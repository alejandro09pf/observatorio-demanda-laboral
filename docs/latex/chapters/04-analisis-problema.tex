\chapter{ANÁLISIS DEL PROBLEMA}

Este capítulo expone un análisis detallado del problema que el proyecto pretende solucionar, estableciendo el marco conceptual que guía el diseño de la solución técnica. Se presentan los principales requerimientos funcionales y no funcionales del sistema, las restricciones técnicas y metodológicas que delimitaron el alcance, y la especificación funcional de alto nivel de la arquitectura de pipeline. Este capítulo se enfoca en los aspectos fundamentales que determinaron las decisiones arquitectónicas posteriores.

\section{Requerimientos del sistema}

El observatorio debe satisfacer requerimientos funcionales, no funcionales y de datos que garanticen tanto las capacidades operativas como las propiedades de calidad del sistema.

\subsection{Requerimientos funcionales}

Los requerimientos funcionales definen las capacidades operativas que el observatorio debe implementar, organizadas según las siete etapas del pipeline de procesamiento. Los requerimientos clave incluyen:

\begin{itemize}
    \item \textbf{Adquisición automatizada de datos}: Recolección de ofertas laborales desde portales de empleo en Colombia, México y Argentina mediante web scraping que soporte tanto contenido estático (HTML directo) como dinámico (JavaScript rendering) \cite{orozco2019webscraping}.
    \item \textbf{Procesamiento de lenguaje natural}: Normalización y limpieza de texto adaptada al español latinoamericano técnico, incluyendo tokenización y manejo de ``Spanglish'' \cite{rubio2025}.
    \item \textbf{Extracción dual de habilidades}: Pipeline A con NER y regex para habilidades explícitas; Pipeline B con LLMs para inferencia de habilidades implícitas.
    \item \textbf{Normalización taxonómica}: Mapeo de habilidades extraídas contra taxonomía ESCO mediante matching exacto y difuso (fuzzywuzzy $\geq$0.92).
    \item \textbf{Análisis no supervisado}: Generación de embeddings semánticos (768D), reducción dimensional (UMAP), y clustering jerárquico (HDBSCAN) para descubrir perfiles emergentes.
    \item \textbf{Trazabilidad completa}: Registro de metadatos de procesamiento (timestamps, versiones de modelos, parámetros) para reproducibilidad científica.
\end{itemize}

\subsection{Requerimientos no funcionales}

Los requerimientos no funcionales establecen las propiedades de calidad que garantizan la viabilidad técnica y operativa del sistema:

\begin{itemize}
    \item \textbf{Escalabilidad}: Capacidad de procesar más de 30,000 ofertas con latencias aceptables (extracción $<$30 seg/oferta, clustering $<$4 horas).
    \item \textbf{Multilingüismo}: Soporte nativo para español, inglés y mezcla técnica (``Spanglish'') en todos los modelos de NLP.
    \item \textbf{Reproducibilidad científica}: Semillas aleatorias fijas, versionado de modelos, y persistencia de datasets intermedios.
    \item \textbf{Portabilidad y mantenibilidad}: Contenedorización Docker, adherencia a PEP 8, documentación técnica, y arquitectura modular extensible.
\end{itemize}

\subsection{Requerimientos de datos}

Los requerimientos de datos especifican las características del corpus de ofertas laborales:

\begin{itemize}
    \item \textbf{Cobertura geográfica}: Ofertas de Colombia, México y Argentina con al menos 2 portales por país.
    \item \textbf{Volumen y calidad}: Más de 30,000 ofertas con descripción $>$100 caracteres y al menos 2 habilidades identificables, tras deduplicación y filtrado de calidad.
    \item \textbf{Metadatos temporales}: Fecha de publicación, recolección y expiración para análisis de evolución temporal.
    \item \textbf{Representatividad sectorial}: Aunque enfocado en tecnología, captura de múltiples sectores (TI, finanzas, manufactura, salud) para identificar demanda transversal de competencias digitales.
\end{itemize}

\section{Restricciones}

Las restricciones representan limitaciones inherentes al problema, al contexto de operación o a las decisiones de alcance del proyecto. Se organizan en tres categorías: técnicas, de datos y metodológicas.

\subsection{Restricciones técnicas}

Las principales restricciones técnicas que afectaron el diseño del sistema incluyen:

\begin{itemize}
    \item \textbf{Medidas anti-bot de portales}: CAPTCHAs, rate limiting y bloqueos por IP que requieren delays adaptativos, rotación de user-agents y backoff exponencial.
    \item \textbf{Dinamismo del DOM}: Cambios frecuentes en la estructura HTML de portales que generan fragilidad en selectores CSS/XPath, requiriendo monitoreo de fallos y mecanismos de alerta.
    \item \textbf{Recursos computacionales limitados}: LLMs requieren GPU con 8GB+ VRAM, limitando el proyecto a modelos open-source de 3-4B parámetros ejecutables localmente.
    \item \textbf{Latencia de procesamiento LLM}: 40-45 segundos/oferta para procesamiento local versus métodos tradicionales, implicando trade-off entre calidad semántica y tiempos de procesamiento.
\end{itemize}

\subsection{Restricciones de datos}

Las características del dominio de ofertas laborales introducen restricciones significativas:

\begin{itemize}
    \item \textbf{Heterogeneidad de formatos}: Ausencia de estándar para publicación de ofertas, con campos, nomenclaturas y niveles de detalle diferentes entre portales.
    \item \textbf{Incompletitud de información}: Muchas ofertas omiten salario, requisitos detallados o tecnologías específicas, limitando la profundidad del análisis.
    \item \textbf{Ruido lingüístico}: Errores ortográficos, abreviaciones no estándar, mezcla de idiomas y uso informal que reduce efectividad de técnicas NLP entrenadas en corpus formales.
    \item \textbf{Volatilidad temporal}: Vigencia típica de 30-60 días requiere estrategias de recolección periódica y versionado de datos.
\end{itemize}

\subsection{Restricciones metodológicas}

El contexto de español latinoamericano técnico presenta desafíos metodológicos únicos:

\begin{itemize}
    \item \textbf{Ausencia de ground truth}: No existe dataset etiquetado de referencia para habilidades en ofertas laborales en español latinoamericano, dificultando evaluación cuantitativa rigurosa.
    \item \textbf{Sesgo de fuente}: Portales de empleo excluyen ofertas en sitios corporativos directos, redes sociales o canales informales, introduciendo sesgo de formalidad y tamaño de empresa.
\end{itemize}

\section{Especificación funcional}

La especificación funcional describe el comportamiento de alto nivel del sistema mediante la definición de su arquitectura de pipeline de 7 etapas, las interfaces críticas entre módulos, y los casos de uso principales que el observatorio debe soportar.

\subsection{Arquitectura de pipeline de 7 etapas}

El observatorio se estructura como un pipeline secuencial de transformación de datos, donde cada etapa consume la salida de la anterior desde PostgreSQL, ejecuta su transformación especializada, y persiste resultados para la siguiente etapa:

\begin{enumerate}
    \item \textbf{Scraping}: Recolecta ofertas laborales de portales mediante Scrapy + Selenium $\to$ \texttt{raw\_jobs}
    \item \textbf{Normalización}: Limpia y estandariza texto UTF-8 $\to$ \texttt{description\_clean}, \texttt{combined\_text}
    \item \textbf{Extracción Pipeline A}: Aplica NER + Regex $\to$ \texttt{extracted\_skills}
    \item \textbf{Extracción Pipeline B}: Aplica LLM con prompt engineering $\to$ \texttt{enhanced\_skills}
    \item \textbf{Mapeo ESCO}: Normaliza contra taxonomía (exact + fuzzy) $\to$ \texttt{esco\_uri}, \texttt{esco\_preferred\_label}
    \item \textbf{Embeddings}: Genera vectores 768D con E5 Multilingual $\to$ \texttt{skill\_embeddings}
    \item \textbf{Clustering}: UMAP (768D$\to$2-3D) + HDBSCAN $\to$ \texttt{analysis\_results}
\end{enumerate}

La orquestación se gestiona mediante CLI único (Typer) que permite ejecución manual de etapas individuales o automatización completa mediante scheduler.

\subsection{Interfaces críticas}

El sistema define cuatro interfaces críticas que garantizan la comunicación entre módulos:

\begin{itemize}
    \item \textbf{Scraper-Database}: \texttt{JobPostgresPipeline} implementa batch inserts optimizados con deduplicación por hash SHA-256 del contenido, almacenando ofertas en \texttt{raw\_jobs}.
    \item \textbf{Extractor-ESCO}: \texttt{ESCOMatcher3Layers} implementa estrategia de matching de 3 capas (exact, fuzzy, semantic) contra taxonomía ESCO extendida, con caché de resultados para optimizar procesamiento batch.
    \item \textbf{LLM-Processor}: \texttt{LLMHandler} abstrae llamadas a modelos locales (llama.cpp, transformers) o APIs remotas (OpenAI) con interfaz unificada, soportando múltiples backends sin modificar código de negocio.
    \item \textbf{Orquestador-Pipeline}: \texttt{MasterController} coordina scheduler, pipeline automator y health monitoring, con control de estado persistente para reinicio tras fallos.
\end{itemize}

\subsection{Casos de uso principales}

El observatorio soporta cuatro casos de uso fundamentales:

\begin{itemize}
    \item \textbf{Recolección programada}: Scheduler ejecuta spiders periódicamente para mantener corpus actualizado, con registro de métricas y alertas por caída de volumen.
    \item \textbf{Análisis temporal de demanda}: Generación automática de visualizaciones (heatmaps, gráficos de evolución) y reportes JSON con métricas de clustering por trimestre.
    \item \textbf{Validación de pipeline}: Comparación experimental de Pipeline A vs. B mediante métricas de solapamiento (Jaccard) y análisis cualitativo de habilidades únicas.
    \item \textbf{Extensibilidad geográfica}: Arquitectura modular permite agregar nuevos países implementando spiders heredados de \texttt{BaseSpider} sin modificaciones al pipeline de procesamiento.
\end{itemize}

\vspace{0.5cm}

\noindent\textit{Nota: La especificación técnica detallada de los requerimientos, restricciones y casos de uso presentados en este capítulo se encuentra en el Apéndice A. Los detalles de arquitectura física, lógica, despliegue e interfaces se presentan en el capítulo de Desarrollo.}
