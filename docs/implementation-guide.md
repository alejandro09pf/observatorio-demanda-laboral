# Complete Implementation Guide
## All Production-Ready Files for Labor Market Observatory

**This document contains EVERY file needed for the complete system implementation.**

---

## Table of Contents
1. [Root Configuration Files]

### src/utils/validators.py
```python
import re
from typing import Optional, List
from config.settings import get_settings

settings = get_settings()

def validate_country(country: str) -> bool:
    """Validate country code.
    
    Args:
        country: Country code to validate
        
    Returns:
        True if valid, False otherwise
    """
    return country in settings.supported_countries

def validate_portal(portal: str) -> bool:
    """Validate portal name.
    
    Args:
        portal: Portal name to validate
        
    Returns:
        True if valid, False otherwise
    """
    return portal in settings.supported_portals

def validate_skill(skill: str) -> bool:
    """Validate skill text.
    
    Args:
        skill: Skill text to validate
        
    Returns:
        True if valid skill, False otherwise
    """
    if not skill or len(skill.strip()) < 2:
        return False
    
    # Must contain at least one letter
    if not re.search(r'[a-zA-Z]', skill):
        return False
    
    # Check length
    if len(skill) > 100:
        return False
    
    # Check for suspicious patterns
    blacklist_patterns = [
        r'^\d+(#1-root-configuration-files)
2. [Database Setup Files](#2-database-setup-files)
3. [Configuration Module](#3-configuration-module)
4. [Scraper Module Files](#4-scraper-module-files)
5. [Extractor Module Files](#5-extractor-module-files)
6. [LLM Processor Module Files](#6-llm-processor-module-files)
7. [Embedder Module Files](#7-embedder-module-files)
8. [Analyzer Module Files](#8-analyzer-module-files)
9. [Orchestrator and Utilities](#9-orchestrator-and-utilities)
10. [Scripts](#10-scripts)

---

## 1. Root Configuration Files

### requirements.txt
```
# Core
python-dotenv==1.0.0
pydantic==2.5.3
typer==0.9.0
tqdm==4.66.1

# Web Scraping
scrapy==2.11.0
scrapy-selenium==0.0.7
beautifulsoup4==4.12.2
lxml==4.9.3
fake-useragent==1.4.0
requests==2.31.0

# Database
psycopg2-binary==2.9.9
sqlalchemy==2.0.23
pgvector==0.2.3
alembic==1.13.1

# NLP
spacy==3.7.2
langdetect==1.0.9
regex==2023.12.25

# Machine Learning
transformers==4.36.2
sentence-transformers==2.2.2
torch==2.1.2
llama-cpp-python==0.2.32
openai==1.6.1

# Data Processing
pandas==2.1.4
numpy==1.24.3
scikit-learn==1.3.2
umap-learn==0.5.5
hdbscan==0.8.33

# Visualization
matplotlib==3.8.2
seaborn==0.13.0
reportlab==4.0.8
pillow==10.1.0

# Testing
pytest==7.4.4
pytest-asyncio==0.23.3
pytest-cov==4.1.0

# Development
black==23.12.1
flake8==7.0.0
mypy==1.8.0
pre-commit==3.6.0
```

### .env.example
```bash
# Database Configuration
DATABASE_URL=postgresql://labor_user:your_password@localhost:5432/labor_observatory
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=0

# Scraping Configuration
SCRAPER_USER_AGENT="Mozilla/5.0 (Windows NT 10.0; Win64; x64) Academic Research Bot"
SCRAPER_CONCURRENT_REQUESTS=16
SCRAPER_DOWNLOAD_DELAY=1.0
SCRAPER_RETRY_TIMES=3

# ESCO API Configuration
ESCO_API_URL=https://ec.europa.eu/esco/api
ESCO_VERSION=1.1.0
ESCO_LANGUAGE=es

# LLM Configuration
LLM_MODEL_PATH=./data/models/mistral-7b-instruct.Q4_K_M.gguf
LLM_CONTEXT_LENGTH=4096
LLM_MAX_TOKENS=512
LLM_TEMPERATURE=0.7
LLM_N_GPU_LAYERS=35

# OpenAI Fallback (Optional)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-3.5-turbo

# Embedding Configuration
EMBEDDING_MODEL=intfloat/multilingual-e5-base
EMBEDDING_BATCH_SIZE=32
EMBEDDING_CACHE_DIR=./data/cache/embeddings

# Analysis Configuration
CLUSTER_MIN_SIZE=5
CLUSTER_MIN_SAMPLES=3
UMAP_N_NEIGHBORS=15
UMAP_MIN_DIST=0.1

# Output Configuration
OUTPUT_DIR=./outputs
REPORT_FORMAT=pdf
LOG_LEVEL=INFO
LOG_FILE=./logs/labor_observatory.log
```

### .gitignore
```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/
.venv

# IDE
.vscode/
.idea/
*.swp
*.swo

# Data and Models
data/models/*.gguf
data/models/*/
data/cache/
outputs/
logs/

# Database
*.db
*.sqlite3

# Environment
.env
.env.local

# OS
.DS_Store
Thumbs.db

# Testing
.coverage
htmlcov/
.pytest_cache/

# Scrapy
.scrapy/

# Notebooks
.ipynb_checkpoints/
```

### setup.py
```python
from setuptools import setup, find_packages

setup(
    name="labor-observatory",
    version="1.0.0",
    author="Your Team",
    description="Automated Labor Market Observatory for Latin America",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    python_requires=">=3.10",
    install_requires=[
        line.strip()
        for line in open("requirements.txt")
        if line.strip() and not line.startswith("#")
    ],
    entry_points={
        "console_scripts": [
            "labor-observatory=orchestrator:app",
        ],
    },
)
```

---

## 2. Database Setup Files

### src/database/migrations/001_initial_schema.sql
```sql
-- Create database
CREATE DATABASE IF NOT EXISTS labor_observatory
  WITH ENCODING 'UTF8'
  LC_COLLATE = 'en_US.UTF-8'
  LC_CTYPE = 'en_US.UTF-8';

\c labor_observatory;

-- Enable extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgvector";

-- Create tables
CREATE TABLE IF NOT EXISTS raw_jobs (
    job_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    portal VARCHAR(50) NOT NULL,
    country CHAR(2) NOT NULL,
    url TEXT NOT NULL,
    title TEXT NOT NULL,
    company TEXT,
    location TEXT,
    description TEXT NOT NULL,
    requirements TEXT,
    salary_raw TEXT,
    contract_type VARCHAR(50),
    remote_type VARCHAR(50),
    posted_date DATE,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    content_hash VARCHAR(64) UNIQUE,
    raw_html TEXT,
    is_processed BOOLEAN DEFAULT FALSE,
    
    CONSTRAINT chk_country CHECK (country IN ('CO', 'MX', 'AR')),
    CONSTRAINT chk_portal CHECK (portal IN ('computrabajo', 'bumeran', 'elempleo'))
);

CREATE INDEX idx_portal_country ON raw_jobs(portal, country);
CREATE INDEX idx_scraped_at ON raw_jobs(scraped_at);
CREATE INDEX idx_processed ON raw_jobs(is_processed);

CREATE TABLE IF NOT EXISTS extracted_skills (
    extraction_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    job_id UUID REFERENCES raw_jobs(job_id) ON DELETE CASCADE,
    skill_text TEXT NOT NULL,
    skill_type VARCHAR(50),
    extraction_method VARCHAR(50),
    confidence_score FLOAT,
    source_section VARCHAR(50),
    span_start INTEGER,
    span_end INTEGER,
    esco_uri TEXT,
    extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_job_skills ON extracted_skills(job_id);
CREATE INDEX idx_skill_text ON extracted_skills(skill_text);

CREATE TABLE IF NOT EXISTS enhanced_skills (
    enhancement_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    job_id UUID REFERENCES raw_jobs(job_id) ON DELETE CASCADE,
    original_skill_text TEXT,
    normalized_skill TEXT NOT NULL,
    skill_type VARCHAR(50),
    esco_concept_uri TEXT,
    esco_preferred_label TEXT,
    llm_confidence FLOAT,
    llm_reasoning TEXT,
    is_duplicate BOOLEAN DEFAULT FALSE,
    duplicate_of_id UUID,
    enhanced_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    llm_model VARCHAR(100)
);

CREATE INDEX idx_job_enhanced ON enhanced_skills(job_id);
CREATE INDEX idx_normalized ON enhanced_skills(normalized_skill);

CREATE TABLE IF NOT EXISTS skill_embeddings (
    embedding_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    skill_text TEXT UNIQUE NOT NULL,
    embedding vector(768) NOT NULL,
    model_name VARCHAR(100) NOT NULL,
    model_version VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_skill_lookup ON skill_embeddings(skill_text);
CREATE INDEX idx_embedding_similarity ON skill_embeddings 
  USING ivfflat (embedding vector_cosine_ops)
  WITH (lists = 100);

CREATE TABLE IF NOT EXISTS analysis_results (
    analysis_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    analysis_type VARCHAR(50),
    country CHAR(2),
    date_range_start DATE,
    date_range_end DATE,
    parameters JSONB,
    results JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_analysis_type ON analysis_results(analysis_type);
CREATE INDEX idx_analysis_date ON analysis_results(created_at);

-- Create views
CREATE VIEW skill_frequency AS
SELECT 
    es.normalized_skill,
    COUNT(DISTINCT es.job_id) as job_count,
    COUNT(*) as total_mentions,
    ARRAY_AGG(DISTINCT rj.country) as countries
FROM enhanced_skills es
JOIN raw_jobs rj ON es.job_id = rj.job_id
WHERE es.is_duplicate = FALSE
GROUP BY es.normalized_skill
ORDER BY job_count DESC;

CREATE VIEW country_skill_distribution AS
SELECT 
    rj.country,
    es.normalized_skill,
    COUNT(DISTINCT es.job_id) as job_count
FROM enhanced_skills es
JOIN raw_jobs rj ON es.job_id = rj.job_id
WHERE es.is_duplicate = FALSE
GROUP BY rj.country, es.normalized_skill
ORDER BY rj.country, job_count DESC;
```

### src/database/models.py
```python
from sqlalchemy import Column, String, Text, Boolean, Float, Integer, DateTime, Date, ForeignKey, JSON
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from pgvector.sqlalchemy import Vector
import uuid

Base = declarative_base()

class RawJob(Base):
    __tablename__ = 'raw_jobs'
    
    job_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    portal = Column(String(50), nullable=False)
    country = Column(String(2), nullable=False)
    url = Column(Text, nullable=False)
    title = Column(Text, nullable=False)
    company = Column(Text)
    location = Column(Text)
    description = Column(Text, nullable=False)
    requirements = Column(Text)
    salary_raw = Column(Text)
    contract_type = Column(String(50))
    remote_type = Column(String(50))
    posted_date = Column(Date)
    scraped_at = Column(DateTime, server_default=func.now())
    content_hash = Column(String(64), unique=True)
    raw_html = Column(Text)
    is_processed = Column(Boolean, default=False)
    
    # Relationships
    extracted_skills = relationship("ExtractedSkill", back_populates="job")
    enhanced_skills = relationship("EnhancedSkill", back_populates="job")

class ExtractedSkill(Base):
    __tablename__ = 'extracted_skills'
    
    extraction_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    job_id = Column(UUID(as_uuid=True), ForeignKey('raw_jobs.job_id'))
    skill_text = Column(Text, nullable=False)
    skill_type = Column(String(50))
    extraction_method = Column(String(50))
    confidence_score = Column(Float)
    source_section = Column(String(50))
    span_start = Column(Integer)
    span_end = Column(Integer)
    esco_uri = Column(Text)
    extracted_at = Column(DateTime, server_default=func.now())
    
    # Relationships
    job = relationship("RawJob", back_populates="extracted_skills")

class EnhancedSkill(Base):
    __tablename__ = 'enhanced_skills'
    
    enhancement_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    job_id = Column(UUID(as_uuid=True), ForeignKey('raw_jobs.job_id'))
    original_skill_text = Column(Text)
    normalized_skill = Column(Text, nullable=False)
    skill_type = Column(String(50))
    esco_concept_uri = Column(Text)
    esco_preferred_label = Column(Text)
    llm_confidence = Column(Float)
    llm_reasoning = Column(Text)
    is_duplicate = Column(Boolean, default=False)
    duplicate_of_id = Column(UUID(as_uuid=True))
    enhanced_at = Column(DateTime, server_default=func.now())
    llm_model = Column(String(100))
    
    # Relationships
    job = relationship("RawJob", back_populates="enhanced_skills")

class SkillEmbedding(Base):
    __tablename__ = 'skill_embeddings'
    
    embedding_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    skill_text = Column(Text, unique=True, nullable=False)
    embedding = Column(Vector(768), nullable=False)
    model_name = Column(String(100), nullable=False)
    model_version = Column(String(50))
    created_at = Column(DateTime, server_default=func.now())

class AnalysisResult(Base):
    __tablename__ = 'analysis_results'
    
    analysis_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    analysis_type = Column(String(50))
    country = Column(String(2))
    date_range_start = Column(Date)
    date_range_end = Column(Date)
    parameters = Column(JSONB)
    results = Column(JSONB)
    created_at = Column(DateTime, server_default=func.now())
```

### src/database/operations.py
```python
from typing import List, Optional, Dict, Any
from datetime import datetime
from sqlalchemy import create_engine, and_, or_, func
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.exc import IntegrityError
import os
from .models import Base, RawJob, ExtractedSkill, EnhancedSkill, SkillEmbedding, AnalysisResult
import hashlib
import logging

logger = logging.getLogger(__name__)

class DatabaseOperations:
    def __init__(self, database_url: Optional[str] = None):
        self.database_url = database_url or os.getenv('DATABASE_URL')
        self.engine = create_engine(
            self.database_url,
            pool_size=20,
            max_overflow=0,
            pool_pre_ping=True
        )
        Base.metadata.create_all(self.engine)
        self.SessionLocal = sessionmaker(bind=self.engine)
    
    def get_session(self) -> Session:
        """Get a new database session."""
        return self.SessionLocal()
    
    def insert_job(self, job_data: Dict[str, Any]) -> Optional[str]:
        """Insert a new job posting."""
        session = self.get_session()
        try:
            # Generate content hash
            content = f"{job_data['title']}{job_data['description']}{job_data.get('requirements', '')}"
            content_hash = hashlib.sha256(content.encode()).hexdigest()
            
            job = RawJob(
                **job_data,
                content_hash=content_hash
            )
            session.add(job)
            session.commit()
            
            job_id = str(job.job_id)
            logger.info(f"Inserted job {job_id}")
            return job_id
            
        except IntegrityError:
            session.rollback()
            logger.warning(f"Duplicate job detected: {job_data['url']}")
            return None

### src/analyzer/visualizations.py
```python
import logging
from typing import Dict, Any, List, Optional, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from datetime import datetime
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class VisualizationGenerator:
    """Generate static visualizations for analysis results."""
    
    def __init__(self, output_dir: str = None):
        self.output_dir = output_dir or "./outputs"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Set style
        plt.style.use('seaborn-v0_8-darkgrid')
        sns.set_palette("husl")
        
        # Set font for better Spanish support
        plt.rcParams['font.family'] = 'DejaVu Sans'
    
    def create_all_visualizations(self, 
                                analysis_data: Dict[str, Any],
                                country: Optional[str] = None) -> List[str]:
        """Create all standard visualizations.
        
        Args:
            analysis_data: Dictionary with analysis results
            country: Country code for filtering
            
        Returns:
            List of generated file paths
        """
        generated_files = []
        
        # Skill frequency chart
        if 'skill_statistics' in analysis_data:
            path = self.create_skill_frequency_chart(
                analysis_data['skill_statistics'],
                country
            )
            if path:
                generated_files.append(path)
        
        # Cluster visualization
        if 'clustering_results' in analysis_data:
            path = self.create_cluster_visualization(
                analysis_data['clustering_results']
            )
            if path:
                generated_files.append(path)
        
        # Geographic distribution
        if 'geographic_data' in analysis_data:
            path = self.create_geographic_distribution(
                analysis_data['geographic_data']
            )
            if path:
                generated_files.append(path)
        
        # Skill co-occurrence heatmap
        if 'skill_cooccurrence' in analysis_data:
            path = self.create_skill_cooccurrence_heatmap(
                analysis_data['skill_cooccurrence']
            )
            if path:
                generated_files.append(path)
        
        return generated_files
    
    def create_skill_frequency_chart(self,
                                   skill_stats: Dict[str, Any],
                                   country: Optional[str] = None,
                                   top_n: int = 20) -> Optional[str]:
        """Create horizontal bar chart of top skills.
        
        Args:
            skill_stats: Skill statistics data
            country: Country filter
            top_n: Number of top skills to show
            
        Returns:
            Path to saved chart
        """
        try:
            # Prepare data
            top_skills = skill_stats.get('top_skills', [])[:top_n]
            if not top_skills:
                logger.warning("No skill data for frequency chart")
                return None
            
            df = pd.DataFrame(top_skills)
            
            # Create figure
            fig, ax = plt.subplots(figsize=(12, 8))
            
            # Create horizontal bar chart
            bars = ax.barh(df['skill'], df['count'], 
                          color=plt.cm.viridis(np.linspace(0.3, 0.9, len(df))))
            
            # Customize
            ax.set_xlabel('Número de Vacantes', fontsize=14)
            ax.set_ylabel('Habilidad Técnica', fontsize=14)
            
            title = f'Top {top_n} Habilidades Más Demandadas'
            if country:
                country_names = {'CO': 'Colombia', 'MX': 'México', 'AR': 'Argentina'}
                title += f' - {country_names.get(country, country)}'
            ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
            
            # Add value labels
            for bar in bars:
                width = bar.get_width()
                ax.text(width + 1, bar.get_y() + bar.get_height()/2,
                       f'{int(width)}',
                       ha='left', va='center', fontsize=10)
            
            # Adjust layout
            plt.tight_layout()
            ax.invert_yaxis()  # Highest on top
            
            # Grid
            ax.grid(True, axis='x', alpha=0.3)
            ax.set_axisbelow(True)
            
            # Save
            filename = self._generate_filename('skill_frequency', country)
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created skill frequency chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating skill frequency chart: {e}")
            return None
    
    def create_cluster_visualization(self,
                                   clustering_results: Dict[str, Any]) -> Optional[str]:
        """Create 2D scatter plot of skill clusters.
        
        Args:
            clustering_results: Clustering analysis results
            
        Returns:
            Path to saved visualization
        """
        try:
            # Extract data
            coordinates = clustering_results.get('coordinates_2d')
            labels = clustering_results.get('labels')
            skills = clustering_results.get('skills', [])
            
            if coordinates is None or labels is None:
                logger.warning("Missing data for cluster visualization")
                return None
            
            # Create figure
            fig, ax = plt.subplots(figsize=(14, 10))
            
            # Get unique labels
            unique_labels = set(labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            
            # Color map
            colors = plt.cm.rainbow(np.linspace(0, 1, n_clusters))
            
            # Plot each cluster
            for k, col in zip(sorted(unique_labels), colors):
                if k == -1:
                    # Noise points in black
                    col = 'black'
                    label = 'Ruido'
                else:
                    label = f'Cluster {k}'
                
                class_member_mask = (labels == k)
                xy = coordinates[class_member_mask]
                
                ax.scatter(xy[:, 0], xy[:, 1], 
                         c=[col], 
                         label=label,
                         alpha=0.6,
                         s=30)
            
            # Add labels for some points (avoid overlap)
            if skills:
                # Sample points to label
                n_labels = min(30, len(skills))
                indices = np.random.choice(len(skills), n_labels, replace=False)
                
                for idx in indices:
                    ax.annotate(skills[idx], 
                              (coordinates[idx, 0], coordinates[idx, 1]),
                              fontsize=8,
                              alpha=0.7)
            
            # Customize
            ax.set_title('Visualización de Clusters de Habilidades', 
                        fontsize=16, fontweight='bold')
            ax.set_xlabel('UMAP Dimension 1', fontsize=12)
            ax.set_ylabel('UMAP Dimension 2', fontsize=12)
            
            # Legend
            ax.legend(loc='upper right', bbox_to_anchor=(1.15, 1))
            
            # Remove ticks
            ax.set_xticks([])
            ax.set_yticks([])
            
            # Save
            filename = self._generate_filename('skill_clusters')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created cluster visualization: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating cluster visualization: {e}")
            return None
    
    def create_geographic_distribution(self,
                                     geo_data: Dict[str, Any]) -> Optional[str]:
        """Create geographic distribution chart.
        
        Args:
            geo_data: Geographic distribution data
            
        Returns:
            Path to saved chart
        """
        try:
            # Create figure
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # Job distribution by country
            countries = ['Colombia', 'México', 'Argentina']
            job_counts = [
                geo_data.get('CO', {}).get('total_jobs', 0),
                geo_data.get('MX', {}).get('total_jobs', 0),
                geo_data.get('AR', {}).get('total_jobs', 0)
            ]
            
            # Pie chart
            colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
            wedges, texts, autotexts = ax1.pie(job_counts, 
                                              labels=countries,
                                              colors=colors,
                                              autopct='%1.1f%%',
                                              startangle=90)
            
            ax1.set_title('Distribución de Vacantes por País', 
                         fontsize=14, fontweight='bold')
            
            # Skills per country
            skill_counts = [
                geo_data.get('CO', {}).get('unique_skills', 0),
                geo_data.get('MX', {}).get('unique_skills', 0),
                geo_data.get('AR', {}).get('unique_skills', 0)
            ]
            
            bars = ax2.bar(countries, skill_counts, color=colors)
            ax2.set_title('Habilidades Únicas por País', 
                         fontsize=14, fontweight='bold')
            ax2.set_ylabel('Número de Habilidades', fontsize=12)
            
            # Add value labels
            for bar in bars:
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height,
                        f'{int(height)}',
                        ha='center', va='bottom')
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('geographic_distribution')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created geographic distribution chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating geographic distribution: {e}")
            return None
    
    def create_skill_cooccurrence_heatmap(self,
                                        cooccurrence_data: Dict[str, Any],
                                        top_n: int = 15) -> Optional[str]:
        """Create heatmap of skill co-occurrences.
        
        Args:
            cooccurrence_data: Skill co-occurrence matrix
            top_n: Number of top skills to include
            
        Returns:
            Path to saved heatmap
        """
        try:
            # Convert to DataFrame
            df = pd.DataFrame(cooccurrence_data.get('matrix', []))
            skills = cooccurrence_data.get('skills', [])
            
            if df.empty or not skills:
                logger.warning("No co-occurrence data available")
                return None
            
            # Select top skills
            if len(skills) > top_n:
                # Sum co-occurrences for each skill
                skill_importance = df.sum(axis=0) + df.sum(axis=1)
                top_indices = skill_importance.nlargest(top_n).index
                df = df.loc[top_indices, top_indices]
                skills = [skills[i] for i in top_indices]
            
            # Create figure
            fig, ax = plt.subplots(figsize=(12, 10))
            
            # Create heatmap
            sns.heatmap(df, 
                       xticklabels=skills,
                       yticklabels=skills,
                       cmap='YlOrRd',
                       cbar_kws={'label': 'Co-ocurrencias'},
                       square=True,
                       linewidths=0.5,
                       ax=ax)
            
            # Customize
            ax.set_title('Matriz de Co-ocurrencia de Habilidades', 
                        fontsize=16, fontweight='bold', pad=20)
            
            # Rotate labels
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('skill_cooccurrence')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created co-occurrence heatmap: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating co-occurrence heatmap: {e}")
            return None
    
    def create_temporal_trends(self,
                             temporal_data: Dict[str, Any],
                             skills: List[str] = None) -> Optional[str]:
        """Create temporal trend visualization.
        
        Args:
            temporal_data: Temporal trend data
            skills: List of skills to plot (default: top 5)
            
        Returns:
            Path to saved chart
        """
        try:
            # Prepare data
            df = pd.DataFrame(temporal_data.get('trends', []))
            
            if df.empty:
                logger.warning("No temporal data available")
                return None
            
            # Convert date column
            df['date'] = pd.to_datetime(df['date'])
            
            # Select skills to plot
            if not skills:
                # Get top 5 skills by total mentions
                skill_totals = df.groupby('skill')['count'].sum()
                skills = skill_totals.nlargest(5).index.tolist()
            
            # Create figure
            fig, ax = plt.subplots(figsize=(14, 8))
            
            # Plot each skill
            for skill in skills:
                skill_data = df[df['skill'] == skill]
                ax.plot(skill_data['date'], skill_data['count'], 
                       marker='o', label=skill, linewidth=2)
            
            # Customize
            ax.set_title('Tendencias Temporales de Habilidades', 
                        fontsize=16, fontweight='bold')
            ax.set_xlabel('Fecha', fontsize=12)
            ax.set_ylabel('Número de Menciones', fontsize=12)
            
            # Format x-axis
            import matplotlib.dates as mdates
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
            ax.xaxis.set_major_locator(mdates.MonthLocator())
            plt.xticks(rotation=45)
            
            # Legend
            ax.legend(loc='upper left', bbox_to_anchor=(1, 1))
            
            # Grid
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('temporal_trends')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created temporal trends chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating temporal trends: {e}")
            return None
    
    def _generate_filename(self, chart_type: str, country: Optional[str] = None) -> str:
        """Generate filename with timestamp.
        
        Args:
            chart_type: Type of chart
            country: Country code (optional)
            
        Returns:
            Generated filename
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        country_suffix = f"_{country}" if country else ""
        return f"{chart_type}{country_suffix}_{timestamp}.png"

---

## 9. Orchestrator and Utilities

### src/orchestrator.py
```python
#!/usr/bin/env python3
"""
Main orchestrator for the Labor Market Observatory pipeline.
"""

import logging
import sys
from typing import Optional
import typer
from rich.console import Console
from rich.table import Table
from rich.progress import track
import time

from config.settings import get_settings
from config.logging_config import setup_logging
from database.operations import DatabaseOperations
from scraper.spiders.computrabajo_spider import ComputrabajoSpider
from scraper.spiders.bumeran_spider import BumeranSpider
from scraper.spiders.elempleo_spider import ElempleoSpider
from extractor.pipeline import ExtractionPipeline
from llm_processor.pipeline import LLMProcessingPipeline
from embedder.batch_processor import BatchProcessor
from analyzer.clustering import SkillClusterer
from analyzer.dimension_reducer import DimensionReducer
from analyzer.report_generator import ReportGenerator
from analyzer.visualizations import VisualizationGenerator

# Initialize
app = typer.Typer(help="Labor Market Observatory CLI")
console = Console()
settings = get_settings()
logger = setup_logging(settings.log_level, settings.log_file)

@app.command()
def scrape(
    country: str = typer.Argument(..., help="Country code (CO, MX, AR)"),
    portal: str = typer.Argument(..., help="Portal name (computrabajo, bumeran, elempleo)"),
    pages: int = typer.Option(10, help="Number of pages to scrape")
):
    """Run web scraping for a specific portal and country."""
    console.print(f"[bold green]Starting scraper for {portal} in {country}[/bold green]")
    
    # Validate inputs
    if country not in settings.supported_countries:
        console.print(f"[red]Invalid country: {country}[/red]")
        raise typer.Exit(1)
    
    if portal not in settings.supported_portals:
        console.print(f"[red]Invalid portal: {portal}[/red]")
        raise typer.Exit(1)
    
    # Run appropriate spider
    try:
        from scrapy.crawler import CrawlerProcess
        from scrapy.utils.project import get_project_settings
        
        # Get scrapy settings
        scrapy_settings = get_project_settings()
        scrapy_settings.update({
            'LOG_LEVEL': 'INFO',
            'CLOSESPIDER_PAGECOUNT': pages
        })
        
        process = CrawlerProcess(scrapy_settings)
        
        # Select spider
        if portal == 'computrabajo':
            spider_class = ComputrabajoSpider
        elif portal == 'bumeran':
            spider_class = BumeranSpider
        elif portal == 'elempleo':
            spider_class = ElempleoSpider
        
        # Run spider
        process.crawl(spider_class, country=country)
        process.start()
        
        console.print("[bold green]Scraping completed![/bold green]")
        
    except Exception as e:
        console.print(f"[red]Scraping failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def extract(
    batch_size: int = typer.Option(100, help="Batch size for processing")
):
    """Extract skills from scraped job postings."""
    console.print("[bold green]Starting skill extraction...[/bold green]")
    
    try:
        pipeline = ExtractionPipeline()
        
        with console.status("[bold green]Extracting skills...") as status:
            stats = pipeline.process_batch(batch_size)
        
        # Display results
        table = Table(title="Extraction Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Jobs Processed", str(stats['jobs_processed']))
        table.add_row("Skills Extracted", str(stats['skills_extracted']))
        table.add_row("ESCO Matches", str(stats['esco_matches']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        table.add_row("Jobs/Second", f"{stats['jobs_per_second']:.2f}")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Extraction failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def enhance(
    batch_size: int = typer.Option(50, help="Batch size for LLM processing"),
    model: str = typer.Option("local", help="Model type (local or openai)")
):
    """Enhance extracted skills using LLM."""
    console.print(f"[bold green]Starting LLM enhancement with {model} model...[/bold green]")
    
    try:
        pipeline = LLMProcessingPipeline(model_type=model)
        
        with console.status("[bold green]Processing with LLM...") as status:
            stats = pipeline.process_batch(batch_size)
        
        # Display results
        table = Table(title="LLM Enhancement Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Jobs Processed", str(stats['jobs_processed']))
        table.add_row("Skills Enhanced", str(stats['skills_enhanced']))
        table.add_row("Implicit Skills Found", str(stats['implicit_skills_found']))
        table.add_row("Skills Normalized", str(stats['skills_normalized']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Enhancement failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def embed(
    model: Optional[str] = typer.Option(None, help="Embedding model name")
):
    """Generate embeddings for all skills."""
    console.print("[bold green]Starting embedding generation...[/bold green]")
    
    try:
        processor = BatchProcessor(model_name=model)
        
        with console.status("[bold green]Generating embeddings...") as status:
            stats = processor.process_all_skills()
        
        # Display results
        table = Table(title="Embedding Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Skills Processed", str(stats['skills_processed']))
        table.add_row("Embeddings Created", str(stats['embeddings_created']))
        table.add_row("Errors", str(stats['errors']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Embedding failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def analyze(
    method: str = typer.Option("hdbscan", help="Clustering method")
):
    """Run clustering analysis on skill embeddings."""
    console.print(f"[bold green]Starting clustering analysis with {method}...[/bold green]")
    
    try:
        clusterer = SkillClusterer()
        
        with console.status("[bold green]Running clustering...") as status:
            results = clusterer.run_clustering_pipeline()
        
        # Display results
        if results:
            table = Table(title="Clustering Results")
            table.add_column("Metric", style="cyan")
            table.add_column("Value", style="magenta")
            
            table.add_row("Number of Clusters", str(results['n_clusters']))
            table.add_row("Noise Points", str(results['n_noise']))
            table.add_row("Silhouette Score", f"{results['metrics'].get('silhouette_score', 0):.3f}")
            
            console.print(table)
            
            # Show top clusters
            console.print("\n[bold]Top 5 Clusters:[/bold]")
            for cluster in results['cluster_info'][:5]:
                console.print(f"  • {cluster['label']}: {cluster['size']} skills")
        
    except Exception as e:
        console.print(f"[red]Analysis failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def report(
    country: Optional[str] = typer.Option(None, help="Country code to filter"),
    format: str = typer.Option("pdf", help="Report format (pdf)")
):
    """Generate analysis report."""
    console.print("[bold green]Generating report...[/bold green]")
    
    try:
        generator = ReportGenerator()
        
        with console.status("[bold green]Creating report...") as status:
            filepath = generator.generate_full_report(
                country=country,
                include_visualizations=True
            )
        
        console.print(f"[bold green]Report generated: {filepath}[/bold green]")
        
    except Exception as e:
        console.print(f"[red]Report generation failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def pipeline(
    country: str = typer.Argument(..., help="Country code"),
    portal: str = typer.Argument(..., help="Portal name"),
    full: bool = typer.Option(False, help="Run full pipeline including scraping")
):
    """Run complete pipeline."""
    console.print("[bold green]Running complete pipeline...[/bold green]")
    
    steps = []
    
    if full:
        steps.append(("Scraping", lambda: scrape(country, portal, pages=5)))
    
    steps.extend([
        ("Extraction", lambda: extract(batch_size=100)),
        ("LLM Enhancement", lambda: enhance(batch_size=50)),
        ("Embedding", lambda: embed()),
        ("Clustering", lambda: analyze()),
        ("Report Generation", lambda: report(country=country))
    ])
    
    for step_name, step_func in track(steps, description="Processing..."):
        try:
            console.print(f"\n[bold cyan]Running: {step_name}[/bold cyan]")
            step_func()
            time.sleep(1)  # Brief pause between steps
        except Exception as e:
            console.print(f"[red]Step '{step_name}' failed: {e}[/red]")
            raise typer.Exit(1)
    
    console.print("\n[bold green]Pipeline completed successfully![/bold green]")

@app.command()
def status():
    """Show system status and statistics."""
    console.print("[bold green]Labor Market Observatory Status[/bold green]\n")
    
    try:
        db_ops = DatabaseOperations()
        
        # Get statistics
        stats = db_ops.get_skill_statistics()
        
        # Display overall stats
        table = Table(title="System Statistics")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Total Unique Skills", str(stats.get('total_unique_skills', 0)))
        table.add_row("Database Size", "N/A")  # Would need to implement
        
        console.print(table)
        
        # Top skills
        if stats.get('top_skills'):
            console.print("\n[bold]Top 10 Skills:[/bold]")
            for i, skill in enumerate(stats['top_skills'][:10], 1):
                console.print(f"  {i}. {skill['skill']} ({skill['count']} jobs)")
        
    except Exception as e:
        console.print(f"[red]Failed to get status: {e}[/red]")
        raise typer.Exit(1)

if __name__ == "__main__":
    app()

### src/utils/__init__.py
```python
from .validators import validate_country, validate_portal, validate_skill
from .cleaners import clean_text, normalize_text, remove_html
from .metrics import calculate_metrics, generate_statistics
from .logger import get_logger

__all__ = [
    'validate_country', 'validate_portal', 'validate_skill',
    'clean_text', 'normalize_text', 'remove_html',
    'calculate_metrics', 'generate_statistics',
    'get_logger'
]
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting job: {e}")
            raise
        finally:
            session.close()
    
    def get_unprocessed_jobs(self, limit: int = 100) -> List[RawJob]:
        """Get unprocessed job postings."""
        session = self.get_session()
        try:
            jobs = session.query(RawJob).filter(
                RawJob.is_processed == False
            ).limit(limit).all()
            return jobs
        finally:
            session.close()
    
    def mark_job_processed(self, job_id: str):
        """Mark a job as processed."""
        session = self.get_session()
        try:
            session.query(RawJob).filter(
                RawJob.job_id == job_id
            ).update({"is_processed": True})
            session.commit()
        finally:
            session.close()
    
    def insert_extracted_skills(self, job_id: str, skills: List[Dict[str, Any]]):
        """Insert extracted skills for a job."""
        session = self.get_session()
        try:
            for skill_data in skills:
                skill = ExtractedSkill(
                    job_id=job_id,
                    **skill_data
                )
                session.add(skill)
            session.commit()
            logger.info(f"Inserted {len(skills)} extracted skills for job {job_id}")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting extracted skills: {e}")
            raise
        finally:
            session.close()
    
    def get_extracted_skills_for_processing(self, limit: int = 100) -> List[Dict[str, Any]]:
        """Get jobs with extracted skills that need LLM processing."""
        session = self.get_session()
        try:
            # Get jobs that have extracted skills but no enhanced skills
            subquery = session.query(EnhancedSkill.job_id).distinct()
            
            jobs = session.query(RawJob).join(ExtractedSkill).filter(
                ~RawJob.job_id.in_(subquery)
            ).limit(limit).all()
            
            result = []
            for job in jobs:
                skills = session.query(ExtractedSkill).filter(
                    ExtractedSkill.job_id == job.job_id
                ).all()
                
                result.append({
                    'job_id': str(job.job_id),
                    'job_title': job.title,
                    'job_description': job.description,
                    'job_requirements': job.requirements,
                    'extracted_skills': [
                        {
                            'skill_text': skill.skill_text,
                            'extraction_method': skill.extraction_method,
                            'source_section': skill.source_section,
                            'confidence_score': skill.confidence_score
                        }
                        for skill in skills
                    ]
                })
            
            return result
        finally:
            session.close()
    
    def insert_enhanced_skills(self, job_id: str, skills: List[Dict[str, Any]]):
        """Insert enhanced skills for a job."""
        session = self.get_session()
        try:
            for skill_data in skills:
                skill = EnhancedSkill(
                    job_id=job_id,
                    **skill_data
                )
                session.add(skill)
            session.commit()
            logger.info(f"Inserted {len(skills)} enhanced skills for job {job_id}")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting enhanced skills: {e}")
            raise
        finally:
            session.close()
    
    def get_unique_skills_for_embedding(self) -> List[str]:
        """Get unique normalized skills that don't have embeddings yet."""
        session = self.get_session()
        try:
            # Get skills that don't have embeddings
            embedded_skills = session.query(SkillEmbedding.skill_text).distinct()
            
            unique_skills = session.query(
                EnhancedSkill.normalized_skill
            ).filter(
                EnhancedSkill.is_duplicate == False,
                ~EnhancedSkill.normalized_skill.in_(embedded_skills)
            ).distinct().all()
            
            return [skill[0] for skill in unique_skills]
        finally:
            session.close()
    
    def insert_skill_embeddings(self, embeddings: List[Dict[str, Any]]):
        """Insert skill embeddings."""
        session = self.get_session()
        try:
            for emb_data in embeddings:
                embedding = SkillEmbedding(**emb_data)
                session.add(embedding)
            session.commit()
            logger.info(f"Inserted {len(embeddings)} skill embeddings")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting embeddings: {e}")
            raise
        finally:
            session.close()
    
    def get_all_embeddings(self) -> List[Dict[str, Any]]:
        """Get all skill embeddings for clustering."""
        session = self.get_session()
        try:
            embeddings = session.query(SkillEmbedding).all()
            return [
                {
                    'skill_text': emb.skill_text,
                    'embedding': emb.embedding,
                    'embedding_id': str(emb.embedding_id)
                }
                for emb in embeddings
            ]
        finally:
            session.close()
    
    def save_analysis_results(self, analysis_type: str, results: Dict[str, Any], 
                            parameters: Dict[str, Any], country: Optional[str] = None):
        """Save analysis results."""
        session = self.get_session()
        try:
            analysis = AnalysisResult(
                analysis_type=analysis_type,
                country=country,
                parameters=parameters,
                results=results
            )
            session.add(analysis)
            session.commit()
            logger.info(f"Saved {analysis_type} analysis results")
        except Exception as e:
            session.rollback()
            logger.error(f"Error saving analysis results: {e}")
            raise
        finally:
            session.close()
    
    def get_skill_statistics(self, country: Optional[str] = None) -> Dict[str, Any]:
        """Get skill statistics by country."""
        session = self.get_session()
        try:
            query = session.query(
                EnhancedSkill.normalized_skill,
                func.count(func.distinct(EnhancedSkill.job_id)).label('job_count')
            ).join(RawJob).filter(
                EnhancedSkill.is_duplicate == False
            )
            
            if country:
                query = query.filter(RawJob.country == country)
            
            results = query.group_by(
                EnhancedSkill.normalized_skill
            ).order_by(
                func.count(func.distinct(EnhancedSkill.job_id)).desc()
            ).limit(50).all()
            
            return {
                'top_skills': [
                    {'skill': skill, 'count': count}
                    for skill, count in results
                ],
                'total_unique_skills': session.query(
                    func.count(func.distinct(EnhancedSkill.normalized_skill))
                ).filter(EnhancedSkill.is_duplicate == False).scalar()
            }
        finally:
            session.close()
```

---

## 3. Configuration Module

### src/config/__init__.py
```python
from .settings import Settings, get_settings
from .database import get_database_url
from .logging_config import setup_logging

__all__ = ['Settings', 'get_settings', 'get_database_url', 'setup_logging']
```

### src/config/settings.py
```python
from pydantic_settings import BaseSettings
from pydantic import Field, validator
from typing import Optional, List
import os
from functools import lru_cache

class Settings(BaseSettings):
    # Database
    database_url: str = Field(..., env='DATABASE_URL')
    database_pool_size: int = Field(20, env='DATABASE_POOL_SIZE')
    
    # Scraping
    scraper_user_agent: str = Field(..., env='SCRAPER_USER_AGENT')
    scraper_concurrent_requests: int = Field(16, env='SCRAPER_CONCURRENT_REQUESTS')
    scraper_download_delay: float = Field(1.0, env='SCRAPER_DOWNLOAD_DELAY')
    scraper_retry_times: int = Field(3, env='SCRAPER_RETRY_TIMES')
    
    # ESCO
    esco_api_url: str = Field('https://ec.europa.eu/esco/api', env='ESCO_API_URL')
    esco_version: str = Field('1.1.0', env='ESCO_VERSION')
    esco_language: str = Field('es', env='ESCO_LANGUAGE')
    
    # LLM
    llm_model_path: str = Field(..., env='LLM_MODEL_PATH')
    llm_context_length: int = Field(4096, env='LLM_CONTEXT_LENGTH')
    llm_max_tokens: int = Field(512, env='LLM_MAX_TOKENS')
    llm_temperature: float = Field(0.7, env='LLM_TEMPERATURE')
    llm_n_gpu_layers: int = Field(35, env='LLM_N_GPU_LAYERS')
    
    # OpenAI (Optional)
    openai_api_key: Optional[str] = Field(None, env='OPENAI_API_KEY')
    openai_model: str = Field('gpt-3.5-turbo', env='OPENAI_MODEL')
    
    # Embeddings
    embedding_model: str = Field('intfloat/multilingual-e5-base', env='EMBEDDING_MODEL')
    embedding_batch_size: int = Field(32, env='EMBEDDING_BATCH_SIZE')
    embedding_cache_dir: str = Field('./data/cache/embeddings', env='EMBEDDING_CACHE_DIR')
    
    # Analysis
    cluster_min_size: int = Field(5, env='CLUSTER_MIN_SIZE')
    cluster_min_samples: int = Field(3, env='CLUSTER_MIN_SAMPLES')
    umap_n_neighbors: int = Field(15, env='UMAP_N_NEIGHBORS')
    umap_min_dist: float = Field(0.1, env='UMAP_MIN_DIST')
    
    # Output
    output_dir: str = Field('./outputs', env='OUTPUT_DIR')
    report_format: str = Field('pdf', env='REPORT_FORMAT')
    log_level: str = Field('INFO', env='LOG_LEVEL')
    log_file: str = Field('./logs/labor_observatory.log', env='LOG_FILE')
    
    # Supported countries and portals
    supported_countries: List[str] = ['CO', 'MX', 'AR']
    supported_portals: List[str] = ['computrabajo', 'bumeran', 'elempleo']
    
    class Config:
        env_file = '.env'
        env_file_encoding = 'utf-8'
    
    @validator('output_dir', 'log_file', 'embedding_cache_dir')
    def create_directories(cls, v):
        os.makedirs(os.path.dirname(v) if os.path.dirname(v) else v, exist_ok=True)
        return v

@lru_cache()
def get_settings() -> Settings:
    """Get cached settings instance."""
    return Settings()
```

### src/config/database.py
```python
import os
from urllib.parse import urlparse

def get_database_url() -> str:
    """Get database URL from environment or construct from components."""
    if os.getenv('DATABASE_URL'):
        return os.getenv('DATABASE_URL')
    
    # Construct from individual components
    user = os.getenv('DB_USER', 'labor_user')
    password = os.getenv('DB_PASSWORD', 'password')
    host = os.getenv('DB_HOST', 'localhost')
    port = os.getenv('DB_PORT', '5432')
    name = os.getenv('DB_NAME', 'labor_observatory')
    
    return f"postgresql://{user}:{password}@{host}:{port}/{name}"

def get_database_config() -> dict:
    """Parse database URL into components."""
    url = get_database_url()
    parsed = urlparse(url)
    
    return {
        'host': parsed.hostname,
        'port': parsed.port or 5432,
        'user': parsed.username,
        'password': parsed.password,
        'database': parsed.path.lstrip('/')
    }
```

### src/config/logging_config.py
```python
import logging
import logging.handlers
import os
from datetime import datetime

def setup_logging(log_level: str = 'INFO', log_file: str = None):
    """Configure logging for the entire application."""
    
    # Create logs directory if needed
    if log_file:
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    # Create formatters
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
    )
    console_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )
    
    # Get root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, log_level.upper()))
    
    # Remove existing handlers
    root_logger.handlers.clear()
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)
    
    # File handler with rotation
    if log_file:
        file_handler = logging.handlers.RotatingFileHandler(
            log_file,
            maxBytes=10 * 1024 * 1024,  # 10MB
            backupCount=5
        )
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(file_formatter)
        root_logger.addHandler(file_handler)
    
    # Specific loggers configuration
    logging.getLogger('scrapy').setLevel(logging.WARNING)
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('transformers').setLevel(logging.WARNING)
    
    return root_logger
```

### config/esco_config.yaml
```yaml
# ESCO Configuration for Spanish Language
esco:
  version: "1.1.0"
  base_url: "https://ec.europa.eu/esco/api"
  
  # Endpoints
  endpoints:
    skills: "/resource/skill"
    occupations: "/resource/occupation"
    search: "/search"
  
  # Spanish language configuration
  language:
    primary: "es"
    fallback: "en"
  
  # Skill types to extract
  skill_types:
    - "skill/competence"
    - "knowledge"
    - "skill"
  
  # Search parameters
  search:
    limit: 100
    fields:
      - "preferredLabel"
      - "altLabels"
      - "description"
      - "broaderConcept"
    
  # Mapping rules for common tech terms
  tech_mappings:
    # Programming languages
    "python": "http://data.europa.eu/esco/skill/3897c3c6-556b-4b8a-bfeb-234b0f716950"
    "java": "http://data.europa.eu/esco/skill/b4b36f5a-d5e6-4d78-b4ed-5b0b0e4f7a8a"
    "javascript": "http://data.europa.eu/esco/skill/7c7c5c49-0122-4b2e-8f6e-4e6b3f3e5d5f"
    "react": "http://data.europa.eu/esco/skill/react-framework"
    "node.js": "http://data.europa.eu/esco/skill/nodejs-runtime"
    
    # Databases
    "sql": "http://data.europa.eu/esco/skill/sql-language"
    "mysql": "http://data.europa.eu/esco/skill/mysql-database"
    "postgresql": "http://data.europa.eu/esco/skill/postgresql-database"
    "mongodb": "http://data.europa.eu/esco/skill/mongodb-database"
    
    # Cloud platforms
    "aws": "http://data.europa.eu/esco/skill/amazon-web-services"
    "azure": "http://data.europa.eu/esco/skill/microsoft-azure"
    "gcp": "http://data.europa.eu/esco/skill/google-cloud-platform"
    
    # DevOps
    "docker": "http://data.europa.eu/esco/skill/docker-containerization"
    "kubernetes": "http://data.europa.eu/esco/skill/kubernetes-orchestration"
    "git": "http://data.europa.eu/esco/skill/git-version-control"
    
    # Soft skills (Spanish)
    "trabajo en equipo": "http://data.europa.eu/esco/skill/teamwork"
    "comunicación": "http://data.europa.eu/esco/skill/communication"
    "liderazgo": "http://data.europa.eu/esco/skill/leadership"
    "resolución de problemas": "http://data.europa.eu/esco/skill/problem-solving"
```

---

## 4. Scraper Module Files

### src/scraper/__init__.py
```python
from .spiders.computrabajo_spider import ComputrabajoSpider
from .spiders.bumeran_spider import BumeranSpider
from .spiders.elempleo_spider import ElempleoSpider

__all__ = ['ComputrabajoSpider', 'BumeranSpider', 'ElempleoSpider']
```

### src/scraper/scrapy.cfg
```ini
[settings]
default = scraper.settings

[deploy]
project = labor_observatory_scraper
```

### src/scraper/items.py
```python
import scrapy
from scrapy.item import Field
from datetime import datetime
import re

class JobItem(scrapy.Item):
    # Required fields
    portal = Field()
    country = Field()
    url = Field()
    title = Field()
    description = Field()
    
    # Optional fields
    company = Field()
    location = Field()
    requirements = Field()
    salary_raw = Field()
    contract_type = Field()
    remote_type = Field()
    posted_date = Field()
    raw_html = Field()
    
    # Metadata
    scraped_at = Field()
    
    def clean_text(self, text):
        """Clean and normalize text fields."""
        if not text:
            return None
        
        # Remove extra whitespace
        text = ' '.join(text.split())
        
        # Remove HTML entities
        text = re.sub(r'&[a-zA-Z]+;', ' ', text)
        
        return text.strip()
    
    def __setitem__(self, key, value):
        # Clean text fields
        if key in ['title', 'description', 'requirements', 'company', 'location'] and value:
            value = self.clean_text(value)
        
        # Set scraped_at automatically
        if key == 'scraped_at':
            value = datetime.now()
        
        super().__setitem__(key, value)
```

### src/scraper/pipelines.py
```python
import logging
from datetime import datetime
from typing import Optional
import re
from scrapy.exceptions import DropItem
from database.operations import DatabaseOperations

logger = logging.getLogger(__name__)

class ValidationPipeline:
    """Validate scraped items."""
    
    def process_item(self, item, spider):
        # Check required fields
        required_fields = ['portal', 'country', 'url', 'title', 'description']
        
        for field in required_fields:
            if not item.get(field):
                raise DropItem(f"Missing required field: {field}")
        
        # Validate country code
        if item['country'] not in ['CO', 'MX', 'AR']:
            raise DropItem(f"Invalid country code: {item['country']}")
        
        # Validate portal
        if item['portal'] not in ['computrabajo', 'bumeran', 'elempleo']:
            raise DropItem(f"Invalid portal: {item['portal']}")
        
        # Ensure minimum description length
        if len(item['description']) < 50:
            raise DropItem("Description too short")
        
        return item

class NormalizationPipeline:
    """Normalize item fields."""
    
    def process_item(self, item, spider):
        # Normalize contract type
        if item.get('contract_type'):
            item['contract_type'] = self.normalize_contract_type(item['contract_type'])
        
        # Normalize remote type
        if item.get('remote_type'):
            item['remote_type'] = self.normalize_remote_type(item['remote_type'])
        
        # Parse posted date
        if item.get('posted_date'):
            item['posted_date'] = self.parse_date(item['posted_date'])
        
        # Set scraped_at
        item['scraped_at'] = datetime.now()
        
        return item
    
    def normalize_contract_type(self, contract: str) -> str:
        """Normalize contract type to standard values."""
        contract_lower = contract.lower()
        
        if any(term in contract_lower for term in ['tiempo completo', 'full time', 'completo']):
            return 'full_time'
        elif any(term in contract_lower for term in ['medio tiempo', 'part time', 'parcial']):
            return 'part_time'
        elif any(term in contract_lower for term in ['freelance', 'independiente', 'autonomo']):
            return 'freelance'
        elif any(term in contract_lower for term in ['contrato', 'temporal', 'proyecto']):
            return 'contract'
        elif any(term in contract_lower for term in ['pasantia', 'practica', 'internship']):
            return 'internship'
        else:
            return 'other'
    
    def normalize_remote_type(self, remote: str) -> str:
        """Normalize remote work type."""
        remote_lower = remote.lower()
        
        if any(term in remote_lower for term in ['remoto', 'remote', 'teletrabajo']):
            return 'remote'
        elif any(term in remote_lower for term in ['hibrido', 'hybrid', 'mixto']):
            return 'hybrid'
        elif any(term in remote_lower for term in ['presencial', 'oficina', 'on-site']):
            return 'on_site'
        else:
            return 'not_specified'
    
    def parse_date(self, date_str: str) -> Optional[datetime]:
        """Parse various date formats."""
        # Common Spanish date patterns
        patterns = [
            r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})',  # DD/MM/YYYY or DD-MM-YYYY
            r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})',  # YYYY/MM/DD or YYYY-MM-DD
            r'hace (\d+) días?',  # "hace X días"
            r'(\d+) días? atrás',  # "X días atrás"
            r'hoy',  # "hoy"
            r'ayer',  # "ayer"
        ]
        
        # Try to match patterns
        for pattern in patterns:
            match = re.search(pattern, date_str, re.IGNORECASE)
            if match:
                if 'hace' in pattern or 'atrás' in pattern:
                    days_ago = int(match.group(1))
                    return datetime.now() - timedelta(days=days_ago)
                elif pattern == r'hoy':
                    return datetime.now().date()
                elif pattern == r'ayer':
                    return datetime.now() - timedelta(days=1)
                else:
                    # Handle date formats
                    try:
                        if len(match.groups()) == 3:
                            if match.group(1).isdigit() and len(match.group(1)) == 4:
                                # YYYY/MM/DD format
                                return datetime(
                                    int(match.group(1)),
                                    int(match.group(2)),
                                    int(match.group(3))
                                ).date()
                            else:
                                # DD/MM/YYYY format
                                return datetime(
                                    int(match.group(3)),
                                    int(match.group(2)),
                                    int(match.group(1))
                                ).date()
                    except ValueError:
                        pass
        
        return None

class DatabasePipeline:
    """Save items to PostgreSQL database."""
    
    def __init__(self):
        self.db_ops = None
    
    def open_spider(self, spider):
        self.db_ops = DatabaseOperations()
        logger.info(f"Database pipeline opened for spider: {spider.name}")
    
    def process_item(self, item, spider):
        try:
            # Convert item to dict
            job_data = dict(item)
            
            # Remove metadata fields
            job_data.pop('scraped_at', None)
            
            # Insert into database
            job_id = self.db_ops.insert_job(job_data)
            
            if job_id:
                logger.info(f"Saved job {job_id}: {item['title']}")
            else:
                logger.warning(f"Duplicate job skipped: {item['url']}")
            
            return item
            
        except Exception as e:
            logger.error(f"Error saving job to database: {e}")
            raise
```

### src/scraper/settings.py
```python
import os
from config.settings import get_settings

settings = get_settings()

BOT_NAME = 'labor_observatory_scraper'

SPIDER_MODULES = ['scraper.spiders']
NEWSPIDER_MODULE = 'scraper.spiders'

# Obey robots.txt rules
ROBOTSTXT_OBEY = True

# Configure maximum concurrent requests
CONCURRENT_REQUESTS = settings.scraper_concurrent_requests
CONCURRENT_REQUESTS_PER_DOMAIN = 8

# Configure delay
DOWNLOAD_DELAY = settings.scraper_download_delay
RANDOMIZE_DOWNLOAD_DELAY = True

# Disable cookies
COOKIES_ENABLED = False

# User agent
USER_AGENT = settings.scraper_user_agent

# Override default headers
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'Cache-Control': 'no-cache',
    'Pragma': 'no-cache',
}

# Retry configuration
RETRY_TIMES = settings.scraper_retry_times
RETRY_HTTP_CODES = [500, 502, 503, 504, 408, 429]

# Configure pipelines
ITEM_PIPELINES = {
    'scraper.pipelines.ValidationPipeline': 100,
    'scraper.pipelines.NormalizationPipeline': 200,
    'scraper.pipelines.DatabasePipeline': 300,
}

# AutoThrottle extension
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 1
AUTOTHROTTLE_MAX_DELAY = 10
AUTOTHROTTLE_TARGET_CONCURRENCY = 8.0
AUTOTHROTTLE_DEBUG = False

# Memory usage
MEMUSAGE_ENABLED = True
MEMUSAGE_LIMIT_MB = 2048
MEMUSAGE_WARNING_MB = 1536

# Logging
LOG_LEVEL = 'INFO'
LOG_FORMAT = '%(levelname)s: %(message)s'

# Cache
HTTPCACHE_ENABLED = False

# Download timeout
DOWNLOAD_TIMEOUT = 30

# Telnet Console (disabled for production)
TELNETCONSOLE_ENABLED = False

# Middleware settings
DOWNLOADER_MIDDLEWARES = {
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,
    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110,
}
```

### src/scraper/middlewares.py
```python
import random
import logging
from scrapy.downloadermiddlewares.retry import RetryMiddleware
from scrapy.utils.response import response_status_message
from fake_useragent import UserAgent

logger = logging.getLogger(__name__)

class RotateUserAgentMiddleware:
    """Rotate user agents for each request."""
    
    def __init__(self):
        self.ua = UserAgent()
        self.user_agent_list = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
        ]
    
    def process_request(self, request, spider):
        try:
            # Try to use fake-useragent
            ua = self.ua.random
        except:
            # Fallback to predefined list
            ua = random.choice(self.user_agent_list)
        
        request.headers['User-Agent'] = ua + ' Academic Research Bot'

class CustomRetryMiddleware(RetryMiddleware):
    """Custom retry middleware with exponential backoff."""
    
    def process_response(self, request, response, spider):
        if response.status in self.retry_http_codes:
            reason = response_status_message(response.status)
            
            # Log retry attempt
            retry_times = request.meta.get('retry_times', 0) + 1
            logger.warning(
                f"Retrying {request.url} (attempt {retry_times}): {reason}"
            )
            
            # Exponential backoff
            request.meta['download_delay'] = 2 ** retry_times
            
            return self._retry(request, reason, spider) or response
        
        return response
```

### src/scraper/spiders/__init__.py
```python
# Spider modules initialization
```

### src/scraper/spiders/base_spider.py
```python
import scrapy
from abc import ABC, abstractmethod
import logging
from datetime import datetime
from urllib.parse import urljoin

logger = logging.getLogger(__name__)

class BaseJobSpider(scrapy.Spider, ABC):
    """Base spider class for job portals."""
    
    def __init__(self, country=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.country = country
        self.total_scraped = 0
        self.start_time = datetime.now()
    
    @abstractmethod
    def parse_job(self, response):
        """Parse individual job posting. Must be implemented by subclasses."""
        pass
    
    def extract_text(self, selector, xpath_or_css, method='xpath'):
        """Safely extract text from selector."""
        try:
            if method == 'xpath':
                texts = selector.xpath(xpath_or_css).getall()
            else:
                texts = selector.css(xpath_or_css).getall()
            
            # Join and clean text
            text = ' '.join(texts)
            return ' '.join(text.split()) if text else None
        except Exception as e:
            logger.error(f"Error extracting text: {e}")
            return None
    
    def build_absolute_url(self, response, relative_url):
        """Build absolute URL from relative URL."""
        return urljoin(response.url, relative_url)
    
    def log_progress(self):
        """Log scraping progress."""
        elapsed = (datetime.now() - self.start_time).total_seconds()
        rate = self.total_scraped / elapsed if elapsed > 0 else 0
        
        logger.info(
            f"Spider {self.name} - Country: {self.country} - "
            f"Scraped: {self.total_scraped} - Rate: {rate:.2f} jobs/sec"
        )
```

### src/scraper/spiders/computrabajo_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import re
from urllib.parse import urlencode
import logging

logger = logging.getLogger(__name__)

class ComputrabajoSpider(BaseJobSpider):
    name = 'computrabajo'
    allowed_domains = ['computrabajo.com', 'computrabajo.com.co', 
                      'computrabajo.com.mx', 'computrabajo.com.ar']
    
    # URL patterns by country
    country_urls = {
        'CO': 'https://www.computrabajo.com.co',
        'MX': 'https://www.computrabajo.com.mx',
        'AR': 'https://www.computrabajo.com.ar'
    }
    
    # Tech-related search terms
    tech_keywords = [
        'desarrollador', 'developer', 'programador', 'software',
        'data', 'analyst', 'engineer', 'fullstack', 'frontend',
        'backend', 'devops', 'cloud', 'mobile', 'web'
    ]
    
    def start_requests(self):
        if not self.country or self.country not in self.country_urls:
            raise ValueError(f"Invalid country: {self.country}")
        
        base_url = self.country_urls[self.country]
        
        # Generate search URLs for tech keywords
        for keyword in self.tech_keywords:
            search_url = f"{base_url}/trabajo-de-{keyword}"
            yield Request(
                url=search_url,
                callback=self.parse_search_results,
                meta={'keyword': keyword, 'page': 1}
            )
    
    def parse_search_results(self, response):
        """Parse search results page."""
        # Extract job listings
        job_cards = response.css('article.js-o-card')
        
        for card in job_cards:
            # Extract job URL
            job_url = card.css('a.js-o-card__link::attr(href)').get()
            if job_url:
                absolute_url = self.build_absolute_url(response, job_url)
                yield Request(
                    url=absolute_url,
                    callback=self.parse_job,
                    meta={'search_keyword': response.meta.get('keyword')}
                )
        
        # Check for next page
        current_page = response.meta.get('page', 1)
        next_page_link = response.css('a[aria-label="Siguiente"]::attr(href)').get()
        
        if next_page_link and current_page < 10:  # Limit to 10 pages per keyword
            next_url = self.build_absolute_url(response, next_page_link)
            yield Request(
                url=next_url,
                callback=self.parse_search_results,
                meta={
                    'keyword': response.meta.get('keyword'),
                    'page': current_page + 1
                }
            )
    
    def parse_job(self, response):
        """Parse individual job posting."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'computrabajo'
        item['country'] = self.country
        item['url'] = response.url
        
        # Title
        item['title'] = self.extract_text(
            response, 
            '//h1[@class="fwB fs24"]//text()',
            'xpath'
        )
        
        # Company
        item['company'] = self.extract_text(
            response,
            '//a[@class="dIB fs16 js-o-link"]//text()',
            'xpath'
        )
        
        # Location
        location_parts = response.xpath(
            '//div[@class="fs16 fc_base mt5"]//span//text()'
        ).getall()
        item['location'] = ', '.join(location_parts) if location_parts else None
        
        # Description and requirements
        description_sections = response.xpath(
            '//div[@class="mbB"]//p//text() | //div[@class="mbB"]//li//text()'
        ).getall()
        
        full_text = ' '.join(description_sections)
        
        # Try to separate requirements
        req_pattern = r'(?:requisitos|requerimientos|requirements|experiencia|conocimientos):(.*?)(?:funciones|responsabilidades|beneficios|$)'
        req_match = re.search(req_pattern, full_text, re.IGNORECASE | re.DOTALL)
        
        if req_match:
            item['requirements'] = req_match.group(1).strip()
            item['description'] = full_text.replace(req_match.group(0), '').strip()
        else:
            item['description'] = full_text
            item['requirements'] = None
        
        # Salary
        salary_text = self.extract_text(
            response,
            '//span[@class="fs16 fc_aux"]//text()[contains(., "$")]',
            'xpath'
        )
        item['salary_raw'] = salary_text
        
        # Contract type
        contract_info = response.xpath(
            '//span[@class="fs13 fc_aux"]//text()'
        ).getall()
        
        for info in contract_info:
            info_lower = info.lower()
            if any(term in info_lower for term in ['tiempo completo', 'full time', 'part time']):
                item['contract_type'] = info
            elif any(term in info_lower for term in ['remoto', 'presencial', 'hibrido']):
                item['remote_type'] = info
        
        # Posted date
        date_text = self.extract_text(
            response,
            '//span[@class="fs13 fc_aux"][contains(text(), "Publicado")]//text()',
            'xpath'
        )
        if date_text:
            item['posted_date'] = date_text.replace('Publicado', '').strip()
        
        # Raw HTML (for debugging/reprocessing)
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

### src/scraper/spiders/bumeran_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import json
import re
import logging

logger = logging.getLogger(__name__)

class BumeranSpider(BaseJobSpider):
    name = 'bumeran'
    allowed_domains = ['bumeran.com', 'bumeran.com.mx', 'bumeran.com.ar']
    
    # URL patterns by country
    country_urls = {
        'MX': 'https://www.bumeran.com.mx',
        'AR': 'https://www.bumeran.com.ar'
    }
    
    # Tech categories
    tech_categories = [
        'informatica-telecomunicaciones',
        'tecnologia-sistemas',
        'desarrollo-programacion'
    ]
    
    def start_requests(self):
        if self.country not in self.country_urls:
            raise ValueError(f"Bumeran not available for country: {self.country}")
        
        base_url = self.country_urls[self.country]
        
        # Generate category URLs
        for category in self.tech_categories:
            category_url = f"{base_url}/empleos-{category}.html"
            yield Request(
                url=category_url,
                callback=self.parse_category,
                meta={'category': category, 'page': 1}
            )
    
    def parse_category(self, response):
        """Parse category listing page."""
        # Check if page uses React/JSON data
        scripts = response.xpath('//script[contains(text(), "__INITIAL_STATE__")]/text()').getall()
        
        if scripts:
            # Extract JSON data from script
            for script in scripts:
                match = re.search(r'__INITIAL_STATE__\s*=\s*({.*?});', script, re.DOTALL)
                if match:
                    try:
                        data = json.loads(match.group(1))
                        jobs = self.extract_jobs_from_json(data)
                        
                        for job in jobs:
                            yield Request(
                                url=job['url'],
                                callback=self.parse_job,
                                meta={'job_data': job}
                            )
                    except json.JSONDecodeError:
                        logger.error("Failed to parse JSON data")
        else:
            # Fallback to HTML parsing
            job_links = response.css('div.Card__CardContentWrapper a::attr(href)').getall()
            
            for link in job_links:
                absolute_url = self.build_absolute_url(response, link)
                yield Request(url=absolute_url, callback=self.parse_job)
        
        # Handle pagination
        current_page = response.meta.get('page', 1)
        if current_page < 10:  # Limit pages
            next_page = current_page + 1
            next_url = response.url.replace(
                f'.html', 
                f'-pagina-{next_page}.html'
            )
            yield Request(
                url=next_url,
                callback=self.parse_category,
                meta={
                    'category': response.meta.get('category'),
                    'page': next_page
                }
            )
    
    def extract_jobs_from_json(self, data):
        """Extract job data from JSON structure."""
        jobs = []
        
        # Navigate through possible JSON structures
        try:
            if 'results' in data:
                job_list = data['results'].get('jobs', [])
            elif 'jobs' in data:
                job_list = data['jobs']
            else:
                return jobs
            
            for job in job_list:
                job_info = {
                    'url': job.get('url', ''),
                    'title': job.get('title', ''),
                    'company': job.get('company', {}).get('name', ''),
                    'location': job.get('location', '')
                }
                if job_info['url']:
                    jobs.append(job_info)
        except Exception as e:
            logger.error(f"Error extracting jobs from JSON: {e}")
        
        return jobs
    
    def parse_job(self, response):
        """Parse individual job posting."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'bumeran'
        item['country'] = self.country
        item['url'] = response.url
        
        # Try to get pre-parsed data
        job_data = response.meta.get('job_data', {})
        
        # Title
        item['title'] = job_data.get('title') or self.extract_text(
            response,
            'h1[class*="Title"]::text',
            'css'
        )
        
        # Company
        item['company'] = job_data.get('company') or self.extract_text(
            response,
            'h2[class*="Company"]::text',
            'css'
        )
        
        # Location
        item['location'] = job_data.get('location') or self.extract_text(
            response,
            'span[class*="Location"]::text',
            'css'
        )
        
        # Description
        description_selectors = [
            'div[class*="Description"]',
            'div.detalle-aviso',
            'div#description'
        ]
        
        for selector in description_selectors:
            desc_elements = response.css(f'{selector} ::text').getall()
            if desc_elements:
                item['description'] = ' '.join(desc_elements)
                break
        
        # Requirements - often within description
        if item.get('description'):
            req_pattern = r'(?:requisitos|requerimientos|experiencia|competencias):(.*?)(?:beneficios|funciones|responsabilidades|$)'
            req_match = re.search(req_pattern, item['description'], re.IGNORECASE | re.DOTALL)
            
            if req_match:
                item['requirements'] = req_match.group(1).strip()
        
        # Salary
        salary_selectors = [
            'span[class*="Salary"]::text',
            'div[class*="salary"]::text',
            'span:contains("$")::text'
        ]
        
        for selector in salary_selectors:
            salary = response.css(selector).get()
            if salary and '$' in salary:
                item['salary_raw'] = salary
                break
        
        # Contract type and remote
        tags = response.css('span[class*="Tag"]::text').getall()
        for tag in tags:
            tag_lower = tag.lower()
            if any(term in tag_lower for term in ['tiempo completo', 'part time', 'freelance']):
                item['contract_type'] = tag
            elif any(term in tag_lower for term in ['remoto', 'presencial', 'hibrido']):
                item['remote_type'] = tag
        
        # Posted date
        date_text = response.css('span[class*="Date"]::text').get()
        if date_text:
            item['posted_date'] = date_text
        
        # Raw HTML
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

### src/scraper/spiders/elempleo_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import re
import logging
from urllib.parse import urljoin, urlparse, parse_qs

logger = logging.getLogger(__name__)

class ElempleoSpider(BaseJobSpider):
    name = 'elempleo'
    allowed_domains = ['elempleo.com']
    
    # Only available for Colombia
    country_urls = {
        'CO': 'https://www.elempleo.com'
    }
    
    # Tech-related categories in elempleo
    tech_categories = {
        'tecnologia': '1100',
        'sistemas': '1100',
        'informatica': '1100'
    }
    
    def start_requests(self):
        if self.country != 'CO':
            raise ValueError("elempleo.com is only available for Colombia (CO)")
        
        base_url = self.country_urls['CO']
        
        # Search URLs for technology jobs
        search_base = f"{base_url}/colombia/empleos"
        
        # Generate search requests
        for category_name, category_id in self.tech_categories.items():
            search_params = {
                'categoria': category_id,
                'pagina': 1
            }
            
            search_url = f"{search_base}?categoria={category_id}"
            
            yield Request(
                url=search_url,
                callback=self.parse_search_results,
                meta={
                    'category': category_name,
                    'page': 1
                }
            )
    
    def parse_search_results(self, response):
        """Parse search results from elempleo."""
        # Extract job cards
        job_cards = response.css('div.result-item')
        
        if not job_cards:
            # Try alternative selectors
            job_cards = response.css('article.js-offer')
        
        for card in job_cards:
            # Extract job URL
            job_link = card.css('a.js-offer-title::attr(href)').get()
            if not job_link:
                job_link = card.css('h2 a::attr(href)').get()
            
            if job_link:
                # Handle both relative and absolute URLs
                if not job_link.startswith('http'):
                    job_link = urljoin(response.url, job_link)
                
                # Extract basic info from card
                card_data = {
                    'title': card.css('h2 a::text').get(),
                    'company': card.css('span.info-company-name::text').get(),
                    'location': card.css('span.info-city::text').get()
                }
                
                yield Request(
                    url=job_link,
                    callback=self.parse_job,
                    meta={'card_data': card_data}
                )
        
        # Handle pagination
        current_page = response.meta.get('page', 1)
        next_page_link = response.css('a.js-btn-next::attr(href)').get()
        
        if not next_page_link:
            # Alternative pagination
            pagination_links = response.css('ul.pagination a::attr(href)').getall()
            for link in pagination_links:
                if f'pagina={current_page + 1}' in link:
                    next_page_link = link
                    break
        
        if next_page_link and current_page < 10:  # Limit to 10 pages
            next_url = urljoin(response.url, next_page_link)
            yield Request(
                url=next_url,
                callback=self.parse_search_results,
                meta={
                    'category': response.meta.get('category'),
                    'page': current_page + 1
                }
            )
    
    def parse_job(self, response):
        """Parse individual job posting from elempleo."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'elempleo'
        item['country'] = 'CO'
        item['url'] = response.url
        
        # Get card data if available
        card_data = response.meta.get('card_data', {})
        
        # Title
        item['title'] = self.extract_text(
            response,
            'h1.offer-title::text',
            'css'
        ) or card_data.get('title')
        
        # Company
        item['company'] = self.extract_text(
            response,
            'div.company-name a::text',
            'css'
        ) or self.extract_text(
            response,
            'span.offer-company::text',
            'css'
        ) or card_data.get('company')
        
        # Location
        location_parts = response.css('div.offer-location span::text').getall()
        if location_parts:
            item['location'] = ', '.join(location_parts)
        else:
            item['location'] = card_data.get('location')
        
        # Main content sections
        content_sections = response.css('div.offer-description')
        
        # Description
        description_html = content_sections.css('div#description').get()
        if description_html:
            # Clean HTML and extract text
            desc_text = re.sub(r'<[^>]+>', ' ', description_html)
            item['description'] = ' '.join(desc_text.split())
        
        # Requirements
        requirements_section = content_sections.css('div#requirements')
        if requirements_section:
            req_items = requirements_section.css('li::text').getall()
            if req_items:
                item['requirements'] = ' '.join(req_items)
            else:
                req_text = requirements_section.css('::text').getall()
                item['requirements'] = ' '.join(req_text)
        
        # If requirements not in separate section, try to extract from description
        if not item.get('requirements') and item.get('description'):
            req_pattern = r'(?:requisitos|perfil|experiencia|conocimientos):(.*?)(?:funciones|responsabilidades|ofrecemos|$)'
            req_match = re.search(req_pattern, item['description'], re.IGNORECASE | re.DOTALL)
            if req_match:
                item['requirements'] = req_match.group(1).strip()
        
        # Salary
        salary_element = response.css('span.offer-salary::text').get()
        if salary_element:
            item['salary_raw'] = salary_element
        else:
            # Look for salary in description
            salary_pattern = r'\$[\d.,]+ (?:millones|COP|pesos)'
            salary_match = re.search(salary_pattern, item.get('description', ''))
            if salary_match:
                item['salary_raw'] = salary_match.group(0)
        
        # Contract type
        contract_info = response.css('div.offer-info span::text').getall()
        for info in contract_info:
            info_lower = info.lower()
            if any(term in info_lower for term in ['contrato', 'tiempo completo', 'medio tiempo']):
                item['contract_type'] = info
            elif any(term in info_lower for term in ['remoto', 'presencial', 'teletrabajo']):
                item['remote_type'] = info
        
        # Posted date
        date_element = response.css('span.offer-date::text').get()
        if date_element:
            item['posted_date'] = date_element
        else:
            # Try to extract from meta tags
            date_meta = response.css('meta[property="article:published_time"]::attr(content)').get()
            if date_meta:
                item['posted_date'] = date_meta.split('T')[0]
        
        # Additional fields from structured data
        try:
            # Check for JSON-LD structured data
            json_ld = response.css('script[type="application/ld+json"]::text').get()
            if json_ld:
                import json
                data = json.loads(json_ld)
                
                # Extract additional info if available
                if isinstance(data, dict):
                    if 'title' in data and not item.get('title'):
                        item['title'] = data['title']
                    if 'hiringOrganization' in data and not item.get('company'):
                        item['company'] = data['hiringOrganization'].get('name')
                    if 'jobLocation' in data and not item.get('location'):
                        location = data['jobLocation']
                        if isinstance(location, dict):
                            item['location'] = location.get('address', {}).get('addressLocality')
        except Exception as e:
            logger.debug(f"Could not parse structured data: {e}")
        
        # Raw HTML
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

---

## 5. Extractor Module Files

### src/extractor/__init__.py
```python
from .pipeline import ExtractionPipeline
from .ner_extractor import NERExtractor
from .regex_patterns import RegexExtractor
from .esco_matcher import ESCOMatcher

__all__ = ['ExtractionPipeline', 'NERExtractor', 'RegexExtractor', 'ESCOMatcher']
```

### src/extractor/ner_extractor.py
```python
import spacy
from spacy.tokens import Doc, Span
from typing import List, Dict, Tuple, Optional
import logging
import os
from config.settings import get_settings

logger = logging.getLogger(__name__)

class NERExtractor:
    """Extract skills using Named Entity Recognition."""
    
    def __init__(self, model_path: Optional[str] = None):
        self.settings = get_settings()
        
        # Load spaCy model
        if model_path and os.path.exists(model_path):
            self.nlp = spacy.load(model_path)
            logger.info(f"Loaded custom NER model from {model_path}")
        else:
            # Load default Spanish model
            try:
                self.nlp = spacy.load("es_core_news_lg")
                logger.info("Loaded default Spanish model")
            except:
                logger.warning("Spanish model not found, downloading...")
                os.system("python -m spacy download es_core_news_lg")
                self.nlp = spacy.load("es_core_news_lg")
        
        # Add custom pipeline components
        self._add_tech_entity_ruler()
    
    def _add_tech_entity_ruler(self):
        """Add rule-based entity recognition for tech terms."""
        ruler = self.nlp.add_pipe("entity_ruler", before="ner")
        
        # Define patterns for common tech skills
        patterns = [
            # Programming languages
            {"label": "SKILL", "pattern": [{"LOWER": "python"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "java"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "javascript"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "typescript"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "c++"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "c#"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "php"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "ruby"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "go"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "golang"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "rust"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "kotlin"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "swift"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "r"}]},
            
            # Frameworks
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "react"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "react"}, {"LOWER": "native"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "angular"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "vue"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "vue"}, {"LOWER": "js"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "django"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "flask"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "spring"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "spring"}, {"LOWER": "boot"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "node"}, {"LOWER": "js"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "nodejs"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "express"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": ".net"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "laravel"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "rails"}]},
            
            # Databases
            {"label": "DATABASE", "pattern": [{"LOWER": "mysql"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "postgresql"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "postgres"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "mongodb"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "redis"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "elasticsearch"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "oracle"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "sql"}, {"LOWER": "server"}]},
            
            # Cloud & DevOps
            {"label": "PLATFORM", "pattern": [{"LOWER": "aws"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "amazon"}, {"LOWER": "web"}, {"LOWER": "services"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "azure"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "gcp"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "google"}, {"LOWER": "cloud"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "docker"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "kubernetes"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "jenkins"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "git"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "github"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "gitlab"}]},
            
            # Data & ML
            {"label": "SKILL", "pattern": [{"LOWER": "machine"}, {"LOWER": "learning"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "deep"}, {"LOWER": "learning"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "data"}, {"LOWER": "science"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "big"}, {"LOWER": "data"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "tensorflow"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "pytorch"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "scikit"}, {"LOWER": "learn"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "pandas"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "numpy"}]},
            
            # Methodologies
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "agile"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "scrum"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "kanban"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "devops"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "ci"}, {"LOWER": "cd"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "tdd"}]},
        ]
        
        # Add Spanish variations
        spanish_patterns = [
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "web"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "movil"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "móvil"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "base"}, {"LOWER": "de"}, {"LOWER": "datos"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "inteligencia"}, {"LOWER": "artificial"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "aprendizaje"}, {"LOWER": "automatico"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "aprendizaje"}, {"LOWER": "automático"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "ciencia"}, {"LOWER": "de"}, {"LOWER": "datos"}]},
        ]
        
        ruler.add_patterns(patterns + spanish_patterns)
    
    def extract(self, text: str, source_section: str = "unknown") -> List[Dict[str, any]]:
        """Extract skills from text using NER.
        
        Args:
            text: Text to extract skills from
            source_section: Section of job posting (title, description, requirements)
            
        Returns:
            List of extracted skills with metadata
        """
        if not text:
            return []
        
        # Process text with spaCy
        doc = self.nlp(text)
        
        extracted_skills = []
        seen_skills = set()
        
        # Extract entities
        for ent in doc.ents:
            if ent.label_ in ["SKILL", "FRAMEWORK", "DATABASE", "PLATFORM", "TOOL", "METHODOLOGY"]:
                skill_text = ent.text.lower().strip()
                
                # Skip if already seen
                if skill_text in seen_skills:
                    continue
                
                seen_skills.add(skill_text)
                
                extracted_skills.append({
                    "skill_text": skill_text,
                    "skill_type": "explicit",
                    "extraction_method": "ner",
                    "entity_label": ent.label_,
                    "confidence_score": 0.9,  # High confidence for NER
                    "source_section": source_section,
                    "span_start": ent.start_char,
                    "span_end": ent.end_char,
                    "context": text[max(0, ent.start_char-50):ent.end_char+50]
                })
        
        logger.debug(f"NER extracted {len(extracted_skills)} skills from {source_section}")
        
        return extracted_skills
    
    def extract_from_job(self, job_data: Dict[str, str]) -> List[Dict[str, any]]:
        """Extract skills from all sections of a job posting.
        
        Args:
            job_data: Dictionary with job fields (title, description, requirements)
            
        Returns:
            List of all extracted skills
        """
        all_skills = []
        
        # Extract from title
        if job_data.get('title'):
            title_skills = self.extract(job_data['title'], 'title')
            all_skills.extend(title_skills)
        
        # Extract from description
        if job_data.get('description'):
            desc_skills = self.extract(job_data['description'], 'description')
            all_skills.extend(desc_skills)
        
        # Extract from requirements
        if job_data.get('requirements'):
            req_skills = self.extract(job_data['requirements'], 'requirements')
            all_skills.extend(req_skills)
        
        return all_skills
```

### src/extractor/regex_patterns.py
```python
import re
from typing import List, Dict, Tuple
import logging

logger = logging.getLogger(__name__)

class RegexExtractor:
    """Extract skills using regular expressions."""
    
    def __init__(self):
        # Define regex patterns for skill extraction
        self.patterns = self._build_patterns()
    
    def _build_patterns(self) -> List[Tuple[str, re.Pattern, str]]:
        """Build regex patterns for skill extraction.
        
        Returns:
            List of tuples (pattern_name, compiled_regex, skill_type)
        """
        patterns = []
        
        # Experience patterns in Spanish
        experience_patterns = [
            (
                "experiencia_en",
                re.compile(
                    r"experiencia\s+(?:en|con)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "conocimientos_de",
                re.compile(
                    r"conocimientos?\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "manejo_de",
                re.compile(
                    r"manejo\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "tool"
            ),
            (
                "dominio_de",
                re.compile(
                    r"dominio\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "desarrollo_en",
                re.compile(
                    r"desarrollo\s+(?:en|con)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
        ]
        
        # Required skills patterns
        required_patterns = [
            (
                "requerimos",
                re.compile(
                    r"(?:requerimos|buscamos|necesitamos)\s+(?:personas?\s+con\s+)?(?:conocimientos?\s+en\s+)?([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+para\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "indispensable",
                re.compile(
                    r"(?:indispensable|fundamental|esencial)\s+(?:contar\s+con\s+)?(?:conocimientos?\s+en\s+)?([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
        ]
        
        # Technology stack patterns
        tech_stack_patterns = [
            (
                "tecnologias",
                re.compile(
                    r"(?:tecnologías?|herramientas?|lenguajes?)\s*:\s*([A-Za-z0-9\+\#\.\s,/\-\(\)]+?)(?:\n|$)",
                    re.IGNORECASE
                ),
                "mixed"
            ),
            (
                "stack_tecnologico",
                re.compile(
                    r"stack\s+(?:tecnológico|tech)\s*:\s*([A-Za-z0-9\+\#\.\s,/\-\(\)]+?)(?:\n|$)",
                    re.IGNORECASE
                ),
                "mixed"
            ),
        ]
        
        # List patterns (bullet points, numbered lists)
        list_patterns = [
            (
                "bullet_skills",
                re.compile(
                    r"(?:^|\n)\s*[\-\*\•]\s*([A-Za-z0-9\+\#\.\s]+?)(?:\n|$)",
                    re.MULTILINE
                ),
                "skill"
            ),
            (
                "numbered_skills",
                re.compile(
                    r"(?:^|\n)\s*\d+[\.\)]\s*([A-Za-z0-9\+\#\.\s]+?)(?:\n|$)",
                    re.MULTILINE
                ),
                "skill"
            ),
        ]
        
        # Certification patterns
        cert_patterns = [
            (
                "certificacion",
                re.compile(
                    r"(?:certificación|certificado)\s+(?:en\s+)?([A-Za-z0-9\+\#\.\s\-]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "certification"
            ),
        ]
        
        # Years of experience patterns
        experience_years_patterns = [
            (
                "años_experiencia",
                re.compile(
                    r"(\d+)\s*\+?\s*años?\s+(?:de\s+)?experiencia\s+(?:en\s+)?([A-Za-z0-9\+\#\.\s]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "skill_with_years"
            ),
        ]
        
        # Combine all patterns
        patterns.extend(experience_patterns)
        patterns.extend(required_patterns)
        patterns.extend(tech_stack_patterns)
        patterns.extend(list_patterns)
        patterns.extend(cert_patterns)
        patterns.extend(experience_years_patterns)
        
        return patterns
    
    def extract(self, text: str, source_section: str = "unknown") -> List[Dict[str, any]]:
        """Extract skills using regex patterns.
        
        Args:
            text: Text to extract skills from
            source_section: Section of job posting
            
        Returns:
            List of extracted skills with metadata
        """
        if not text:
            return []
        
        extracted_skills = []
        seen_skills = set()
        
        for pattern_name, regex, skill_type in self.patterns:
            matches = regex.finditer(text)
            
            for match in matches:
                if skill_type == "skill_with_years":
                    # Special handling for years of experience
                    years = match.group(1)
                    skill_text = match.group(2).strip().lower()
                    
                    if skill_text and skill_text not in seen_skills:
                        seen_skills.add(skill_text)
                        extracted_skills.append({
                            "skill_text": skill_text,
                            "skill_type": "explicit",
                            "extraction_method": "regex",
                            "pattern_name": pattern_name,
                            "confidence_score": 0.8,
                            "source_section": source_section,
                            "span_start": match.start(2),
                            "span_end": match.end(2),
                            "years_required": int(years),
                            "context": text[max(0, match.start()-30):match.end()+30]
                        })
                else:
                    # Normal skill extraction
                    skill_text = match.group(1).strip().lower()
                    
                    # Clean up extracted text
                    skill_text = self._clean_skill_text(skill_text)
                    
                    if skill_text and len(skill_text) > 1 and skill_text not in seen_skills:
                        # Additional validation
                        if self._is_valid_skill(skill_text):
                            seen_skills.add(skill_text)
                            
                            extracted_skills.append({
                                "skill_text": skill_text,
                                "skill_type": "explicit",
                                "extraction_method": "regex",
                                "pattern_name": pattern_name,
                                "confidence_score": 0.7,  # Lower than NER
                                "source_section": source_section,
                                "span_start": match.start(1),
                                "span_end": match.end(1),
                                "context": text[max(0, match.start()-30):match.end()+30]
                            })
        
        # Handle comma-separated lists within extracted skills
        expanded_skills = []
        for skill in extracted_skills:
            if ',' in skill['skill_text'] or ' y ' in skill['skill_text']:
                # Split and create individual skills
                parts = re.split(r'[,\s]+y\s+|,\s*', skill['skill_text'])
                for part in parts:
                    part = part.strip()
                    if part and self._is_valid_skill(part):
                        new_skill = skill.copy()
                        new_skill['skill_text'] = part
                        expanded_skills.append(new_skill)
            else:
                expanded_skills.append(skill)
        
        logger.debug(f"Regex extracted {len(expanded_skills)} skills from {source_section}")
        
        return expanded_skills
    
    def _clean_skill_text(self, text: str) -> str:
        """Clean extracted skill text.
        
        Args:
            text: Raw extracted text
            
        Returns:
            Cleaned skill text
        """
        # Remove common stop words at the beginning/end
        stop_words = [
            'el', 'la', 'los', 'las', 'un', 'una', 'de', 'del', 'al',
            'y', 'o', 'con', 'para', 'por', 'en', 'a'
        ]
        
        words = text.split()
        
        # Remove stop words from beginning
        while words and words[0].lower() in stop_words:
            words.pop(0)
        
        # Remove stop words from end
        while words and words[-1].lower() in stop_words:
            words.pop()
        
        cleaned = ' '.join(words)
        
        # Remove extra spaces and punctuation
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = re.sub(r'[^\w\s\+\#\.\-/]', '', cleaned)
        
        return cleaned.strip()
    
    def _is_valid_skill(self, skill_text: str) -> bool:
        """Validate if extracted text is likely a valid skill.
        
        Args:
            skill_text: Text to validate
            
        Returns:
            True if valid skill, False otherwise
        """
        # Check minimum length
        if len(skill_text) < 2:
            return False
        
        # Check if it's just numbers
        if skill_text.isdigit():
            return False
        
        # Check against blacklist of common false positives
        blacklist = [
            'años', 'año', 'experiencia', 'conocimiento', 'manejo',
            'desarrollo', 'persona', 'profesional', 'trabajo',
            'empresa', 'cliente', 'proyecto', 'equipo', 'area',
            'sistemas', 'tecnologia', 'informatica'  # Too generic
        ]
        
        if skill_text.lower() in blacklist:
            return False
        
        # Must contain at least one letter
        if not any(c.isalpha() for c in skill_text):
            return False
        
        return True
    
    def extract_from_job(self, job_data: Dict[str, str]) -> List[Dict[str, any]]:
        """Extract skills from all sections of a job posting.
        
        Args:
            job_data: Dictionary with job fields
            
        Returns:
            List of all extracted skills
        """
        all_skills = []
        
        # Process each section
        for section in ['title', 'description', 'requirements']:
            if job_data.get(section):
                section_skills = self.extract(job_data[section], section)
                all_skills.extend(section_skills)
        
        return all_skills
```

### src/extractor/esco_matcher.py
```python
import json
import logging
from typing import List, Dict, Optional, Set
from fuzzywuzzy import fuzz, process
import requests
from pathlib import Path
import yaml

logger = logging.getLogger(__name__)

class ESCOMatcher:
    """Match extracted skills to ESCO taxonomy."""
    
    def __init__(self, config_path: str = "config/esco_config.yaml"):
        self.config = self._load_config(config_path)
        self.skills_cache = {}
        self.local_mappings = self.config.get('tech_mappings', {})
        
        # Load local ESCO data if available
        self.local_esco_data = self._load_local_esco_data()
    
    def _load_config(self, config_path: str) -> dict:
        """Load ESCO configuration."""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config['esco']
        except Exception as e:
            logger.error(f"Failed to load ESCO config: {e}")
            return {}
    
    def _load_local_esco_data(self) -> Dict[str, Dict]:
        """Load local ESCO data files if available."""
        esco_data = {}
        
        # Try to load skills data
        skills_path = Path("data/esco/skills_es.csv")
        if skills_path.exists():
            try:
                import pandas as pd
                df = pd.read_csv(skills_path)
                
                for _, row in df.iterrows():
                    skill_uri = row.get('conceptUri', '')
                    esco_data[skill_uri] = {
                        'preferredLabel': row.get('preferredLabel', ''),
                        'altLabels': row.get('altLabels', '').split('|') if row.get('altLabels') else [],
                        'description': row.get('description', ''),
                        'skillType': row.get('skillType', '')
                    }
                
                logger.info(f"Loaded {len(esco_data)} ESCO skills from local file")
            except Exception as e:
                logger.error(f"Failed to load local ESCO data: {e}")
        
        return esco_data
    
    def match_skill(self, skill_text: str, threshold: float = 0.8) -> Optional[Dict[str, any]]:
        """Match a skill to ESCO taxonomy.
        
        Args:
            skill_text: Skill text to match
            threshold: Minimum similarity threshold (0-1)
            
        Returns:
            ESCO match data or None
        """
        skill_lower = skill_text.lower().strip()
        
        # Check direct mappings first
        if skill_lower in self.local_mappings:
            esco_uri = self.local_mappings[skill_lower]
            
            # Get details from local data or cache
            if esco_uri in self.local_esco_data:
                return {
                    'esco_uri': esco_uri,
                    'esco_preferred_label': self.local_esco_data[esco_uri]['preferredLabel'],
                    'match_type': 'direct',
                    'match_score': 1.0
                }
        
        # Try fuzzy matching against local data
        if self.local_esco_data:
            best_match = self._fuzzy_match_local(skill_text, threshold)
            if best_match:
                return best_match
        
        # Try API lookup (if configured and no local match)
        if self.config.get('base_url'):
            api_match = self._api_lookup(skill_text)
            if api_match:
                return api_match
        
        return None
    
    def _fuzzy_match_local(self, skill_text: str, threshold: float) -> Optional[Dict[str, any]]:
        """Fuzzy match against local ESCO data.
        
        Args:
            skill_text: Skill to match
            threshold: Minimum score threshold
            
        Returns:
            Best match or None
        """
        # Collect all labels for matching
        all_labels = []
        for uri, data in self.local_esco_data.items():
            # Add preferred label
            all_labels.append((data['preferredLabel'].lower(), uri, 'preferred'))
            
            # Add alternative labels
            for alt_label in data.get('altLabels', []):
                if alt_label:
                    all_labels.append((alt_label.lower(), uri, 'alternative'))
        
        # Find best match
        if all_labels:
            # Use token sort ratio for better matching of multi-word skills
            matches = process.extract(
                skill_text.lower(),
                [label[0] for label in all_labels],
                scorer=fuzz.token_sort_ratio,
                limit=3
            )
            
            for match_text, score in matches:
                if score >= threshold * 100:  # fuzzywuzzy uses 0-100 scale
                    # Find the corresponding URI
                    for label_text, uri, label_type in all_labels:
                        if label_text == match_text:
                            return {
                                'esco_uri': uri,
                                'esco_preferred_label': self.local_esco_data[uri]['preferredLabel'],
                                'match_type': f'fuzzy_{label_type}',
                                'match_score': score / 100.0,
                                'matched_text': match_text
                            }
        
        return None
    
    def _api_lookup(self, skill_text: str) -> Optional[Dict[str, any]]:
        """Look up skill using ESCO API.
        
        Args:
            skill_text: Skill to look up
            
        Returns:
            API match data or None
        """
        try:
            # Check cache first
            if skill_text in self.skills_cache:
                return self.skills_cache[skill_text]
            
            # Prepare API request
            api_url = f"{self.config['base_url']}{self.config['endpoints']['search']}"
            
            params = {
                'text': skill_text,
                'language': self.config['language']['primary'],
                'type': 'skill',
                'limit': 5
            }
            
            headers = {
                'Accept': 'application/json',
                'Accept-Language': self.config['language']['primary']
            }
            
            # Make request
            response = requests.get(api_url, params=params, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                if data.get('results'):
                    # Get first result
                    result = data['results'][0]
                    
                    match_data = {
                        'esco_uri': result.get('uri', ''),
                        'esco_preferred_label': result.get('preferredLabel', {}).get(
                            self.config['language']['primary'],
                            result.get('preferredLabel', {}).get('en', '')
                        ),
                        'match_type': 'api_search',
                        'match_score': result.get('score', 0.0)
                    }
                    
                    # Cache the result
                    self.skills_cache[skill_text] = match_data
                    
                    return match_data
            
        except Exception as e:
            logger.error(f"ESCO API lookup failed for '{skill_text}': {e}")
        
        return None
    
    def match_skills_batch(self, skills: List[str]) -> Dict[str, Optional[Dict]]:
        """Match multiple skills to ESCO taxonomy.
        
        Args:
            skills: List of skill texts
            
        Returns:
            Dictionary mapping skill text to ESCO match data
        """
        results = {}
        
        for skill in skills:
            match = self.match_skill(skill)
            results[skill] = match
        
        # Log statistics
        matched = sum(1 for v in results.values() if v is not None)
        logger.info(
            f"ESCO matching: {matched}/{len(skills)} skills matched "
            f"({matched/len(skills)*100:.1f}%)"
        )
        
        return results

### src/analyzer/dimension_reducer.py
```python
import logging
from typing import Tuple, Dict, Any, Optional
import numpy as np
from umap import UMAP
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import time

logger = logging.getLogger(__name__)

class DimensionReducer:
    """Reduce dimensionality of embeddings for visualization and clustering."""
    
    def __init__(self):
        self.reducers = {}
    
    def reduce_dimensions(self,
                         embeddings: np.ndarray,
                         method: str = 'umap',
                         n_components: int = 2,
                         **kwargs) -> Tuple[np.ndarray, Dict[str, Any]]:
        """Reduce dimensionality of embeddings.
        
        Args:
            embeddings: High-dimensional embeddings
            method: Reduction method ('umap', 'pca', 'tsne')
            n_components: Number of output dimensions
            **kwargs: Additional parameters for the method
            
        Returns:
            Tuple of (reduced embeddings, metadata)
        """
        logger.info(
            f"Reducing {embeddings.shape} to {n_components}D using {method}"
        )
        
        start_time = time.time()
        
        if method == 'umap':
            reduced, reducer = self._reduce_umap(embeddings, n_components, **kwargs)
        elif method == 'pca':
            reduced, reducer = self._reduce_pca(embeddings, n_components, **kwargs)
        elif method == 'tsne':
            reduced, reducer = self._reduce_tsne(embeddings, n_components, **kwargs)
        else:
            raise ValueError(f"Unknown reduction method: {method}")
        
        processing_time = time.time() - start_time
        
        # Store reducer for later use
        self.reducers[method] = reducer
        
        metadata = {
            'method': method,
            'n_components': n_components,
            'original_shape': embeddings.shape,
            'reduced_shape': reduced.shape,
            'processing_time': processing_time,
            'parameters': kwargs
        }
        
        logger.info(f"Dimension reduction complete in {processing_time:.2f}s")
        
        return reduced, metadata
    
    def _reduce_umap(self,
                    embeddings: np.ndarray,
                    n_components: int,
                    n_neighbors: int = 15,
                    min_dist: float = 0.1,
                    metric: str = 'cosine') -> Tuple[np.ndarray, UMAP]:
        """Reduce dimensions using UMAP.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            n_neighbors: UMAP n_neighbors parameter
            min_dist: UMAP min_dist parameter
            metric: Distance metric
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        reducer = UMAP(
            n_components=n_components,
            n_neighbors=n_neighbors,
            min_dist=min_dist,
            metric=metric,
            random_state=42,
            verbose=True
        )
        
        reduced = reducer.fit_transform(embeddings)
        
        return reduced, reducer
    
    def _reduce_pca(self,
                   embeddings: np.ndarray,
                   n_components: int) -> Tuple[np.ndarray, PCA]:
        """Reduce dimensions using PCA.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        reducer = PCA(n_components=n_components, random_state=42)
        reduced = reducer.fit_transform(embeddings)
        
        # Log explained variance
        if hasattr(reducer, 'explained_variance_ratio_'):
            total_variance = np.sum(reducer.explained_variance_ratio_)
            logger.info(f"PCA explained variance: {total_variance:.2%}")
        
        return reduced, reducer
    
    def _reduce_tsne(self,
                    embeddings: np.ndarray,
                    n_components: int,
                    perplexity: float = 30.0,
                    learning_rate: float = 200.0) -> Tuple[np.ndarray, TSNE]:
        """Reduce dimensions using t-SNE.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            perplexity: t-SNE perplexity parameter
            learning_rate: t-SNE learning rate
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        # For t-SNE, first reduce with PCA if dimensions > 50
        if embeddings.shape[1] > 50:
            logger.info("Pre-reducing with PCA for t-SNE")
            pca = PCA(n_components=50, random_state=42)
            embeddings = pca.fit_transform(embeddings)
        
        reducer = TSNE(
            n_components=n_components,
            perplexity=perplexity,
            learning_rate=learning_rate,
            random_state=42,
            verbose=1
        )
        
        reduced = reducer.fit_transform(embeddings)
        
        return reduced, reducer
    
    def transform_new_points(self,
                           embeddings: np.ndarray,
                           method: str = 'umap') -> np.ndarray:
        """Transform new points using existing reducer.
        
        Args:
            embeddings: New embeddings to transform
            method: Which reducer to use
            
        Returns:
            Transformed embeddings
        """
        if method not in self.reducers:
            raise ValueError(f"No {method} reducer available. Run reduce_dimensions first.")
        
        reducer = self.reducers[method]
        
        if method == 'tsne':
            logger.warning("t-SNE doesn't support transform. Returning original embeddings.")
            return embeddings
        
        return reducer.transform(embeddings)
    
    def create_embedding_map(self,
                           embeddings: np.ndarray,
                           labels: Optional[np.ndarray] = None,
                           skill_texts: Optional[list] = None) -> Dict[str, Any]:
        """Create a complete embedding map with 2D coordinates.
        
        Args:
            embeddings: Original embeddings
            labels: Cluster labels (optional)
            skill_texts: Skill names (optional)
            
        Returns:
            Dictionary with 2D coordinates and metadata
        """
        # Reduce to 2D
        coords_2d, metadata = self.reduce_dimensions(embeddings, method='umap', n_components=2)
        
        # Create map
        embedding_map = {
            'coordinates': coords_2d,
            'metadata': metadata
        }
        
        if labels is not None:
            embedding_map['labels'] = labels
        
        if skill_texts is not None:
            embedding_map['skills'] = skill_texts
        
        # Add statistics
        embedding_map['stats'] = {
            'x_range': (float(np.min(coords_2d[:, 0])), float(np.max(coords_2d[:, 0]))),
            'y_range': (float(np.min(coords_2d[:, 1])), float(np.max(coords_2d[:, 1]))),
            'center': (float(np.mean(coords_2d[:, 0])), float(np.mean(coords_2d[:, 1])))
        }
        
        return embedding_map

### src/analyzer/report_generator.py
```python
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import os
from pathlib import Path
from reportlab.lib import colors
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, PageBreak, Image
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from database.operations import DatabaseOperations

logger = logging.getLogger(__name__)

class ReportGenerator:
    """Generate PDF reports with analysis results."""
    
    def __init__(self, output_dir: str = None):
        self.output_dir = output_dir or "./outputs"
        os.makedirs(self.output_dir, exist_ok=True)
        self.db_ops = DatabaseOperations()
        self.styles = getSampleStyleSheet()
        self._add_custom_styles()
    
    def _add_custom_styles(self):
        """Add custom styles for the report."""
        self.styles.add(ParagraphStyle(
            name='CustomTitle',
            parent=self.styles['Title'],
            fontSize=24,
            textColor=colors.HexColor('#1a73e8'),
            spaceAfter=30
        ))
        
        self.styles.add(ParagraphStyle(
            name='SectionHeader',
            parent=self.styles['Heading1'],
            fontSize=16,
            textColor=colors.HexColor('#1a73e8'),
            spaceAfter=12
        ))
        
        self.styles.add(ParagraphStyle(
            name='SubsectionHeader',
            parent=self.styles['Heading2'],
            fontSize=14,
            textColor=colors.HexColor('#5f6368'),
            spaceAfter=10
        ))
    
    def generate_full_report(self, 
                           country: Optional[str] = None,
                           include_visualizations: bool = True) -> str:
        """Generate comprehensive analysis report.
        
        Args:
            country: Country code to filter by (optional)
            include_visualizations: Whether to include charts
            
        Returns:
            Path to generated report
        """
        # Create timestamp for filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        country_suffix = f"_{country}" if country else "_all"
        filename = f"labor_market_analysis{country_suffix}_{timestamp}.pdf"
        filepath = os.path.join(self.output_dir, filename)
        
        # Create document
        doc = SimpleDocTemplate(
            filepath,
            pagesize=A4,
            rightMargin=72,
            leftMargin=72,
            topMargin=72,
            bottomMargin=18
        )
        
        # Build content
        story = []
        
        # Title page
        story.extend(self._create_title_page(country))
        story.append(PageBreak())
        
        # Executive summary
        story.extend(self._create_executive_summary(country))
        story.append(PageBreak())
        
        # Skills analysis
        story.extend(self._create_skills_analysis(country, include_visualizations))
        story.append(PageBreak())
        
        # Clustering results
        story.extend(self._create_clustering_analysis(include_visualizations))
        story.append(PageBreak())
        
        # Temporal trends (if available)
        story.extend(self._create_temporal_analysis(country, include_visualizations))
        story.append(PageBreak())
        
        # Methodology
        story.extend(self._create_methodology_section())
        
        # Build PDF
        doc.build(story)
        
        logger.info(f"Report generated: {filepath}")
        return filepath
    
    def _create_title_page(self, country: Optional[str]) -> List:
        """Create title page elements."""
        elements = []
        
        # Title
        title_text = "Observatorio de Demanda Laboral Tecnológica"
        if country:
            country_names = {'CO': 'Colombia', 'MX': 'México', 'AR': 'Argentina'}
            title_text += f"\n{country_names.get(country, country)}"
        else:
            title_text += "\nAmérica Latina"
        
        elements.append(Paragraph(title_text, self.styles['CustomTitle']))
        elements.append(Spacer(1, 0.5*inch))
        
        # Subtitle
        subtitle = "Análisis Automatizado de Habilidades Técnicas"
        elements.append(Paragraph(subtitle, self.styles['Heading2']))
        elements.append(Spacer(1, 0.3*inch))
        
        # Date
        date_text = f"Fecha de generación: {datetime.now().strftime('%d de %B de %Y')}"
        elements.append(Paragraph(date_text, self.styles['Normal']))
        elements.append(Spacer(1, 2*inch))
        
        # Authors/Institution
        elements.append(Paragraph("Universidad XYZ", self.styles['Normal']))
        elements.append(Paragraph("Facultad de Ingeniería", self.styles['Normal']))
        
        return elements
    
    def _create_executive_summary(self, country: Optional[str]) -> List:
        """Create executive summary section."""
        elements = []
        
        elements.append(Paragraph("Resumen Ejecutivo", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get summary statistics
        stats = self.db_ops.get_skill_statistics(country)
        
        # Summary text
        summary_points = [
            f"Se analizaron un total de {stats.get('total_unique_skills', 0)} habilidades técnicas únicas.",
            f"Las 5 habilidades más demandadas son: {', '.join([s['skill'] for s in stats.get('top_skills', [])[:5]])}.",
            "El análisis revela una fuerte demanda de habilidades en desarrollo web, cloud computing y ciencia de datos.",
            "Se identificaron patrones emergentes en tecnologías de inteligencia artificial y DevOps."
        ]
        
        for point in summary_points:
            elements.append(Paragraph(f"• {point}", self.styles['Normal']))
            elements.append(Spacer(1, 0.1*inch))
        
        return elements
    
    def _create_skills_analysis(self, country: Optional[str], include_viz: bool) -> List:
        """Create skills analysis section."""
        elements = []
        
        elements.append(Paragraph("Análisis de Habilidades", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get skill statistics
        stats = self.db_ops.get_skill_statistics(country)
        
        # Top skills table
        elements.append(Paragraph("Top 20 Habilidades Más Demandadas", self.styles['SubsectionHeader']))
        
        if stats.get('top_skills'):
            # Create table data
            table_data = [['Posición', 'Habilidad', 'Frecuencia']]
            for i, skill_data in enumerate(stats['top_skills'][:20], 1):
                table_data.append([
                    str(i),
                    skill_data['skill'],
                    str(skill_data['count'])
                ])
            
            # Create table
            table = Table(table_data, colWidths=[1*inch, 3*inch, 1.5*inch])
            table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            
            elements.append(table)
            elements.append(Spacer(1, 0.3*inch))
        
        # Add visualization if requested
        if include_viz:
            viz_path = self._create_skill_frequency_chart(stats.get('top_skills', [])[:15])
            if viz_path:
                elements.append(Image(viz_path, width=6*inch, height=4*inch))
                elements.append(Spacer(1, 0.2*inch))
        
        return elements
    
    def _create_clustering_analysis(self, include_viz: bool) -> List:
        """Create clustering analysis section."""
        elements = []
        
        elements.append(Paragraph("Análisis de Clustering", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get latest clustering results
        # Note: This would need to be implemented in DatabaseOperations
        # For now, we'll use placeholder text
        
        elements.append(Paragraph(
            "El análisis de clustering identificó grupos coherentes de habilidades "
            "que típicamente aparecen juntas en las ofertas laborales:",
            self.styles['Normal']
        ))
        elements.append(Spacer(1, 0.1*inch))
        
        # Placeholder cluster descriptions
        clusters = [
            "Frontend Development: React, Vue.js, CSS, JavaScript, HTML5",
            "Backend Development: Node.js, Python, Django, Flask, API REST",
            "Data Science: Python, R, Machine Learning, SQL, Pandas",
            "DevOps: Docker, Kubernetes, AWS, CI/CD, Jenkins",
            "Mobile Development: React Native, Flutter, iOS, Android"
        ]
        
        for cluster in clusters:
            elements.append(Paragraph(f"• {cluster}", self.styles['Normal']))
            elements.append(Spacer(1, 0.05*inch))
        
        return elements
    
    def _create_temporal_analysis(self, country: Optional[str], include_viz: bool) -> List:
        """Create temporal trends analysis section."""
        elements = []
        
        elements.append(Paragraph("Tendencias Temporales", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        elements.append(Paragraph(
            "El análisis temporal permite identificar la evolución de la demanda "
            "de habilidades técnicas a lo largo del tiempo.",
            self.styles['Normal']
        ))
        
        # Placeholder for temporal analysis
        trends = [
            "Crecimiento sostenido en demanda de habilidades cloud (AWS, Azure)",
            "Aumento significativo en tecnologías de IA/ML en los últimos 6 meses",
            "Estabilidad en frameworks tradicionales (Spring, .NET)",
            "Emergencia de nuevas herramientas DevOps"
        ]
        
        elements.append(Spacer(1, 0.1*inch))
        for trend in trends:
            elements.append(Paragraph(f"• {trend}", self.styles['Normal']))
            elements.append(Spacer(1, 0.05*inch))
        
        return elements
    
    def _create_methodology_section(self) -> List:
        """Create methodology section."""
        elements = []
        
        elements.append(Paragraph("Metodología", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        methodology_text = """
        Este análisis se realizó mediante un pipeline automatizado que incluye:
        
        1. Web Scraping: Recolección automática de ofertas laborales de portales como 
           Computrabajo, Bumeran y elempleo.com.
        
        2. Extracción de Habilidades: Combinación de técnicas de NER (Named Entity Recognition) 
           y expresiones regulares para identificar menciones de habilidades técnicas.
        
        3. Enriquecimiento con LLM: Uso de modelos de lenguaje para identificar habilidades 
           implícitas y normalizar variaciones.
        
        4. Análisis Semántico: Generación de embeddings multilingües y clustering para 
           identificar grupos de habilidades relacionadas.
        
        5. Visualización: Generación de reportes estáticos con métricas agregadas y 
           visualizaciones interpretables.
        """
        
        elements.append(Paragraph(methodology_text, self.styles['Normal']))
        
        return elements
    
    def _create_skill_frequency_chart(self, top_skills: List[Dict[str, Any]]) -> Optional[str]:
        """Create skill frequency bar chart.
        
        Args:
            top_skills: List of top skills with counts
            
        Returns:
            Path to saved chart image
        """
        if not top_skills:
            return None
        
        try:
            # Prepare data
            skills = [s['skill'] for s in top_skills]
            counts = [s['count'] for s in top_skills]
            
            # Create figure
            plt.figure(figsize=(10, 6))
            
            # Create horizontal bar chart
            bars = plt.barh(skills, counts, color='#1a73e8')
            
            # Customize
            plt.xlabel('Número de Vacantes', fontsize=12)
            plt.title('Habilidades Más Demandadas', fontsize=14, fontweight='bold')
            plt.gca().invert_yaxis()  # Highest on top
            
            # Add value labels
            for i, bar in enumerate(bars):
                width = bar.get_width()
                plt.text(width + 1, bar.get_y() + bar.get_height()/2, 
                        f'{counts[i]}', 
                        ha='left', va='center')
            
            plt.tight_layout()
            
            # Save
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(self.output_dir, f"skill_frequency_{timestamp}.png")
            plt.savefig(filepath, dpi=300, bbox_inches='tight')
            plt.close()
            
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating skill frequency chart: {e}")
            return None
    
    def enrich_extracted_skills(self, extracted_skills: List[Dict]) -> List[Dict]:
        """Enrich extracted skills with ESCO matches.
        
        Args:
            extracted_skills: List of extracted skill dictionaries
            
        Returns:
            Enriched skill list
        """
        # Get unique skill texts
        unique_skills = list(set(skill['skill_text'] for skill in extracted_skills))
        
        # Match all unique skills
        esco_matches = self.match_skills_batch(unique_skills)
        
        # Enrich original skills
        enriched = []
        for skill in extracted_skills:
            enriched_skill = skill.copy()
            
            match = esco_matches.get(skill['skill_text'])
            if match:
                enriched_skill.update({
                    'esco_uri': match['esco_uri'],
                    'esco_preferred_label': match['esco_preferred_label'],
                    'esco_match_type': match['match_type'],
                    'esco_match_score': match['match_score']
                })
            
            enriched.append(enriched_skill)
        
        return enriched
```

### src/extractor/pipeline.py
```python
import logging
from typing import List, Dict, Optional
from database.operations import DatabaseOperations
from .ner_extractor import NERExtractor
from .regex_patterns import RegexExtractor
from .esco_matcher import ESCOMatcher
from config.settings import get_settings
import time

logger = logging.getLogger(__name__)

class ExtractionPipeline:
    """Main pipeline for skill extraction from job postings."""
    
    def __init__(self):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        
        # Initialize extractors
        logger.info("Initializing extraction components...")
        self.ner_extractor = NERExtractor()
        self.regex_extractor = RegexExtractor()
        self.esco_matcher = ESCOMatcher()
        
        logger.info("Extraction pipeline initialized")
    
    def process_batch(self, batch_size: int = 100) -> Dict[str, any]:
        """Process a batch of unprocessed jobs.
        
        Args:
            batch_size: Number of jobs to process
            
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'jobs_processed': 0,
            'skills_extracted': 0,
            'esco_matches': 0,
            'errors': 0
        }
        
        try:
            # Get unprocessed jobs
            jobs = self.db_ops.get_unprocessed_jobs(limit=batch_size)
            logger.info(f"Processing {len(jobs)} jobs")
            
            for job in jobs:
                try:
                    # Process individual job
                    skills = self.process_job(job)
                    
                    if skills:
                        # Save extracted skills
                        self.db_ops.insert_extracted_skills(
                            str(job.job_id),
                            skills
                        )
                        
                        # Mark job as processed
                        self.db_ops.mark_job_processed(str(job.job_id))
                        
                        # Update stats
                        stats['jobs_processed'] += 1
                        stats['skills_extracted'] += len(skills)
                        stats['esco_matches'] += sum(
                            1 for s in skills if s.get('esco_uri')
                        )
                    
                except Exception as e:
                    logger.error(f"Error processing job {job.job_id}: {e}")
                    stats['errors'] += 1
            
            # Calculate processing time
            stats['processing_time'] = time.time() - start_time
            stats['jobs_per_second'] = stats['jobs_processed'] / stats['processing_time'] if stats['processing_time'] > 0 else 0
            
            logger.info(
                f"Batch complete: {stats['jobs_processed']} jobs, "
                f"{stats['skills_extracted']} skills extracted, "
                f"{stats['esco_matches']} ESCO matches, "
                f"{stats['errors']} errors, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"Batch processing failed: {e}")
            raise
    
    def process_job(self, job) -> List[Dict[str, any]]:
        """Process a single job to extract skills.
        
        Args:
            job: Job object from database
            
        Returns:
            List of extracted and enriched skills
        """
        # Prepare job data
        job_data = {
            'title': job.title,
            'description': job.description,
            'requirements': job.requirements
        }
        
        # Extract skills using NER
        ner_skills = self.ner_extractor.extract_from_job(job_data)
        
        # Extract skills using regex
        regex_skills = self.regex_extractor.extract_from_job(job_data)
        
        # Combine and deduplicate
        all_skills = self._combine_skills(ner_skills, regex_skills)
        
        # Enrich with ESCO matches
        enriched_skills = self.esco_matcher.enrich_extracted_skills(all_skills)
        
        logger.debug(
            f"Job {job.job_id}: {len(ner_skills)} NER skills, "
            f"{len(regex_skills)} regex skills, "
            f"{len(enriched_skills)} total after deduplication"
        )
        
        return enriched_skills
    
    def _combine_skills(self, ner_skills: List[Dict], regex_skills: List[Dict]) -> List[Dict]:
        """Combine and deduplicate skills from different extractors.
        
        Args:
            ner_skills: Skills from NER
            regex_skills: Skills from regex
            
        Returns:
            Combined and deduplicated skill list
        """
        # Use skill text and source section as unique key
        seen_skills = {}
        
        # Process NER skills first (higher confidence)
        for skill in ner_skills:
            key = (skill['skill_text'], skill['source_section'])
            
            if key not in seen_skills:
                seen_skills[key] = skill
            else:
                # Update confidence if higher
                if skill['confidence_score'] > seen_skills[key]['confidence_score']:
                    seen_skills[key] = skill
        
        # Process regex skills
        for skill in regex_skills:
            key = (skill['skill_text'], skill['source_section'])
            
            if key not in seen_skills:
                seen_skills[key] = skill
            else:
                # Merge extraction methods
                existing = seen_skills[key]
                if existing['extraction_method'] != skill['extraction_method']:
                    existing['extraction_method'] = 'ner+regex'
                    existing['confidence_score'] = min(0.95, existing['confidence_score'] + 0.1)
        
        return list(seen_skills.values())
    
    def run_continuous(self, batch_size: int = 100, wait_time: int = 60):
        """Run extraction continuously.
        
        Args:
            batch_size: Jobs per batch
            wait_time: Seconds to wait between batches
        """
        logger.info(f"Starting continuous extraction (batch_size={batch_size}, wait={wait_time}s)")
        
        while True:
            try:
                stats = self.process_batch(batch_size)
                
                if stats['jobs_processed'] == 0:
                    logger.info(f"No jobs to process. Waiting {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    # Short pause between batches
                    time.sleep(5)
                    
            except KeyboardInterrupt:
                logger.info("Extraction stopped by user")
                break
            except Exception as e:
                logger.error(f"Extraction error: {e}")
                logger.info(f"Waiting {wait_time}s before retry...")
                time.sleep(wait_time)
```

---

## 6. LLM Processor Module Files

### src/llm_processor/__init__.py
```python
from .llm_handler import LLMHandler
from .prompts import PromptGenerator
from .esco_normalizer import ESCONormalizer
from .validator import SkillValidator

__all__ = ['LLMHandler', 'PromptGenerator', 'ESCONormalizer', 'SkillValidator']
```

### src/llm_processor/llm_handler.py
```python
import logging
from typing import List, Dict, Optional, Any
from llama_cpp import Llama
import openai
from config.settings import get_settings
import json
import time

logger = logging.getLogger(__name__)

class LLMHandler:
    """Handle LLM interactions for skill processing."""
    
    def __init__(self, model_type: str = "local"):
        self.settings = get_settings()
        self.model_type = model_type
        
        if model_type == "local":
            self._init_local_model()
        elif model_type == "openai":
            self._init_openai()
        else:
            raise ValueError(f"Unknown model type: {model_type}")
    
    def _init_local_model(self):
        """Initialize local LLaMA/Mistral model."""
        try:
            logger.info(f"Loading local model from {self.settings.llm_model_path}")
            
            self.model = Llama(
                model_path=self.settings.llm_model_path,
                n_ctx=self.settings.llm_context_length,
                n_gpu_layers=self.settings.llm_n_gpu_layers,
                verbose=False
            )
            
            logger.info("Local model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load local model: {e}")
            raise
    
    def _init_openai(self):
        """Initialize OpenAI API client."""
        if not self.settings.openai_api_key:
            raise ValueError("OpenAI API key not configured")
        
        openai.api_key = self.settings.openai_api_key
        self.model_name = self.settings.openai_model
        logger.info(f"OpenAI API initialized with model {self.model_name}")
    
    def process_skills(self, 
                      job_data: Dict[str, Any],
                      extracted_skills: List[Dict[str, Any]],
                      prompt_template: str) -> Dict[str, Any]:
        """Process skills using LLM.
        
        Args:
            job_data: Job information
            extracted_skills: Initially extracted skills
            prompt_template: Formatted prompt template
            
        Returns:
            LLM response with processed skills
        """
        start_time = time.time()
        
        try:
            if self.model_type == "local":
                response = self._process_local(prompt_template)
            else:
                response = self._process_openai(prompt_template)
            
            # Parse response
            result = self._parse_response(response)
            
            # Add metadata
            result['processing_time'] = time.time() - start_time
            result['model_type'] = self.model_type
            result['model_name'] = getattr(self, 'model_name', 'local_mistral')
            
            return result
            
        except Exception as e:
            logger.error(f"LLM processing failed: {e}")
            raise
    
    def _process_local(self, prompt: str) -> str:
        """Process using local model.
        
        Args:
            prompt: Complete prompt
            
        Returns:
            Model response
        """
        response = self.model(
            prompt,
            max_tokens=self.settings.llm_max_tokens,
            temperature=self.settings.llm_temperature,
            stop=["</response>", "\n\n\n"],
            echo=False
        )
        
        return response['choices'][0]['text']
    
    def _process_openai(self, prompt: str) -> str:
        """Process using OpenAI API.
        
        Args:
            prompt: Complete prompt
            
        Returns:
            Model response
        """
        response = openai.ChatCompletion.create(
            model=self.model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert in analyzing job postings and extracting technical skills. Respond in Spanish."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            max_tokens=self.settings.llm_max_tokens,
            temperature=self.settings.llm_temperature
        )
        
        return response.choices[0].message.content
    
    def _parse_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response.
        
        Args:
            response: Raw LLM response
            
        Returns:
            Parsed skills data
        """
        # Try to extract JSON if present
        if "```json" in response:
            # Extract JSON block
            start = response.find("```json") + 7
            end = response.find("```", start)
            json_str = response[start:end].strip()
            
            try:
                return json.loads(json_str)
            except json.JSONDecodeError:
                logger.warning("Failed to parse JSON from response")
        
        # Fallback: Parse structured text response
        result = {
            "explicit_skills": [],
            "implicit_skills": [],
            "normalized_skills": [],
            "deduplicated_skills": []
        }
        
        lines = response.strip().split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            
            # Detect sections
            if "habilidades explícitas" in line.lower():
                current_section = "explicit_skills"
            elif "habilidades implícitas" in line.lower():
                current_section = "implicit_skills"
            elif "habilidades normalizadas" in line.lower():
                current_section = "normalized_skills"
            elif "deduplicadas" in line.lower():
                current_section = "deduplicated_skills"
            elif line and current_section and (line.startswith('-') or line.startswith('*')):
                # Extract skill from bullet point
                skill_text = line.lstrip('-*').strip()
                
                # Parse skill with reasoning if present
                if ':' in skill_text:
                    skill, reasoning = skill_text.split(':', 1)
                    result[current_section].append({
                        "skill": skill.strip(),
                        "reasoning": reasoning.strip()
                    })
                else:
                    result[current_section].append({
                        "skill": skill_text
                    })
        
        return result
```

### src/llm_processor/prompts.py
```python
from typing import List, Dict, Any
import json

class PromptGenerator:
    """Generate prompts for LLM skill processing."""
    
    def __init__(self):
        self.templates = self._load_templates()
    
    def _load_templates(self) -> Dict[str, str]:
        """Load prompt templates."""
        return {
            "skill_processing": """Eres un experto en análisis de ofertas laborales tecnológicas en América Latina.

Datos de la vacante:
- Título: {job_title}
- Descripción: {job_description}
- Requisitos: {job_requirements}

Habilidades extraídas inicialmente:
{extracted_skills}

Por favor, realiza las siguientes tareas:

1. **Validación de habilidades explícitas**: Revisa las habilidades extraídas y confirma cuáles son realmente habilidades técnicas relevantes.

2. **Detección de habilidades implícitas**: Basándote en el contexto del puesto, identifica habilidades técnicas que serían necesarias pero no están mencionadas explícitamente. Por ejemplo:
   - Si menciona "desarrollo web full stack" → probablemente necesite Git, bases de datos, APIs REST
   - Si menciona "análisis de datos" → probablemente necesite SQL, Python/R, visualización
   - Si menciona "DevOps" → probablemente necesite CI/CD, contenedores, cloud

3. **Normalización con ESCO**: Para cada habilidad, proporciona la forma normalizada según estándares internacionales:
   - Usa nombres estándar (ej: "JS" → "JavaScript", "React.js" → "React")
   - Mantén el español cuando sea apropiado
   - Agrupa variaciones (ej: "MySQL/MariaDB" → "MySQL")

4. **Deduplicación**: Elimina habilidades duplicadas o redundantes:
   - Combina variaciones del mismo concepto
   - Elimina términos demasiado genéricos
   - Mantén el término más específico cuando haya jerarquía

Responde en el siguiente formato JSON:
```json
{{
  "explicit_skills": [
    {{"skill": "nombre", "confidence": 0.9, "original": "texto_original"}}
  ],
  "implicit_skills": [
    {{"skill": "nombre", "confidence": 0.7, "reasoning": "justificación"}}
  ],
  "normalized_skills": [
    {{"original": "skill_original", "normalized": "skill_normalizado", "esco_match": "posible_uri"}}
  ],
  "deduplicated_skills": [
    {{"skill": "nombre_final", "type": "explicit|implicit", "category": "programming|database|framework|tool|soft_skill"}}
  ]
}}
```""",

            "simple_inference": """Analiza esta oferta de trabajo y extrae SOLO las habilidades técnicas implícitas que no están mencionadas pero serían necesarias.

Título: {job_title}
Descripción resumida: {job_summary}
Habilidades ya identificadas: {known_skills}

Lista únicamente las habilidades técnicas implícitas con su justificación:
""",

            "normalization": """Normaliza las siguientes habilidades técnicas según estándares internacionales y la taxonomía ESCO:

Habilidades a normalizar:
{skills_list}

Para cada habilidad, proporciona:
- Forma normalizada
- Categoría (lenguaje/framework/base de datos/herramienta/metodología)
- Término ESCO equivalente si existe

Responde en formato de lista:
""",

            "deduplication": """Elimina duplicados y agrupa las siguientes habilidades:

Habilidades:
{skills_list}

Reglas:
- Combina variaciones del mismo concepto (ej: JS, JavaScript, javascript → JavaScript)
- Mantén el término más específico cuando haya jerarquía
- Elimina términos genéricos si hay específicos

Lista final sin duplicados:
"""
        }
    
    def generate_skill_processing_prompt(self, 
                                       job_data: Dict[str, Any],
                                       extracted_skills: List[Dict[str, Any]]) -> str:
        """Generate prompt for complete skill processing.
        
        Args:
            job_data: Job information
            extracted_skills: Initially extracted skills
            
        Returns:
            Formatted prompt
        """
        # Format extracted skills
        skills_text = self._format_extracted_skills(extracted_skills)
        
        prompt = self.templates["skill_processing"].format(
            job_title=job_data.get('title', 'No especificado'),
            job_description=job_data.get('description', 'No especificado'),
            job_requirements=job_data.get('requirements', 'No especificado'),
            extracted_skills=skills_text
        )
        
        return prompt
    
    def generate_inference_prompt(self,
                                job_title: str,
                                job_summary: str,
                                known_skills: List[str]) -> str:
        """Generate prompt for implicit skill inference.
        
        Args:
            job_title: Job title
            job_summary: Brief job description
            known_skills: Already identified skills
            
        Returns:
            Formatted prompt
        """
        skills_list = ", ".join(known_skills) if known_skills else "Ninguna"
        
        return self.templates["simple_inference"].format(
            job_title=job_title,
            job_summary=job_summary,
            known_skills=skills_list
        )
    
    def generate_normalization_prompt(self, skills: List[str]) -> str:
        """Generate prompt for skill normalization.
        
        Args:
            skills: List of skills to normalize
            
        Returns:
            Formatted prompt
        """
        skills_text = "\n".join(f"- {skill}" for skill in skills)
        
        return self.templates["normalization"].format(
            skills_list=skills_text
        )
    
    def generate_deduplication_prompt(self, skills: List[str]) -> str:
        """Generate prompt for skill deduplication.
        
        Args:
            skills: List of skills to deduplicate
            
        Returns:
            Formatted prompt
        """
        skills_text = "\n".join(f"- {skill}" for skill in skills)
        
        return self.templates["deduplication"].format(
            skills_list=skills_text
        )
    
    def _format_extracted_skills(self, skills: List[Dict[str, Any]]) -> str:
        """Format extracted skills for prompt.
        
        Args:
            skills: List of skill dictionaries
            
        Returns:
            Formatted text
        """
        formatted_skills = []
        
        # Group by source section
        by_section = {}
        for skill in skills:
            section = skill.get('source_section', 'unknown')
            if section not in by_section:
                by_section[section] = []
            by_section[section].append(skill)
        
        # Format each section
        for section, section_skills in by_section.items():
            formatted_skills.append(f"\nDe {section}:")
            for skill in section_skills:
                method = skill.get('extraction_method', 'unknown')
                confidence = skill.get('confidence_score', 0)
                text = skill.get('skill_text', '')
                
                formatted_skills.append(
                    f"  - {text} (método: {method}, confianza: {confidence:.2f})"
                )
        
        return "\n".join(formatted_skills)

### src/llm_processor/esco_normalizer.py
```python
import logging
from typing import List, Dict, Any, Optional
from fuzzywuzzy import fuzz
import yaml

logger = logging.getLogger(__name__)

class ESCONormalizer:
    """Normalize skills using ESCO taxonomy with LLM assistance."""
    
    def __init__(self, config_path: str = "config/esco_config.yaml"):
        self.config = self._load_config(config_path)
        self.normalization_rules = self._build_normalization_rules()
    
    def _load_config(self, config_path: str) -> dict:
        """Load ESCO configuration."""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config['esco']
        except Exception as e:
            logger.error(f"Failed to load ESCO config: {e}")
            return {}
    
    def _build_normalization_rules(self) -> Dict[str, str]:
        """Build normalization rules from config and common variations."""
        rules = {}
        
        # Load from config
        if 'tech_mappings' in self.config:
            rules.update(self.config['tech_mappings'])
        
        # Add common variations
        common_variations = {
            # JavaScript variations
            'js': 'JavaScript',
            'javascript': 'JavaScript',
            'java script': 'JavaScript',
            
            # Python variations
            'python3': 'Python',
            'python 3': 'Python',
            'python2': 'Python',
            
            # React variations
            'reactjs': 'React',
            'react.js': 'React',
            'react js': 'React',
            'react native': 'React Native',
            
            # Node variations
            'nodejs': 'Node.js',
            'node js': 'Node.js',
            'node': 'Node.js',
            
            # Database variations
            'postgres': 'PostgreSQL',
            'mysql': 'MySQL',
            'maria db': 'MariaDB',
            'mariadb': 'MariaDB',
            'mongo': 'MongoDB',
            'mongo db': 'MongoDB',
            
            # .NET variations
            'dotnet': '.NET',
            'dot net': '.NET',
            '.net core': '.NET Core',
            'asp.net': 'ASP.NET',
            
            # Other common variations
            'c++': 'C++',
            'c#': 'C#',
            'c sharp': 'C#',
            'objective c': 'Objective-C',
            'obj-c': 'Objective-C',
            
            # Spanish to English mappings
            'base de datos': 'Database',
            'desarrollo web': 'Web Development',
            'desarrollo móvil': 'Mobile Development',
            'aprendizaje automático': 'Machine Learning',
            'inteligencia artificial': 'Artificial Intelligence',
            'ciencia de datos': 'Data Science',
            'computación en la nube': 'Cloud Computing',
            'control de versiones': 'Version Control',
        }
        
        # Convert all keys to lowercase
        for key, value in common_variations.items():
            rules[key.lower()] = value
        
        return rules
    
    def normalize_skill(self, skill: str) -> Dict[str, Any]:
        """Normalize a single skill.
        
        Args:
            skill: Raw skill text
            
        Returns:
            Normalized skill data
        """
        skill_lower = skill.lower().strip()
        
        # Direct mapping
        if skill_lower in self.normalization_rules:
            return {
                'original': skill,
                'normalized': self.normalization_rules[skill_lower],
                'method': 'direct_mapping',
                'confidence': 1.0
            }
        
        # Try fuzzy matching
        best_match = None
        best_score = 0
        
        for pattern, normalized in self.normalization_rules.items():
            score = fuzz.ratio(skill_lower, pattern)
            if score > best_score and score >= 85:
                best_score = score
                best_match = normalized
        
        if best_match:
            return {
                'original': skill,
                'normalized': best_match,
                'method': 'fuzzy_mapping',
                'confidence': best_score / 100.0
            }
        
        # Category-based normalization
        normalized = self._category_normalization(skill)
        if normalized != skill:
            return {
                'original': skill,
                'normalized': normalized,
                'method': 'category_rules',
                'confidence': 0.8
            }
        
        # No normalization found
        return {
            'original': skill,
            'normalized': skill,
            'method': 'no_normalization',
            'confidence': 0.5
        }
    
    def _category_normalization(self, skill: str) -> str:
        """Apply category-based normalization rules.
        
        Args:
            skill: Skill to normalize
            
        Returns:
            Normalized skill
        """
        skill_lower = skill.lower()
        
        # Framework detection
        if 'framework' in skill_lower:
            skill = skill.replace('framework', '').replace('Framework', '').strip()
        
        # Version removal for certain technologies
        version_patterns = [
            (r'python\s*\d+\.?\d*', 'Python'),
            (r'java\s*\d+', 'Java'),
            (r'angular\s*\d+', 'Angular'),
            (r'vue\s*\d+', 'Vue.js'),
            (r'react\s*\d+', 'React'),
        ]
        
        import re
        for pattern, replacement in version_patterns:
            if re.search(pattern, skill_lower):
                return replacement
        
        # Capitalize properly
        # Special cases
        special_cases = {
            'mysql': 'MySQL',
            'postgresql': 'PostgreSQL',
            'mongodb': 'MongoDB',
            'javascript': 'JavaScript',
            'typescript': 'TypeScript',
            'graphql': 'GraphQL',
            'nodejs': 'Node.js',
            'reactjs': 'React',
            'vuejs': 'Vue.js',
        }
        
        if skill_lower in special_cases:
            return special_cases[skill_lower]
        
        # Default: capitalize first letter of each word
        return ' '.join(word.capitalize() for word in skill.split())
    
    def normalize_skills_batch(self, skills: List[str]) -> List[Dict[str, Any]]:
        """Normalize multiple skills.
        
        Args:
            skills: List of skills to normalize
            
        Returns:
            List of normalized skill data
        """
        normalized = []
        
        for skill in skills:
            result = self.normalize_skill(skill)
            normalized.append(result)
        
        return normalized
    
    def deduplicate_normalized_skills(self, normalized_skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Deduplicate normalized skills.
        
        Args:
            normalized_skills: List of normalized skill dictionaries
            
        Returns:
            Deduplicated list
        """
        # Group by normalized form
        skill_groups = {}
        
        for skill_data in normalized_skills:
            normalized = skill_data['normalized']
            
            if normalized not in skill_groups:
                skill_groups[normalized] = {
                    'normalized': normalized,
                    'originals': [],
                    'best_confidence': 0,
                    'methods': set()
                }
            
            skill_groups[normalized]['originals'].append(skill_data['original'])
            skill_groups[normalized]['methods'].add(skill_data['method'])
            skill_groups[normalized]['best_confidence'] = max(
                skill_groups[normalized]['best_confidence'],
                skill_data['confidence']
            )
        
        # Convert back to list
        deduplicated = []
        for normalized, group_data in skill_groups.items():
            deduplicated.append({
                'normalized': normalized,
                'original_variations': group_data['originals'],
                'confidence': group_data['best_confidence'],
                'methods': list(group_data['methods'])
            })
        
        return deduplicated

### src/llm_processor/validator.py
```python
import logging
from typing import List, Dict, Any, Set
import re

logger = logging.getLogger(__name__)

class SkillValidator:
    """Validate and filter skills."""
    
    def __init__(self):
        self.blacklist = self._build_blacklist()
        self.whitelist = self._build_whitelist()
        self.categories = self._build_categories()
    
    def _build_blacklist(self) -> Set[str]:
        """Build blacklist of non-skill terms."""
        return {
            # Generic terms
            'experiencia', 'años', 'año', 'conocimiento', 'conocimientos',
            'habilidad', 'habilidades', 'capacidad', 'competencia',
            'desarrollo', 'trabajo', 'empresa', 'cliente', 'proyecto',
            'equipo', 'persona', 'profesional', 'área', 'proceso',
            
            # Too generic tech terms
            'tecnología', 'tecnologías', 'sistema', 'sistemas',
            'informática', 'computación', 'software', 'hardware',
            'programación', 'desarrollo de software',
            
            # Common words
            'bueno', 'excelente', 'alto', 'nivel', 'manejo',
            'uso', 'gestión', 'administración', 'análisis',
            
            # Methodologies too generic
            'metodología', 'metodologías', 'mejores prácticas',
            
            # Soft skills (we focus on technical)
            'comunicación', 'liderazgo', 'trabajo en equipo',
            'responsabilidad', 'proactividad', 'creatividad'
        }
    
    def _build_whitelist(self) -> Set[str]:
        """Build whitelist of known valid skills."""
        return {
            # Programming languages
            'python', 'java', 'javascript', 'typescript', 'c++', 'c#',
            'php', 'ruby', 'go', 'golang', 'rust', 'kotlin', 'swift',
            'objective-c', 'r', 'scala', 'perl', 'matlab', 'julia',
            
            # Frameworks and libraries
            'react', 'angular', 'vue', 'django', 'flask', 'spring',
            'express', 'laravel', 'rails', 'fastapi', 'nextjs',
            '.net', 'asp.net', 'tensorflow', 'pytorch', 'keras',
            
            # Databases
            'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',
            'cassandra', 'dynamodb', 'oracle', 'sql server', 'sqlite',
            'neo4j', 'couchdb', 'firebase', 'supabase',
            
            # Cloud and DevOps
            'aws', 'azure', 'gcp', 'google cloud', 'docker', 'kubernetes',
            'jenkins', 'terraform', 'ansible', 'puppet', 'chef',
            'gitlab', 'github', 'bitbucket', 'circleci', 'travis',
            
            # Tools and platforms
            'git', 'jira', 'confluence', 'slack', 'linux', 'windows',
            'macos', 'ubuntu', 'centos', 'debian', 'nginx', 'apache',
            'grafana', 'prometheus', 'elasticsearch', 'kibana',
            
            # Data and ML
            'machine learning', 'deep learning', 'data science',
            'big data', 'spark', 'hadoop', 'kafka', 'airflow',
            'pandas', 'numpy', 'scikit-learn', 'matplotlib',
            
            # Mobile
            'android', 'ios', 'react native', 'flutter', 'xamarin',
            'swift', 'kotlin', 'objective-c', 'cordova', 'ionic',
            
            # Other
            'api', 'rest', 'graphql', 'websocket', 'microservices',
            'ci/cd', 'agile', 'scrum', 'kanban', 'tdd', 'bdd'
        }
    
    def _build_categories(self) -> Dict[str, Set[str]]:
        """Build skill categories."""
        return {
            'programming_language': {
                'python', 'java', 'javascript', 'typescript', 'c++', 'c#',
                'php', 'ruby', 'go', 'rust', 'kotlin', 'swift', 'r'
            },
            'framework': {
                'react', 'angular', 'vue', 'django', 'flask', 'spring',
                'express', 'laravel', 'rails', '.net', 'nextjs'
            },
            'database': {
                'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',
                'cassandra', 'dynamodb', 'oracle', 'sql server'
            },
            'cloud_platform': {
                'aws', 'azure', 'gcp', 'google cloud', 'heroku', 'digitalocean'
            },
            'devops_tool': {
                'docker', 'kubernetes', 'jenkins', 'terraform', 'ansible',
                'git', 'github', 'gitlab', 'ci/cd'
            },
            'data_ml': {
                'machine learning', 'deep learning', 'tensorflow', 'pytorch',
                'pandas', 'numpy', 'spark', 'hadoop'
            },
            'mobile': {
                'android', 'ios', 'react native', 'flutter', 'xamarin'
            },
            'methodology': {
                'agile', 'scrum', 'kanban', 'devops', 'tdd', 'bdd'
            }
        }
    
    def validate_skill(self, skill: str) -> Dict[str, Any]:
        """Validate a single skill.
        
        Args:
            skill: Skill to validate
            
        Returns:
            Validation result
        """
        skill_lower = skill.lower().strip()
        
        # Check blacklist
        if skill_lower in self.blacklist:
            return {
                'skill': skill,
                'valid': False,
                'reason': 'blacklisted',
                'confidence': 0.0
            }
        
        # Check whitelist
        if skill_lower in self.whitelist:
            category = self._categorize_skill(skill_lower)
            return {
                'skill': skill,
                'valid': True,
                'reason': 'whitelisted',
                'category': category,
                'confidence': 1.0
            }
        
        # Length check
        if len(skill_lower) < 2 or len(skill_lower) > 50:
            return {
                'skill': skill,
                'valid': False,
                'reason': 'invalid_length',
                'confidence': 0.0
            }
        
        # Pattern validation
        if not self._validate_pattern(skill_lower):
            return {
                'skill': skill,
                'valid': False,
                'reason': 'invalid_pattern',
                'confidence': 0.0
            }
        
        # Partial matches in whitelist
        for valid_skill in self.whitelist:
            if valid_skill in skill_lower or skill_lower in valid_skill:
                category = self._categorize_skill(valid_skill)
                return {
                    'skill': skill,
                    'valid': True,
                    'reason': 'partial_match',
                    'category': category,
                    'confidence': 0.7
                }
        
        # If not in whitelist but passes other checks
        return {
            'skill': skill,
            'valid': True,
            'reason': 'pattern_valid',
            'category': 'uncategorized',
            'confidence': 0.5
        }
    
    def _validate_pattern(self, skill: str) -> bool:
        """Validate skill pattern.
        
        Args:
            skill: Skill to validate
            
        Returns:
            True if pattern is valid
        """
        # Must contain at least one letter
        if not re.search(r'[a-zA-Z]', skill):
            return False
        
        # Check for suspicious patterns
        suspicious_patterns = [
            r'^\d+,  # Only numbers
            r'^[^a-zA-Z0-9\s\.\+\#\-/]+,  # Only special chars
            r'\b(años?|experiencia|conocimientos?)\b',  # Common non-skills
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, skill, re.IGNORECASE):
                return False
        
        return True
    
    def _categorize_skill(self, skill: str) -> str:
        """Categorize a skill.
        
        Args:
            skill: Skill to categorize
            
        Returns:
            Category name
        """
        skill_lower = skill.lower()
        
        for category, skills in self.categories.items():
            if skill_lower in skills:
                return category
        
        # Try partial matches
        for category, skills in self.categories.items():
            for cat_skill in skills:
                if cat_skill in skill_lower or skill_lower in cat_skill:
                    return category
        
        return 'uncategorized'
    
    def validate_skills_batch(self, skills: List[str]) -> List[Dict[str, Any]]:
        """Validate multiple skills.
        
        Args:
            skills: List of skills to validate
            
        Returns:
            List of validation results
        """
        results = []
        
        for skill in skills:
            result = self.validate_skill(skill)
            results.append(result)
        
        # Log statistics
        valid_count = sum(1 for r in results if r['valid'])
        logger.info(
            f"Skill validation: {valid_count}/{len(skills)} valid "
            f"({valid_count/len(skills)*100:.1f}%)"
        )
        
        return results
    
    def filter_valid_skills(self, skills: List[str], min_confidence: float = 0.5) -> List[str]:
        """Filter only valid skills.
        
        Args:
            skills: List of skills
            min_confidence: Minimum confidence threshold
            
        Returns:
            List of valid skills
        """
        results = self.validate_skills_batch(skills)
        
        valid_skills = [
            r['skill'] for r in results
            if r['valid'] and r['confidence'] >= min_confidence
        ]
        
        return valid_skills

### src/llm_processor/pipeline.py
```python
import logging
from typing import List, Dict, Any
from database.operations import DatabaseOperations
from .llm_handler import LLMHandler
from .prompts import PromptGenerator
from .esco_normalizer import ESCONormalizer
from .validator import SkillValidator
from config.settings import get_settings
import time

logger = logging.getLogger(__name__)

class LLMProcessingPipeline:
    """Main pipeline for LLM-based skill processing."""
    
    def __init__(self, model_type: str = "local"):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        
        logger.info("Initializing LLM processing components...")
        self.llm_handler = LLMHandler(model_type)
        self.prompt_generator = PromptGenerator()
        self.normalizer = ESCONormalizer()
        self.validator = SkillValidator()
        
        logger.info("LLM processing pipeline initialized")
    
    def process_batch(self, batch_size: int = 50) -> Dict[str, Any]:
        """Process a batch of jobs with extracted skills.
        
        Args:
            batch_size: Number of jobs to process
            
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'jobs_processed': 0,
            'skills_enhanced': 0,
            'implicit_skills_found': 0,
            'skills_normalized': 0,
            'errors': 0
        }
        
        try:
            # Get jobs with extracted skills needing LLM processing
            jobs_data = self.db_ops.get_extracted_skills_for_processing(limit=batch_size)
            logger.info(f"Processing {len(jobs_data)} jobs with LLM")
            
            for job_data in jobs_data:
                try:
                    # Process individual job
                    enhanced_skills = self.process_job(job_data)
                    
                    if enhanced_skills:
                        # Save enhanced skills
                        self.db_ops.insert_enhanced_skills(
                            job_data['job_id'],
                            enhanced_skills
                        )
                        
                        # Update statistics
                        stats['jobs_processed'] += 1
                        stats['skills_enhanced'] += len(enhanced_skills)
                        stats['implicit_skills_found'] += sum(
                            1 for s in enhanced_skills 
                            if s.get('skill_type') == 'implicit'
                        )
                        stats['skills_normalized'] += sum(
                            1 for s in enhanced_skills
                            if s.get('normalized_skill') != s.get('original_skill_text')
                        )
                    
                except Exception as e:
                    logger.error(f"Error processing job {job_data['job_id']}: {e}")
                    stats['errors'] += 1
            
            # Calculate processing time
            stats['processing_time'] = time.time() - start_time
            stats['jobs_per_minute'] = (stats['jobs_processed'] / stats['processing_time']) * 60 if stats['processing_time'] > 0 else 0
            
            logger.info(
                f"LLM batch complete: {stats['jobs_processed']} jobs, "
                f"{stats['skills_enhanced']} skills enhanced, "
                f"{stats['implicit_skills_found']} implicit skills found, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"LLM batch processing failed: {e}")
            raise
    
    def process_job(self, job_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Process a single job with LLM.
        
        Args:
            job_data: Job data with extracted skills
            
        Returns:
            List of enhanced skills
        """
        # Generate prompt
        prompt = self.prompt_generator.generate_skill_processing_prompt(
            {
                'title': job_data['job_title'],
                'description': job_data['job_description'],
                'requirements': job_data['job_requirements']
            },
            job_data['extracted_skills']
        )
        
        # Process with LLM
        llm_response = self.llm_handler.process_skills(
            job_data,
            job_data['extracted_skills'],
            prompt
        )
        
        # Process LLM response
        enhanced_skills = self._process_llm_response(
            llm_response,
            job_data['extracted_skills']
        )
        
        return enhanced_skills
    
    def _process_llm_response(self, 
                            llm_response: Dict[str, Any],
                            original_skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process LLM response into enhanced skills.
        
        Args:
            llm_response: Response from LLM
            original_skills: Original extracted skills
            
        Returns:
            List of enhanced skill records
        """
        enhanced_skills = []
        
        # Create a mapping of original skills
        original_map = {
            skill['skill_text'].lower(): skill 
            for skill in original_skills
        }
        
        # Process explicit skills (validated by LLM)
        for skill_data in llm_response.get('explicit_skills', []):
            skill_text = skill_data.get('skill', '').lower()
            
            # Validate skill
            validation = self.validator.validate_skill(skill_text)
            if not validation['valid']:
                continue
            
            # Normalize skill
            normalization = self.normalizer.normalize_skill(skill_text)
            
            enhanced_skills.append({
                'original_skill_text': skill_data.get('original', skill_text),
                'normalized_skill': normalization['normalized'],
                'skill_type': 'explicit',
                'esco_concept_uri': None,  # To be matched later
                'esco_preferred_label': None,
                'llm_confidence': skill_data.get('confidence', 0.8),
                'llm_reasoning': None,
                'is_duplicate': False,
                'llm_model': llm_response.get('model_name', 'unknown')
            })
        
        # Process implicit skills (inferred by LLM)
        for skill_data in llm_response.get('implicit_skills', []):
            skill_text = skill_data.get('skill', '').lower()
            
            # Validate skill
            validation = self.validator.validate_skill(skill_text)
            if not validation['valid']:
                continue
            
            # Normalize skill
            normalization = self.normalizer.normalize_skill(skill_text)
            
            enhanced_skills.append({
                'original_skill_text': None,
                'normalized_skill': normalization['normalized'],
                'skill_type': 'implicit',
                'esco_concept_uri': None,
                'esco_preferred_label': None,
                'llm_confidence': skill_data.get('confidence', 0.7),
                'llm_reasoning': skill_data.get('reasoning', ''),
                'is_duplicate': False,
                'llm_model': llm_response.get('model_name', 'unknown')
            })
        
        # Deduplicate skills
        enhanced_skills = self._deduplicate_skills(enhanced_skills)
        
        return enhanced_skills
    
    def _deduplicate_skills(self, skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Deduplicate enhanced skills.
        
        Args:
            skills: List of enhanced skills
            
        Returns:
            Deduplicated list with duplicates marked
        """
        seen_normalized = {}
        deduplicated = []
        
        # Sort by confidence (highest first)
        sorted_skills = sorted(
            skills, 
            key=lambda x: x.get('llm_confidence', 0),
            reverse=True
        )
        
        for skill in sorted_skills:
            normalized = skill['normalized_skill'].lower()
            
            if normalized not in seen_normalized:
                # First occurrence
                seen_normalized[normalized] = skill
                deduplicated.append(skill)
            else:
                # Duplicate found
                skill['is_duplicate'] = True
                skill['duplicate_of_id'] = id(seen_normalized[normalized])
                deduplicated.append(skill)
        
        return deduplicated
    
    def run_continuous(self, batch_size: int = 50, wait_time: int = 60):
        """Run LLM processing continuously.
        
        Args:
            batch_size: Jobs per batch
            wait_time: Seconds to wait between batches
        """
        logger.info(f"Starting continuous LLM processing (batch_size={batch_size}, wait={wait_time}s)")
        
        while True:
            try:
                stats = self.process_batch(batch_size)
                
                if stats['jobs_processed'] == 0:
                    logger.info(f"No jobs to process. Waiting {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    # Short pause between batches
                    time.sleep(10)
                    
            except KeyboardInterrupt:
                logger.info("LLM processing stopped by user")
                break
            except Exception as e:
                logger.error(f"LLM processing error: {e}")
                logger.info(f"Waiting {wait_time}s before retry...")
                time.sleep(wait_time)
```

---

## 7. Embedder Module Files

### src/embedder/__init__.py
```python
from .vectorizer import SkillVectorizer
from .model_loader import EmbeddingModelLoader
from .batch_processor import BatchProcessor

__all__ = ['SkillVectorizer', 'EmbeddingModelLoader', 'BatchProcessor']

### src/embedder/vectorizer.py
```python
import logging
from typing import List, Dict, Any, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import torch

logger = logging.getLogger(__name__)

class SkillVectorizer:
    """Generate embeddings for skills."""
    
    def __init__(self, model_name: str = None):
        self.model_name = model_name or "intfloat/multilingual-e5-base"
        self.model = None
        self.device = None
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialize the embedding model."""
        try:
            # Check for GPU availability
            if torch.cuda.is_available():
                self.device = 'cuda'
                logger.info(f"Using GPU for embeddings")
            else:
                self.device = 'cpu'
                logger.info(f"Using CPU for embeddings")
            
            # Load model
            logger.info(f"Loading embedding model: {self.model_name}")
            self.model = SentenceTransformer(self.model_name, device=self.device)
            
            # Get embedding dimension
            self.embedding_dim = self.model.get_sentence_embedding_dimension()
            logger.info(f"Model loaded. Embedding dimension: {self.embedding_dim}")
            
        except Exception as e:
            logger.error(f"Failed to initialize embedding model: {e}")
            raise
    
    def vectorize(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """Generate embeddings for a list of texts.
        
        Args:
            texts: List of texts to embed
            batch_size: Batch size for processing
            
        Returns:
            Array of embeddings
        """
        if not texts:
            return np.array([])
        
        try:
            # For E5 models, add instruction prefix
            if 'e5' in self.model_name.lower():
                texts = [f"query: {text}" for text in texts]
            
            # Generate embeddings
            embeddings = self.model.encode(
                texts,
                batch_size=batch_size,
                show_progress_bar=len(texts) > 100,
                convert_to_numpy=True,
                normalize_embeddings=True  # L2 normalization
            )
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Vectorization failed: {e}")
            raise
    
    def vectorize_single(self, text: str) -> np.ndarray:
        """Generate embedding for a single text.
        
        Args:
            text: Text to embed
            
        Returns:
            Embedding vector
        """
        return self.vectorize([text])[0]
    
    def compute_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """Compute cosine similarity between two embeddings.
        
        Args:
            embedding1: First embedding
            embedding2: Second embedding
            
        Returns:
            Cosine similarity score
        """
        # Assuming normalized embeddings, cosine similarity is just dot product
        return float(np.dot(embedding1, embedding2))
    
    def find_similar_skills(self, 
                          query_embedding: np.ndarray,
                          skill_embeddings: List[Dict[str, Any]],
                          top_k: int = 10,
                          threshold: float = 0.7) -> List[Dict[str, Any]]:
        """Find similar skills based on embeddings.
        
        Args:
            query_embedding: Query skill embedding
            skill_embeddings: List of skill embedding dictionaries
            top_k: Number of top results to return
            threshold: Minimum similarity threshold
            
        Returns:
            List of similar skills with scores
        """
        similarities = []
        
        for skill_data in skill_embeddings:
            embedding = skill_data['embedding']
            similarity = self.compute_similarity(query_embedding, embedding)
            
            if similarity >= threshold:
                similarities.append({
                    'skill': skill_data['skill_text'],
                    'similarity': similarity,
                    'embedding_id': skill_data.get('embedding_id')
                })
        
        # Sort by similarity
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        return similarities[:top_k]

### src/embedder/model_loader.py
```python
import logging
from typing import Dict, Any, Optional
import os
import json
from pathlib import Path
import torch
from transformers import AutoModel, AutoTokenizer
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)

class EmbeddingModelLoader:
    """Load and manage embedding models."""
    
    # Model configurations
    MODEL_CONFIGS = {
        'multilingual-e5-base': {
            'name': 'intfloat/multilingual-e5-base',
            'dimension': 768,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': 'query: '
        },
        'multilingual-e5-large': {
            'name': 'intfloat/multilingual-e5-large',
            'dimension': 1024,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': 'query: '
        },
        'beto': {
            'name': 'dccuchile/bert-base-spanish-wwm-cased',
            'dimension': 768,
            'max_length': 512,
            'type': 'transformers',
            'instruction_prefix': None
        },
        'labse': {
            'name': 'sentence-transformers/LaBSE',
            'dimension': 768,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': None
        },
        'multilingual-minilm': {
            'name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
            'dimension': 384,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': None
        }
    }
    
    def __init__(self, cache_dir: Optional[str] = None):
        self.cache_dir = cache_dir or "./data/cache/embeddings"
        os.makedirs(self.cache_dir, exist_ok=True)
        self.loaded_models = {}
    
    def load_model(self, model_key: str) -> Any:
        """Load an embedding model.
        
        Args:
            model_key: Key from MODEL_CONFIGS
            
        Returns:
            Loaded model
        """
        if model_key in self.loaded_models:
            logger.info(f"Model {model_key} already loaded")
            return self.loaded_models[model_key]
        
        if model_key not in self.MODEL_CONFIGS:
            raise ValueError(f"Unknown model key: {model_key}")
        
        config = self.MODEL_CONFIGS[model_key]
        
        try:
            if config['type'] == 'sentence-transformers':
                model = self._load_sentence_transformer(config)
            elif config['type'] == 'transformers':
                model = self._load_transformers_model(config)
            else:
                raise ValueError(f"Unknown model type: {config['type']}")
            
            self.loaded_models[model_key] = model
            logger.info(f"Successfully loaded model: {model_key}")
            
            return model
            
        except Exception as e:
            logger.error(f"Failed to load model {model_key}: {e}")
            raise
    
    def _load_sentence_transformer(self, config: Dict[str, Any]) -> SentenceTransformer:
        """Load a sentence-transformers model.
        
        Args:
            config: Model configuration
            
        Returns:
            Loaded model
        """
        model = SentenceTransformer(
            config['name'],
            cache_folder=self.cache_dir
        )
        
        # Verify dimension
        actual_dim = model.get_sentence_embedding_dimension()
        if actual_dim != config['dimension']:
            logger.warning(
                f"Model dimension mismatch: expected {config['dimension']}, "
                f"got {actual_dim}"
            )
        
        return model
    
    def _load_transformers_model(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Load a transformers model with tokenizer.
        
        Args:
            config: Model configuration
            
        Returns:
            Dictionary with model and tokenizer
        """
        model = AutoModel.from_pretrained(
            config['name'],
            cache_dir=self.cache_dir
        )
        
        tokenizer = AutoTokenizer.from_pretrained(
            config['name'],
            cache_dir=self.cache_dir
        )
        
        return {
            'model': model,
            'tokenizer': tokenizer,
            'config': config
        }
    
    def get_model_info(self, model_key: str) -> Dict[str, Any]:
        """Get information about a model.
        
        Args:
            model_key: Model key
            
        Returns:
            Model information
        """
        if model_key not in self.MODEL_CONFIGS:
            raise ValueError(f"Unknown model key: {model_key}")
        
        return self.MODEL_CONFIGS[model_key].copy()
    
    def list_available_models(self) -> Dict[str, Dict[str, Any]]:
        """List all available models.
        
        Returns:
            Dictionary of model configurations
        """
        return self.MODEL_CONFIGS.copy()
    
    def download_all_models(self):
        """Download all configured models."""
        logger.info("Downloading all configured embedding models...")
        
        for model_key in self.MODEL_CONFIGS:
            try:
                logger.info(f"Downloading {model_key}...")
                self.load_model(model_key)
            except Exception as e:
                logger.error(f"Failed to download {model_key}: {e}")
    
    def clear_cache(self):
        """Clear model cache."""
        import shutil
        
        if os.path.exists(self.cache_dir):
            shutil.rmtree(self.cache_dir)
            os.makedirs(self.cache_dir)
            logger.info("Model cache cleared")
        
        self.loaded_models.clear()

### src/embedder/batch_processor.py
```python
import logging
from typing import List, Dict, Any
import numpy as np
from database.operations import DatabaseOperations
from .vectorizer import SkillVectorizer
from config.settings import get_settings
import time
from tqdm import tqdm

logger = logging.getLogger(__name__)

class BatchProcessor:
    """Process skill embeddings in batches."""
    
    def __init__(self, model_name: str = None):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        self.vectorizer = SkillVectorizer(model_name)
        self.batch_size = self.settings.embedding_batch_size
    
    def process_all_skills(self) -> Dict[str, Any]:
        """Process all skills that need embeddings.
        
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'skills_processed': 0,
            'embeddings_created': 0,
            'errors': 0,
            'processing_time': 0
        }
        
        try:
            # Get skills without embeddings
            skills = self.db_ops.get_unique_skills_for_embedding()
            logger.info(f"Found {len(skills)} skills without embeddings")
            
            if not skills:
                return stats
            
            # Process in batches
            for i in tqdm(range(0, len(skills), self.batch_size), desc="Embedding skills"):
                batch = skills[i:i + self.batch_size]
                
                try:
                    # Generate embeddings
                    embeddings = self.vectorizer.vectorize(batch)
                    
                    # Prepare data for database
                    embedding_data = []
                    for skill_text, embedding in zip(batch, embeddings):
                        embedding_data.append({
                            'skill_text': skill_text,
                            'embedding': embedding.tolist(),  # Convert to list for pgvector
                            'model_name': self.vectorizer.model_name,
                            'model_version': '1.0'
                        })
                    
                    # Save to database
                    self.db_ops.insert_skill_embeddings(embedding_data)
                    
                    stats['skills_processed'] += len(batch)
                    stats['embeddings_created'] += len(embedding_data)
                    
                except Exception as e:
                    logger.error(f"Error processing batch {i//self.batch_size}: {e}")
                    stats['errors'] += 1
            
            stats['processing_time'] = time.time() - start_time
            
            logger.info(
                f"Embedding complete: {stats['skills_processed']} skills processed, "
                f"{stats['embeddings_created']} embeddings created, "
                f"{stats['errors']} errors, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"Embedding batch processing failed: {e}")
            raise
    
    def update_embeddings(self, force: bool = False) -> Dict[str, Any]:
        """Update embeddings for new or modified skills.
        
        Args:
            force: Force re-embedding of all skills
            
        Returns:
            Update statistics
        """
        if force:
            logger.warning("Force update requested - this will re-embed all skills")
            # Clear existing embeddings
            # Note: Implement this method in DatabaseOperations if needed
        
        return self.process_all_skills()
    
    def compute_similarity_matrix(self, skill_list: List[str] = None) -> np.ndarray:
        """Compute similarity matrix for skills.
        
        Args:
            skill_list: List of skills to compare (None for all)
            
        Returns:
            Similarity matrix
        """
        # Get embeddings from database
        all_embeddings = self.db_ops.get_all_embeddings()
        
        if skill_list:
            # Filter to requested skills
            embeddings_data = [
                e for e in all_embeddings 
                if e['skill_text'] in skill_list
            ]
        else:
            embeddings_data = all_embeddings
        
        if not embeddings_data:
            return np.array([])
        
        # Extract embedding vectors
        embeddings = np.array([e['embedding'] for e in embeddings_data])
        
        # Compute similarity matrix
        # Since embeddings are normalized, cosine similarity is just dot product
        similarity_matrix = np.dot(embeddings, embeddings.T)
        
        return similarity_matrix
    
    def find_duplicate_skills(self, threshold: float = 0.95) -> List[Dict[str, Any]]:
        """Find potential duplicate skills based on embedding similarity.
        
        Args:
            threshold: Similarity threshold for duplicates
            
        Returns:
            List of potential duplicates
        """
        all_embeddings = self.db_ops.get_all_embeddings()
        
        if len(all_embeddings) < 2:
            return []
        
        duplicates = []
        
        # Compare all pairs
        for i in range(len(all_embeddings)):
            for j in range(i + 1, len(all_embeddings)):
                skill1 = all_embeddings[i]
                skill2 = all_embeddings[j]
                
                similarity = self.vectorizer.compute_similarity(
                    np.array(skill1['embedding']),
                    np.array(skill2['embedding'])
                )
                
                if similarity >= threshold:
                    duplicates.append({
                        'skill1': skill1['skill_text'],
                        'skill2': skill2['skill_text'],
                        'similarity': similarity
                    })
        
        # Sort by similarity
        duplicates.sort(key=lambda x: x['similarity'], reverse=True)
        
        logger.info(f"Found {len(duplicates)} potential duplicate skill pairs")
        
        return duplicates
    
    def get_skill_recommendations(self, 
                                job_skills: List[str],
                                top_k: int = 10) -> List[Dict[str, Any]]:
        """Get skill recommendations based on current job skills.
        
        Args:
            job_skills: Current skills in job
            top_k: Number of recommendations
            
        Returns:
            List of recommended skills with scores
        """
        # Get embeddings for input skills
        input_embeddings = self.vectorizer.vectorize(job_skills)
        
        # Average the embeddings to get job profile
        job_profile = np.mean(input_embeddings, axis=0)
        
        # Get all skill embeddings
        all_embeddings = self.db_ops.get_all_embeddings()
        
        # Find similar skills
        recommendations = []
        
        for skill_data in all_embeddings:
            # Skip if already in job skills
            if skill_data['skill_text'] in job_skills:
                continue
            
            similarity = self.vectorizer.compute_similarity(
                job_profile,
                np.array(skill_data['embedding'])
            )
            
            recommendations.append({
                'skill': skill_data['skill_text'],
                'score': similarity
            })
        
        # Sort and return top K
        recommendations.sort(key=lambda x: x['score'], reverse=True)
        
        return recommendations[:top_k]

---

## 8. Analyzer Module Files

### src/analyzer/__init__.py
```python
from .clustering import SkillClusterer
from .dimension_reducer import DimensionReducer
from .report_generator import ReportGenerator
from .visualizations import VisualizationGenerator

__all__ = ['SkillClusterer', 'DimensionReducer', 'ReportGenerator', 'VisualizationGenerator']

### src/analyzer/clustering.py
```python
import logging
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from sklearn.cluster import HDBSCAN, KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score
import pandas as pd
from database.operations import DatabaseOperations
from config.settings import get_settings

logger = logging.getLogger(__name__)

class SkillClusterer:
    """Perform clustering on skill embeddings."""
    
    def __init__(self):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        self.clustering_method = 'hdbscan'
    
    def cluster_skills(self, 
                      embeddings: np.ndarray,
                      skill_texts: List[str],
                      method: str = 'hdbscan',
                      **kwargs) -> Dict[str, Any]:
        """Cluster skills based on embeddings.
        
        Args:
            embeddings: Skill embedding matrix
            skill_texts: List of skill texts
            method: Clustering method ('hdbscan' or 'kmeans')
            **kwargs: Additional parameters for clustering
            
        Returns:
            Clustering results
        """
        if len(embeddings) < 2:
            logger.warning("Not enough data points for clustering")
            return {}
        
        logger.info(f"Clustering {len(embeddings)} skills using {method}")
        
        if method == 'hdbscan':
            results = self._cluster_hdbscan(embeddings, skill_texts, **kwargs)
        elif method == 'kmeans':
            results = self._cluster_kmeans(embeddings, skill_texts, **kwargs)
        else:
            raise ValueError(f"Unknown clustering method: {method}")
        
        # Calculate metrics
        results['metrics'] = self._calculate_metrics(embeddings, results['labels'])
        
        # Characterize clusters
        results['cluster_info'] = self._characterize_clusters(
            results['labels'],
            skill_texts
        )
        
        return results
    
    def _cluster_hdbscan(self, 
                        embeddings: np.ndarray,
                        skill_texts: List[str],
                        min_cluster_size: int = None,
                        min_samples: int = None) -> Dict[str, Any]:
        """Perform HDBSCAN clustering.
        
        Args:
            embeddings: Embedding matrix
            skill_texts: Skill texts
            min_cluster_size: Minimum cluster size
            min_samples: Minimum samples for core points
            
        Returns:
            Clustering results
        """
        # Use settings if not provided
        min_cluster_size = min_cluster_size or self.settings.cluster_min_size
        min_samples = min_samples or self.settings.cluster_min_samples
        
        # Perform clustering
        clusterer = HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            metric='euclidean',
            cluster_selection_method='eom',
            prediction_data=True
        )
        
        labels = clusterer.fit_predict(embeddings)
        
        # Get cluster persistence (stability)
        cluster_persistence = clusterer.cluster_persistence_
        
        results = {
            'method': 'hdbscan',
            'labels': labels,
            'n_clusters': len(set(labels)) - (1 if -1 in labels else 0),
            'n_noise': list(labels).count(-1),
            'parameters': {
                'min_cluster_size': min_cluster_size,
                'min_samples': min_samples
            },
            'cluster_persistence': cluster_persistence,
            'clusterer': clusterer
        }
        
        logger.info(
            f"HDBSCAN found {results['n_clusters']} clusters "
            f"with {results['n_noise']} noise points"
        )
        
        return results
    
    def _cluster_kmeans(self,
                       embeddings: np.ndarray,
                       skill_texts: List[str],
                       n_clusters: int = 20) -> Dict[str, Any]:
        """Perform K-means clustering.
        
        Args:
            embeddings: Embedding matrix
            skill_texts: Skill texts
            n_clusters: Number of clusters
            
        Returns:
            Clustering results
        """
        # Perform clustering
        clusterer = KMeans(
            n_clusters=n_clusters,
            n_init=10,
            max_iter=300,
            random_state=42
        )
        
        labels = clusterer.fit_predict(embeddings)
        
        results = {
            'method': 'kmeans',
            'labels': labels,
            'n_clusters': n_clusters,
            'n_noise': 0,
            'parameters': {
                'n_clusters': n_clusters
            },
            'cluster_centers': clusterer.cluster_centers_,
            'inertia': clusterer.inertia_,
            'clusterer': clusterer
        }
        
        logger.info(f"K-means created {n_clusters} clusters")
        
        return results
    
    def _calculate_metrics(self, embeddings: np.ndarray, labels: np.ndarray) -> Dict[str, float]:
        """Calculate clustering quality metrics.
        
        Args:
            embeddings: Embedding matrix
            labels: Cluster labels
            
        Returns:
            Dictionary of metrics
        """
        metrics = {}
        
        # Remove noise points for metrics
        mask = labels >= 0
        if np.sum(mask) < 2:
            logger.warning("Not enough clustered points for metrics")
            return metrics
        
        try:
            # Silhouette score
            metrics['silhouette_score'] = silhouette_score(
                embeddings[mask],
                labels[mask]
            )
            
            # Davies-Bouldin index (lower is better)
            metrics['davies_bouldin_index'] = davies_bouldin_score(
                embeddings[mask],
                labels[mask]
            )
            
            # Cluster statistics
            unique_labels = np.unique(labels[labels >= 0])
            cluster_sizes = [np.sum(labels == label) for label in unique_labels]
            
            metrics['avg_cluster_size'] = np.mean(cluster_sizes)
            metrics['std_cluster_size'] = np.std(cluster_sizes)
            metrics['min_cluster_size'] = np.min(cluster_sizes)
            metrics['max_cluster_size'] = np.max(cluster_sizes)
            
        except Exception as e:
            logger.error(f"Error calculating metrics: {e}")
        
        return metrics
    
    def _characterize_clusters(self, 
                             labels: np.ndarray,
                             skill_texts: List[str]) -> List[Dict[str, Any]]:
        """Characterize each cluster.
        
        Args:
            labels: Cluster labels
            skill_texts: List of skill texts
            
        Returns:
            List of cluster characteristics
        """
        cluster_info = []
        
        # Create DataFrame for easier analysis
        df = pd.DataFrame({
            'skill': skill_texts,
            'cluster': labels
        })
        
        # Analyze each cluster
        unique_labels = sorted(set(labels))
        
        for label in unique_labels:
            if label == -1:  # Skip noise cluster
                continue
            
            cluster_skills = df[df['cluster'] == label]['skill'].tolist()
            
            # Get most common skills
            skill_counts = pd.Series(cluster_skills).value_counts()
            
            cluster_data = {
                'cluster_id': int(label),
                'size': len(cluster_skills),
                'top_skills': skill_counts.head(10).to_dict(),
                'all_skills': cluster_skills,
                'label': self._generate_cluster_label(skill_counts.head(5).index.tolist())
            }
            
            cluster_info.append(cluster_data)
        
        # Sort by size
        cluster_info.sort(key=lambda x: x['size'], reverse=True)
        
        return cluster_info
    
    def _generate_cluster_label(self, top_skills: List[str]) -> str:
        """Generate a descriptive label for a cluster.
        
        Args:
            top_skills: Top skills in cluster
            
        Returns:
            Cluster label
        """
        # Simple heuristic-based labeling
        skill_lower = [s.lower() for s in top_skills]
        
        if any('frontend' in s or 'react' in s or 'angular' in s or 'vue' in s for s in skill_lower):
            return "Frontend Development"
        elif any('backend' in s or 'node' in s or 'django' in s or 'spring' in s for s in skill_lower):
            return "Backend Development"
        elif any('data' in s or 'analytics' in s or 'sql' in s for s in skill_lower):
            return "Data & Analytics"
        elif any('machine learning' in s or 'ml' in s or 'ai' in s for s in skill_lower):
            return "Machine Learning & AI"
        elif any('devops' in s or 'docker' in s or 'kubernetes' in s for s in skill_lower):
            return "DevOps & Infrastructure"
        elif any('mobile' in s or 'android' in s or 'ios' in s for s in skill_lower):
            return "Mobile Development"
        elif any('cloud' in s or 'aws' in s or 'azure' in s for s in skill_lower):
            return "Cloud Computing"
        else:
            # Use most common skill as label
            return f"{top_skills[0]} & Related"
    
    def run_clustering_pipeline(self) -> Dict[str, Any]:
        """Run complete clustering pipeline on all skills.
        
        Returns:
            Complete clustering results
        """
        # Get all embeddings
        embeddings_data = self.db_ops.get_all_embeddings()
        
        if not embeddings_data:
            logger.warning("No embeddings found for clustering")
            return {}
        
        # Extract data
        skill_texts = [e['skill_text'] for e in embeddings_data]
        embeddings = np.array([e['embedding'] for e in embeddings_data])
        
        logger.info(f"Running clustering on {len(embeddings)} skills")
        
        # Run clustering
        results = self.cluster_skills(embeddings, skill_texts)
        
        # Save results to database
        self.db_ops.save_analysis_results(
            analysis_type='clustering',
            results={
                'n_clusters': results['n_clusters'],
                'n_noise': results['n_noise'],
                'metrics': results['metrics'],
                'cluster_info': results['cluster_info']
            },
            parameters=results['parameters']
        )
        
        return results,  # Only numbers
        r'^[^\w\s]+(#1-root-configuration-files)
2. [Database Setup Files](#2-database-setup-files)
3. [Configuration Module](#3-configuration-module)
4. [Scraper Module Files](#4-scraper-module-files)
5. [Extractor Module Files](#5-extractor-module-files)
6. [LLM Processor Module Files](#6-llm-processor-module-files)
7. [Embedder Module Files](#7-embedder-module-files)
8. [Analyzer Module Files](#8-analyzer-module-files)
9. [Orchestrator and Utilities](#9-orchestrator-and-utilities)
10. [Scripts](#10-scripts)

---

## 1. Root Configuration Files

### requirements.txt
```
# Core
python-dotenv==1.0.0
pydantic==2.5.3
typer==0.9.0
tqdm==4.66.1

# Web Scraping
scrapy==2.11.0
scrapy-selenium==0.0.7
beautifulsoup4==4.12.2
lxml==4.9.3
fake-useragent==1.4.0
requests==2.31.0

# Database
psycopg2-binary==2.9.9
sqlalchemy==2.0.23
pgvector==0.2.3
alembic==1.13.1

# NLP
spacy==3.7.2
langdetect==1.0.9
regex==2023.12.25

# Machine Learning
transformers==4.36.2
sentence-transformers==2.2.2
torch==2.1.2
llama-cpp-python==0.2.32
openai==1.6.1

# Data Processing
pandas==2.1.4
numpy==1.24.3
scikit-learn==1.3.2
umap-learn==0.5.5
hdbscan==0.8.33

# Visualization
matplotlib==3.8.2
seaborn==0.13.0
reportlab==4.0.8
pillow==10.1.0

# Testing
pytest==7.4.4
pytest-asyncio==0.23.3
pytest-cov==4.1.0

# Development
black==23.12.1
flake8==7.0.0
mypy==1.8.0
pre-commit==3.6.0
```

### .env.example
```bash
# Database Configuration
DATABASE_URL=postgresql://labor_user:your_password@localhost:5432/labor_observatory
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=0

# Scraping Configuration
SCRAPER_USER_AGENT="Mozilla/5.0 (Windows NT 10.0; Win64; x64) Academic Research Bot"
SCRAPER_CONCURRENT_REQUESTS=16
SCRAPER_DOWNLOAD_DELAY=1.0
SCRAPER_RETRY_TIMES=3

# ESCO API Configuration
ESCO_API_URL=https://ec.europa.eu/esco/api
ESCO_VERSION=1.1.0
ESCO_LANGUAGE=es

# LLM Configuration
LLM_MODEL_PATH=./data/models/mistral-7b-instruct.Q4_K_M.gguf
LLM_CONTEXT_LENGTH=4096
LLM_MAX_TOKENS=512
LLM_TEMPERATURE=0.7
LLM_N_GPU_LAYERS=35

# OpenAI Fallback (Optional)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-3.5-turbo

# Embedding Configuration
EMBEDDING_MODEL=intfloat/multilingual-e5-base
EMBEDDING_BATCH_SIZE=32
EMBEDDING_CACHE_DIR=./data/cache/embeddings

# Analysis Configuration
CLUSTER_MIN_SIZE=5
CLUSTER_MIN_SAMPLES=3
UMAP_N_NEIGHBORS=15
UMAP_MIN_DIST=0.1

# Output Configuration
OUTPUT_DIR=./outputs
REPORT_FORMAT=pdf
LOG_LEVEL=INFO
LOG_FILE=./logs/labor_observatory.log
```

### .gitignore
```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/
.venv

# IDE
.vscode/
.idea/
*.swp
*.swo

# Data and Models
data/models/*.gguf
data/models/*/
data/cache/
outputs/
logs/

# Database
*.db
*.sqlite3

# Environment
.env
.env.local

# OS
.DS_Store
Thumbs.db

# Testing
.coverage
htmlcov/
.pytest_cache/

# Scrapy
.scrapy/

# Notebooks
.ipynb_checkpoints/
```

### setup.py
```python
from setuptools import setup, find_packages

setup(
    name="labor-observatory",
    version="1.0.0",
    author="Your Team",
    description="Automated Labor Market Observatory for Latin America",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    python_requires=">=3.10",
    install_requires=[
        line.strip()
        for line in open("requirements.txt")
        if line.strip() and not line.startswith("#")
    ],
    entry_points={
        "console_scripts": [
            "labor-observatory=orchestrator:app",
        ],
    },
)
```

---

## 2. Database Setup Files

### src/database/migrations/001_initial_schema.sql
```sql
-- Create database
CREATE DATABASE IF NOT EXISTS labor_observatory
  WITH ENCODING 'UTF8'
  LC_COLLATE = 'en_US.UTF-8'
  LC_CTYPE = 'en_US.UTF-8';

\c labor_observatory;

-- Enable extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgvector";

-- Create tables
CREATE TABLE IF NOT EXISTS raw_jobs (
    job_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    portal VARCHAR(50) NOT NULL,
    country CHAR(2) NOT NULL,
    url TEXT NOT NULL,
    title TEXT NOT NULL,
    company TEXT,
    location TEXT,
    description TEXT NOT NULL,
    requirements TEXT,
    salary_raw TEXT,
    contract_type VARCHAR(50),
    remote_type VARCHAR(50),
    posted_date DATE,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    content_hash VARCHAR(64) UNIQUE,
    raw_html TEXT,
    is_processed BOOLEAN DEFAULT FALSE,
    
    CONSTRAINT chk_country CHECK (country IN ('CO', 'MX', 'AR')),
    CONSTRAINT chk_portal CHECK (portal IN ('computrabajo', 'bumeran', 'elempleo'))
);

CREATE INDEX idx_portal_country ON raw_jobs(portal, country);
CREATE INDEX idx_scraped_at ON raw_jobs(scraped_at);
CREATE INDEX idx_processed ON raw_jobs(is_processed);

CREATE TABLE IF NOT EXISTS extracted_skills (
    extraction_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    job_id UUID REFERENCES raw_jobs(job_id) ON DELETE CASCADE,
    skill_text TEXT NOT NULL,
    skill_type VARCHAR(50),
    extraction_method VARCHAR(50),
    confidence_score FLOAT,
    source_section VARCHAR(50),
    span_start INTEGER,
    span_end INTEGER,
    esco_uri TEXT,
    extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_job_skills ON extracted_skills(job_id);
CREATE INDEX idx_skill_text ON extracted_skills(skill_text);

CREATE TABLE IF NOT EXISTS enhanced_skills (
    enhancement_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    job_id UUID REFERENCES raw_jobs(job_id) ON DELETE CASCADE,
    original_skill_text TEXT,
    normalized_skill TEXT NOT NULL,
    skill_type VARCHAR(50),
    esco_concept_uri TEXT,
    esco_preferred_label TEXT,
    llm_confidence FLOAT,
    llm_reasoning TEXT,
    is_duplicate BOOLEAN DEFAULT FALSE,
    duplicate_of_id UUID,
    enhanced_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    llm_model VARCHAR(100)
);

CREATE INDEX idx_job_enhanced ON enhanced_skills(job_id);
CREATE INDEX idx_normalized ON enhanced_skills(normalized_skill);

CREATE TABLE IF NOT EXISTS skill_embeddings (
    embedding_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    skill_text TEXT UNIQUE NOT NULL,
    embedding vector(768) NOT NULL,
    model_name VARCHAR(100) NOT NULL,
    model_version VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_skill_lookup ON skill_embeddings(skill_text);
CREATE INDEX idx_embedding_similarity ON skill_embeddings 
  USING ivfflat (embedding vector_cosine_ops)
  WITH (lists = 100);

CREATE TABLE IF NOT EXISTS analysis_results (
    analysis_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    analysis_type VARCHAR(50),
    country CHAR(2),
    date_range_start DATE,
    date_range_end DATE,
    parameters JSONB,
    results JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_analysis_type ON analysis_results(analysis_type);
CREATE INDEX idx_analysis_date ON analysis_results(created_at);

-- Create views
CREATE VIEW skill_frequency AS
SELECT 
    es.normalized_skill,
    COUNT(DISTINCT es.job_id) as job_count,
    COUNT(*) as total_mentions,
    ARRAY_AGG(DISTINCT rj.country) as countries
FROM enhanced_skills es
JOIN raw_jobs rj ON es.job_id = rj.job_id
WHERE es.is_duplicate = FALSE
GROUP BY es.normalized_skill
ORDER BY job_count DESC;

CREATE VIEW country_skill_distribution AS
SELECT 
    rj.country,
    es.normalized_skill,
    COUNT(DISTINCT es.job_id) as job_count
FROM enhanced_skills es
JOIN raw_jobs rj ON es.job_id = rj.job_id
WHERE es.is_duplicate = FALSE
GROUP BY rj.country, es.normalized_skill
ORDER BY rj.country, job_count DESC;
```

### src/database/models.py
```python
from sqlalchemy import Column, String, Text, Boolean, Float, Integer, DateTime, Date, ForeignKey, JSON
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from pgvector.sqlalchemy import Vector
import uuid

Base = declarative_base()

class RawJob(Base):
    __tablename__ = 'raw_jobs'
    
    job_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    portal = Column(String(50), nullable=False)
    country = Column(String(2), nullable=False)
    url = Column(Text, nullable=False)
    title = Column(Text, nullable=False)
    company = Column(Text)
    location = Column(Text)
    description = Column(Text, nullable=False)
    requirements = Column(Text)
    salary_raw = Column(Text)
    contract_type = Column(String(50))
    remote_type = Column(String(50))
    posted_date = Column(Date)
    scraped_at = Column(DateTime, server_default=func.now())
    content_hash = Column(String(64), unique=True)
    raw_html = Column(Text)
    is_processed = Column(Boolean, default=False)
    
    # Relationships
    extracted_skills = relationship("ExtractedSkill", back_populates="job")
    enhanced_skills = relationship("EnhancedSkill", back_populates="job")

class ExtractedSkill(Base):
    __tablename__ = 'extracted_skills'
    
    extraction_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    job_id = Column(UUID(as_uuid=True), ForeignKey('raw_jobs.job_id'))
    skill_text = Column(Text, nullable=False)
    skill_type = Column(String(50))
    extraction_method = Column(String(50))
    confidence_score = Column(Float)
    source_section = Column(String(50))
    span_start = Column(Integer)
    span_end = Column(Integer)
    esco_uri = Column(Text)
    extracted_at = Column(DateTime, server_default=func.now())
    
    # Relationships
    job = relationship("RawJob", back_populates="extracted_skills")

class EnhancedSkill(Base):
    __tablename__ = 'enhanced_skills'
    
    enhancement_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    job_id = Column(UUID(as_uuid=True), ForeignKey('raw_jobs.job_id'))
    original_skill_text = Column(Text)
    normalized_skill = Column(Text, nullable=False)
    skill_type = Column(String(50))
    esco_concept_uri = Column(Text)
    esco_preferred_label = Column(Text)
    llm_confidence = Column(Float)
    llm_reasoning = Column(Text)
    is_duplicate = Column(Boolean, default=False)
    duplicate_of_id = Column(UUID(as_uuid=True))
    enhanced_at = Column(DateTime, server_default=func.now())
    llm_model = Column(String(100))
    
    # Relationships
    job = relationship("RawJob", back_populates="enhanced_skills")

class SkillEmbedding(Base):
    __tablename__ = 'skill_embeddings'
    
    embedding_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    skill_text = Column(Text, unique=True, nullable=False)
    embedding = Column(Vector(768), nullable=False)
    model_name = Column(String(100), nullable=False)
    model_version = Column(String(50))
    created_at = Column(DateTime, server_default=func.now())

class AnalysisResult(Base):
    __tablename__ = 'analysis_results'
    
    analysis_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    analysis_type = Column(String(50))
    country = Column(String(2))
    date_range_start = Column(Date)
    date_range_end = Column(Date)
    parameters = Column(JSONB)
    results = Column(JSONB)
    created_at = Column(DateTime, server_default=func.now())
```

### src/database/operations.py
```python
from typing import List, Optional, Dict, Any
from datetime import datetime
from sqlalchemy import create_engine, and_, or_, func
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.exc import IntegrityError
import os
from .models import Base, RawJob, ExtractedSkill, EnhancedSkill, SkillEmbedding, AnalysisResult
import hashlib
import logging

logger = logging.getLogger(__name__)

class DatabaseOperations:
    def __init__(self, database_url: Optional[str] = None):
        self.database_url = database_url or os.getenv('DATABASE_URL')
        self.engine = create_engine(
            self.database_url,
            pool_size=20,
            max_overflow=0,
            pool_pre_ping=True
        )
        Base.metadata.create_all(self.engine)
        self.SessionLocal = sessionmaker(bind=self.engine)
    
    def get_session(self) -> Session:
        """Get a new database session."""
        return self.SessionLocal()
    
    def insert_job(self, job_data: Dict[str, Any]) -> Optional[str]:
        """Insert a new job posting."""
        session = self.get_session()
        try:
            # Generate content hash
            content = f"{job_data['title']}{job_data['description']}{job_data.get('requirements', '')}"
            content_hash = hashlib.sha256(content.encode()).hexdigest()
            
            job = RawJob(
                **job_data,
                content_hash=content_hash
            )
            session.add(job)
            session.commit()
            
            job_id = str(job.job_id)
            logger.info(f"Inserted job {job_id}")
            return job_id
            
        except IntegrityError:
            session.rollback()
            logger.warning(f"Duplicate job detected: {job_data['url']}")
            return None

### src/analyzer/visualizations.py
```python
import logging
from typing import Dict, Any, List, Optional, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from datetime import datetime
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class VisualizationGenerator:
    """Generate static visualizations for analysis results."""
    
    def __init__(self, output_dir: str = None):
        self.output_dir = output_dir or "./outputs"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Set style
        plt.style.use('seaborn-v0_8-darkgrid')
        sns.set_palette("husl")
        
        # Set font for better Spanish support
        plt.rcParams['font.family'] = 'DejaVu Sans'
    
    def create_all_visualizations(self, 
                                analysis_data: Dict[str, Any],
                                country: Optional[str] = None) -> List[str]:
        """Create all standard visualizations.
        
        Args:
            analysis_data: Dictionary with analysis results
            country: Country code for filtering
            
        Returns:
            List of generated file paths
        """
        generated_files = []
        
        # Skill frequency chart
        if 'skill_statistics' in analysis_data:
            path = self.create_skill_frequency_chart(
                analysis_data['skill_statistics'],
                country
            )
            if path:
                generated_files.append(path)
        
        # Cluster visualization
        if 'clustering_results' in analysis_data:
            path = self.create_cluster_visualization(
                analysis_data['clustering_results']
            )
            if path:
                generated_files.append(path)
        
        # Geographic distribution
        if 'geographic_data' in analysis_data:
            path = self.create_geographic_distribution(
                analysis_data['geographic_data']
            )
            if path:
                generated_files.append(path)
        
        # Skill co-occurrence heatmap
        if 'skill_cooccurrence' in analysis_data:
            path = self.create_skill_cooccurrence_heatmap(
                analysis_data['skill_cooccurrence']
            )
            if path:
                generated_files.append(path)
        
        return generated_files
    
    def create_skill_frequency_chart(self,
                                   skill_stats: Dict[str, Any],
                                   country: Optional[str] = None,
                                   top_n: int = 20) -> Optional[str]:
        """Create horizontal bar chart of top skills.
        
        Args:
            skill_stats: Skill statistics data
            country: Country filter
            top_n: Number of top skills to show
            
        Returns:
            Path to saved chart
        """
        try:
            # Prepare data
            top_skills = skill_stats.get('top_skills', [])[:top_n]
            if not top_skills:
                logger.warning("No skill data for frequency chart")
                return None
            
            df = pd.DataFrame(top_skills)
            
            # Create figure
            fig, ax = plt.subplots(figsize=(12, 8))
            
            # Create horizontal bar chart
            bars = ax.barh(df['skill'], df['count'], 
                          color=plt.cm.viridis(np.linspace(0.3, 0.9, len(df))))
            
            # Customize
            ax.set_xlabel('Número de Vacantes', fontsize=14)
            ax.set_ylabel('Habilidad Técnica', fontsize=14)
            
            title = f'Top {top_n} Habilidades Más Demandadas'
            if country:
                country_names = {'CO': 'Colombia', 'MX': 'México', 'AR': 'Argentina'}
                title += f' - {country_names.get(country, country)}'
            ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
            
            # Add value labels
            for bar in bars:
                width = bar.get_width()
                ax.text(width + 1, bar.get_y() + bar.get_height()/2,
                       f'{int(width)}',
                       ha='left', va='center', fontsize=10)
            
            # Adjust layout
            plt.tight_layout()
            ax.invert_yaxis()  # Highest on top
            
            # Grid
            ax.grid(True, axis='x', alpha=0.3)
            ax.set_axisbelow(True)
            
            # Save
            filename = self._generate_filename('skill_frequency', country)
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created skill frequency chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating skill frequency chart: {e}")
            return None
    
    def create_cluster_visualization(self,
                                   clustering_results: Dict[str, Any]) -> Optional[str]:
        """Create 2D scatter plot of skill clusters.
        
        Args:
            clustering_results: Clustering analysis results
            
        Returns:
            Path to saved visualization
        """
        try:
            # Extract data
            coordinates = clustering_results.get('coordinates_2d')
            labels = clustering_results.get('labels')
            skills = clustering_results.get('skills', [])
            
            if coordinates is None or labels is None:
                logger.warning("Missing data for cluster visualization")
                return None
            
            # Create figure
            fig, ax = plt.subplots(figsize=(14, 10))
            
            # Get unique labels
            unique_labels = set(labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            
            # Color map
            colors = plt.cm.rainbow(np.linspace(0, 1, n_clusters))
            
            # Plot each cluster
            for k, col in zip(sorted(unique_labels), colors):
                if k == -1:
                    # Noise points in black
                    col = 'black'
                    label = 'Ruido'
                else:
                    label = f'Cluster {k}'
                
                class_member_mask = (labels == k)
                xy = coordinates[class_member_mask]
                
                ax.scatter(xy[:, 0], xy[:, 1], 
                         c=[col], 
                         label=label,
                         alpha=0.6,
                         s=30)
            
            # Add labels for some points (avoid overlap)
            if skills:
                # Sample points to label
                n_labels = min(30, len(skills))
                indices = np.random.choice(len(skills), n_labels, replace=False)
                
                for idx in indices:
                    ax.annotate(skills[idx], 
                              (coordinates[idx, 0], coordinates[idx, 1]),
                              fontsize=8,
                              alpha=0.7)
            
            # Customize
            ax.set_title('Visualización de Clusters de Habilidades', 
                        fontsize=16, fontweight='bold')
            ax.set_xlabel('UMAP Dimension 1', fontsize=12)
            ax.set_ylabel('UMAP Dimension 2', fontsize=12)
            
            # Legend
            ax.legend(loc='upper right', bbox_to_anchor=(1.15, 1))
            
            # Remove ticks
            ax.set_xticks([])
            ax.set_yticks([])
            
            # Save
            filename = self._generate_filename('skill_clusters')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created cluster visualization: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating cluster visualization: {e}")
            return None
    
    def create_geographic_distribution(self,
                                     geo_data: Dict[str, Any]) -> Optional[str]:
        """Create geographic distribution chart.
        
        Args:
            geo_data: Geographic distribution data
            
        Returns:
            Path to saved chart
        """
        try:
            # Create figure
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # Job distribution by country
            countries = ['Colombia', 'México', 'Argentina']
            job_counts = [
                geo_data.get('CO', {}).get('total_jobs', 0),
                geo_data.get('MX', {}).get('total_jobs', 0),
                geo_data.get('AR', {}).get('total_jobs', 0)
            ]
            
            # Pie chart
            colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
            wedges, texts, autotexts = ax1.pie(job_counts, 
                                              labels=countries,
                                              colors=colors,
                                              autopct='%1.1f%%',
                                              startangle=90)
            
            ax1.set_title('Distribución de Vacantes por País', 
                         fontsize=14, fontweight='bold')
            
            # Skills per country
            skill_counts = [
                geo_data.get('CO', {}).get('unique_skills', 0),
                geo_data.get('MX', {}).get('unique_skills', 0),
                geo_data.get('AR', {}).get('unique_skills', 0)
            ]
            
            bars = ax2.bar(countries, skill_counts, color=colors)
            ax2.set_title('Habilidades Únicas por País', 
                         fontsize=14, fontweight='bold')
            ax2.set_ylabel('Número de Habilidades', fontsize=12)
            
            # Add value labels
            for bar in bars:
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height,
                        f'{int(height)}',
                        ha='center', va='bottom')
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('geographic_distribution')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created geographic distribution chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating geographic distribution: {e}")
            return None
    
    def create_skill_cooccurrence_heatmap(self,
                                        cooccurrence_data: Dict[str, Any],
                                        top_n: int = 15) -> Optional[str]:
        """Create heatmap of skill co-occurrences.
        
        Args:
            cooccurrence_data: Skill co-occurrence matrix
            top_n: Number of top skills to include
            
        Returns:
            Path to saved heatmap
        """
        try:
            # Convert to DataFrame
            df = pd.DataFrame(cooccurrence_data.get('matrix', []))
            skills = cooccurrence_data.get('skills', [])
            
            if df.empty or not skills:
                logger.warning("No co-occurrence data available")
                return None
            
            # Select top skills
            if len(skills) > top_n:
                # Sum co-occurrences for each skill
                skill_importance = df.sum(axis=0) + df.sum(axis=1)
                top_indices = skill_importance.nlargest(top_n).index
                df = df.loc[top_indices, top_indices]
                skills = [skills[i] for i in top_indices]
            
            # Create figure
            fig, ax = plt.subplots(figsize=(12, 10))
            
            # Create heatmap
            sns.heatmap(df, 
                       xticklabels=skills,
                       yticklabels=skills,
                       cmap='YlOrRd',
                       cbar_kws={'label': 'Co-ocurrencias'},
                       square=True,
                       linewidths=0.5,
                       ax=ax)
            
            # Customize
            ax.set_title('Matriz de Co-ocurrencia de Habilidades', 
                        fontsize=16, fontweight='bold', pad=20)
            
            # Rotate labels
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('skill_cooccurrence')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created co-occurrence heatmap: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating co-occurrence heatmap: {e}")
            return None
    
    def create_temporal_trends(self,
                             temporal_data: Dict[str, Any],
                             skills: List[str] = None) -> Optional[str]:
        """Create temporal trend visualization.
        
        Args:
            temporal_data: Temporal trend data
            skills: List of skills to plot (default: top 5)
            
        Returns:
            Path to saved chart
        """
        try:
            # Prepare data
            df = pd.DataFrame(temporal_data.get('trends', []))
            
            if df.empty:
                logger.warning("No temporal data available")
                return None
            
            # Convert date column
            df['date'] = pd.to_datetime(df['date'])
            
            # Select skills to plot
            if not skills:
                # Get top 5 skills by total mentions
                skill_totals = df.groupby('skill')['count'].sum()
                skills = skill_totals.nlargest(5).index.tolist()
            
            # Create figure
            fig, ax = plt.subplots(figsize=(14, 8))
            
            # Plot each skill
            for skill in skills:
                skill_data = df[df['skill'] == skill]
                ax.plot(skill_data['date'], skill_data['count'], 
                       marker='o', label=skill, linewidth=2)
            
            # Customize
            ax.set_title('Tendencias Temporales de Habilidades', 
                        fontsize=16, fontweight='bold')
            ax.set_xlabel('Fecha', fontsize=12)
            ax.set_ylabel('Número de Menciones', fontsize=12)
            
            # Format x-axis
            import matplotlib.dates as mdates
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
            ax.xaxis.set_major_locator(mdates.MonthLocator())
            plt.xticks(rotation=45)
            
            # Legend
            ax.legend(loc='upper left', bbox_to_anchor=(1, 1))
            
            # Grid
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('temporal_trends')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created temporal trends chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating temporal trends: {e}")
            return None
    
    def _generate_filename(self, chart_type: str, country: Optional[str] = None) -> str:
        """Generate filename with timestamp.
        
        Args:
            chart_type: Type of chart
            country: Country code (optional)
            
        Returns:
            Generated filename
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        country_suffix = f"_{country}" if country else ""
        return f"{chart_type}{country_suffix}_{timestamp}.png"

---

## 9. Orchestrator and Utilities

### src/orchestrator.py
```python
#!/usr/bin/env python3
"""
Main orchestrator for the Labor Market Observatory pipeline.
"""

import logging
import sys
from typing import Optional
import typer
from rich.console import Console
from rich.table import Table
from rich.progress import track
import time

from config.settings import get_settings
from config.logging_config import setup_logging
from database.operations import DatabaseOperations
from scraper.spiders.computrabajo_spider import ComputrabajoSpider
from scraper.spiders.bumeran_spider import BumeranSpider
from scraper.spiders.elempleo_spider import ElempleoSpider
from extractor.pipeline import ExtractionPipeline
from llm_processor.pipeline import LLMProcessingPipeline
from embedder.batch_processor import BatchProcessor
from analyzer.clustering import SkillClusterer
from analyzer.dimension_reducer import DimensionReducer
from analyzer.report_generator import ReportGenerator
from analyzer.visualizations import VisualizationGenerator

# Initialize
app = typer.Typer(help="Labor Market Observatory CLI")
console = Console()
settings = get_settings()
logger = setup_logging(settings.log_level, settings.log_file)

@app.command()
def scrape(
    country: str = typer.Argument(..., help="Country code (CO, MX, AR)"),
    portal: str = typer.Argument(..., help="Portal name (computrabajo, bumeran, elempleo)"),
    pages: int = typer.Option(10, help="Number of pages to scrape")
):
    """Run web scraping for a specific portal and country."""
    console.print(f"[bold green]Starting scraper for {portal} in {country}[/bold green]")
    
    # Validate inputs
    if country not in settings.supported_countries:
        console.print(f"[red]Invalid country: {country}[/red]")
        raise typer.Exit(1)
    
    if portal not in settings.supported_portals:
        console.print(f"[red]Invalid portal: {portal}[/red]")
        raise typer.Exit(1)
    
    # Run appropriate spider
    try:
        from scrapy.crawler import CrawlerProcess
        from scrapy.utils.project import get_project_settings
        
        # Get scrapy settings
        scrapy_settings = get_project_settings()
        scrapy_settings.update({
            'LOG_LEVEL': 'INFO',
            'CLOSESPIDER_PAGECOUNT': pages
        })
        
        process = CrawlerProcess(scrapy_settings)
        
        # Select spider
        if portal == 'computrabajo':
            spider_class = ComputrabajoSpider
        elif portal == 'bumeran':
            spider_class = BumeranSpider
        elif portal == 'elempleo':
            spider_class = ElempleoSpider
        
        # Run spider
        process.crawl(spider_class, country=country)
        process.start()
        
        console.print("[bold green]Scraping completed![/bold green]")
        
    except Exception as e:
        console.print(f"[red]Scraping failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def extract(
    batch_size: int = typer.Option(100, help="Batch size for processing")
):
    """Extract skills from scraped job postings."""
    console.print("[bold green]Starting skill extraction...[/bold green]")
    
    try:
        pipeline = ExtractionPipeline()
        
        with console.status("[bold green]Extracting skills...") as status:
            stats = pipeline.process_batch(batch_size)
        
        # Display results
        table = Table(title="Extraction Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Jobs Processed", str(stats['jobs_processed']))
        table.add_row("Skills Extracted", str(stats['skills_extracted']))
        table.add_row("ESCO Matches", str(stats['esco_matches']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        table.add_row("Jobs/Second", f"{stats['jobs_per_second']:.2f}")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Extraction failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def enhance(
    batch_size: int = typer.Option(50, help="Batch size for LLM processing"),
    model: str = typer.Option("local", help="Model type (local or openai)")
):
    """Enhance extracted skills using LLM."""
    console.print(f"[bold green]Starting LLM enhancement with {model} model...[/bold green]")
    
    try:
        pipeline = LLMProcessingPipeline(model_type=model)
        
        with console.status("[bold green]Processing with LLM...") as status:
            stats = pipeline.process_batch(batch_size)
        
        # Display results
        table = Table(title="LLM Enhancement Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Jobs Processed", str(stats['jobs_processed']))
        table.add_row("Skills Enhanced", str(stats['skills_enhanced']))
        table.add_row("Implicit Skills Found", str(stats['implicit_skills_found']))
        table.add_row("Skills Normalized", str(stats['skills_normalized']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Enhancement failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def embed(
    model: Optional[str] = typer.Option(None, help="Embedding model name")
):
    """Generate embeddings for all skills."""
    console.print("[bold green]Starting embedding generation...[/bold green]")
    
    try:
        processor = BatchProcessor(model_name=model)
        
        with console.status("[bold green]Generating embeddings...") as status:
            stats = processor.process_all_skills()
        
        # Display results
        table = Table(title="Embedding Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Skills Processed", str(stats['skills_processed']))
        table.add_row("Embeddings Created", str(stats['embeddings_created']))
        table.add_row("Errors", str(stats['errors']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Embedding failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def analyze(
    method: str = typer.Option("hdbscan", help="Clustering method")
):
    """Run clustering analysis on skill embeddings."""
    console.print(f"[bold green]Starting clustering analysis with {method}...[/bold green]")
    
    try:
        clusterer = SkillClusterer()
        
        with console.status("[bold green]Running clustering...") as status:
            results = clusterer.run_clustering_pipeline()
        
        # Display results
        if results:
            table = Table(title="Clustering Results")
            table.add_column("Metric", style="cyan")
            table.add_column("Value", style="magenta")
            
            table.add_row("Number of Clusters", str(results['n_clusters']))
            table.add_row("Noise Points", str(results['n_noise']))
            table.add_row("Silhouette Score", f"{results['metrics'].get('silhouette_score', 0):.3f}")
            
            console.print(table)
            
            # Show top clusters
            console.print("\n[bold]Top 5 Clusters:[/bold]")
            for cluster in results['cluster_info'][:5]:
                console.print(f"  • {cluster['label']}: {cluster['size']} skills")
        
    except Exception as e:
        console.print(f"[red]Analysis failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def report(
    country: Optional[str] = typer.Option(None, help="Country code to filter"),
    format: str = typer.Option("pdf", help="Report format (pdf)")
):
    """Generate analysis report."""
    console.print("[bold green]Generating report...[/bold green]")
    
    try:
        generator = ReportGenerator()
        
        with console.status("[bold green]Creating report...") as status:
            filepath = generator.generate_full_report(
                country=country,
                include_visualizations=True
            )
        
        console.print(f"[bold green]Report generated: {filepath}[/bold green]")
        
    except Exception as e:
        console.print(f"[red]Report generation failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def pipeline(
    country: str = typer.Argument(..., help="Country code"),
    portal: str = typer.Argument(..., help="Portal name"),
    full: bool = typer.Option(False, help="Run full pipeline including scraping")
):
    """Run complete pipeline."""
    console.print("[bold green]Running complete pipeline...[/bold green]")
    
    steps = []
    
    if full:
        steps.append(("Scraping", lambda: scrape(country, portal, pages=5)))
    
    steps.extend([
        ("Extraction", lambda: extract(batch_size=100)),
        ("LLM Enhancement", lambda: enhance(batch_size=50)),
        ("Embedding", lambda: embed()),
        ("Clustering", lambda: analyze()),
        ("Report Generation", lambda: report(country=country))
    ])
    
    for step_name, step_func in track(steps, description="Processing..."):
        try:
            console.print(f"\n[bold cyan]Running: {step_name}[/bold cyan]")
            step_func()
            time.sleep(1)  # Brief pause between steps
        except Exception as e:
            console.print(f"[red]Step '{step_name}' failed: {e}[/red]")
            raise typer.Exit(1)
    
    console.print("\n[bold green]Pipeline completed successfully![/bold green]")

@app.command()
def status():
    """Show system status and statistics."""
    console.print("[bold green]Labor Market Observatory Status[/bold green]\n")
    
    try:
        db_ops = DatabaseOperations()
        
        # Get statistics
        stats = db_ops.get_skill_statistics()
        
        # Display overall stats
        table = Table(title="System Statistics")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Total Unique Skills", str(stats.get('total_unique_skills', 0)))
        table.add_row("Database Size", "N/A")  # Would need to implement
        
        console.print(table)
        
        # Top skills
        if stats.get('top_skills'):
            console.print("\n[bold]Top 10 Skills:[/bold]")
            for i, skill in enumerate(stats['top_skills'][:10], 1):
                console.print(f"  {i}. {skill['skill']} ({skill['count']} jobs)")
        
    except Exception as e:
        console.print(f"[red]Failed to get status: {e}[/red]")
        raise typer.Exit(1)

if __name__ == "__main__":
    app()

### src/utils/__init__.py
```python
from .validators import validate_country, validate_portal, validate_skill
from .cleaners import clean_text, normalize_text, remove_html
from .metrics import calculate_metrics, generate_statistics
from .logger import get_logger

__all__ = [
    'validate_country', 'validate_portal', 'validate_skill',
    'clean_text', 'normalize_text', 'remove_html',
    'calculate_metrics', 'generate_statistics',
    'get_logger'
]
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting job: {e}")
            raise
        finally:
            session.close()
    
    def get_unprocessed_jobs(self, limit: int = 100) -> List[RawJob]:
        """Get unprocessed job postings."""
        session = self.get_session()
        try:
            jobs = session.query(RawJob).filter(
                RawJob.is_processed == False
            ).limit(limit).all()
            return jobs
        finally:
            session.close()
    
    def mark_job_processed(self, job_id: str):
        """Mark a job as processed."""
        session = self.get_session()
        try:
            session.query(RawJob).filter(
                RawJob.job_id == job_id
            ).update({"is_processed": True})
            session.commit()
        finally:
            session.close()
    
    def insert_extracted_skills(self, job_id: str, skills: List[Dict[str, Any]]):
        """Insert extracted skills for a job."""
        session = self.get_session()
        try:
            for skill_data in skills:
                skill = ExtractedSkill(
                    job_id=job_id,
                    **skill_data
                )
                session.add(skill)
            session.commit()
            logger.info(f"Inserted {len(skills)} extracted skills for job {job_id}")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting extracted skills: {e}")
            raise
        finally:
            session.close()
    
    def get_extracted_skills_for_processing(self, limit: int = 100) -> List[Dict[str, Any]]:
        """Get jobs with extracted skills that need LLM processing."""
        session = self.get_session()
        try:
            # Get jobs that have extracted skills but no enhanced skills
            subquery = session.query(EnhancedSkill.job_id).distinct()
            
            jobs = session.query(RawJob).join(ExtractedSkill).filter(
                ~RawJob.job_id.in_(subquery)
            ).limit(limit).all()
            
            result = []
            for job in jobs:
                skills = session.query(ExtractedSkill).filter(
                    ExtractedSkill.job_id == job.job_id
                ).all()
                
                result.append({
                    'job_id': str(job.job_id),
                    'job_title': job.title,
                    'job_description': job.description,
                    'job_requirements': job.requirements,
                    'extracted_skills': [
                        {
                            'skill_text': skill.skill_text,
                            'extraction_method': skill.extraction_method,
                            'source_section': skill.source_section,
                            'confidence_score': skill.confidence_score
                        }
                        for skill in skills
                    ]
                })
            
            return result
        finally:
            session.close()
    
    def insert_enhanced_skills(self, job_id: str, skills: List[Dict[str, Any]]):
        """Insert enhanced skills for a job."""
        session = self.get_session()
        try:
            for skill_data in skills:
                skill = EnhancedSkill(
                    job_id=job_id,
                    **skill_data
                )
                session.add(skill)
            session.commit()
            logger.info(f"Inserted {len(skills)} enhanced skills for job {job_id}")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting enhanced skills: {e}")
            raise
        finally:
            session.close()
    
    def get_unique_skills_for_embedding(self) -> List[str]:
        """Get unique normalized skills that don't have embeddings yet."""
        session = self.get_session()
        try:
            # Get skills that don't have embeddings
            embedded_skills = session.query(SkillEmbedding.skill_text).distinct()
            
            unique_skills = session.query(
                EnhancedSkill.normalized_skill
            ).filter(
                EnhancedSkill.is_duplicate == False,
                ~EnhancedSkill.normalized_skill.in_(embedded_skills)
            ).distinct().all()
            
            return [skill[0] for skill in unique_skills]
        finally:
            session.close()
    
    def insert_skill_embeddings(self, embeddings: List[Dict[str, Any]]):
        """Insert skill embeddings."""
        session = self.get_session()
        try:
            for emb_data in embeddings:
                embedding = SkillEmbedding(**emb_data)
                session.add(embedding)
            session.commit()
            logger.info(f"Inserted {len(embeddings)} skill embeddings")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting embeddings: {e}")
            raise
        finally:
            session.close()
    
    def get_all_embeddings(self) -> List[Dict[str, Any]]:
        """Get all skill embeddings for clustering."""
        session = self.get_session()
        try:
            embeddings = session.query(SkillEmbedding).all()
            return [
                {
                    'skill_text': emb.skill_text,
                    'embedding': emb.embedding,
                    'embedding_id': str(emb.embedding_id)
                }
                for emb in embeddings
            ]
        finally:
            session.close()
    
    def save_analysis_results(self, analysis_type: str, results: Dict[str, Any], 
                            parameters: Dict[str, Any], country: Optional[str] = None):
        """Save analysis results."""
        session = self.get_session()
        try:
            analysis = AnalysisResult(
                analysis_type=analysis_type,
                country=country,
                parameters=parameters,
                results=results
            )
            session.add(analysis)
            session.commit()
            logger.info(f"Saved {analysis_type} analysis results")
        except Exception as e:
            session.rollback()
            logger.error(f"Error saving analysis results: {e}")
            raise
        finally:
            session.close()
    
    def get_skill_statistics(self, country: Optional[str] = None) -> Dict[str, Any]:
        """Get skill statistics by country."""
        session = self.get_session()
        try:
            query = session.query(
                EnhancedSkill.normalized_skill,
                func.count(func.distinct(EnhancedSkill.job_id)).label('job_count')
            ).join(RawJob).filter(
                EnhancedSkill.is_duplicate == False
            )
            
            if country:
                query = query.filter(RawJob.country == country)
            
            results = query.group_by(
                EnhancedSkill.normalized_skill
            ).order_by(
                func.count(func.distinct(EnhancedSkill.job_id)).desc()
            ).limit(50).all()
            
            return {
                'top_skills': [
                    {'skill': skill, 'count': count}
                    for skill, count in results
                ],
                'total_unique_skills': session.query(
                    func.count(func.distinct(EnhancedSkill.normalized_skill))
                ).filter(EnhancedSkill.is_duplicate == False).scalar()
            }
        finally:
            session.close()
```

---

## 3. Configuration Module

### src/config/__init__.py
```python
from .settings import Settings, get_settings
from .database import get_database_url
from .logging_config import setup_logging

__all__ = ['Settings', 'get_settings', 'get_database_url', 'setup_logging']
```

### src/config/settings.py
```python
from pydantic_settings import BaseSettings
from pydantic import Field, validator
from typing import Optional, List
import os
from functools import lru_cache

class Settings(BaseSettings):
    # Database
    database_url: str = Field(..., env='DATABASE_URL')
    database_pool_size: int = Field(20, env='DATABASE_POOL_SIZE')
    
    # Scraping
    scraper_user_agent: str = Field(..., env='SCRAPER_USER_AGENT')
    scraper_concurrent_requests: int = Field(16, env='SCRAPER_CONCURRENT_REQUESTS')
    scraper_download_delay: float = Field(1.0, env='SCRAPER_DOWNLOAD_DELAY')
    scraper_retry_times: int = Field(3, env='SCRAPER_RETRY_TIMES')
    
    # ESCO
    esco_api_url: str = Field('https://ec.europa.eu/esco/api', env='ESCO_API_URL')
    esco_version: str = Field('1.1.0', env='ESCO_VERSION')
    esco_language: str = Field('es', env='ESCO_LANGUAGE')
    
    # LLM
    llm_model_path: str = Field(..., env='LLM_MODEL_PATH')
    llm_context_length: int = Field(4096, env='LLM_CONTEXT_LENGTH')
    llm_max_tokens: int = Field(512, env='LLM_MAX_TOKENS')
    llm_temperature: float = Field(0.7, env='LLM_TEMPERATURE')
    llm_n_gpu_layers: int = Field(35, env='LLM_N_GPU_LAYERS')
    
    # OpenAI (Optional)
    openai_api_key: Optional[str] = Field(None, env='OPENAI_API_KEY')
    openai_model: str = Field('gpt-3.5-turbo', env='OPENAI_MODEL')
    
    # Embeddings
    embedding_model: str = Field('intfloat/multilingual-e5-base', env='EMBEDDING_MODEL')
    embedding_batch_size: int = Field(32, env='EMBEDDING_BATCH_SIZE')
    embedding_cache_dir: str = Field('./data/cache/embeddings', env='EMBEDDING_CACHE_DIR')
    
    # Analysis
    cluster_min_size: int = Field(5, env='CLUSTER_MIN_SIZE')
    cluster_min_samples: int = Field(3, env='CLUSTER_MIN_SAMPLES')
    umap_n_neighbors: int = Field(15, env='UMAP_N_NEIGHBORS')
    umap_min_dist: float = Field(0.1, env='UMAP_MIN_DIST')
    
    # Output
    output_dir: str = Field('./outputs', env='OUTPUT_DIR')
    report_format: str = Field('pdf', env='REPORT_FORMAT')
    log_level: str = Field('INFO', env='LOG_LEVEL')
    log_file: str = Field('./logs/labor_observatory.log', env='LOG_FILE')
    
    # Supported countries and portals
    supported_countries: List[str] = ['CO', 'MX', 'AR']
    supported_portals: List[str] = ['computrabajo', 'bumeran', 'elempleo']
    
    class Config:
        env_file = '.env'
        env_file_encoding = 'utf-8'
    
    @validator('output_dir', 'log_file', 'embedding_cache_dir')
    def create_directories(cls, v):
        os.makedirs(os.path.dirname(v) if os.path.dirname(v) else v, exist_ok=True)
        return v

@lru_cache()
def get_settings() -> Settings:
    """Get cached settings instance."""
    return Settings()
```

### src/config/database.py
```python
import os
from urllib.parse import urlparse

def get_database_url() -> str:
    """Get database URL from environment or construct from components."""
    if os.getenv('DATABASE_URL'):
        return os.getenv('DATABASE_URL')
    
    # Construct from individual components
    user = os.getenv('DB_USER', 'labor_user')
    password = os.getenv('DB_PASSWORD', 'password')
    host = os.getenv('DB_HOST', 'localhost')
    port = os.getenv('DB_PORT', '5432')
    name = os.getenv('DB_NAME', 'labor_observatory')
    
    return f"postgresql://{user}:{password}@{host}:{port}/{name}"

def get_database_config() -> dict:
    """Parse database URL into components."""
    url = get_database_url()
    parsed = urlparse(url)
    
    return {
        'host': parsed.hostname,
        'port': parsed.port or 5432,
        'user': parsed.username,
        'password': parsed.password,
        'database': parsed.path.lstrip('/')
    }
```

### src/config/logging_config.py
```python
import logging
import logging.handlers
import os
from datetime import datetime

def setup_logging(log_level: str = 'INFO', log_file: str = None):
    """Configure logging for the entire application."""
    
    # Create logs directory if needed
    if log_file:
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    # Create formatters
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
    )
    console_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )
    
    # Get root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, log_level.upper()))
    
    # Remove existing handlers
    root_logger.handlers.clear()
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)
    
    # File handler with rotation
    if log_file:
        file_handler = logging.handlers.RotatingFileHandler(
            log_file,
            maxBytes=10 * 1024 * 1024,  # 10MB
            backupCount=5
        )
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(file_formatter)
        root_logger.addHandler(file_handler)
    
    # Specific loggers configuration
    logging.getLogger('scrapy').setLevel(logging.WARNING)
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('transformers').setLevel(logging.WARNING)
    
    return root_logger
```

### config/esco_config.yaml
```yaml
# ESCO Configuration for Spanish Language
esco:
  version: "1.1.0"
  base_url: "https://ec.europa.eu/esco/api"
  
  # Endpoints
  endpoints:
    skills: "/resource/skill"
    occupations: "/resource/occupation"
    search: "/search"
  
  # Spanish language configuration
  language:
    primary: "es"
    fallback: "en"
  
  # Skill types to extract
  skill_types:
    - "skill/competence"
    - "knowledge"
    - "skill"
  
  # Search parameters
  search:
    limit: 100
    fields:
      - "preferredLabel"
      - "altLabels"
      - "description"
      - "broaderConcept"
    
  # Mapping rules for common tech terms
  tech_mappings:
    # Programming languages
    "python": "http://data.europa.eu/esco/skill/3897c3c6-556b-4b8a-bfeb-234b0f716950"
    "java": "http://data.europa.eu/esco/skill/b4b36f5a-d5e6-4d78-b4ed-5b0b0e4f7a8a"
    "javascript": "http://data.europa.eu/esco/skill/7c7c5c49-0122-4b2e-8f6e-4e6b3f3e5d5f"
    "react": "http://data.europa.eu/esco/skill/react-framework"
    "node.js": "http://data.europa.eu/esco/skill/nodejs-runtime"
    
    # Databases
    "sql": "http://data.europa.eu/esco/skill/sql-language"
    "mysql": "http://data.europa.eu/esco/skill/mysql-database"
    "postgresql": "http://data.europa.eu/esco/skill/postgresql-database"
    "mongodb": "http://data.europa.eu/esco/skill/mongodb-database"
    
    # Cloud platforms
    "aws": "http://data.europa.eu/esco/skill/amazon-web-services"
    "azure": "http://data.europa.eu/esco/skill/microsoft-azure"
    "gcp": "http://data.europa.eu/esco/skill/google-cloud-platform"
    
    # DevOps
    "docker": "http://data.europa.eu/esco/skill/docker-containerization"
    "kubernetes": "http://data.europa.eu/esco/skill/kubernetes-orchestration"
    "git": "http://data.europa.eu/esco/skill/git-version-control"
    
    # Soft skills (Spanish)
    "trabajo en equipo": "http://data.europa.eu/esco/skill/teamwork"
    "comunicación": "http://data.europa.eu/esco/skill/communication"
    "liderazgo": "http://data.europa.eu/esco/skill/leadership"
    "resolución de problemas": "http://data.europa.eu/esco/skill/problem-solving"
```

---

## 4. Scraper Module Files

### src/scraper/__init__.py
```python
from .spiders.computrabajo_spider import ComputrabajoSpider
from .spiders.bumeran_spider import BumeranSpider
from .spiders.elempleo_spider import ElempleoSpider

__all__ = ['ComputrabajoSpider', 'BumeranSpider', 'ElempleoSpider']
```

### src/scraper/scrapy.cfg
```ini
[settings]
default = scraper.settings

[deploy]
project = labor_observatory_scraper
```

### src/scraper/items.py
```python
import scrapy
from scrapy.item import Field
from datetime import datetime
import re

class JobItem(scrapy.Item):
    # Required fields
    portal = Field()
    country = Field()
    url = Field()
    title = Field()
    description = Field()
    
    # Optional fields
    company = Field()
    location = Field()
    requirements = Field()
    salary_raw = Field()
    contract_type = Field()
    remote_type = Field()
    posted_date = Field()
    raw_html = Field()
    
    # Metadata
    scraped_at = Field()
    
    def clean_text(self, text):
        """Clean and normalize text fields."""
        if not text:
            return None
        
        # Remove extra whitespace
        text = ' '.join(text.split())
        
        # Remove HTML entities
        text = re.sub(r'&[a-zA-Z]+;', ' ', text)
        
        return text.strip()
    
    def __setitem__(self, key, value):
        # Clean text fields
        if key in ['title', 'description', 'requirements', 'company', 'location'] and value:
            value = self.clean_text(value)
        
        # Set scraped_at automatically
        if key == 'scraped_at':
            value = datetime.now()
        
        super().__setitem__(key, value)
```

### src/scraper/pipelines.py
```python
import logging
from datetime import datetime
from typing import Optional
import re
from scrapy.exceptions import DropItem
from database.operations import DatabaseOperations

logger = logging.getLogger(__name__)

class ValidationPipeline:
    """Validate scraped items."""
    
    def process_item(self, item, spider):
        # Check required fields
        required_fields = ['portal', 'country', 'url', 'title', 'description']
        
        for field in required_fields:
            if not item.get(field):
                raise DropItem(f"Missing required field: {field}")
        
        # Validate country code
        if item['country'] not in ['CO', 'MX', 'AR']:
            raise DropItem(f"Invalid country code: {item['country']}")
        
        # Validate portal
        if item['portal'] not in ['computrabajo', 'bumeran', 'elempleo']:
            raise DropItem(f"Invalid portal: {item['portal']}")
        
        # Ensure minimum description length
        if len(item['description']) < 50:
            raise DropItem("Description too short")
        
        return item

class NormalizationPipeline:
    """Normalize item fields."""
    
    def process_item(self, item, spider):
        # Normalize contract type
        if item.get('contract_type'):
            item['contract_type'] = self.normalize_contract_type(item['contract_type'])
        
        # Normalize remote type
        if item.get('remote_type'):
            item['remote_type'] = self.normalize_remote_type(item['remote_type'])
        
        # Parse posted date
        if item.get('posted_date'):
            item['posted_date'] = self.parse_date(item['posted_date'])
        
        # Set scraped_at
        item['scraped_at'] = datetime.now()
        
        return item
    
    def normalize_contract_type(self, contract: str) -> str:
        """Normalize contract type to standard values."""
        contract_lower = contract.lower()
        
        if any(term in contract_lower for term in ['tiempo completo', 'full time', 'completo']):
            return 'full_time'
        elif any(term in contract_lower for term in ['medio tiempo', 'part time', 'parcial']):
            return 'part_time'
        elif any(term in contract_lower for term in ['freelance', 'independiente', 'autonomo']):
            return 'freelance'
        elif any(term in contract_lower for term in ['contrato', 'temporal', 'proyecto']):
            return 'contract'
        elif any(term in contract_lower for term in ['pasantia', 'practica', 'internship']):
            return 'internship'
        else:
            return 'other'
    
    def normalize_remote_type(self, remote: str) -> str:
        """Normalize remote work type."""
        remote_lower = remote.lower()
        
        if any(term in remote_lower for term in ['remoto', 'remote', 'teletrabajo']):
            return 'remote'
        elif any(term in remote_lower for term in ['hibrido', 'hybrid', 'mixto']):
            return 'hybrid'
        elif any(term in remote_lower for term in ['presencial', 'oficina', 'on-site']):
            return 'on_site'
        else:
            return 'not_specified'
    
    def parse_date(self, date_str: str) -> Optional[datetime]:
        """Parse various date formats."""
        # Common Spanish date patterns
        patterns = [
            r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})',  # DD/MM/YYYY or DD-MM-YYYY
            r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})',  # YYYY/MM/DD or YYYY-MM-DD
            r'hace (\d+) días?',  # "hace X días"
            r'(\d+) días? atrás',  # "X días atrás"
            r'hoy',  # "hoy"
            r'ayer',  # "ayer"
        ]
        
        # Try to match patterns
        for pattern in patterns:
            match = re.search(pattern, date_str, re.IGNORECASE)
            if match:
                if 'hace' in pattern or 'atrás' in pattern:
                    days_ago = int(match.group(1))
                    return datetime.now() - timedelta(days=days_ago)
                elif pattern == r'hoy':
                    return datetime.now().date()
                elif pattern == r'ayer':
                    return datetime.now() - timedelta(days=1)
                else:
                    # Handle date formats
                    try:
                        if len(match.groups()) == 3:
                            if match.group(1).isdigit() and len(match.group(1)) == 4:
                                # YYYY/MM/DD format
                                return datetime(
                                    int(match.group(1)),
                                    int(match.group(2)),
                                    int(match.group(3))
                                ).date()
                            else:
                                # DD/MM/YYYY format
                                return datetime(
                                    int(match.group(3)),
                                    int(match.group(2)),
                                    int(match.group(1))
                                ).date()
                    except ValueError:
                        pass
        
        return None

class DatabasePipeline:
    """Save items to PostgreSQL database."""
    
    def __init__(self):
        self.db_ops = None
    
    def open_spider(self, spider):
        self.db_ops = DatabaseOperations()
        logger.info(f"Database pipeline opened for spider: {spider.name}")
    
    def process_item(self, item, spider):
        try:
            # Convert item to dict
            job_data = dict(item)
            
            # Remove metadata fields
            job_data.pop('scraped_at', None)
            
            # Insert into database
            job_id = self.db_ops.insert_job(job_data)
            
            if job_id:
                logger.info(f"Saved job {job_id}: {item['title']}")
            else:
                logger.warning(f"Duplicate job skipped: {item['url']}")
            
            return item
            
        except Exception as e:
            logger.error(f"Error saving job to database: {e}")
            raise
```

### src/scraper/settings.py
```python
import os
from config.settings import get_settings

settings = get_settings()

BOT_NAME = 'labor_observatory_scraper'

SPIDER_MODULES = ['scraper.spiders']
NEWSPIDER_MODULE = 'scraper.spiders'

# Obey robots.txt rules
ROBOTSTXT_OBEY = True

# Configure maximum concurrent requests
CONCURRENT_REQUESTS = settings.scraper_concurrent_requests
CONCURRENT_REQUESTS_PER_DOMAIN = 8

# Configure delay
DOWNLOAD_DELAY = settings.scraper_download_delay
RANDOMIZE_DOWNLOAD_DELAY = True

# Disable cookies
COOKIES_ENABLED = False

# User agent
USER_AGENT = settings.scraper_user_agent

# Override default headers
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'Cache-Control': 'no-cache',
    'Pragma': 'no-cache',
}

# Retry configuration
RETRY_TIMES = settings.scraper_retry_times
RETRY_HTTP_CODES = [500, 502, 503, 504, 408, 429]

# Configure pipelines
ITEM_PIPELINES = {
    'scraper.pipelines.ValidationPipeline': 100,
    'scraper.pipelines.NormalizationPipeline': 200,
    'scraper.pipelines.DatabasePipeline': 300,
}

# AutoThrottle extension
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 1
AUTOTHROTTLE_MAX_DELAY = 10
AUTOTHROTTLE_TARGET_CONCURRENCY = 8.0
AUTOTHROTTLE_DEBUG = False

# Memory usage
MEMUSAGE_ENABLED = True
MEMUSAGE_LIMIT_MB = 2048
MEMUSAGE_WARNING_MB = 1536

# Logging
LOG_LEVEL = 'INFO'
LOG_FORMAT = '%(levelname)s: %(message)s'

# Cache
HTTPCACHE_ENABLED = False

# Download timeout
DOWNLOAD_TIMEOUT = 30

# Telnet Console (disabled for production)
TELNETCONSOLE_ENABLED = False

# Middleware settings
DOWNLOADER_MIDDLEWARES = {
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,
    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110,
}
```

### src/scraper/middlewares.py
```python
import random
import logging
from scrapy.downloadermiddlewares.retry import RetryMiddleware
from scrapy.utils.response import response_status_message
from fake_useragent import UserAgent

logger = logging.getLogger(__name__)

class RotateUserAgentMiddleware:
    """Rotate user agents for each request."""
    
    def __init__(self):
        self.ua = UserAgent()
        self.user_agent_list = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
        ]
    
    def process_request(self, request, spider):
        try:
            # Try to use fake-useragent
            ua = self.ua.random
        except:
            # Fallback to predefined list
            ua = random.choice(self.user_agent_list)
        
        request.headers['User-Agent'] = ua + ' Academic Research Bot'

class CustomRetryMiddleware(RetryMiddleware):
    """Custom retry middleware with exponential backoff."""
    
    def process_response(self, request, response, spider):
        if response.status in self.retry_http_codes:
            reason = response_status_message(response.status)
            
            # Log retry attempt
            retry_times = request.meta.get('retry_times', 0) + 1
            logger.warning(
                f"Retrying {request.url} (attempt {retry_times}): {reason}"
            )
            
            # Exponential backoff
            request.meta['download_delay'] = 2 ** retry_times
            
            return self._retry(request, reason, spider) or response
        
        return response
```

### src/scraper/spiders/__init__.py
```python
# Spider modules initialization
```

### src/scraper/spiders/base_spider.py
```python
import scrapy
from abc import ABC, abstractmethod
import logging
from datetime import datetime
from urllib.parse import urljoin

logger = logging.getLogger(__name__)

class BaseJobSpider(scrapy.Spider, ABC):
    """Base spider class for job portals."""
    
    def __init__(self, country=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.country = country
        self.total_scraped = 0
        self.start_time = datetime.now()
    
    @abstractmethod
    def parse_job(self, response):
        """Parse individual job posting. Must be implemented by subclasses."""
        pass
    
    def extract_text(self, selector, xpath_or_css, method='xpath'):
        """Safely extract text from selector."""
        try:
            if method == 'xpath':
                texts = selector.xpath(xpath_or_css).getall()
            else:
                texts = selector.css(xpath_or_css).getall()
            
            # Join and clean text
            text = ' '.join(texts)
            return ' '.join(text.split()) if text else None
        except Exception as e:
            logger.error(f"Error extracting text: {e}")
            return None
    
    def build_absolute_url(self, response, relative_url):
        """Build absolute URL from relative URL."""
        return urljoin(response.url, relative_url)
    
    def log_progress(self):
        """Log scraping progress."""
        elapsed = (datetime.now() - self.start_time).total_seconds()
        rate = self.total_scraped / elapsed if elapsed > 0 else 0
        
        logger.info(
            f"Spider {self.name} - Country: {self.country} - "
            f"Scraped: {self.total_scraped} - Rate: {rate:.2f} jobs/sec"
        )
```

### src/scraper/spiders/computrabajo_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import re
from urllib.parse import urlencode
import logging

logger = logging.getLogger(__name__)

class ComputrabajoSpider(BaseJobSpider):
    name = 'computrabajo'
    allowed_domains = ['computrabajo.com', 'computrabajo.com.co', 
                      'computrabajo.com.mx', 'computrabajo.com.ar']
    
    # URL patterns by country
    country_urls = {
        'CO': 'https://www.computrabajo.com.co',
        'MX': 'https://www.computrabajo.com.mx',
        'AR': 'https://www.computrabajo.com.ar'
    }
    
    # Tech-related search terms
    tech_keywords = [
        'desarrollador', 'developer', 'programador', 'software',
        'data', 'analyst', 'engineer', 'fullstack', 'frontend',
        'backend', 'devops', 'cloud', 'mobile', 'web'
    ]
    
    def start_requests(self):
        if not self.country or self.country not in self.country_urls:
            raise ValueError(f"Invalid country: {self.country}")
        
        base_url = self.country_urls[self.country]
        
        # Generate search URLs for tech keywords
        for keyword in self.tech_keywords:
            search_url = f"{base_url}/trabajo-de-{keyword}"
            yield Request(
                url=search_url,
                callback=self.parse_search_results,
                meta={'keyword': keyword, 'page': 1}
            )
    
    def parse_search_results(self, response):
        """Parse search results page."""
        # Extract job listings
        job_cards = response.css('article.js-o-card')
        
        for card in job_cards:
            # Extract job URL
            job_url = card.css('a.js-o-card__link::attr(href)').get()
            if job_url:
                absolute_url = self.build_absolute_url(response, job_url)
                yield Request(
                    url=absolute_url,
                    callback=self.parse_job,
                    meta={'search_keyword': response.meta.get('keyword')}
                )
        
        # Check for next page
        current_page = response.meta.get('page', 1)
        next_page_link = response.css('a[aria-label="Siguiente"]::attr(href)').get()
        
        if next_page_link and current_page < 10:  # Limit to 10 pages per keyword
            next_url = self.build_absolute_url(response, next_page_link)
            yield Request(
                url=next_url,
                callback=self.parse_search_results,
                meta={
                    'keyword': response.meta.get('keyword'),
                    'page': current_page + 1
                }
            )
    
    def parse_job(self, response):
        """Parse individual job posting."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'computrabajo'
        item['country'] = self.country
        item['url'] = response.url
        
        # Title
        item['title'] = self.extract_text(
            response, 
            '//h1[@class="fwB fs24"]//text()',
            'xpath'
        )
        
        # Company
        item['company'] = self.extract_text(
            response,
            '//a[@class="dIB fs16 js-o-link"]//text()',
            'xpath'
        )
        
        # Location
        location_parts = response.xpath(
            '//div[@class="fs16 fc_base mt5"]//span//text()'
        ).getall()
        item['location'] = ', '.join(location_parts) if location_parts else None
        
        # Description and requirements
        description_sections = response.xpath(
            '//div[@class="mbB"]//p//text() | //div[@class="mbB"]//li//text()'
        ).getall()
        
        full_text = ' '.join(description_sections)
        
        # Try to separate requirements
        req_pattern = r'(?:requisitos|requerimientos|requirements|experiencia|conocimientos):(.*?)(?:funciones|responsabilidades|beneficios|$)'
        req_match = re.search(req_pattern, full_text, re.IGNORECASE | re.DOTALL)
        
        if req_match:
            item['requirements'] = req_match.group(1).strip()
            item['description'] = full_text.replace(req_match.group(0), '').strip()
        else:
            item['description'] = full_text
            item['requirements'] = None
        
        # Salary
        salary_text = self.extract_text(
            response,
            '//span[@class="fs16 fc_aux"]//text()[contains(., "$")]',
            'xpath'
        )
        item['salary_raw'] = salary_text
        
        # Contract type
        contract_info = response.xpath(
            '//span[@class="fs13 fc_aux"]//text()'
        ).getall()
        
        for info in contract_info:
            info_lower = info.lower()
            if any(term in info_lower for term in ['tiempo completo', 'full time', 'part time']):
                item['contract_type'] = info
            elif any(term in info_lower for term in ['remoto', 'presencial', 'hibrido']):
                item['remote_type'] = info
        
        # Posted date
        date_text = self.extract_text(
            response,
            '//span[@class="fs13 fc_aux"][contains(text(), "Publicado")]//text()',
            'xpath'
        )
        if date_text:
            item['posted_date'] = date_text.replace('Publicado', '').strip()
        
        # Raw HTML (for debugging/reprocessing)
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

### src/scraper/spiders/bumeran_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import json
import re
import logging

logger = logging.getLogger(__name__)

class BumeranSpider(BaseJobSpider):
    name = 'bumeran'
    allowed_domains = ['bumeran.com', 'bumeran.com.mx', 'bumeran.com.ar']
    
    # URL patterns by country
    country_urls = {
        'MX': 'https://www.bumeran.com.mx',
        'AR': 'https://www.bumeran.com.ar'
    }
    
    # Tech categories
    tech_categories = [
        'informatica-telecomunicaciones',
        'tecnologia-sistemas',
        'desarrollo-programacion'
    ]
    
    def start_requests(self):
        if self.country not in self.country_urls:
            raise ValueError(f"Bumeran not available for country: {self.country}")
        
        base_url = self.country_urls[self.country]
        
        # Generate category URLs
        for category in self.tech_categories:
            category_url = f"{base_url}/empleos-{category}.html"
            yield Request(
                url=category_url,
                callback=self.parse_category,
                meta={'category': category, 'page': 1}
            )
    
    def parse_category(self, response):
        """Parse category listing page."""
        # Check if page uses React/JSON data
        scripts = response.xpath('//script[contains(text(), "__INITIAL_STATE__")]/text()').getall()
        
        if scripts:
            # Extract JSON data from script
            for script in scripts:
                match = re.search(r'__INITIAL_STATE__\s*=\s*({.*?});', script, re.DOTALL)
                if match:
                    try:
                        data = json.loads(match.group(1))
                        jobs = self.extract_jobs_from_json(data)
                        
                        for job in jobs:
                            yield Request(
                                url=job['url'],
                                callback=self.parse_job,
                                meta={'job_data': job}
                            )
                    except json.JSONDecodeError:
                        logger.error("Failed to parse JSON data")
        else:
            # Fallback to HTML parsing
            job_links = response.css('div.Card__CardContentWrapper a::attr(href)').getall()
            
            for link in job_links:
                absolute_url = self.build_absolute_url(response, link)
                yield Request(url=absolute_url, callback=self.parse_job)
        
        # Handle pagination
        current_page = response.meta.get('page', 1)
        if current_page < 10:  # Limit pages
            next_page = current_page + 1
            next_url = response.url.replace(
                f'.html', 
                f'-pagina-{next_page}.html'
            )
            yield Request(
                url=next_url,
                callback=self.parse_category,
                meta={
                    'category': response.meta.get('category'),
                    'page': next_page
                }
            )
    
    def extract_jobs_from_json(self, data):
        """Extract job data from JSON structure."""
        jobs = []
        
        # Navigate through possible JSON structures
        try:
            if 'results' in data:
                job_list = data['results'].get('jobs', [])
            elif 'jobs' in data:
                job_list = data['jobs']
            else:
                return jobs
            
            for job in job_list:
                job_info = {
                    'url': job.get('url', ''),
                    'title': job.get('title', ''),
                    'company': job.get('company', {}).get('name', ''),
                    'location': job.get('location', '')
                }
                if job_info['url']:
                    jobs.append(job_info)
        except Exception as e:
            logger.error(f"Error extracting jobs from JSON: {e}")
        
        return jobs
    
    def parse_job(self, response):
        """Parse individual job posting."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'bumeran'
        item['country'] = self.country
        item['url'] = response.url
        
        # Try to get pre-parsed data
        job_data = response.meta.get('job_data', {})
        
        # Title
        item['title'] = job_data.get('title') or self.extract_text(
            response,
            'h1[class*="Title"]::text',
            'css'
        )
        
        # Company
        item['company'] = job_data.get('company') or self.extract_text(
            response,
            'h2[class*="Company"]::text',
            'css'
        )
        
        # Location
        item['location'] = job_data.get('location') or self.extract_text(
            response,
            'span[class*="Location"]::text',
            'css'
        )
        
        # Description
        description_selectors = [
            'div[class*="Description"]',
            'div.detalle-aviso',
            'div#description'
        ]
        
        for selector in description_selectors:
            desc_elements = response.css(f'{selector} ::text').getall()
            if desc_elements:
                item['description'] = ' '.join(desc_elements)
                break
        
        # Requirements - often within description
        if item.get('description'):
            req_pattern = r'(?:requisitos|requerimientos|experiencia|competencias):(.*?)(?:beneficios|funciones|responsabilidades|$)'
            req_match = re.search(req_pattern, item['description'], re.IGNORECASE | re.DOTALL)
            
            if req_match:
                item['requirements'] = req_match.group(1).strip()
        
        # Salary
        salary_selectors = [
            'span[class*="Salary"]::text',
            'div[class*="salary"]::text',
            'span:contains("$")::text'
        ]
        
        for selector in salary_selectors:
            salary = response.css(selector).get()
            if salary and '$' in salary:
                item['salary_raw'] = salary
                break
        
        # Contract type and remote
        tags = response.css('span[class*="Tag"]::text').getall()
        for tag in tags:
            tag_lower = tag.lower()
            if any(term in tag_lower for term in ['tiempo completo', 'part time', 'freelance']):
                item['contract_type'] = tag
            elif any(term in tag_lower for term in ['remoto', 'presencial', 'hibrido']):
                item['remote_type'] = tag
        
        # Posted date
        date_text = response.css('span[class*="Date"]::text').get()
        if date_text:
            item['posted_date'] = date_text
        
        # Raw HTML
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

### src/scraper/spiders/elempleo_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import re
import logging
from urllib.parse import urljoin, urlparse, parse_qs

logger = logging.getLogger(__name__)

class ElempleoSpider(BaseJobSpider):
    name = 'elempleo'
    allowed_domains = ['elempleo.com']
    
    # Only available for Colombia
    country_urls = {
        'CO': 'https://www.elempleo.com'
    }
    
    # Tech-related categories in elempleo
    tech_categories = {
        'tecnologia': '1100',
        'sistemas': '1100',
        'informatica': '1100'
    }
    
    def start_requests(self):
        if self.country != 'CO':
            raise ValueError("elempleo.com is only available for Colombia (CO)")
        
        base_url = self.country_urls['CO']
        
        # Search URLs for technology jobs
        search_base = f"{base_url}/colombia/empleos"
        
        # Generate search requests
        for category_name, category_id in self.tech_categories.items():
            search_params = {
                'categoria': category_id,
                'pagina': 1
            }
            
            search_url = f"{search_base}?categoria={category_id}"
            
            yield Request(
                url=search_url,
                callback=self.parse_search_results,
                meta={
                    'category': category_name,
                    'page': 1
                }
            )
    
    def parse_search_results(self, response):
        """Parse search results from elempleo."""
        # Extract job cards
        job_cards = response.css('div.result-item')
        
        if not job_cards:
            # Try alternative selectors
            job_cards = response.css('article.js-offer')
        
        for card in job_cards:
            # Extract job URL
            job_link = card.css('a.js-offer-title::attr(href)').get()
            if not job_link:
                job_link = card.css('h2 a::attr(href)').get()
            
            if job_link:
                # Handle both relative and absolute URLs
                if not job_link.startswith('http'):
                    job_link = urljoin(response.url, job_link)
                
                # Extract basic info from card
                card_data = {
                    'title': card.css('h2 a::text').get(),
                    'company': card.css('span.info-company-name::text').get(),
                    'location': card.css('span.info-city::text').get()
                }
                
                yield Request(
                    url=job_link,
                    callback=self.parse_job,
                    meta={'card_data': card_data}
                )
        
        # Handle pagination
        current_page = response.meta.get('page', 1)
        next_page_link = response.css('a.js-btn-next::attr(href)').get()
        
        if not next_page_link:
            # Alternative pagination
            pagination_links = response.css('ul.pagination a::attr(href)').getall()
            for link in pagination_links:
                if f'pagina={current_page + 1}' in link:
                    next_page_link = link
                    break
        
        if next_page_link and current_page < 10:  # Limit to 10 pages
            next_url = urljoin(response.url, next_page_link)
            yield Request(
                url=next_url,
                callback=self.parse_search_results,
                meta={
                    'category': response.meta.get('category'),
                    'page': current_page + 1
                }
            )
    
    def parse_job(self, response):
        """Parse individual job posting from elempleo."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'elempleo'
        item['country'] = 'CO'
        item['url'] = response.url
        
        # Get card data if available
        card_data = response.meta.get('card_data', {})
        
        # Title
        item['title'] = self.extract_text(
            response,
            'h1.offer-title::text',
            'css'
        ) or card_data.get('title')
        
        # Company
        item['company'] = self.extract_text(
            response,
            'div.company-name a::text',
            'css'
        ) or self.extract_text(
            response,
            'span.offer-company::text',
            'css'
        ) or card_data.get('company')
        
        # Location
        location_parts = response.css('div.offer-location span::text').getall()
        if location_parts:
            item['location'] = ', '.join(location_parts)
        else:
            item['location'] = card_data.get('location')
        
        # Main content sections
        content_sections = response.css('div.offer-description')
        
        # Description
        description_html = content_sections.css('div#description').get()
        if description_html:
            # Clean HTML and extract text
            desc_text = re.sub(r'<[^>]+>', ' ', description_html)
            item['description'] = ' '.join(desc_text.split())
        
        # Requirements
        requirements_section = content_sections.css('div#requirements')
        if requirements_section:
            req_items = requirements_section.css('li::text').getall()
            if req_items:
                item['requirements'] = ' '.join(req_items)
            else:
                req_text = requirements_section.css('::text').getall()
                item['requirements'] = ' '.join(req_text)
        
        # If requirements not in separate section, try to extract from description
        if not item.get('requirements') and item.get('description'):
            req_pattern = r'(?:requisitos|perfil|experiencia|conocimientos):(.*?)(?:funciones|responsabilidades|ofrecemos|$)'
            req_match = re.search(req_pattern, item['description'], re.IGNORECASE | re.DOTALL)
            if req_match:
                item['requirements'] = req_match.group(1).strip()
        
        # Salary
        salary_element = response.css('span.offer-salary::text').get()
        if salary_element:
            item['salary_raw'] = salary_element
        else:
            # Look for salary in description
            salary_pattern = r'\$[\d.,]+ (?:millones|COP|pesos)'
            salary_match = re.search(salary_pattern, item.get('description', ''))
            if salary_match:
                item['salary_raw'] = salary_match.group(0)
        
        # Contract type
        contract_info = response.css('div.offer-info span::text').getall()
        for info in contract_info:
            info_lower = info.lower()
            if any(term in info_lower for term in ['contrato', 'tiempo completo', 'medio tiempo']):
                item['contract_type'] = info
            elif any(term in info_lower for term in ['remoto', 'presencial', 'teletrabajo']):
                item['remote_type'] = info
        
        # Posted date
        date_element = response.css('span.offer-date::text').get()
        if date_element:
            item['posted_date'] = date_element
        else:
            # Try to extract from meta tags
            date_meta = response.css('meta[property="article:published_time"]::attr(content)').get()
            if date_meta:
                item['posted_date'] = date_meta.split('T')[0]
        
        # Additional fields from structured data
        try:
            # Check for JSON-LD structured data
            json_ld = response.css('script[type="application/ld+json"]::text').get()
            if json_ld:
                import json
                data = json.loads(json_ld)
                
                # Extract additional info if available
                if isinstance(data, dict):
                    if 'title' in data and not item.get('title'):
                        item['title'] = data['title']
                    if 'hiringOrganization' in data and not item.get('company'):
                        item['company'] = data['hiringOrganization'].get('name')
                    if 'jobLocation' in data and not item.get('location'):
                        location = data['jobLocation']
                        if isinstance(location, dict):
                            item['location'] = location.get('address', {}).get('addressLocality')
        except Exception as e:
            logger.debug(f"Could not parse structured data: {e}")
        
        # Raw HTML
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

---

## 5. Extractor Module Files

### src/extractor/__init__.py
```python
from .pipeline import ExtractionPipeline
from .ner_extractor import NERExtractor
from .regex_patterns import RegexExtractor
from .esco_matcher import ESCOMatcher

__all__ = ['ExtractionPipeline', 'NERExtractor', 'RegexExtractor', 'ESCOMatcher']
```

### src/extractor/ner_extractor.py
```python
import spacy
from spacy.tokens import Doc, Span
from typing import List, Dict, Tuple, Optional
import logging
import os
from config.settings import get_settings

logger = logging.getLogger(__name__)

class NERExtractor:
    """Extract skills using Named Entity Recognition."""
    
    def __init__(self, model_path: Optional[str] = None):
        self.settings = get_settings()
        
        # Load spaCy model
        if model_path and os.path.exists(model_path):
            self.nlp = spacy.load(model_path)
            logger.info(f"Loaded custom NER model from {model_path}")
        else:
            # Load default Spanish model
            try:
                self.nlp = spacy.load("es_core_news_lg")
                logger.info("Loaded default Spanish model")
            except:
                logger.warning("Spanish model not found, downloading...")
                os.system("python -m spacy download es_core_news_lg")
                self.nlp = spacy.load("es_core_news_lg")
        
        # Add custom pipeline components
        self._add_tech_entity_ruler()
    
    def _add_tech_entity_ruler(self):
        """Add rule-based entity recognition for tech terms."""
        ruler = self.nlp.add_pipe("entity_ruler", before="ner")
        
        # Define patterns for common tech skills
        patterns = [
            # Programming languages
            {"label": "SKILL", "pattern": [{"LOWER": "python"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "java"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "javascript"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "typescript"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "c++"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "c#"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "php"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "ruby"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "go"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "golang"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "rust"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "kotlin"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "swift"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "r"}]},
            
            # Frameworks
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "react"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "react"}, {"LOWER": "native"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "angular"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "vue"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "vue"}, {"LOWER": "js"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "django"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "flask"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "spring"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "spring"}, {"LOWER": "boot"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "node"}, {"LOWER": "js"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "nodejs"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "express"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": ".net"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "laravel"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "rails"}]},
            
            # Databases
            {"label": "DATABASE", "pattern": [{"LOWER": "mysql"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "postgresql"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "postgres"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "mongodb"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "redis"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "elasticsearch"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "oracle"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "sql"}, {"LOWER": "server"}]},
            
            # Cloud & DevOps
            {"label": "PLATFORM", "pattern": [{"LOWER": "aws"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "amazon"}, {"LOWER": "web"}, {"LOWER": "services"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "azure"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "gcp"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "google"}, {"LOWER": "cloud"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "docker"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "kubernetes"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "jenkins"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "git"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "github"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "gitlab"}]},
            
            # Data & ML
            {"label": "SKILL", "pattern": [{"LOWER": "machine"}, {"LOWER": "learning"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "deep"}, {"LOWER": "learning"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "data"}, {"LOWER": "science"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "big"}, {"LOWER": "data"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "tensorflow"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "pytorch"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "scikit"}, {"LOWER": "learn"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "pandas"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "numpy"}]},
            
            # Methodologies
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "agile"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "scrum"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "kanban"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "devops"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "ci"}, {"LOWER": "cd"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "tdd"}]},
        ]
        
        # Add Spanish variations
        spanish_patterns = [
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "web"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "movil"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "móvil"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "base"}, {"LOWER": "de"}, {"LOWER": "datos"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "inteligencia"}, {"LOWER": "artificial"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "aprendizaje"}, {"LOWER": "automatico"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "aprendizaje"}, {"LOWER": "automático"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "ciencia"}, {"LOWER": "de"}, {"LOWER": "datos"}]},
        ]
        
        ruler.add_patterns(patterns + spanish_patterns)
    
    def extract(self, text: str, source_section: str = "unknown") -> List[Dict[str, any]]:
        """Extract skills from text using NER.
        
        Args:
            text: Text to extract skills from
            source_section: Section of job posting (title, description, requirements)
            
        Returns:
            List of extracted skills with metadata
        """
        if not text:
            return []
        
        # Process text with spaCy
        doc = self.nlp(text)
        
        extracted_skills = []
        seen_skills = set()
        
        # Extract entities
        for ent in doc.ents:
            if ent.label_ in ["SKILL", "FRAMEWORK", "DATABASE", "PLATFORM", "TOOL", "METHODOLOGY"]:
                skill_text = ent.text.lower().strip()
                
                # Skip if already seen
                if skill_text in seen_skills:
                    continue
                
                seen_skills.add(skill_text)
                
                extracted_skills.append({
                    "skill_text": skill_text,
                    "skill_type": "explicit",
                    "extraction_method": "ner",
                    "entity_label": ent.label_,
                    "confidence_score": 0.9,  # High confidence for NER
                    "source_section": source_section,
                    "span_start": ent.start_char,
                    "span_end": ent.end_char,
                    "context": text[max(0, ent.start_char-50):ent.end_char+50]
                })
        
        logger.debug(f"NER extracted {len(extracted_skills)} skills from {source_section}")
        
        return extracted_skills
    
    def extract_from_job(self, job_data: Dict[str, str]) -> List[Dict[str, any]]:
        """Extract skills from all sections of a job posting.
        
        Args:
            job_data: Dictionary with job fields (title, description, requirements)
            
        Returns:
            List of all extracted skills
        """
        all_skills = []
        
        # Extract from title
        if job_data.get('title'):
            title_skills = self.extract(job_data['title'], 'title')
            all_skills.extend(title_skills)
        
        # Extract from description
        if job_data.get('description'):
            desc_skills = self.extract(job_data['description'], 'description')
            all_skills.extend(desc_skills)
        
        # Extract from requirements
        if job_data.get('requirements'):
            req_skills = self.extract(job_data['requirements'], 'requirements')
            all_skills.extend(req_skills)
        
        return all_skills
```

### src/extractor/regex_patterns.py
```python
import re
from typing import List, Dict, Tuple
import logging

logger = logging.getLogger(__name__)

class RegexExtractor:
    """Extract skills using regular expressions."""
    
    def __init__(self):
        # Define regex patterns for skill extraction
        self.patterns = self._build_patterns()
    
    def _build_patterns(self) -> List[Tuple[str, re.Pattern, str]]:
        """Build regex patterns for skill extraction.
        
        Returns:
            List of tuples (pattern_name, compiled_regex, skill_type)
        """
        patterns = []
        
        # Experience patterns in Spanish
        experience_patterns = [
            (
                "experiencia_en",
                re.compile(
                    r"experiencia\s+(?:en|con)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "conocimientos_de",
                re.compile(
                    r"conocimientos?\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "manejo_de",
                re.compile(
                    r"manejo\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "tool"
            ),
            (
                "dominio_de",
                re.compile(
                    r"dominio\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "desarrollo_en",
                re.compile(
                    r"desarrollo\s+(?:en|con)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
        ]
        
        # Required skills patterns
        required_patterns = [
            (
                "requerimos",
                re.compile(
                    r"(?:requerimos|buscamos|necesitamos)\s+(?:personas?\s+con\s+)?(?:conocimientos?\s+en\s+)?([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+para\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "indispensable",
                re.compile(
                    r"(?:indispensable|fundamental|esencial)\s+(?:contar\s+con\s+)?(?:conocimientos?\s+en\s+)?([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
        ]
        
        # Technology stack patterns
        tech_stack_patterns = [
            (
                "tecnologias",
                re.compile(
                    r"(?:tecnologías?|herramientas?|lenguajes?)\s*:\s*([A-Za-z0-9\+\#\.\s,/\-\(\)]+?)(?:\n|$)",
                    re.IGNORECASE
                ),
                "mixed"
            ),
            (
                "stack_tecnologico",
                re.compile(
                    r"stack\s+(?:tecnológico|tech)\s*:\s*([A-Za-z0-9\+\#\.\s,/\-\(\)]+?)(?:\n|$)",
                    re.IGNORECASE
                ),
                "mixed"
            ),
        ]
        
        # List patterns (bullet points, numbered lists)
        list_patterns = [
            (
                "bullet_skills",
                re.compile(
                    r"(?:^|\n)\s*[\-\*\•]\s*([A-Za-z0-9\+\#\.\s]+?)(?:\n|$)",
                    re.MULTILINE
                ),
                "skill"
            ),
            (
                "numbered_skills",
                re.compile(
                    r"(?:^|\n)\s*\d+[\.\)]\s*([A-Za-z0-9\+\#\.\s]+?)(?:\n|$)",
                    re.MULTILINE
                ),
                "skill"
            ),
        ]
        
        # Certification patterns
        cert_patterns = [
            (
                "certificacion",
                re.compile(
                    r"(?:certificación|certificado)\s+(?:en\s+)?([A-Za-z0-9\+\#\.\s\-]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "certification"
            ),
        ]
        
        # Years of experience patterns
        experience_years_patterns = [
            (
                "años_experiencia",
                re.compile(
                    r"(\d+)\s*\+?\s*años?\s+(?:de\s+)?experiencia\s+(?:en\s+)?([A-Za-z0-9\+\#\.\s]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "skill_with_years"
            ),
        ]
        
        # Combine all patterns
        patterns.extend(experience_patterns)
        patterns.extend(required_patterns)
        patterns.extend(tech_stack_patterns)
        patterns.extend(list_patterns)
        patterns.extend(cert_patterns)
        patterns.extend(experience_years_patterns)
        
        return patterns
    
    def extract(self, text: str, source_section: str = "unknown") -> List[Dict[str, any]]:
        """Extract skills using regex patterns.
        
        Args:
            text: Text to extract skills from
            source_section: Section of job posting
            
        Returns:
            List of extracted skills with metadata
        """
        if not text:
            return []
        
        extracted_skills = []
        seen_skills = set()
        
        for pattern_name, regex, skill_type in self.patterns:
            matches = regex.finditer(text)
            
            for match in matches:
                if skill_type == "skill_with_years":
                    # Special handling for years of experience
                    years = match.group(1)
                    skill_text = match.group(2).strip().lower()
                    
                    if skill_text and skill_text not in seen_skills:
                        seen_skills.add(skill_text)
                        extracted_skills.append({
                            "skill_text": skill_text,
                            "skill_type": "explicit",
                            "extraction_method": "regex",
                            "pattern_name": pattern_name,
                            "confidence_score": 0.8,
                            "source_section": source_section,
                            "span_start": match.start(2),
                            "span_end": match.end(2),
                            "years_required": int(years),
                            "context": text[max(0, match.start()-30):match.end()+30]
                        })
                else:
                    # Normal skill extraction
                    skill_text = match.group(1).strip().lower()
                    
                    # Clean up extracted text
                    skill_text = self._clean_skill_text(skill_text)
                    
                    if skill_text and len(skill_text) > 1 and skill_text not in seen_skills:
                        # Additional validation
                        if self._is_valid_skill(skill_text):
                            seen_skills.add(skill_text)
                            
                            extracted_skills.append({
                                "skill_text": skill_text,
                                "skill_type": "explicit",
                                "extraction_method": "regex",
                                "pattern_name": pattern_name,
                                "confidence_score": 0.7,  # Lower than NER
                                "source_section": source_section,
                                "span_start": match.start(1),
                                "span_end": match.end(1),
                                "context": text[max(0, match.start()-30):match.end()+30]
                            })
        
        # Handle comma-separated lists within extracted skills
        expanded_skills = []
        for skill in extracted_skills:
            if ',' in skill['skill_text'] or ' y ' in skill['skill_text']:
                # Split and create individual skills
                parts = re.split(r'[,\s]+y\s+|,\s*', skill['skill_text'])
                for part in parts:
                    part = part.strip()
                    if part and self._is_valid_skill(part):
                        new_skill = skill.copy()
                        new_skill['skill_text'] = part
                        expanded_skills.append(new_skill)
            else:
                expanded_skills.append(skill)
        
        logger.debug(f"Regex extracted {len(expanded_skills)} skills from {source_section}")
        
        return expanded_skills
    
    def _clean_skill_text(self, text: str) -> str:
        """Clean extracted skill text.
        
        Args:
            text: Raw extracted text
            
        Returns:
            Cleaned skill text
        """
        # Remove common stop words at the beginning/end
        stop_words = [
            'el', 'la', 'los', 'las', 'un', 'una', 'de', 'del', 'al',
            'y', 'o', 'con', 'para', 'por', 'en', 'a'
        ]
        
        words = text.split()
        
        # Remove stop words from beginning
        while words and words[0].lower() in stop_words:
            words.pop(0)
        
        # Remove stop words from end
        while words and words[-1].lower() in stop_words:
            words.pop()
        
        cleaned = ' '.join(words)
        
        # Remove extra spaces and punctuation
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = re.sub(r'[^\w\s\+\#\.\-/]', '', cleaned)
        
        return cleaned.strip()
    
    def _is_valid_skill(self, skill_text: str) -> bool:
        """Validate if extracted text is likely a valid skill.
        
        Args:
            skill_text: Text to validate
            
        Returns:
            True if valid skill, False otherwise
        """
        # Check minimum length
        if len(skill_text) < 2:
            return False
        
        # Check if it's just numbers
        if skill_text.isdigit():
            return False
        
        # Check against blacklist of common false positives
        blacklist = [
            'años', 'año', 'experiencia', 'conocimiento', 'manejo',
            'desarrollo', 'persona', 'profesional', 'trabajo',
            'empresa', 'cliente', 'proyecto', 'equipo', 'area',
            'sistemas', 'tecnologia', 'informatica'  # Too generic
        ]
        
        if skill_text.lower() in blacklist:
            return False
        
        # Must contain at least one letter
        if not any(c.isalpha() for c in skill_text):
            return False
        
        return True
    
    def extract_from_job(self, job_data: Dict[str, str]) -> List[Dict[str, any]]:
        """Extract skills from all sections of a job posting.
        
        Args:
            job_data: Dictionary with job fields
            
        Returns:
            List of all extracted skills
        """
        all_skills = []
        
        # Process each section
        for section in ['title', 'description', 'requirements']:
            if job_data.get(section):
                section_skills = self.extract(job_data[section], section)
                all_skills.extend(section_skills)
        
        return all_skills
```

### src/extractor/esco_matcher.py
```python
import json
import logging
from typing import List, Dict, Optional, Set
from fuzzywuzzy import fuzz, process
import requests
from pathlib import Path
import yaml

logger = logging.getLogger(__name__)

class ESCOMatcher:
    """Match extracted skills to ESCO taxonomy."""
    
    def __init__(self, config_path: str = "config/esco_config.yaml"):
        self.config = self._load_config(config_path)
        self.skills_cache = {}
        self.local_mappings = self.config.get('tech_mappings', {})
        
        # Load local ESCO data if available
        self.local_esco_data = self._load_local_esco_data()
    
    def _load_config(self, config_path: str) -> dict:
        """Load ESCO configuration."""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config['esco']
        except Exception as e:
            logger.error(f"Failed to load ESCO config: {e}")
            return {}
    
    def _load_local_esco_data(self) -> Dict[str, Dict]:
        """Load local ESCO data files if available."""
        esco_data = {}
        
        # Try to load skills data
        skills_path = Path("data/esco/skills_es.csv")
        if skills_path.exists():
            try:
                import pandas as pd
                df = pd.read_csv(skills_path)
                
                for _, row in df.iterrows():
                    skill_uri = row.get('conceptUri', '')
                    esco_data[skill_uri] = {
                        'preferredLabel': row.get('preferredLabel', ''),
                        'altLabels': row.get('altLabels', '').split('|') if row.get('altLabels') else [],
                        'description': row.get('description', ''),
                        'skillType': row.get('skillType', '')
                    }
                
                logger.info(f"Loaded {len(esco_data)} ESCO skills from local file")
            except Exception as e:
                logger.error(f"Failed to load local ESCO data: {e}")
        
        return esco_data
    
    def match_skill(self, skill_text: str, threshold: float = 0.8) -> Optional[Dict[str, any]]:
        """Match a skill to ESCO taxonomy.
        
        Args:
            skill_text: Skill text to match
            threshold: Minimum similarity threshold (0-1)
            
        Returns:
            ESCO match data or None
        """
        skill_lower = skill_text.lower().strip()
        
        # Check direct mappings first
        if skill_lower in self.local_mappings:
            esco_uri = self.local_mappings[skill_lower]
            
            # Get details from local data or cache
            if esco_uri in self.local_esco_data:
                return {
                    'esco_uri': esco_uri,
                    'esco_preferred_label': self.local_esco_data[esco_uri]['preferredLabel'],
                    'match_type': 'direct',
                    'match_score': 1.0
                }
        
        # Try fuzzy matching against local data
        if self.local_esco_data:
            best_match = self._fuzzy_match_local(skill_text, threshold)
            if best_match:
                return best_match
        
        # Try API lookup (if configured and no local match)
        if self.config.get('base_url'):
            api_match = self._api_lookup(skill_text)
            if api_match:
                return api_match
        
        return None
    
    def _fuzzy_match_local(self, skill_text: str, threshold: float) -> Optional[Dict[str, any]]:
        """Fuzzy match against local ESCO data.
        
        Args:
            skill_text: Skill to match
            threshold: Minimum score threshold
            
        Returns:
            Best match or None
        """
        # Collect all labels for matching
        all_labels = []
        for uri, data in self.local_esco_data.items():
            # Add preferred label
            all_labels.append((data['preferredLabel'].lower(), uri, 'preferred'))
            
            # Add alternative labels
            for alt_label in data.get('altLabels', []):
                if alt_label:
                    all_labels.append((alt_label.lower(), uri, 'alternative'))
        
        # Find best match
        if all_labels:
            # Use token sort ratio for better matching of multi-word skills
            matches = process.extract(
                skill_text.lower(),
                [label[0] for label in all_labels],
                scorer=fuzz.token_sort_ratio,
                limit=3
            )
            
            for match_text, score in matches:
                if score >= threshold * 100:  # fuzzywuzzy uses 0-100 scale
                    # Find the corresponding URI
                    for label_text, uri, label_type in all_labels:
                        if label_text == match_text:
                            return {
                                'esco_uri': uri,
                                'esco_preferred_label': self.local_esco_data[uri]['preferredLabel'],
                                'match_type': f'fuzzy_{label_type}',
                                'match_score': score / 100.0,
                                'matched_text': match_text
                            }
        
        return None
    
    def _api_lookup(self, skill_text: str) -> Optional[Dict[str, any]]:
        """Look up skill using ESCO API.
        
        Args:
            skill_text: Skill to look up
            
        Returns:
            API match data or None
        """
        try:
            # Check cache first
            if skill_text in self.skills_cache:
                return self.skills_cache[skill_text]
            
            # Prepare API request
            api_url = f"{self.config['base_url']}{self.config['endpoints']['search']}"
            
            params = {
                'text': skill_text,
                'language': self.config['language']['primary'],
                'type': 'skill',
                'limit': 5
            }
            
            headers = {
                'Accept': 'application/json',
                'Accept-Language': self.config['language']['primary']
            }
            
            # Make request
            response = requests.get(api_url, params=params, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                if data.get('results'):
                    # Get first result
                    result = data['results'][0]
                    
                    match_data = {
                        'esco_uri': result.get('uri', ''),
                        'esco_preferred_label': result.get('preferredLabel', {}).get(
                            self.config['language']['primary'],
                            result.get('preferredLabel', {}).get('en', '')
                        ),
                        'match_type': 'api_search',
                        'match_score': result.get('score', 0.0)
                    }
                    
                    # Cache the result
                    self.skills_cache[skill_text] = match_data
                    
                    return match_data
            
        except Exception as e:
            logger.error(f"ESCO API lookup failed for '{skill_text}': {e}")
        
        return None
    
    def match_skills_batch(self, skills: List[str]) -> Dict[str, Optional[Dict]]:
        """Match multiple skills to ESCO taxonomy.
        
        Args:
            skills: List of skill texts
            
        Returns:
            Dictionary mapping skill text to ESCO match data
        """
        results = {}
        
        for skill in skills:
            match = self.match_skill(skill)
            results[skill] = match
        
        # Log statistics
        matched = sum(1 for v in results.values() if v is not None)
        logger.info(
            f"ESCO matching: {matched}/{len(skills)} skills matched "
            f"({matched/len(skills)*100:.1f}%)"
        )
        
        return results

### src/analyzer/dimension_reducer.py
```python
import logging
from typing import Tuple, Dict, Any, Optional
import numpy as np
from umap import UMAP
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import time

logger = logging.getLogger(__name__)

class DimensionReducer:
    """Reduce dimensionality of embeddings for visualization and clustering."""
    
    def __init__(self):
        self.reducers = {}
    
    def reduce_dimensions(self,
                         embeddings: np.ndarray,
                         method: str = 'umap',
                         n_components: int = 2,
                         **kwargs) -> Tuple[np.ndarray, Dict[str, Any]]:
        """Reduce dimensionality of embeddings.
        
        Args:
            embeddings: High-dimensional embeddings
            method: Reduction method ('umap', 'pca', 'tsne')
            n_components: Number of output dimensions
            **kwargs: Additional parameters for the method
            
        Returns:
            Tuple of (reduced embeddings, metadata)
        """
        logger.info(
            f"Reducing {embeddings.shape} to {n_components}D using {method}"
        )
        
        start_time = time.time()
        
        if method == 'umap':
            reduced, reducer = self._reduce_umap(embeddings, n_components, **kwargs)
        elif method == 'pca':
            reduced, reducer = self._reduce_pca(embeddings, n_components, **kwargs)
        elif method == 'tsne':
            reduced, reducer = self._reduce_tsne(embeddings, n_components, **kwargs)
        else:
            raise ValueError(f"Unknown reduction method: {method}")
        
        processing_time = time.time() - start_time
        
        # Store reducer for later use
        self.reducers[method] = reducer
        
        metadata = {
            'method': method,
            'n_components': n_components,
            'original_shape': embeddings.shape,
            'reduced_shape': reduced.shape,
            'processing_time': processing_time,
            'parameters': kwargs
        }
        
        logger.info(f"Dimension reduction complete in {processing_time:.2f}s")
        
        return reduced, metadata
    
    def _reduce_umap(self,
                    embeddings: np.ndarray,
                    n_components: int,
                    n_neighbors: int = 15,
                    min_dist: float = 0.1,
                    metric: str = 'cosine') -> Tuple[np.ndarray, UMAP]:
        """Reduce dimensions using UMAP.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            n_neighbors: UMAP n_neighbors parameter
            min_dist: UMAP min_dist parameter
            metric: Distance metric
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        reducer = UMAP(
            n_components=n_components,
            n_neighbors=n_neighbors,
            min_dist=min_dist,
            metric=metric,
            random_state=42,
            verbose=True
        )
        
        reduced = reducer.fit_transform(embeddings)
        
        return reduced, reducer
    
    def _reduce_pca(self,
                   embeddings: np.ndarray,
                   n_components: int) -> Tuple[np.ndarray, PCA]:
        """Reduce dimensions using PCA.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        reducer = PCA(n_components=n_components, random_state=42)
        reduced = reducer.fit_transform(embeddings)
        
        # Log explained variance
        if hasattr(reducer, 'explained_variance_ratio_'):
            total_variance = np.sum(reducer.explained_variance_ratio_)
            logger.info(f"PCA explained variance: {total_variance:.2%}")
        
        return reduced, reducer
    
    def _reduce_tsne(self,
                    embeddings: np.ndarray,
                    n_components: int,
                    perplexity: float = 30.0,
                    learning_rate: float = 200.0) -> Tuple[np.ndarray, TSNE]:
        """Reduce dimensions using t-SNE.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            perplexity: t-SNE perplexity parameter
            learning_rate: t-SNE learning rate
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        # For t-SNE, first reduce with PCA if dimensions > 50
        if embeddings.shape[1] > 50:
            logger.info("Pre-reducing with PCA for t-SNE")
            pca = PCA(n_components=50, random_state=42)
            embeddings = pca.fit_transform(embeddings)
        
        reducer = TSNE(
            n_components=n_components,
            perplexity=perplexity,
            learning_rate=learning_rate,
            random_state=42,
            verbose=1
        )
        
        reduced = reducer.fit_transform(embeddings)
        
        return reduced, reducer
    
    def transform_new_points(self,
                           embeddings: np.ndarray,
                           method: str = 'umap') -> np.ndarray:
        """Transform new points using existing reducer.
        
        Args:
            embeddings: New embeddings to transform
            method: Which reducer to use
            
        Returns:
            Transformed embeddings
        """
        if method not in self.reducers:
            raise ValueError(f"No {method} reducer available. Run reduce_dimensions first.")
        
        reducer = self.reducers[method]
        
        if method == 'tsne':
            logger.warning("t-SNE doesn't support transform. Returning original embeddings.")
            return embeddings
        
        return reducer.transform(embeddings)
    
    def create_embedding_map(self,
                           embeddings: np.ndarray,
                           labels: Optional[np.ndarray] = None,
                           skill_texts: Optional[list] = None) -> Dict[str, Any]:
        """Create a complete embedding map with 2D coordinates.
        
        Args:
            embeddings: Original embeddings
            labels: Cluster labels (optional)
            skill_texts: Skill names (optional)
            
        Returns:
            Dictionary with 2D coordinates and metadata
        """
        # Reduce to 2D
        coords_2d, metadata = self.reduce_dimensions(embeddings, method='umap', n_components=2)
        
        # Create map
        embedding_map = {
            'coordinates': coords_2d,
            'metadata': metadata
        }
        
        if labels is not None:
            embedding_map['labels'] = labels
        
        if skill_texts is not None:
            embedding_map['skills'] = skill_texts
        
        # Add statistics
        embedding_map['stats'] = {
            'x_range': (float(np.min(coords_2d[:, 0])), float(np.max(coords_2d[:, 0]))),
            'y_range': (float(np.min(coords_2d[:, 1])), float(np.max(coords_2d[:, 1]))),
            'center': (float(np.mean(coords_2d[:, 0])), float(np.mean(coords_2d[:, 1])))
        }
        
        return embedding_map

### src/analyzer/report_generator.py
```python
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import os
from pathlib import Path
from reportlab.lib import colors
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, PageBreak, Image
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from database.operations import DatabaseOperations

logger = logging.getLogger(__name__)

class ReportGenerator:
    """Generate PDF reports with analysis results."""
    
    def __init__(self, output_dir: str = None):
        self.output_dir = output_dir or "./outputs"
        os.makedirs(self.output_dir, exist_ok=True)
        self.db_ops = DatabaseOperations()
        self.styles = getSampleStyleSheet()
        self._add_custom_styles()
    
    def _add_custom_styles(self):
        """Add custom styles for the report."""
        self.styles.add(ParagraphStyle(
            name='CustomTitle',
            parent=self.styles['Title'],
            fontSize=24,
            textColor=colors.HexColor('#1a73e8'),
            spaceAfter=30
        ))
        
        self.styles.add(ParagraphStyle(
            name='SectionHeader',
            parent=self.styles['Heading1'],
            fontSize=16,
            textColor=colors.HexColor('#1a73e8'),
            spaceAfter=12
        ))
        
        self.styles.add(ParagraphStyle(
            name='SubsectionHeader',
            parent=self.styles['Heading2'],
            fontSize=14,
            textColor=colors.HexColor('#5f6368'),
            spaceAfter=10
        ))
    
    def generate_full_report(self, 
                           country: Optional[str] = None,
                           include_visualizations: bool = True) -> str:
        """Generate comprehensive analysis report.
        
        Args:
            country: Country code to filter by (optional)
            include_visualizations: Whether to include charts
            
        Returns:
            Path to generated report
        """
        # Create timestamp for filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        country_suffix = f"_{country}" if country else "_all"
        filename = f"labor_market_analysis{country_suffix}_{timestamp}.pdf"
        filepath = os.path.join(self.output_dir, filename)
        
        # Create document
        doc = SimpleDocTemplate(
            filepath,
            pagesize=A4,
            rightMargin=72,
            leftMargin=72,
            topMargin=72,
            bottomMargin=18
        )
        
        # Build content
        story = []
        
        # Title page
        story.extend(self._create_title_page(country))
        story.append(PageBreak())
        
        # Executive summary
        story.extend(self._create_executive_summary(country))
        story.append(PageBreak())
        
        # Skills analysis
        story.extend(self._create_skills_analysis(country, include_visualizations))
        story.append(PageBreak())
        
        # Clustering results
        story.extend(self._create_clustering_analysis(include_visualizations))
        story.append(PageBreak())
        
        # Temporal trends (if available)
        story.extend(self._create_temporal_analysis(country, include_visualizations))
        story.append(PageBreak())
        
        # Methodology
        story.extend(self._create_methodology_section())
        
        # Build PDF
        doc.build(story)
        
        logger.info(f"Report generated: {filepath}")
        return filepath
    
    def _create_title_page(self, country: Optional[str]) -> List:
        """Create title page elements."""
        elements = []
        
        # Title
        title_text = "Observatorio de Demanda Laboral Tecnológica"
        if country:
            country_names = {'CO': 'Colombia', 'MX': 'México', 'AR': 'Argentina'}
            title_text += f"\n{country_names.get(country, country)}"
        else:
            title_text += "\nAmérica Latina"
        
        elements.append(Paragraph(title_text, self.styles['CustomTitle']))
        elements.append(Spacer(1, 0.5*inch))
        
        # Subtitle
        subtitle = "Análisis Automatizado de Habilidades Técnicas"
        elements.append(Paragraph(subtitle, self.styles['Heading2']))
        elements.append(Spacer(1, 0.3*inch))
        
        # Date
        date_text = f"Fecha de generación: {datetime.now().strftime('%d de %B de %Y')}"
        elements.append(Paragraph(date_text, self.styles['Normal']))
        elements.append(Spacer(1, 2*inch))
        
        # Authors/Institution
        elements.append(Paragraph("Universidad XYZ", self.styles['Normal']))
        elements.append(Paragraph("Facultad de Ingeniería", self.styles['Normal']))
        
        return elements
    
    def _create_executive_summary(self, country: Optional[str]) -> List:
        """Create executive summary section."""
        elements = []
        
        elements.append(Paragraph("Resumen Ejecutivo", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get summary statistics
        stats = self.db_ops.get_skill_statistics(country)
        
        # Summary text
        summary_points = [
            f"Se analizaron un total de {stats.get('total_unique_skills', 0)} habilidades técnicas únicas.",
            f"Las 5 habilidades más demandadas son: {', '.join([s['skill'] for s in stats.get('top_skills', [])[:5]])}.",
            "El análisis revela una fuerte demanda de habilidades en desarrollo web, cloud computing y ciencia de datos.",
            "Se identificaron patrones emergentes en tecnologías de inteligencia artificial y DevOps."
        ]
        
        for point in summary_points:
            elements.append(Paragraph(f"• {point}", self.styles['Normal']))
            elements.append(Spacer(1, 0.1*inch))
        
        return elements
    
    def _create_skills_analysis(self, country: Optional[str], include_viz: bool) -> List:
        """Create skills analysis section."""
        elements = []
        
        elements.append(Paragraph("Análisis de Habilidades", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get skill statistics
        stats = self.db_ops.get_skill_statistics(country)
        
        # Top skills table
        elements.append(Paragraph("Top 20 Habilidades Más Demandadas", self.styles['SubsectionHeader']))
        
        if stats.get('top_skills'):
            # Create table data
            table_data = [['Posición', 'Habilidad', 'Frecuencia']]
            for i, skill_data in enumerate(stats['top_skills'][:20], 1):
                table_data.append([
                    str(i),
                    skill_data['skill'],
                    str(skill_data['count'])
                ])
            
            # Create table
            table = Table(table_data, colWidths=[1*inch, 3*inch, 1.5*inch])
            table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            
            elements.append(table)
            elements.append(Spacer(1, 0.3*inch))
        
        # Add visualization if requested
        if include_viz:
            viz_path = self._create_skill_frequency_chart(stats.get('top_skills', [])[:15])
            if viz_path:
                elements.append(Image(viz_path, width=6*inch, height=4*inch))
                elements.append(Spacer(1, 0.2*inch))
        
        return elements
    
    def _create_clustering_analysis(self, include_viz: bool) -> List:
        """Create clustering analysis section."""
        elements = []
        
        elements.append(Paragraph("Análisis de Clustering", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get latest clustering results
        # Note: This would need to be implemented in DatabaseOperations
        # For now, we'll use placeholder text
        
        elements.append(Paragraph(
            "El análisis de clustering identificó grupos coherentes de habilidades "
            "que típicamente aparecen juntas en las ofertas laborales:",
            self.styles['Normal']
        ))
        elements.append(Spacer(1, 0.1*inch))
        
        # Placeholder cluster descriptions
        clusters = [
            "Frontend Development: React, Vue.js, CSS, JavaScript, HTML5",
            "Backend Development: Node.js, Python, Django, Flask, API REST",
            "Data Science: Python, R, Machine Learning, SQL, Pandas",
            "DevOps: Docker, Kubernetes, AWS, CI/CD, Jenkins",
            "Mobile Development: React Native, Flutter, iOS, Android"
        ]
        
        for cluster in clusters:
            elements.append(Paragraph(f"• {cluster}", self.styles['Normal']))
            elements.append(Spacer(1, 0.05*inch))
        
        return elements
    
    def _create_temporal_analysis(self, country: Optional[str], include_viz: bool) -> List:
        """Create temporal trends analysis section."""
        elements = []
        
        elements.append(Paragraph("Tendencias Temporales", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        elements.append(Paragraph(
            "El análisis temporal permite identificar la evolución de la demanda "
            "de habilidades técnicas a lo largo del tiempo.",
            self.styles['Normal']
        ))
        
        # Placeholder for temporal analysis
        trends = [
            "Crecimiento sostenido en demanda de habilidades cloud (AWS, Azure)",
            "Aumento significativo en tecnologías de IA/ML en los últimos 6 meses",
            "Estabilidad en frameworks tradicionales (Spring, .NET)",
            "Emergencia de nuevas herramientas DevOps"
        ]
        
        elements.append(Spacer(1, 0.1*inch))
        for trend in trends:
            elements.append(Paragraph(f"• {trend}", self.styles['Normal']))
            elements.append(Spacer(1, 0.05*inch))
        
        return elements
    
    def _create_methodology_section(self) -> List:
        """Create methodology section."""
        elements = []
        
        elements.append(Paragraph("Metodología", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        methodology_text = """
        Este análisis se realizó mediante un pipeline automatizado que incluye:
        
        1. Web Scraping: Recolección automática de ofertas laborales de portales como 
           Computrabajo, Bumeran y elempleo.com.
        
        2. Extracción de Habilidades: Combinación de técnicas de NER (Named Entity Recognition) 
           y expresiones regulares para identificar menciones de habilidades técnicas.
        
        3. Enriquecimiento con LLM: Uso de modelos de lenguaje para identificar habilidades 
           implícitas y normalizar variaciones.
        
        4. Análisis Semántico: Generación de embeddings multilingües y clustering para 
           identificar grupos de habilidades relacionadas.
        
        5. Visualización: Generación de reportes estáticos con métricas agregadas y 
           visualizaciones interpretables.
        """
        
        elements.append(Paragraph(methodology_text, self.styles['Normal']))
        
        return elements
    
    def _create_skill_frequency_chart(self, top_skills: List[Dict[str, Any]]) -> Optional[str]:
        """Create skill frequency bar chart.
        
        Args:
            top_skills: List of top skills with counts
            
        Returns:
            Path to saved chart image
        """
        if not top_skills:
            return None
        
        try:
            # Prepare data
            skills = [s['skill'] for s in top_skills]
            counts = [s['count'] for s in top_skills]
            
            # Create figure
            plt.figure(figsize=(10, 6))
            
            # Create horizontal bar chart
            bars = plt.barh(skills, counts, color='#1a73e8')
            
            # Customize
            plt.xlabel('Número de Vacantes', fontsize=12)
            plt.title('Habilidades Más Demandadas', fontsize=14, fontweight='bold')
            plt.gca().invert_yaxis()  # Highest on top
            
            # Add value labels
            for i, bar in enumerate(bars):
                width = bar.get_width()
                plt.text(width + 1, bar.get_y() + bar.get_height()/2, 
                        f'{counts[i]}', 
                        ha='left', va='center')
            
            plt.tight_layout()
            
            # Save
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(self.output_dir, f"skill_frequency_{timestamp}.png")
            plt.savefig(filepath, dpi=300, bbox_inches='tight')
            plt.close()
            
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating skill frequency chart: {e}")
            return None
    
    def enrich_extracted_skills(self, extracted_skills: List[Dict]) -> List[Dict]:
        """Enrich extracted skills with ESCO matches.
        
        Args:
            extracted_skills: List of extracted skill dictionaries
            
        Returns:
            Enriched skill list
        """
        # Get unique skill texts
        unique_skills = list(set(skill['skill_text'] for skill in extracted_skills))
        
        # Match all unique skills
        esco_matches = self.match_skills_batch(unique_skills)
        
        # Enrich original skills
        enriched = []
        for skill in extracted_skills:
            enriched_skill = skill.copy()
            
            match = esco_matches.get(skill['skill_text'])
            if match:
                enriched_skill.update({
                    'esco_uri': match['esco_uri'],
                    'esco_preferred_label': match['esco_preferred_label'],
                    'esco_match_type': match['match_type'],
                    'esco_match_score': match['match_score']
                })
            
            enriched.append(enriched_skill)
        
        return enriched
```

### src/extractor/pipeline.py
```python
import logging
from typing import List, Dict, Optional
from database.operations import DatabaseOperations
from .ner_extractor import NERExtractor
from .regex_patterns import RegexExtractor
from .esco_matcher import ESCOMatcher
from config.settings import get_settings
import time

logger = logging.getLogger(__name__)

class ExtractionPipeline:
    """Main pipeline for skill extraction from job postings."""
    
    def __init__(self):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        
        # Initialize extractors
        logger.info("Initializing extraction components...")
        self.ner_extractor = NERExtractor()
        self.regex_extractor = RegexExtractor()
        self.esco_matcher = ESCOMatcher()
        
        logger.info("Extraction pipeline initialized")
    
    def process_batch(self, batch_size: int = 100) -> Dict[str, any]:
        """Process a batch of unprocessed jobs.
        
        Args:
            batch_size: Number of jobs to process
            
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'jobs_processed': 0,
            'skills_extracted': 0,
            'esco_matches': 0,
            'errors': 0
        }
        
        try:
            # Get unprocessed jobs
            jobs = self.db_ops.get_unprocessed_jobs(limit=batch_size)
            logger.info(f"Processing {len(jobs)} jobs")
            
            for job in jobs:
                try:
                    # Process individual job
                    skills = self.process_job(job)
                    
                    if skills:
                        # Save extracted skills
                        self.db_ops.insert_extracted_skills(
                            str(job.job_id),
                            skills
                        )
                        
                        # Mark job as processed
                        self.db_ops.mark_job_processed(str(job.job_id))
                        
                        # Update stats
                        stats['jobs_processed'] += 1
                        stats['skills_extracted'] += len(skills)
                        stats['esco_matches'] += sum(
                            1 for s in skills if s.get('esco_uri')
                        )
                    
                except Exception as e:
                    logger.error(f"Error processing job {job.job_id}: {e}")
                    stats['errors'] += 1
            
            # Calculate processing time
            stats['processing_time'] = time.time() - start_time
            stats['jobs_per_second'] = stats['jobs_processed'] / stats['processing_time'] if stats['processing_time'] > 0 else 0
            
            logger.info(
                f"Batch complete: {stats['jobs_processed']} jobs, "
                f"{stats['skills_extracted']} skills extracted, "
                f"{stats['esco_matches']} ESCO matches, "
                f"{stats['errors']} errors, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"Batch processing failed: {e}")
            raise
    
    def process_job(self, job) -> List[Dict[str, any]]:
        """Process a single job to extract skills.
        
        Args:
            job: Job object from database
            
        Returns:
            List of extracted and enriched skills
        """
        # Prepare job data
        job_data = {
            'title': job.title,
            'description': job.description,
            'requirements': job.requirements
        }
        
        # Extract skills using NER
        ner_skills = self.ner_extractor.extract_from_job(job_data)
        
        # Extract skills using regex
        regex_skills = self.regex_extractor.extract_from_job(job_data)
        
        # Combine and deduplicate
        all_skills = self._combine_skills(ner_skills, regex_skills)
        
        # Enrich with ESCO matches
        enriched_skills = self.esco_matcher.enrich_extracted_skills(all_skills)
        
        logger.debug(
            f"Job {job.job_id}: {len(ner_skills)} NER skills, "
            f"{len(regex_skills)} regex skills, "
            f"{len(enriched_skills)} total after deduplication"
        )
        
        return enriched_skills
    
    def _combine_skills(self, ner_skills: List[Dict], regex_skills: List[Dict]) -> List[Dict]:
        """Combine and deduplicate skills from different extractors.
        
        Args:
            ner_skills: Skills from NER
            regex_skills: Skills from regex
            
        Returns:
            Combined and deduplicated skill list
        """
        # Use skill text and source section as unique key
        seen_skills = {}
        
        # Process NER skills first (higher confidence)
        for skill in ner_skills:
            key = (skill['skill_text'], skill['source_section'])
            
            if key not in seen_skills:
                seen_skills[key] = skill
            else:
                # Update confidence if higher
                if skill['confidence_score'] > seen_skills[key]['confidence_score']:
                    seen_skills[key] = skill
        
        # Process regex skills
        for skill in regex_skills:
            key = (skill['skill_text'], skill['source_section'])
            
            if key not in seen_skills:
                seen_skills[key] = skill
            else:
                # Merge extraction methods
                existing = seen_skills[key]
                if existing['extraction_method'] != skill['extraction_method']:
                    existing['extraction_method'] = 'ner+regex'
                    existing['confidence_score'] = min(0.95, existing['confidence_score'] + 0.1)
        
        return list(seen_skills.values())
    
    def run_continuous(self, batch_size: int = 100, wait_time: int = 60):
        """Run extraction continuously.
        
        Args:
            batch_size: Jobs per batch
            wait_time: Seconds to wait between batches
        """
        logger.info(f"Starting continuous extraction (batch_size={batch_size}, wait={wait_time}s)")
        
        while True:
            try:
                stats = self.process_batch(batch_size)
                
                if stats['jobs_processed'] == 0:
                    logger.info(f"No jobs to process. Waiting {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    # Short pause between batches
                    time.sleep(5)
                    
            except KeyboardInterrupt:
                logger.info("Extraction stopped by user")
                break
            except Exception as e:
                logger.error(f"Extraction error: {e}")
                logger.info(f"Waiting {wait_time}s before retry...")
                time.sleep(wait_time)
```

---

## 6. LLM Processor Module Files

### src/llm_processor/__init__.py
```python
from .llm_handler import LLMHandler
from .prompts import PromptGenerator
from .esco_normalizer import ESCONormalizer
from .validator import SkillValidator

__all__ = ['LLMHandler', 'PromptGenerator', 'ESCONormalizer', 'SkillValidator']
```

### src/llm_processor/llm_handler.py
```python
import logging
from typing import List, Dict, Optional, Any
from llama_cpp import Llama
import openai
from config.settings import get_settings
import json
import time

logger = logging.getLogger(__name__)

class LLMHandler:
    """Handle LLM interactions for skill processing."""
    
    def __init__(self, model_type: str = "local"):
        self.settings = get_settings()
        self.model_type = model_type
        
        if model_type == "local":
            self._init_local_model()
        elif model_type == "openai":
            self._init_openai()
        else:
            raise ValueError(f"Unknown model type: {model_type}")
    
    def _init_local_model(self):
        """Initialize local LLaMA/Mistral model."""
        try:
            logger.info(f"Loading local model from {self.settings.llm_model_path}")
            
            self.model = Llama(
                model_path=self.settings.llm_model_path,
                n_ctx=self.settings.llm_context_length,
                n_gpu_layers=self.settings.llm_n_gpu_layers,
                verbose=False
            )
            
            logger.info("Local model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load local model: {e}")
            raise
    
    def _init_openai(self):
        """Initialize OpenAI API client."""
        if not self.settings.openai_api_key:
            raise ValueError("OpenAI API key not configured")
        
        openai.api_key = self.settings.openai_api_key
        self.model_name = self.settings.openai_model
        logger.info(f"OpenAI API initialized with model {self.model_name}")
    
    def process_skills(self, 
                      job_data: Dict[str, Any],
                      extracted_skills: List[Dict[str, Any]],
                      prompt_template: str) -> Dict[str, Any]:
        """Process skills using LLM.
        
        Args:
            job_data: Job information
            extracted_skills: Initially extracted skills
            prompt_template: Formatted prompt template
            
        Returns:
            LLM response with processed skills
        """
        start_time = time.time()
        
        try:
            if self.model_type == "local":
                response = self._process_local(prompt_template)
            else:
                response = self._process_openai(prompt_template)
            
            # Parse response
            result = self._parse_response(response)
            
            # Add metadata
            result['processing_time'] = time.time() - start_time
            result['model_type'] = self.model_type
            result['model_name'] = getattr(self, 'model_name', 'local_mistral')
            
            return result
            
        except Exception as e:
            logger.error(f"LLM processing failed: {e}")
            raise
    
    def _process_local(self, prompt: str) -> str:
        """Process using local model.
        
        Args:
            prompt: Complete prompt
            
        Returns:
            Model response
        """
        response = self.model(
            prompt,
            max_tokens=self.settings.llm_max_tokens,
            temperature=self.settings.llm_temperature,
            stop=["</response>", "\n\n\n"],
            echo=False
        )
        
        return response['choices'][0]['text']
    
    def _process_openai(self, prompt: str) -> str:
        """Process using OpenAI API.
        
        Args:
            prompt: Complete prompt
            
        Returns:
            Model response
        """
        response = openai.ChatCompletion.create(
            model=self.model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert in analyzing job postings and extracting technical skills. Respond in Spanish."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            max_tokens=self.settings.llm_max_tokens,
            temperature=self.settings.llm_temperature
        )
        
        return response.choices[0].message.content
    
    def _parse_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response.
        
        Args:
            response: Raw LLM response
            
        Returns:
            Parsed skills data
        """
        # Try to extract JSON if present
        if "```json" in response:
            # Extract JSON block
            start = response.find("```json") + 7
            end = response.find("```", start)
            json_str = response[start:end].strip()
            
            try:
                return json.loads(json_str)
            except json.JSONDecodeError:
                logger.warning("Failed to parse JSON from response")
        
        # Fallback: Parse structured text response
        result = {
            "explicit_skills": [],
            "implicit_skills": [],
            "normalized_skills": [],
            "deduplicated_skills": []
        }
        
        lines = response.strip().split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            
            # Detect sections
            if "habilidades explícitas" in line.lower():
                current_section = "explicit_skills"
            elif "habilidades implícitas" in line.lower():
                current_section = "implicit_skills"
            elif "habilidades normalizadas" in line.lower():
                current_section = "normalized_skills"
            elif "deduplicadas" in line.lower():
                current_section = "deduplicated_skills"
            elif line and current_section and (line.startswith('-') or line.startswith('*')):
                # Extract skill from bullet point
                skill_text = line.lstrip('-*').strip()
                
                # Parse skill with reasoning if present
                if ':' in skill_text:
                    skill, reasoning = skill_text.split(':', 1)
                    result[current_section].append({
                        "skill": skill.strip(),
                        "reasoning": reasoning.strip()
                    })
                else:
                    result[current_section].append({
                        "skill": skill_text
                    })
        
        return result
```

### src/llm_processor/prompts.py
```python
from typing import List, Dict, Any
import json

class PromptGenerator:
    """Generate prompts for LLM skill processing."""
    
    def __init__(self):
        self.templates = self._load_templates()
    
    def _load_templates(self) -> Dict[str, str]:
        """Load prompt templates."""
        return {
            "skill_processing": """Eres un experto en análisis de ofertas laborales tecnológicas en América Latina.

Datos de la vacante:
- Título: {job_title}
- Descripción: {job_description}
- Requisitos: {job_requirements}

Habilidades extraídas inicialmente:
{extracted_skills}

Por favor, realiza las siguientes tareas:

1. **Validación de habilidades explícitas**: Revisa las habilidades extraídas y confirma cuáles son realmente habilidades técnicas relevantes.

2. **Detección de habilidades implícitas**: Basándote en el contexto del puesto, identifica habilidades técnicas que serían necesarias pero no están mencionadas explícitamente. Por ejemplo:
   - Si menciona "desarrollo web full stack" → probablemente necesite Git, bases de datos, APIs REST
   - Si menciona "análisis de datos" → probablemente necesite SQL, Python/R, visualización
   - Si menciona "DevOps" → probablemente necesite CI/CD, contenedores, cloud

3. **Normalización con ESCO**: Para cada habilidad, proporciona la forma normalizada según estándares internacionales:
   - Usa nombres estándar (ej: "JS" → "JavaScript", "React.js" → "React")
   - Mantén el español cuando sea apropiado
   - Agrupa variaciones (ej: "MySQL/MariaDB" → "MySQL")

4. **Deduplicación**: Elimina habilidades duplicadas o redundantes:
   - Combina variaciones del mismo concepto
   - Elimina términos demasiado genéricos
   - Mantén el término más específico cuando haya jerarquía

Responde en el siguiente formato JSON:
```json
{{
  "explicit_skills": [
    {{"skill": "nombre", "confidence": 0.9, "original": "texto_original"}}
  ],
  "implicit_skills": [
    {{"skill": "nombre", "confidence": 0.7, "reasoning": "justificación"}}
  ],
  "normalized_skills": [
    {{"original": "skill_original", "normalized": "skill_normalizado", "esco_match": "posible_uri"}}
  ],
  "deduplicated_skills": [
    {{"skill": "nombre_final", "type": "explicit|implicit", "category": "programming|database|framework|tool|soft_skill"}}
  ]
}}
```""",

            "simple_inference": """Analiza esta oferta de trabajo y extrae SOLO las habilidades técnicas implícitas que no están mencionadas pero serían necesarias.

Título: {job_title}
Descripción resumida: {job_summary}
Habilidades ya identificadas: {known_skills}

Lista únicamente las habilidades técnicas implícitas con su justificación:
""",

            "normalization": """Normaliza las siguientes habilidades técnicas según estándares internacionales y la taxonomía ESCO:

Habilidades a normalizar:
{skills_list}

Para cada habilidad, proporciona:
- Forma normalizada
- Categoría (lenguaje/framework/base de datos/herramienta/metodología)
- Término ESCO equivalente si existe

Responde en formato de lista:
""",

            "deduplication": """Elimina duplicados y agrupa las siguientes habilidades:

Habilidades:
{skills_list}

Reglas:
- Combina variaciones del mismo concepto (ej: JS, JavaScript, javascript → JavaScript)
- Mantén el término más específico cuando haya jerarquía
- Elimina términos genéricos si hay específicos

Lista final sin duplicados:
"""
        }
    
    def generate_skill_processing_prompt(self, 
                                       job_data: Dict[str, Any],
                                       extracted_skills: List[Dict[str, Any]]) -> str:
        """Generate prompt for complete skill processing.
        
        Args:
            job_data: Job information
            extracted_skills: Initially extracted skills
            
        Returns:
            Formatted prompt
        """
        # Format extracted skills
        skills_text = self._format_extracted_skills(extracted_skills)
        
        prompt = self.templates["skill_processing"].format(
            job_title=job_data.get('title', 'No especificado'),
            job_description=job_data.get('description', 'No especificado'),
            job_requirements=job_data.get('requirements', 'No especificado'),
            extracted_skills=skills_text
        )
        
        return prompt
    
    def generate_inference_prompt(self,
                                job_title: str,
                                job_summary: str,
                                known_skills: List[str]) -> str:
        """Generate prompt for implicit skill inference.
        
        Args:
            job_title: Job title
            job_summary: Brief job description
            known_skills: Already identified skills
            
        Returns:
            Formatted prompt
        """
        skills_list = ", ".join(known_skills) if known_skills else "Ninguna"
        
        return self.templates["simple_inference"].format(
            job_title=job_title,
            job_summary=job_summary,
            known_skills=skills_list
        )
    
    def generate_normalization_prompt(self, skills: List[str]) -> str:
        """Generate prompt for skill normalization.
        
        Args:
            skills: List of skills to normalize
            
        Returns:
            Formatted prompt
        """
        skills_text = "\n".join(f"- {skill}" for skill in skills)
        
        return self.templates["normalization"].format(
            skills_list=skills_text
        )
    
    def generate_deduplication_prompt(self, skills: List[str]) -> str:
        """Generate prompt for skill deduplication.
        
        Args:
            skills: List of skills to deduplicate
            
        Returns:
            Formatted prompt
        """
        skills_text = "\n".join(f"- {skill}" for skill in skills)
        
        return self.templates["deduplication"].format(
            skills_list=skills_text
        )
    
    def _format_extracted_skills(self, skills: List[Dict[str, Any]]) -> str:
        """Format extracted skills for prompt.
        
        Args:
            skills: List of skill dictionaries
            
        Returns:
            Formatted text
        """
        formatted_skills = []
        
        # Group by source section
        by_section = {}
        for skill in skills:
            section = skill.get('source_section', 'unknown')
            if section not in by_section:
                by_section[section] = []
            by_section[section].append(skill)
        
        # Format each section
        for section, section_skills in by_section.items():
            formatted_skills.append(f"\nDe {section}:")
            for skill in section_skills:
                method = skill.get('extraction_method', 'unknown')
                confidence = skill.get('confidence_score', 0)
                text = skill.get('skill_text', '')
                
                formatted_skills.append(
                    f"  - {text} (método: {method}, confianza: {confidence:.2f})"
                )
        
        return "\n".join(formatted_skills)

### src/llm_processor/esco_normalizer.py
```python
import logging
from typing import List, Dict, Any, Optional
from fuzzywuzzy import fuzz
import yaml

logger = logging.getLogger(__name__)

class ESCONormalizer:
    """Normalize skills using ESCO taxonomy with LLM assistance."""
    
    def __init__(self, config_path: str = "config/esco_config.yaml"):
        self.config = self._load_config(config_path)
        self.normalization_rules = self._build_normalization_rules()
    
    def _load_config(self, config_path: str) -> dict:
        """Load ESCO configuration."""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config['esco']
        except Exception as e:
            logger.error(f"Failed to load ESCO config: {e}")
            return {}
    
    def _build_normalization_rules(self) -> Dict[str, str]:
        """Build normalization rules from config and common variations."""
        rules = {}
        
        # Load from config
        if 'tech_mappings' in self.config:
            rules.update(self.config['tech_mappings'])
        
        # Add common variations
        common_variations = {
            # JavaScript variations
            'js': 'JavaScript',
            'javascript': 'JavaScript',
            'java script': 'JavaScript',
            
            # Python variations
            'python3': 'Python',
            'python 3': 'Python',
            'python2': 'Python',
            
            # React variations
            'reactjs': 'React',
            'react.js': 'React',
            'react js': 'React',
            'react native': 'React Native',
            
            # Node variations
            'nodejs': 'Node.js',
            'node js': 'Node.js',
            'node': 'Node.js',
            
            # Database variations
            'postgres': 'PostgreSQL',
            'mysql': 'MySQL',
            'maria db': 'MariaDB',
            'mariadb': 'MariaDB',
            'mongo': 'MongoDB',
            'mongo db': 'MongoDB',
            
            # .NET variations
            'dotnet': '.NET',
            'dot net': '.NET',
            '.net core': '.NET Core',
            'asp.net': 'ASP.NET',
            
            # Other common variations
            'c++': 'C++',
            'c#': 'C#',
            'c sharp': 'C#',
            'objective c': 'Objective-C',
            'obj-c': 'Objective-C',
            
            # Spanish to English mappings
            'base de datos': 'Database',
            'desarrollo web': 'Web Development',
            'desarrollo móvil': 'Mobile Development',
            'aprendizaje automático': 'Machine Learning',
            'inteligencia artificial': 'Artificial Intelligence',
            'ciencia de datos': 'Data Science',
            'computación en la nube': 'Cloud Computing',
            'control de versiones': 'Version Control',
        }
        
        # Convert all keys to lowercase
        for key, value in common_variations.items():
            rules[key.lower()] = value
        
        return rules
    
    def normalize_skill(self, skill: str) -> Dict[str, Any]:
        """Normalize a single skill.
        
        Args:
            skill: Raw skill text
            
        Returns:
            Normalized skill data
        """
        skill_lower = skill.lower().strip()
        
        # Direct mapping
        if skill_lower in self.normalization_rules:
            return {
                'original': skill,
                'normalized': self.normalization_rules[skill_lower],
                'method': 'direct_mapping',
                'confidence': 1.0
            }
        
        # Try fuzzy matching
        best_match = None
        best_score = 0
        
        for pattern, normalized in self.normalization_rules.items():
            score = fuzz.ratio(skill_lower, pattern)
            if score > best_score and score >= 85:
                best_score = score
                best_match = normalized
        
        if best_match:
            return {
                'original': skill,
                'normalized': best_match,
                'method': 'fuzzy_mapping',
                'confidence': best_score / 100.0
            }
        
        # Category-based normalization
        normalized = self._category_normalization(skill)
        if normalized != skill:
            return {
                'original': skill,
                'normalized': normalized,
                'method': 'category_rules',
                'confidence': 0.8
            }
        
        # No normalization found
        return {
            'original': skill,
            'normalized': skill,
            'method': 'no_normalization',
            'confidence': 0.5
        }
    
    def _category_normalization(self, skill: str) -> str:
        """Apply category-based normalization rules.
        
        Args:
            skill: Skill to normalize
            
        Returns:
            Normalized skill
        """
        skill_lower = skill.lower()
        
        # Framework detection
        if 'framework' in skill_lower:
            skill = skill.replace('framework', '').replace('Framework', '').strip()
        
        # Version removal for certain technologies
        version_patterns = [
            (r'python\s*\d+\.?\d*', 'Python'),
            (r'java\s*\d+', 'Java'),
            (r'angular\s*\d+', 'Angular'),
            (r'vue\s*\d+', 'Vue.js'),
            (r'react\s*\d+', 'React'),
        ]
        
        import re
        for pattern, replacement in version_patterns:
            if re.search(pattern, skill_lower):
                return replacement
        
        # Capitalize properly
        # Special cases
        special_cases = {
            'mysql': 'MySQL',
            'postgresql': 'PostgreSQL',
            'mongodb': 'MongoDB',
            'javascript': 'JavaScript',
            'typescript': 'TypeScript',
            'graphql': 'GraphQL',
            'nodejs': 'Node.js',
            'reactjs': 'React',
            'vuejs': 'Vue.js',
        }
        
        if skill_lower in special_cases:
            return special_cases[skill_lower]
        
        # Default: capitalize first letter of each word
        return ' '.join(word.capitalize() for word in skill.split())
    
    def normalize_skills_batch(self, skills: List[str]) -> List[Dict[str, Any]]:
        """Normalize multiple skills.
        
        Args:
            skills: List of skills to normalize
            
        Returns:
            List of normalized skill data
        """
        normalized = []
        
        for skill in skills:
            result = self.normalize_skill(skill)
            normalized.append(result)
        
        return normalized
    
    def deduplicate_normalized_skills(self, normalized_skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Deduplicate normalized skills.
        
        Args:
            normalized_skills: List of normalized skill dictionaries
            
        Returns:
            Deduplicated list
        """
        # Group by normalized form
        skill_groups = {}
        
        for skill_data in normalized_skills:
            normalized = skill_data['normalized']
            
            if normalized not in skill_groups:
                skill_groups[normalized] = {
                    'normalized': normalized,
                    'originals': [],
                    'best_confidence': 0,
                    'methods': set()
                }
            
            skill_groups[normalized]['originals'].append(skill_data['original'])
            skill_groups[normalized]['methods'].add(skill_data['method'])
            skill_groups[normalized]['best_confidence'] = max(
                skill_groups[normalized]['best_confidence'],
                skill_data['confidence']
            )
        
        # Convert back to list
        deduplicated = []
        for normalized, group_data in skill_groups.items():
            deduplicated.append({
                'normalized': normalized,
                'original_variations': group_data['originals'],
                'confidence': group_data['best_confidence'],
                'methods': list(group_data['methods'])
            })
        
        return deduplicated

### src/llm_processor/validator.py
```python
import logging
from typing import List, Dict, Any, Set
import re

logger = logging.getLogger(__name__)

class SkillValidator:
    """Validate and filter skills."""
    
    def __init__(self):
        self.blacklist = self._build_blacklist()
        self.whitelist = self._build_whitelist()
        self.categories = self._build_categories()
    
    def _build_blacklist(self) -> Set[str]:
        """Build blacklist of non-skill terms."""
        return {
            # Generic terms
            'experiencia', 'años', 'año', 'conocimiento', 'conocimientos',
            'habilidad', 'habilidades', 'capacidad', 'competencia',
            'desarrollo', 'trabajo', 'empresa', 'cliente', 'proyecto',
            'equipo', 'persona', 'profesional', 'área', 'proceso',
            
            # Too generic tech terms
            'tecnología', 'tecnologías', 'sistema', 'sistemas',
            'informática', 'computación', 'software', 'hardware',
            'programación', 'desarrollo de software',
            
            # Common words
            'bueno', 'excelente', 'alto', 'nivel', 'manejo',
            'uso', 'gestión', 'administración', 'análisis',
            
            # Methodologies too generic
            'metodología', 'metodologías', 'mejores prácticas',
            
            # Soft skills (we focus on technical)
            'comunicación', 'liderazgo', 'trabajo en equipo',
            'responsabilidad', 'proactividad', 'creatividad'
        }
    
    def _build_whitelist(self) -> Set[str]:
        """Build whitelist of known valid skills."""
        return {
            # Programming languages
            'python', 'java', 'javascript', 'typescript', 'c++', 'c#',
            'php', 'ruby', 'go', 'golang', 'rust', 'kotlin', 'swift',
            'objective-c', 'r', 'scala', 'perl', 'matlab', 'julia',
            
            # Frameworks and libraries
            'react', 'angular', 'vue', 'django', 'flask', 'spring',
            'express', 'laravel', 'rails', 'fastapi', 'nextjs',
            '.net', 'asp.net', 'tensorflow', 'pytorch', 'keras',
            
            # Databases
            'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',
            'cassandra', 'dynamodb', 'oracle', 'sql server', 'sqlite',
            'neo4j', 'couchdb', 'firebase', 'supabase',
            
            # Cloud and DevOps
            'aws', 'azure', 'gcp', 'google cloud', 'docker', 'kubernetes',
            'jenkins', 'terraform', 'ansible', 'puppet', 'chef',
            'gitlab', 'github', 'bitbucket', 'circleci', 'travis',
            
            # Tools and platforms
            'git', 'jira', 'confluence', 'slack', 'linux', 'windows',
            'macos', 'ubuntu', 'centos', 'debian', 'nginx', 'apache',
            'grafana', 'prometheus', 'elasticsearch', 'kibana',
            
            # Data and ML
            'machine learning', 'deep learning', 'data science',
            'big data', 'spark', 'hadoop', 'kafka', 'airflow',
            'pandas', 'numpy', 'scikit-learn', 'matplotlib',
            
            # Mobile
            'android', 'ios', 'react native', 'flutter', 'xamarin',
            'swift', 'kotlin', 'objective-c', 'cordova', 'ionic',
            
            # Other
            'api', 'rest', 'graphql', 'websocket', 'microservices',
            'ci/cd', 'agile', 'scrum', 'kanban', 'tdd', 'bdd'
        }
    
    def _build_categories(self) -> Dict[str, Set[str]]:
        """Build skill categories."""
        return {
            'programming_language': {
                'python', 'java', 'javascript', 'typescript', 'c++', 'c#',
                'php', 'ruby', 'go', 'rust', 'kotlin', 'swift', 'r'
            },
            'framework': {
                'react', 'angular', 'vue', 'django', 'flask', 'spring',
                'express', 'laravel', 'rails', '.net', 'nextjs'
            },
            'database': {
                'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',
                'cassandra', 'dynamodb', 'oracle', 'sql server'
            },
            'cloud_platform': {
                'aws', 'azure', 'gcp', 'google cloud', 'heroku', 'digitalocean'
            },
            'devops_tool': {
                'docker', 'kubernetes', 'jenkins', 'terraform', 'ansible',
                'git', 'github', 'gitlab', 'ci/cd'
            },
            'data_ml': {
                'machine learning', 'deep learning', 'tensorflow', 'pytorch',
                'pandas', 'numpy', 'spark', 'hadoop'
            },
            'mobile': {
                'android', 'ios', 'react native', 'flutter', 'xamarin'
            },
            'methodology': {
                'agile', 'scrum', 'kanban', 'devops', 'tdd', 'bdd'
            }
        }
    
    def validate_skill(self, skill: str) -> Dict[str, Any]:
        """Validate a single skill.
        
        Args:
            skill: Skill to validate
            
        Returns:
            Validation result
        """
        skill_lower = skill.lower().strip()
        
        # Check blacklist
        if skill_lower in self.blacklist:
            return {
                'skill': skill,
                'valid': False,
                'reason': 'blacklisted',
                'confidence': 0.0
            }
        
        # Check whitelist
        if skill_lower in self.whitelist:
            category = self._categorize_skill(skill_lower)
            return {
                'skill': skill,
                'valid': True,
                'reason': 'whitelisted',
                'category': category,
                'confidence': 1.0
            }
        
        # Length check
        if len(skill_lower) < 2 or len(skill_lower) > 50:
            return {
                'skill': skill,
                'valid': False,
                'reason': 'invalid_length',
                'confidence': 0.0
            }
        
        # Pattern validation
        if not self._validate_pattern(skill_lower):
            return {
                'skill': skill,
                'valid': False,
                'reason': 'invalid_pattern',
                'confidence': 0.0
            }
        
        # Partial matches in whitelist
        for valid_skill in self.whitelist:
            if valid_skill in skill_lower or skill_lower in valid_skill:
                category = self._categorize_skill(valid_skill)
                return {
                    'skill': skill,
                    'valid': True,
                    'reason': 'partial_match',
                    'category': category,
                    'confidence': 0.7
                }
        
        # If not in whitelist but passes other checks
        return {
            'skill': skill,
            'valid': True,
            'reason': 'pattern_valid',
            'category': 'uncategorized',
            'confidence': 0.5
        }
    
    def _validate_pattern(self, skill: str) -> bool:
        """Validate skill pattern.
        
        Args:
            skill: Skill to validate
            
        Returns:
            True if pattern is valid
        """
        # Must contain at least one letter
        if not re.search(r'[a-zA-Z]', skill):
            return False
        
        # Check for suspicious patterns
        suspicious_patterns = [
            r'^\d+,  # Only numbers
            r'^[^a-zA-Z0-9\s\.\+\#\-/]+,  # Only special chars
            r'\b(años?|experiencia|conocimientos?)\b',  # Common non-skills
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, skill, re.IGNORECASE):
                return False
        
        return True
    
    def _categorize_skill(self, skill: str) -> str:
        """Categorize a skill.
        
        Args:
            skill: Skill to categorize
            
        Returns:
            Category name
        """
        skill_lower = skill.lower()
        
        for category, skills in self.categories.items():
            if skill_lower in skills:
                return category
        
        # Try partial matches
        for category, skills in self.categories.items():
            for cat_skill in skills:
                if cat_skill in skill_lower or skill_lower in cat_skill:
                    return category
        
        return 'uncategorized'
    
    def validate_skills_batch(self, skills: List[str]) -> List[Dict[str, Any]]:
        """Validate multiple skills.
        
        Args:
            skills: List of skills to validate
            
        Returns:
            List of validation results
        """
        results = []
        
        for skill in skills:
            result = self.validate_skill(skill)
            results.append(result)
        
        # Log statistics
        valid_count = sum(1 for r in results if r['valid'])
        logger.info(
            f"Skill validation: {valid_count}/{len(skills)} valid "
            f"({valid_count/len(skills)*100:.1f}%)"
        )
        
        return results
    
    def filter_valid_skills(self, skills: List[str], min_confidence: float = 0.5) -> List[str]:
        """Filter only valid skills.
        
        Args:
            skills: List of skills
            min_confidence: Minimum confidence threshold
            
        Returns:
            List of valid skills
        """
        results = self.validate_skills_batch(skills)
        
        valid_skills = [
            r['skill'] for r in results
            if r['valid'] and r['confidence'] >= min_confidence
        ]
        
        return valid_skills

### src/llm_processor/pipeline.py
```python
import logging
from typing import List, Dict, Any
from database.operations import DatabaseOperations
from .llm_handler import LLMHandler
from .prompts import PromptGenerator
from .esco_normalizer import ESCONormalizer
from .validator import SkillValidator
from config.settings import get_settings
import time

logger = logging.getLogger(__name__)

class LLMProcessingPipeline:
    """Main pipeline for LLM-based skill processing."""
    
    def __init__(self, model_type: str = "local"):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        
        logger.info("Initializing LLM processing components...")
        self.llm_handler = LLMHandler(model_type)
        self.prompt_generator = PromptGenerator()
        self.normalizer = ESCONormalizer()
        self.validator = SkillValidator()
        
        logger.info("LLM processing pipeline initialized")
    
    def process_batch(self, batch_size: int = 50) -> Dict[str, Any]:
        """Process a batch of jobs with extracted skills.
        
        Args:
            batch_size: Number of jobs to process
            
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'jobs_processed': 0,
            'skills_enhanced': 0,
            'implicit_skills_found': 0,
            'skills_normalized': 0,
            'errors': 0
        }
        
        try:
            # Get jobs with extracted skills needing LLM processing
            jobs_data = self.db_ops.get_extracted_skills_for_processing(limit=batch_size)
            logger.info(f"Processing {len(jobs_data)} jobs with LLM")
            
            for job_data in jobs_data:
                try:
                    # Process individual job
                    enhanced_skills = self.process_job(job_data)
                    
                    if enhanced_skills:
                        # Save enhanced skills
                        self.db_ops.insert_enhanced_skills(
                            job_data['job_id'],
                            enhanced_skills
                        )
                        
                        # Update statistics
                        stats['jobs_processed'] += 1
                        stats['skills_enhanced'] += len(enhanced_skills)
                        stats['implicit_skills_found'] += sum(
                            1 for s in enhanced_skills 
                            if s.get('skill_type') == 'implicit'
                        )
                        stats['skills_normalized'] += sum(
                            1 for s in enhanced_skills
                            if s.get('normalized_skill') != s.get('original_skill_text')
                        )
                    
                except Exception as e:
                    logger.error(f"Error processing job {job_data['job_id']}: {e}")
                    stats['errors'] += 1
            
            # Calculate processing time
            stats['processing_time'] = time.time() - start_time
            stats['jobs_per_minute'] = (stats['jobs_processed'] / stats['processing_time']) * 60 if stats['processing_time'] > 0 else 0
            
            logger.info(
                f"LLM batch complete: {stats['jobs_processed']} jobs, "
                f"{stats['skills_enhanced']} skills enhanced, "
                f"{stats['implicit_skills_found']} implicit skills found, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"LLM batch processing failed: {e}")
            raise
    
    def process_job(self, job_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Process a single job with LLM.
        
        Args:
            job_data: Job data with extracted skills
            
        Returns:
            List of enhanced skills
        """
        # Generate prompt
        prompt = self.prompt_generator.generate_skill_processing_prompt(
            {
                'title': job_data['job_title'],
                'description': job_data['job_description'],
                'requirements': job_data['job_requirements']
            },
            job_data['extracted_skills']
        )
        
        # Process with LLM
        llm_response = self.llm_handler.process_skills(
            job_data,
            job_data['extracted_skills'],
            prompt
        )
        
        # Process LLM response
        enhanced_skills = self._process_llm_response(
            llm_response,
            job_data['extracted_skills']
        )
        
        return enhanced_skills
    
    def _process_llm_response(self, 
                            llm_response: Dict[str, Any],
                            original_skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process LLM response into enhanced skills.
        
        Args:
            llm_response: Response from LLM
            original_skills: Original extracted skills
            
        Returns:
            List of enhanced skill records
        """
        enhanced_skills = []
        
        # Create a mapping of original skills
        original_map = {
            skill['skill_text'].lower(): skill 
            for skill in original_skills
        }
        
        # Process explicit skills (validated by LLM)
        for skill_data in llm_response.get('explicit_skills', []):
            skill_text = skill_data.get('skill', '').lower()
            
            # Validate skill
            validation = self.validator.validate_skill(skill_text)
            if not validation['valid']:
                continue
            
            # Normalize skill
            normalization = self.normalizer.normalize_skill(skill_text)
            
            enhanced_skills.append({
                'original_skill_text': skill_data.get('original', skill_text),
                'normalized_skill': normalization['normalized'],
                'skill_type': 'explicit',
                'esco_concept_uri': None,  # To be matched later
                'esco_preferred_label': None,
                'llm_confidence': skill_data.get('confidence', 0.8),
                'llm_reasoning': None,
                'is_duplicate': False,
                'llm_model': llm_response.get('model_name', 'unknown')
            })
        
        # Process implicit skills (inferred by LLM)
        for skill_data in llm_response.get('implicit_skills', []):
            skill_text = skill_data.get('skill', '').lower()
            
            # Validate skill
            validation = self.validator.validate_skill(skill_text)
            if not validation['valid']:
                continue
            
            # Normalize skill
            normalization = self.normalizer.normalize_skill(skill_text)
            
            enhanced_skills.append({
                'original_skill_text': None,
                'normalized_skill': normalization['normalized'],
                'skill_type': 'implicit',
                'esco_concept_uri': None,
                'esco_preferred_label': None,
                'llm_confidence': skill_data.get('confidence', 0.7),
                'llm_reasoning': skill_data.get('reasoning', ''),
                'is_duplicate': False,
                'llm_model': llm_response.get('model_name', 'unknown')
            })
        
        # Deduplicate skills
        enhanced_skills = self._deduplicate_skills(enhanced_skills)
        
        return enhanced_skills
    
    def _deduplicate_skills(self, skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Deduplicate enhanced skills.
        
        Args:
            skills: List of enhanced skills
            
        Returns:
            Deduplicated list with duplicates marked
        """
        seen_normalized = {}
        deduplicated = []
        
        # Sort by confidence (highest first)
        sorted_skills = sorted(
            skills, 
            key=lambda x: x.get('llm_confidence', 0),
            reverse=True
        )
        
        for skill in sorted_skills:
            normalized = skill['normalized_skill'].lower()
            
            if normalized not in seen_normalized:
                # First occurrence
                seen_normalized[normalized] = skill
                deduplicated.append(skill)
            else:
                # Duplicate found
                skill['is_duplicate'] = True
                skill['duplicate_of_id'] = id(seen_normalized[normalized])
                deduplicated.append(skill)
        
        return deduplicated
    
    def run_continuous(self, batch_size: int = 50, wait_time: int = 60):
        """Run LLM processing continuously.
        
        Args:
            batch_size: Jobs per batch
            wait_time: Seconds to wait between batches
        """
        logger.info(f"Starting continuous LLM processing (batch_size={batch_size}, wait={wait_time}s)")
        
        while True:
            try:
                stats = self.process_batch(batch_size)
                
                if stats['jobs_processed'] == 0:
                    logger.info(f"No jobs to process. Waiting {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    # Short pause between batches
                    time.sleep(10)
                    
            except KeyboardInterrupt:
                logger.info("LLM processing stopped by user")
                break
            except Exception as e:
                logger.error(f"LLM processing error: {e}")
                logger.info(f"Waiting {wait_time}s before retry...")
                time.sleep(wait_time)
```

---

## 7. Embedder Module Files

### src/embedder/__init__.py
```python
from .vectorizer import SkillVectorizer
from .model_loader import EmbeddingModelLoader
from .batch_processor import BatchProcessor

__all__ = ['SkillVectorizer', 'EmbeddingModelLoader', 'BatchProcessor']

### src/embedder/vectorizer.py
```python
import logging
from typing import List, Dict, Any, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import torch

logger = logging.getLogger(__name__)

class SkillVectorizer:
    """Generate embeddings for skills."""
    
    def __init__(self, model_name: str = None):
        self.model_name = model_name or "intfloat/multilingual-e5-base"
        self.model = None
        self.device = None
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialize the embedding model."""
        try:
            # Check for GPU availability
            if torch.cuda.is_available():
                self.device = 'cuda'
                logger.info(f"Using GPU for embeddings")
            else:
                self.device = 'cpu'
                logger.info(f"Using CPU for embeddings")
            
            # Load model
            logger.info(f"Loading embedding model: {self.model_name}")
            self.model = SentenceTransformer(self.model_name, device=self.device)
            
            # Get embedding dimension
            self.embedding_dim = self.model.get_sentence_embedding_dimension()
            logger.info(f"Model loaded. Embedding dimension: {self.embedding_dim}")
            
        except Exception as e:
            logger.error(f"Failed to initialize embedding model: {e}")
            raise
    
    def vectorize(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """Generate embeddings for a list of texts.
        
        Args:
            texts: List of texts to embed
            batch_size: Batch size for processing
            
        Returns:
            Array of embeddings
        """
        if not texts:
            return np.array([])
        
        try:
            # For E5 models, add instruction prefix
            if 'e5' in self.model_name.lower():
                texts = [f"query: {text}" for text in texts]
            
            # Generate embeddings
            embeddings = self.model.encode(
                texts,
                batch_size=batch_size,
                show_progress_bar=len(texts) > 100,
                convert_to_numpy=True,
                normalize_embeddings=True  # L2 normalization
            )
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Vectorization failed: {e}")
            raise
    
    def vectorize_single(self, text: str) -> np.ndarray:
        """Generate embedding for a single text.
        
        Args:
            text: Text to embed
            
        Returns:
            Embedding vector
        """
        return self.vectorize([text])[0]
    
    def compute_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """Compute cosine similarity between two embeddings.
        
        Args:
            embedding1: First embedding
            embedding2: Second embedding
            
        Returns:
            Cosine similarity score
        """
        # Assuming normalized embeddings, cosine similarity is just dot product
        return float(np.dot(embedding1, embedding2))
    
    def find_similar_skills(self, 
                          query_embedding: np.ndarray,
                          skill_embeddings: List[Dict[str, Any]],
                          top_k: int = 10,
                          threshold: float = 0.7) -> List[Dict[str, Any]]:
        """Find similar skills based on embeddings.
        
        Args:
            query_embedding: Query skill embedding
            skill_embeddings: List of skill embedding dictionaries
            top_k: Number of top results to return
            threshold: Minimum similarity threshold
            
        Returns:
            List of similar skills with scores
        """
        similarities = []
        
        for skill_data in skill_embeddings:
            embedding = skill_data['embedding']
            similarity = self.compute_similarity(query_embedding, embedding)
            
            if similarity >= threshold:
                similarities.append({
                    'skill': skill_data['skill_text'],
                    'similarity': similarity,
                    'embedding_id': skill_data.get('embedding_id')
                })
        
        # Sort by similarity
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        return similarities[:top_k]

### src/embedder/model_loader.py
```python
import logging
from typing import Dict, Any, Optional
import os
import json
from pathlib import Path
import torch
from transformers import AutoModel, AutoTokenizer
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)

class EmbeddingModelLoader:
    """Load and manage embedding models."""
    
    # Model configurations
    MODEL_CONFIGS = {
        'multilingual-e5-base': {
            'name': 'intfloat/multilingual-e5-base',
            'dimension': 768,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': 'query: '
        },
        'multilingual-e5-large': {
            'name': 'intfloat/multilingual-e5-large',
            'dimension': 1024,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': 'query: '
        },
        'beto': {
            'name': 'dccuchile/bert-base-spanish-wwm-cased',
            'dimension': 768,
            'max_length': 512,
            'type': 'transformers',
            'instruction_prefix': None
        },
        'labse': {
            'name': 'sentence-transformers/LaBSE',
            'dimension': 768,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': None
        },
        'multilingual-minilm': {
            'name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
            'dimension': 384,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': None
        }
    }
    
    def __init__(self, cache_dir: Optional[str] = None):
        self.cache_dir = cache_dir or "./data/cache/embeddings"
        os.makedirs(self.cache_dir, exist_ok=True)
        self.loaded_models = {}
    
    def load_model(self, model_key: str) -> Any:
        """Load an embedding model.
        
        Args:
            model_key: Key from MODEL_CONFIGS
            
        Returns:
            Loaded model
        """
        if model_key in self.loaded_models:
            logger.info(f"Model {model_key} already loaded")
            return self.loaded_models[model_key]
        
        if model_key not in self.MODEL_CONFIGS:
            raise ValueError(f"Unknown model key: {model_key}")
        
        config = self.MODEL_CONFIGS[model_key]
        
        try:
            if config['type'] == 'sentence-transformers':
                model = self._load_sentence_transformer(config)
            elif config['type'] == 'transformers':
                model = self._load_transformers_model(config)
            else:
                raise ValueError(f"Unknown model type: {config['type']}")
            
            self.loaded_models[model_key] = model
            logger.info(f"Successfully loaded model: {model_key}")
            
            return model
            
        except Exception as e:
            logger.error(f"Failed to load model {model_key}: {e}")
            raise
    
    def _load_sentence_transformer(self, config: Dict[str, Any]) -> SentenceTransformer:
        """Load a sentence-transformers model.
        
        Args:
            config: Model configuration
            
        Returns:
            Loaded model
        """
        model = SentenceTransformer(
            config['name'],
            cache_folder=self.cache_dir
        )
        
        # Verify dimension
        actual_dim = model.get_sentence_embedding_dimension()
        if actual_dim != config['dimension']:
            logger.warning(
                f"Model dimension mismatch: expected {config['dimension']}, "
                f"got {actual_dim}"
            )
        
        return model
    
    def _load_transformers_model(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Load a transformers model with tokenizer.
        
        Args:
            config: Model configuration
            
        Returns:
            Dictionary with model and tokenizer
        """
        model = AutoModel.from_pretrained(
            config['name'],
            cache_dir=self.cache_dir
        )
        
        tokenizer = AutoTokenizer.from_pretrained(
            config['name'],
            cache_dir=self.cache_dir
        )
        
        return {
            'model': model,
            'tokenizer': tokenizer,
            'config': config
        }
    
    def get_model_info(self, model_key: str) -> Dict[str, Any]:
        """Get information about a model.
        
        Args:
            model_key: Model key
            
        Returns:
            Model information
        """
        if model_key not in self.MODEL_CONFIGS:
            raise ValueError(f"Unknown model key: {model_key}")
        
        return self.MODEL_CONFIGS[model_key].copy()
    
    def list_available_models(self) -> Dict[str, Dict[str, Any]]:
        """List all available models.
        
        Returns:
            Dictionary of model configurations
        """
        return self.MODEL_CONFIGS.copy()
    
    def download_all_models(self):
        """Download all configured models."""
        logger.info("Downloading all configured embedding models...")
        
        for model_key in self.MODEL_CONFIGS:
            try:
                logger.info(f"Downloading {model_key}...")
                self.load_model(model_key)
            except Exception as e:
                logger.error(f"Failed to download {model_key}: {e}")
    
    def clear_cache(self):
        """Clear model cache."""
        import shutil
        
        if os.path.exists(self.cache_dir):
            shutil.rmtree(self.cache_dir)
            os.makedirs(self.cache_dir)
            logger.info("Model cache cleared")
        
        self.loaded_models.clear()

### src/embedder/batch_processor.py
```python
import logging
from typing import List, Dict, Any
import numpy as np
from database.operations import DatabaseOperations
from .vectorizer import SkillVectorizer
from config.settings import get_settings
import time
from tqdm import tqdm

logger = logging.getLogger(__name__)

class BatchProcessor:
    """Process skill embeddings in batches."""
    
    def __init__(self, model_name: str = None):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        self.vectorizer = SkillVectorizer(model_name)
        self.batch_size = self.settings.embedding_batch_size
    
    def process_all_skills(self) -> Dict[str, Any]:
        """Process all skills that need embeddings.
        
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'skills_processed': 0,
            'embeddings_created': 0,
            'errors': 0,
            'processing_time': 0
        }
        
        try:
            # Get skills without embeddings
            skills = self.db_ops.get_unique_skills_for_embedding()
            logger.info(f"Found {len(skills)} skills without embeddings")
            
            if not skills:
                return stats
            
            # Process in batches
            for i in tqdm(range(0, len(skills), self.batch_size), desc="Embedding skills"):
                batch = skills[i:i + self.batch_size]
                
                try:
                    # Generate embeddings
                    embeddings = self.vectorizer.vectorize(batch)
                    
                    # Prepare data for database
                    embedding_data = []
                    for skill_text, embedding in zip(batch, embeddings):
                        embedding_data.append({
                            'skill_text': skill_text,
                            'embedding': embedding.tolist(),  # Convert to list for pgvector
                            'model_name': self.vectorizer.model_name,
                            'model_version': '1.0'
                        })
                    
                    # Save to database
                    self.db_ops.insert_skill_embeddings(embedding_data)
                    
                    stats['skills_processed'] += len(batch)
                    stats['embeddings_created'] += len(embedding_data)
                    
                except Exception as e:
                    logger.error(f"Error processing batch {i//self.batch_size}: {e}")
                    stats['errors'] += 1
            
            stats['processing_time'] = time.time() - start_time
            
            logger.info(
                f"Embedding complete: {stats['skills_processed']} skills processed, "
                f"{stats['embeddings_created']} embeddings created, "
                f"{stats['errors']} errors, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"Embedding batch processing failed: {e}")
            raise
    
    def update_embeddings(self, force: bool = False) -> Dict[str, Any]:
        """Update embeddings for new or modified skills.
        
        Args:
            force: Force re-embedding of all skills
            
        Returns:
            Update statistics
        """
        if force:
            logger.warning("Force update requested - this will re-embed all skills")
            # Clear existing embeddings
            # Note: Implement this method in DatabaseOperations if needed
        
        return self.process_all_skills()
    
    def compute_similarity_matrix(self, skill_list: List[str] = None) -> np.ndarray:
        """Compute similarity matrix for skills.
        
        Args:
            skill_list: List of skills to compare (None for all)
            
        Returns:
            Similarity matrix
        """
        # Get embeddings from database
        all_embeddings = self.db_ops.get_all_embeddings()
        
        if skill_list:
            # Filter to requested skills
            embeddings_data = [
                e for e in all_embeddings 
                if e['skill_text'] in skill_list
            ]
        else:
            embeddings_data = all_embeddings
        
        if not embeddings_data:
            return np.array([])
        
        # Extract embedding vectors
        embeddings = np.array([e['embedding'] for e in embeddings_data])
        
        # Compute similarity matrix
        # Since embeddings are normalized, cosine similarity is just dot product
        similarity_matrix = np.dot(embeddings, embeddings.T)
        
        return similarity_matrix
    
    def find_duplicate_skills(self, threshold: float = 0.95) -> List[Dict[str, Any]]:
        """Find potential duplicate skills based on embedding similarity.
        
        Args:
            threshold: Similarity threshold for duplicates
            
        Returns:
            List of potential duplicates
        """
        all_embeddings = self.db_ops.get_all_embeddings()
        
        if len(all_embeddings) < 2:
            return []
        
        duplicates = []
        
        # Compare all pairs
        for i in range(len(all_embeddings)):
            for j in range(i + 1, len(all_embeddings)):
                skill1 = all_embeddings[i]
                skill2 = all_embeddings[j]
                
                similarity = self.vectorizer.compute_similarity(
                    np.array(skill1['embedding']),
                    np.array(skill2['embedding'])
                )
                
                if similarity >= threshold:
                    duplicates.append({
                        'skill1': skill1['skill_text'],
                        'skill2': skill2['skill_text'],
                        'similarity': similarity
                    })
        
        # Sort by similarity
        duplicates.sort(key=lambda x: x['similarity'], reverse=True)
        
        logger.info(f"Found {len(duplicates)} potential duplicate skill pairs")
        
        return duplicates
    
    def get_skill_recommendations(self, 
                                job_skills: List[str],
                                top_k: int = 10) -> List[Dict[str, Any]]:
        """Get skill recommendations based on current job skills.
        
        Args:
            job_skills: Current skills in job
            top_k: Number of recommendations
            
        Returns:
            List of recommended skills with scores
        """
        # Get embeddings for input skills
        input_embeddings = self.vectorizer.vectorize(job_skills)
        
        # Average the embeddings to get job profile
        job_profile = np.mean(input_embeddings, axis=0)
        
        # Get all skill embeddings
        all_embeddings = self.db_ops.get_all_embeddings()
        
        # Find similar skills
        recommendations = []
        
        for skill_data in all_embeddings:
            # Skip if already in job skills
            if skill_data['skill_text'] in job_skills:
                continue
            
            similarity = self.vectorizer.compute_similarity(
                job_profile,
                np.array(skill_data['embedding'])
            )
            
            recommendations.append({
                'skill': skill_data['skill_text'],
                'score': similarity
            })
        
        # Sort and return top K
        recommendations.sort(key=lambda x: x['score'], reverse=True)
        
        return recommendations[:top_k]

---

## 8. Analyzer Module Files

### src/analyzer/__init__.py
```python
from .clustering import SkillClusterer
from .dimension_reducer import DimensionReducer
from .report_generator import ReportGenerator
from .visualizations import VisualizationGenerator

__all__ = ['SkillClusterer', 'DimensionReducer', 'ReportGenerator', 'VisualizationGenerator']

### src/analyzer/clustering.py
```python
import logging
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from sklearn.cluster import HDBSCAN, KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score
import pandas as pd
from database.operations import DatabaseOperations
from config.settings import get_settings

logger = logging.getLogger(__name__)

class SkillClusterer:
    """Perform clustering on skill embeddings."""
    
    def __init__(self):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        self.clustering_method = 'hdbscan'
    
    def cluster_skills(self, 
                      embeddings: np.ndarray,
                      skill_texts: List[str],
                      method: str = 'hdbscan',
                      **kwargs) -> Dict[str, Any]:
        """Cluster skills based on embeddings.
        
        Args:
            embeddings: Skill embedding matrix
            skill_texts: List of skill texts
            method: Clustering method ('hdbscan' or 'kmeans')
            **kwargs: Additional parameters for clustering
            
        Returns:
            Clustering results
        """
        if len(embeddings) < 2:
            logger.warning("Not enough data points for clustering")
            return {}
        
        logger.info(f"Clustering {len(embeddings)} skills using {method}")
        
        if method == 'hdbscan':
            results = self._cluster_hdbscan(embeddings, skill_texts, **kwargs)
        elif method == 'kmeans':
            results = self._cluster_kmeans(embeddings, skill_texts, **kwargs)
        else:
            raise ValueError(f"Unknown clustering method: {method}")
        
        # Calculate metrics
        results['metrics'] = self._calculate_metrics(embeddings, results['labels'])
        
        # Characterize clusters
        results['cluster_info'] = self._characterize_clusters(
            results['labels'],
            skill_texts
        )
        
        return results
    
    def _cluster_hdbscan(self, 
                        embeddings: np.ndarray,
                        skill_texts: List[str],
                        min_cluster_size: int = None,
                        min_samples: int = None) -> Dict[str, Any]:
        """Perform HDBSCAN clustering.
        
        Args:
            embeddings: Embedding matrix
            skill_texts: Skill texts
            min_cluster_size: Minimum cluster size
            min_samples: Minimum samples for core points
            
        Returns:
            Clustering results
        """
        # Use settings if not provided
        min_cluster_size = min_cluster_size or self.settings.cluster_min_size
        min_samples = min_samples or self.settings.cluster_min_samples
        
        # Perform clustering
        clusterer = HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            metric='euclidean',
            cluster_selection_method='eom',
            prediction_data=True
        )
        
        labels = clusterer.fit_predict(embeddings)
        
        # Get cluster persistence (stability)
        cluster_persistence = clusterer.cluster_persistence_
        
        results = {
            'method': 'hdbscan',
            'labels': labels,
            'n_clusters': len(set(labels)) - (1 if -1 in labels else 0),
            'n_noise': list(labels).count(-1),
            'parameters': {
                'min_cluster_size': min_cluster_size,
                'min_samples': min_samples
            },
            'cluster_persistence': cluster_persistence,
            'clusterer': clusterer
        }
        
        logger.info(
            f"HDBSCAN found {results['n_clusters']} clusters "
            f"with {results['n_noise']} noise points"
        )
        
        return results
    
    def _cluster_kmeans(self,
                       embeddings: np.ndarray,
                       skill_texts: List[str],
                       n_clusters: int = 20) -> Dict[str, Any]:
        """Perform K-means clustering.
        
        Args:
            embeddings: Embedding matrix
            skill_texts: Skill texts
            n_clusters: Number of clusters
            
        Returns:
            Clustering results
        """
        # Perform clustering
        clusterer = KMeans(
            n_clusters=n_clusters,
            n_init=10,
            max_iter=300,
            random_state=42
        )
        
        labels = clusterer.fit_predict(embeddings)
        
        results = {
            'method': 'kmeans',
            'labels': labels,
            'n_clusters': n_clusters,
            'n_noise': 0,
            'parameters': {
                'n_clusters': n_clusters
            },
            'cluster_centers': clusterer.cluster_centers_,
            'inertia': clusterer.inertia_,
            'clusterer': clusterer
        }
        
        logger.info(f"K-means created {n_clusters} clusters")
        
        return results
    
    def _calculate_metrics(self, embeddings: np.ndarray, labels: np.ndarray) -> Dict[str, float]:
        """Calculate clustering quality metrics.
        
        Args:
            embeddings: Embedding matrix
            labels: Cluster labels
            
        Returns:
            Dictionary of metrics
        """
        metrics = {}
        
        # Remove noise points for metrics
        mask = labels >= 0
        if np.sum(mask) < 2:
            logger.warning("Not enough clustered points for metrics")
            return metrics
        
        try:
            # Silhouette score
            metrics['silhouette_score'] = silhouette_score(
                embeddings[mask],
                labels[mask]
            )
            
            # Davies-Bouldin index (lower is better)
            metrics['davies_bouldin_index'] = davies_bouldin_score(
                embeddings[mask],
                labels[mask]
            )
            
            # Cluster statistics
            unique_labels = np.unique(labels[labels >= 0])
            cluster_sizes = [np.sum(labels == label) for label in unique_labels]
            
            metrics['avg_cluster_size'] = np.mean(cluster_sizes)
            metrics['std_cluster_size'] = np.std(cluster_sizes)
            metrics['min_cluster_size'] = np.min(cluster_sizes)
            metrics['max_cluster_size'] = np.max(cluster_sizes)
            
        except Exception as e:
            logger.error(f"Error calculating metrics: {e}")
        
        return metrics
    
    def _characterize_clusters(self, 
                             labels: np.ndarray,
                             skill_texts: List[str]) -> List[Dict[str, Any]]:
        """Characterize each cluster.
        
        Args:
            labels: Cluster labels
            skill_texts: List of skill texts
            
        Returns:
            List of cluster characteristics
        """
        cluster_info = []
        
        # Create DataFrame for easier analysis
        df = pd.DataFrame({
            'skill': skill_texts,
            'cluster': labels
        })
        
        # Analyze each cluster
        unique_labels = sorted(set(labels))
        
        for label in unique_labels:
            if label == -1:  # Skip noise cluster
                continue
            
            cluster_skills = df[df['cluster'] == label]['skill'].tolist()
            
            # Get most common skills
            skill_counts = pd.Series(cluster_skills).value_counts()
            
            cluster_data = {
                'cluster_id': int(label),
                'size': len(cluster_skills),
                'top_skills': skill_counts.head(10).to_dict(),
                'all_skills': cluster_skills,
                'label': self._generate_cluster_label(skill_counts.head(5).index.tolist())
            }
            
            cluster_info.append(cluster_data)
        
        # Sort by size
        cluster_info.sort(key=lambda x: x['size'], reverse=True)
        
        return cluster_info
    
    def _generate_cluster_label(self, top_skills: List[str]) -> str:
        """Generate a descriptive label for a cluster.
        
        Args:
            top_skills: Top skills in cluster
            
        Returns:
            Cluster label
        """
        # Simple heuristic-based labeling
        skill_lower = [s.lower() for s in top_skills]
        
        if any('frontend' in s or 'react' in s or 'angular' in s or 'vue' in s for s in skill_lower):
            return "Frontend Development"
        elif any('backend' in s or 'node' in s or 'django' in s or 'spring' in s for s in skill_lower):
            return "Backend Development"
        elif any('data' in s or 'analytics' in s or 'sql' in s for s in skill_lower):
            return "Data & Analytics"
        elif any('machine learning' in s or 'ml' in s or 'ai' in s for s in skill_lower):
            return "Machine Learning & AI"
        elif any('devops' in s or 'docker' in s or 'kubernetes' in s for s in skill_lower):
            return "DevOps & Infrastructure"
        elif any('mobile' in s or 'android' in s or 'ios' in s for s in skill_lower):
            return "Mobile Development"
        elif any('cloud' in s or 'aws' in s or 'azure' in s for s in skill_lower):
            return "Cloud Computing"
        else:
            # Use most common skill as label
            return f"{top_skills[0]} & Related"
    
    def run_clustering_pipeline(self) -> Dict[str, Any]:
        """Run complete clustering pipeline on all skills.
        
        Returns:
            Complete clustering results
        """
        # Get all embeddings
        embeddings_data = self.db_ops.get_all_embeddings()
        
        if not embeddings_data:
            logger.warning("No embeddings found for clustering")
            return {}
        
        # Extract data
        skill_texts = [e['skill_text'] for e in embeddings_data]
        embeddings = np.array([e['embedding'] for e in embeddings_data])
        
        logger.info(f"Running clustering on {len(embeddings)} skills")
        
        # Run clustering
        results = self.cluster_skills(embeddings, skill_texts)
        
        # Save results to database
        self.db_ops.save_analysis_results(
            analysis_type='clustering',
            results={
                'n_clusters': results['n_clusters'],
                'n_noise': results['n_noise'],
                'metrics': results['metrics'],
                'cluster_info': results['cluster_info']
            },
            parameters=results['parameters']
        )
        
        return results,  # Only special characters
        r'test|prueba|ejemplo',  # Test data
    ]
    
    for pattern in blacklist_patterns:
        if re.search(pattern, skill, re.IGNORECASE):
            return False
    
    return True

def validate_url(url: str) -> bool:
    """Validate URL format.
    
    Args:
        url: URL to validate
        
    Returns:
        True if valid URL, False otherwise
    """
    url_pattern = re.compile(
        r'^https?://'  # http:// or https://
        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
        r'localhost|'  # localhost...
        r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
        r'(?::\d+)?'  # optional port
        r'(?:/?|[/?]\S+)(#1-root-configuration-files)
2. [Database Setup Files](#2-database-setup-files)
3. [Configuration Module](#3-configuration-module)
4. [Scraper Module Files](#4-scraper-module-files)
5. [Extractor Module Files](#5-extractor-module-files)
6. [LLM Processor Module Files](#6-llm-processor-module-files)
7. [Embedder Module Files](#7-embedder-module-files)
8. [Analyzer Module Files](#8-analyzer-module-files)
9. [Orchestrator and Utilities](#9-orchestrator-and-utilities)
10. [Scripts](#10-scripts)

---

## 1. Root Configuration Files

### requirements.txt
```
# Core
python-dotenv==1.0.0
pydantic==2.5.3
typer==0.9.0
tqdm==4.66.1

# Web Scraping
scrapy==2.11.0
scrapy-selenium==0.0.7
beautifulsoup4==4.12.2
lxml==4.9.3
fake-useragent==1.4.0
requests==2.31.0

# Database
psycopg2-binary==2.9.9
sqlalchemy==2.0.23
pgvector==0.2.3
alembic==1.13.1

# NLP
spacy==3.7.2
langdetect==1.0.9
regex==2023.12.25

# Machine Learning
transformers==4.36.2
sentence-transformers==2.2.2
torch==2.1.2
llama-cpp-python==0.2.32
openai==1.6.1

# Data Processing
pandas==2.1.4
numpy==1.24.3
scikit-learn==1.3.2
umap-learn==0.5.5
hdbscan==0.8.33

# Visualization
matplotlib==3.8.2
seaborn==0.13.0
reportlab==4.0.8
pillow==10.1.0

# Testing
pytest==7.4.4
pytest-asyncio==0.23.3
pytest-cov==4.1.0

# Development
black==23.12.1
flake8==7.0.0
mypy==1.8.0
pre-commit==3.6.0
```

### .env.example
```bash
# Database Configuration
DATABASE_URL=postgresql://labor_user:your_password@localhost:5432/labor_observatory
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=0

# Scraping Configuration
SCRAPER_USER_AGENT="Mozilla/5.0 (Windows NT 10.0; Win64; x64) Academic Research Bot"
SCRAPER_CONCURRENT_REQUESTS=16
SCRAPER_DOWNLOAD_DELAY=1.0
SCRAPER_RETRY_TIMES=3

# ESCO API Configuration
ESCO_API_URL=https://ec.europa.eu/esco/api
ESCO_VERSION=1.1.0
ESCO_LANGUAGE=es

# LLM Configuration
LLM_MODEL_PATH=./data/models/mistral-7b-instruct.Q4_K_M.gguf
LLM_CONTEXT_LENGTH=4096
LLM_MAX_TOKENS=512
LLM_TEMPERATURE=0.7
LLM_N_GPU_LAYERS=35

# OpenAI Fallback (Optional)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-3.5-turbo

# Embedding Configuration
EMBEDDING_MODEL=intfloat/multilingual-e5-base
EMBEDDING_BATCH_SIZE=32
EMBEDDING_CACHE_DIR=./data/cache/embeddings

# Analysis Configuration
CLUSTER_MIN_SIZE=5
CLUSTER_MIN_SAMPLES=3
UMAP_N_NEIGHBORS=15
UMAP_MIN_DIST=0.1

# Output Configuration
OUTPUT_DIR=./outputs
REPORT_FORMAT=pdf
LOG_LEVEL=INFO
LOG_FILE=./logs/labor_observatory.log
```

### .gitignore
```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/
.venv

# IDE
.vscode/
.idea/
*.swp
*.swo

# Data and Models
data/models/*.gguf
data/models/*/
data/cache/
outputs/
logs/

# Database
*.db
*.sqlite3

# Environment
.env
.env.local

# OS
.DS_Store
Thumbs.db

# Testing
.coverage
htmlcov/
.pytest_cache/

# Scrapy
.scrapy/

# Notebooks
.ipynb_checkpoints/
```

### setup.py
```python
from setuptools import setup, find_packages

setup(
    name="labor-observatory",
    version="1.0.0",
    author="Your Team",
    description="Automated Labor Market Observatory for Latin America",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    python_requires=">=3.10",
    install_requires=[
        line.strip()
        for line in open("requirements.txt")
        if line.strip() and not line.startswith("#")
    ],
    entry_points={
        "console_scripts": [
            "labor-observatory=orchestrator:app",
        ],
    },
)
```

---

## 2. Database Setup Files

### src/database/migrations/001_initial_schema.sql
```sql
-- Create database
CREATE DATABASE IF NOT EXISTS labor_observatory
  WITH ENCODING 'UTF8'
  LC_COLLATE = 'en_US.UTF-8'
  LC_CTYPE = 'en_US.UTF-8';

\c labor_observatory;

-- Enable extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgvector";

-- Create tables
CREATE TABLE IF NOT EXISTS raw_jobs (
    job_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    portal VARCHAR(50) NOT NULL,
    country CHAR(2) NOT NULL,
    url TEXT NOT NULL,
    title TEXT NOT NULL,
    company TEXT,
    location TEXT,
    description TEXT NOT NULL,
    requirements TEXT,
    salary_raw TEXT,
    contract_type VARCHAR(50),
    remote_type VARCHAR(50),
    posted_date DATE,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    content_hash VARCHAR(64) UNIQUE,
    raw_html TEXT,
    is_processed BOOLEAN DEFAULT FALSE,
    
    CONSTRAINT chk_country CHECK (country IN ('CO', 'MX', 'AR')),
    CONSTRAINT chk_portal CHECK (portal IN ('computrabajo', 'bumeran', 'elempleo'))
);

CREATE INDEX idx_portal_country ON raw_jobs(portal, country);
CREATE INDEX idx_scraped_at ON raw_jobs(scraped_at);
CREATE INDEX idx_processed ON raw_jobs(is_processed);

CREATE TABLE IF NOT EXISTS extracted_skills (
    extraction_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    job_id UUID REFERENCES raw_jobs(job_id) ON DELETE CASCADE,
    skill_text TEXT NOT NULL,
    skill_type VARCHAR(50),
    extraction_method VARCHAR(50),
    confidence_score FLOAT,
    source_section VARCHAR(50),
    span_start INTEGER,
    span_end INTEGER,
    esco_uri TEXT,
    extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_job_skills ON extracted_skills(job_id);
CREATE INDEX idx_skill_text ON extracted_skills(skill_text);

CREATE TABLE IF NOT EXISTS enhanced_skills (
    enhancement_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    job_id UUID REFERENCES raw_jobs(job_id) ON DELETE CASCADE,
    original_skill_text TEXT,
    normalized_skill TEXT NOT NULL,
    skill_type VARCHAR(50),
    esco_concept_uri TEXT,
    esco_preferred_label TEXT,
    llm_confidence FLOAT,
    llm_reasoning TEXT,
    is_duplicate BOOLEAN DEFAULT FALSE,
    duplicate_of_id UUID,
    enhanced_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    llm_model VARCHAR(100)
);

CREATE INDEX idx_job_enhanced ON enhanced_skills(job_id);
CREATE INDEX idx_normalized ON enhanced_skills(normalized_skill);

CREATE TABLE IF NOT EXISTS skill_embeddings (
    embedding_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    skill_text TEXT UNIQUE NOT NULL,
    embedding vector(768) NOT NULL,
    model_name VARCHAR(100) NOT NULL,
    model_version VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_skill_lookup ON skill_embeddings(skill_text);
CREATE INDEX idx_embedding_similarity ON skill_embeddings 
  USING ivfflat (embedding vector_cosine_ops)
  WITH (lists = 100);

CREATE TABLE IF NOT EXISTS analysis_results (
    analysis_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    analysis_type VARCHAR(50),
    country CHAR(2),
    date_range_start DATE,
    date_range_end DATE,
    parameters JSONB,
    results JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_analysis_type ON analysis_results(analysis_type);
CREATE INDEX idx_analysis_date ON analysis_results(created_at);

-- Create views
CREATE VIEW skill_frequency AS
SELECT 
    es.normalized_skill,
    COUNT(DISTINCT es.job_id) as job_count,
    COUNT(*) as total_mentions,
    ARRAY_AGG(DISTINCT rj.country) as countries
FROM enhanced_skills es
JOIN raw_jobs rj ON es.job_id = rj.job_id
WHERE es.is_duplicate = FALSE
GROUP BY es.normalized_skill
ORDER BY job_count DESC;

CREATE VIEW country_skill_distribution AS
SELECT 
    rj.country,
    es.normalized_skill,
    COUNT(DISTINCT es.job_id) as job_count
FROM enhanced_skills es
JOIN raw_jobs rj ON es.job_id = rj.job_id
WHERE es.is_duplicate = FALSE
GROUP BY rj.country, es.normalized_skill
ORDER BY rj.country, job_count DESC;
```

### src/database/models.py
```python
from sqlalchemy import Column, String, Text, Boolean, Float, Integer, DateTime, Date, ForeignKey, JSON
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from pgvector.sqlalchemy import Vector
import uuid

Base = declarative_base()

class RawJob(Base):
    __tablename__ = 'raw_jobs'
    
    job_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    portal = Column(String(50), nullable=False)
    country = Column(String(2), nullable=False)
    url = Column(Text, nullable=False)
    title = Column(Text, nullable=False)
    company = Column(Text)
    location = Column(Text)
    description = Column(Text, nullable=False)
    requirements = Column(Text)
    salary_raw = Column(Text)
    contract_type = Column(String(50))
    remote_type = Column(String(50))
    posted_date = Column(Date)
    scraped_at = Column(DateTime, server_default=func.now())
    content_hash = Column(String(64), unique=True)
    raw_html = Column(Text)
    is_processed = Column(Boolean, default=False)
    
    # Relationships
    extracted_skills = relationship("ExtractedSkill", back_populates="job")
    enhanced_skills = relationship("EnhancedSkill", back_populates="job")

class ExtractedSkill(Base):
    __tablename__ = 'extracted_skills'
    
    extraction_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    job_id = Column(UUID(as_uuid=True), ForeignKey('raw_jobs.job_id'))
    skill_text = Column(Text, nullable=False)
    skill_type = Column(String(50))
    extraction_method = Column(String(50))
    confidence_score = Column(Float)
    source_section = Column(String(50))
    span_start = Column(Integer)
    span_end = Column(Integer)
    esco_uri = Column(Text)
    extracted_at = Column(DateTime, server_default=func.now())
    
    # Relationships
    job = relationship("RawJob", back_populates="extracted_skills")

class EnhancedSkill(Base):
    __tablename__ = 'enhanced_skills'
    
    enhancement_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    job_id = Column(UUID(as_uuid=True), ForeignKey('raw_jobs.job_id'))
    original_skill_text = Column(Text)
    normalized_skill = Column(Text, nullable=False)
    skill_type = Column(String(50))
    esco_concept_uri = Column(Text)
    esco_preferred_label = Column(Text)
    llm_confidence = Column(Float)
    llm_reasoning = Column(Text)
    is_duplicate = Column(Boolean, default=False)
    duplicate_of_id = Column(UUID(as_uuid=True))
    enhanced_at = Column(DateTime, server_default=func.now())
    llm_model = Column(String(100))
    
    # Relationships
    job = relationship("RawJob", back_populates="enhanced_skills")

class SkillEmbedding(Base):
    __tablename__ = 'skill_embeddings'
    
    embedding_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    skill_text = Column(Text, unique=True, nullable=False)
    embedding = Column(Vector(768), nullable=False)
    model_name = Column(String(100), nullable=False)
    model_version = Column(String(50))
    created_at = Column(DateTime, server_default=func.now())

class AnalysisResult(Base):
    __tablename__ = 'analysis_results'
    
    analysis_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    analysis_type = Column(String(50))
    country = Column(String(2))
    date_range_start = Column(Date)
    date_range_end = Column(Date)
    parameters = Column(JSONB)
    results = Column(JSONB)
    created_at = Column(DateTime, server_default=func.now())
```

### src/database/operations.py
```python
from typing import List, Optional, Dict, Any
from datetime import datetime
from sqlalchemy import create_engine, and_, or_, func
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.exc import IntegrityError
import os
from .models import Base, RawJob, ExtractedSkill, EnhancedSkill, SkillEmbedding, AnalysisResult
import hashlib
import logging

logger = logging.getLogger(__name__)

class DatabaseOperations:
    def __init__(self, database_url: Optional[str] = None):
        self.database_url = database_url or os.getenv('DATABASE_URL')
        self.engine = create_engine(
            self.database_url,
            pool_size=20,
            max_overflow=0,
            pool_pre_ping=True
        )
        Base.metadata.create_all(self.engine)
        self.SessionLocal = sessionmaker(bind=self.engine)
    
    def get_session(self) -> Session:
        """Get a new database session."""
        return self.SessionLocal()
    
    def insert_job(self, job_data: Dict[str, Any]) -> Optional[str]:
        """Insert a new job posting."""
        session = self.get_session()
        try:
            # Generate content hash
            content = f"{job_data['title']}{job_data['description']}{job_data.get('requirements', '')}"
            content_hash = hashlib.sha256(content.encode()).hexdigest()
            
            job = RawJob(
                **job_data,
                content_hash=content_hash
            )
            session.add(job)
            session.commit()
            
            job_id = str(job.job_id)
            logger.info(f"Inserted job {job_id}")
            return job_id
            
        except IntegrityError:
            session.rollback()
            logger.warning(f"Duplicate job detected: {job_data['url']}")
            return None

### src/analyzer/visualizations.py
```python
import logging
from typing import Dict, Any, List, Optional, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from datetime import datetime
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class VisualizationGenerator:
    """Generate static visualizations for analysis results."""
    
    def __init__(self, output_dir: str = None):
        self.output_dir = output_dir or "./outputs"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Set style
        plt.style.use('seaborn-v0_8-darkgrid')
        sns.set_palette("husl")
        
        # Set font for better Spanish support
        plt.rcParams['font.family'] = 'DejaVu Sans'
    
    def create_all_visualizations(self, 
                                analysis_data: Dict[str, Any],
                                country: Optional[str] = None) -> List[str]:
        """Create all standard visualizations.
        
        Args:
            analysis_data: Dictionary with analysis results
            country: Country code for filtering
            
        Returns:
            List of generated file paths
        """
        generated_files = []
        
        # Skill frequency chart
        if 'skill_statistics' in analysis_data:
            path = self.create_skill_frequency_chart(
                analysis_data['skill_statistics'],
                country
            )
            if path:
                generated_files.append(path)
        
        # Cluster visualization
        if 'clustering_results' in analysis_data:
            path = self.create_cluster_visualization(
                analysis_data['clustering_results']
            )
            if path:
                generated_files.append(path)
        
        # Geographic distribution
        if 'geographic_data' in analysis_data:
            path = self.create_geographic_distribution(
                analysis_data['geographic_data']
            )
            if path:
                generated_files.append(path)
        
        # Skill co-occurrence heatmap
        if 'skill_cooccurrence' in analysis_data:
            path = self.create_skill_cooccurrence_heatmap(
                analysis_data['skill_cooccurrence']
            )
            if path:
                generated_files.append(path)
        
        return generated_files
    
    def create_skill_frequency_chart(self,
                                   skill_stats: Dict[str, Any],
                                   country: Optional[str] = None,
                                   top_n: int = 20) -> Optional[str]:
        """Create horizontal bar chart of top skills.
        
        Args:
            skill_stats: Skill statistics data
            country: Country filter
            top_n: Number of top skills to show
            
        Returns:
            Path to saved chart
        """
        try:
            # Prepare data
            top_skills = skill_stats.get('top_skills', [])[:top_n]
            if not top_skills:
                logger.warning("No skill data for frequency chart")
                return None
            
            df = pd.DataFrame(top_skills)
            
            # Create figure
            fig, ax = plt.subplots(figsize=(12, 8))
            
            # Create horizontal bar chart
            bars = ax.barh(df['skill'], df['count'], 
                          color=plt.cm.viridis(np.linspace(0.3, 0.9, len(df))))
            
            # Customize
            ax.set_xlabel('Número de Vacantes', fontsize=14)
            ax.set_ylabel('Habilidad Técnica', fontsize=14)
            
            title = f'Top {top_n} Habilidades Más Demandadas'
            if country:
                country_names = {'CO': 'Colombia', 'MX': 'México', 'AR': 'Argentina'}
                title += f' - {country_names.get(country, country)}'
            ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
            
            # Add value labels
            for bar in bars:
                width = bar.get_width()
                ax.text(width + 1, bar.get_y() + bar.get_height()/2,
                       f'{int(width)}',
                       ha='left', va='center', fontsize=10)
            
            # Adjust layout
            plt.tight_layout()
            ax.invert_yaxis()  # Highest on top
            
            # Grid
            ax.grid(True, axis='x', alpha=0.3)
            ax.set_axisbelow(True)
            
            # Save
            filename = self._generate_filename('skill_frequency', country)
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created skill frequency chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating skill frequency chart: {e}")
            return None
    
    def create_cluster_visualization(self,
                                   clustering_results: Dict[str, Any]) -> Optional[str]:
        """Create 2D scatter plot of skill clusters.
        
        Args:
            clustering_results: Clustering analysis results
            
        Returns:
            Path to saved visualization
        """
        try:
            # Extract data
            coordinates = clustering_results.get('coordinates_2d')
            labels = clustering_results.get('labels')
            skills = clustering_results.get('skills', [])
            
            if coordinates is None or labels is None:
                logger.warning("Missing data for cluster visualization")
                return None
            
            # Create figure
            fig, ax = plt.subplots(figsize=(14, 10))
            
            # Get unique labels
            unique_labels = set(labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            
            # Color map
            colors = plt.cm.rainbow(np.linspace(0, 1, n_clusters))
            
            # Plot each cluster
            for k, col in zip(sorted(unique_labels), colors):
                if k == -1:
                    # Noise points in black
                    col = 'black'
                    label = 'Ruido'
                else:
                    label = f'Cluster {k}'
                
                class_member_mask = (labels == k)
                xy = coordinates[class_member_mask]
                
                ax.scatter(xy[:, 0], xy[:, 1], 
                         c=[col], 
                         label=label,
                         alpha=0.6,
                         s=30)
            
            # Add labels for some points (avoid overlap)
            if skills:
                # Sample points to label
                n_labels = min(30, len(skills))
                indices = np.random.choice(len(skills), n_labels, replace=False)
                
                for idx in indices:
                    ax.annotate(skills[idx], 
                              (coordinates[idx, 0], coordinates[idx, 1]),
                              fontsize=8,
                              alpha=0.7)
            
            # Customize
            ax.set_title('Visualización de Clusters de Habilidades', 
                        fontsize=16, fontweight='bold')
            ax.set_xlabel('UMAP Dimension 1', fontsize=12)
            ax.set_ylabel('UMAP Dimension 2', fontsize=12)
            
            # Legend
            ax.legend(loc='upper right', bbox_to_anchor=(1.15, 1))
            
            # Remove ticks
            ax.set_xticks([])
            ax.set_yticks([])
            
            # Save
            filename = self._generate_filename('skill_clusters')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created cluster visualization: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating cluster visualization: {e}")
            return None
    
    def create_geographic_distribution(self,
                                     geo_data: Dict[str, Any]) -> Optional[str]:
        """Create geographic distribution chart.
        
        Args:
            geo_data: Geographic distribution data
            
        Returns:
            Path to saved chart
        """
        try:
            # Create figure
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # Job distribution by country
            countries = ['Colombia', 'México', 'Argentina']
            job_counts = [
                geo_data.get('CO', {}).get('total_jobs', 0),
                geo_data.get('MX', {}).get('total_jobs', 0),
                geo_data.get('AR', {}).get('total_jobs', 0)
            ]
            
            # Pie chart
            colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
            wedges, texts, autotexts = ax1.pie(job_counts, 
                                              labels=countries,
                                              colors=colors,
                                              autopct='%1.1f%%',
                                              startangle=90)
            
            ax1.set_title('Distribución de Vacantes por País', 
                         fontsize=14, fontweight='bold')
            
            # Skills per country
            skill_counts = [
                geo_data.get('CO', {}).get('unique_skills', 0),
                geo_data.get('MX', {}).get('unique_skills', 0),
                geo_data.get('AR', {}).get('unique_skills', 0)
            ]
            
            bars = ax2.bar(countries, skill_counts, color=colors)
            ax2.set_title('Habilidades Únicas por País', 
                         fontsize=14, fontweight='bold')
            ax2.set_ylabel('Número de Habilidades', fontsize=12)
            
            # Add value labels
            for bar in bars:
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height,
                        f'{int(height)}',
                        ha='center', va='bottom')
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('geographic_distribution')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created geographic distribution chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating geographic distribution: {e}")
            return None
    
    def create_skill_cooccurrence_heatmap(self,
                                        cooccurrence_data: Dict[str, Any],
                                        top_n: int = 15) -> Optional[str]:
        """Create heatmap of skill co-occurrences.
        
        Args:
            cooccurrence_data: Skill co-occurrence matrix
            top_n: Number of top skills to include
            
        Returns:
            Path to saved heatmap
        """
        try:
            # Convert to DataFrame
            df = pd.DataFrame(cooccurrence_data.get('matrix', []))
            skills = cooccurrence_data.get('skills', [])
            
            if df.empty or not skills:
                logger.warning("No co-occurrence data available")
                return None
            
            # Select top skills
            if len(skills) > top_n:
                # Sum co-occurrences for each skill
                skill_importance = df.sum(axis=0) + df.sum(axis=1)
                top_indices = skill_importance.nlargest(top_n).index
                df = df.loc[top_indices, top_indices]
                skills = [skills[i] for i in top_indices]
            
            # Create figure
            fig, ax = plt.subplots(figsize=(12, 10))
            
            # Create heatmap
            sns.heatmap(df, 
                       xticklabels=skills,
                       yticklabels=skills,
                       cmap='YlOrRd',
                       cbar_kws={'label': 'Co-ocurrencias'},
                       square=True,
                       linewidths=0.5,
                       ax=ax)
            
            # Customize
            ax.set_title('Matriz de Co-ocurrencia de Habilidades', 
                        fontsize=16, fontweight='bold', pad=20)
            
            # Rotate labels
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('skill_cooccurrence')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created co-occurrence heatmap: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating co-occurrence heatmap: {e}")
            return None
    
    def create_temporal_trends(self,
                             temporal_data: Dict[str, Any],
                             skills: List[str] = None) -> Optional[str]:
        """Create temporal trend visualization.
        
        Args:
            temporal_data: Temporal trend data
            skills: List of skills to plot (default: top 5)
            
        Returns:
            Path to saved chart
        """
        try:
            # Prepare data
            df = pd.DataFrame(temporal_data.get('trends', []))
            
            if df.empty:
                logger.warning("No temporal data available")
                return None
            
            # Convert date column
            df['date'] = pd.to_datetime(df['date'])
            
            # Select skills to plot
            if not skills:
                # Get top 5 skills by total mentions
                skill_totals = df.groupby('skill')['count'].sum()
                skills = skill_totals.nlargest(5).index.tolist()
            
            # Create figure
            fig, ax = plt.subplots(figsize=(14, 8))
            
            # Plot each skill
            for skill in skills:
                skill_data = df[df['skill'] == skill]
                ax.plot(skill_data['date'], skill_data['count'], 
                       marker='o', label=skill, linewidth=2)
            
            # Customize
            ax.set_title('Tendencias Temporales de Habilidades', 
                        fontsize=16, fontweight='bold')
            ax.set_xlabel('Fecha', fontsize=12)
            ax.set_ylabel('Número de Menciones', fontsize=12)
            
            # Format x-axis
            import matplotlib.dates as mdates
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
            ax.xaxis.set_major_locator(mdates.MonthLocator())
            plt.xticks(rotation=45)
            
            # Legend
            ax.legend(loc='upper left', bbox_to_anchor=(1, 1))
            
            # Grid
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('temporal_trends')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created temporal trends chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating temporal trends: {e}")
            return None
    
    def _generate_filename(self, chart_type: str, country: Optional[str] = None) -> str:
        """Generate filename with timestamp.
        
        Args:
            chart_type: Type of chart
            country: Country code (optional)
            
        Returns:
            Generated filename
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        country_suffix = f"_{country}" if country else ""
        return f"{chart_type}{country_suffix}_{timestamp}.png"

---

## 9. Orchestrator and Utilities

### src/orchestrator.py
```python
#!/usr/bin/env python3
"""
Main orchestrator for the Labor Market Observatory pipeline.
"""

import logging
import sys
from typing import Optional
import typer
from rich.console import Console
from rich.table import Table
from rich.progress import track
import time

from config.settings import get_settings
from config.logging_config import setup_logging
from database.operations import DatabaseOperations
from scraper.spiders.computrabajo_spider import ComputrabajoSpider
from scraper.spiders.bumeran_spider import BumeranSpider
from scraper.spiders.elempleo_spider import ElempleoSpider
from extractor.pipeline import ExtractionPipeline
from llm_processor.pipeline import LLMProcessingPipeline
from embedder.batch_processor import BatchProcessor
from analyzer.clustering import SkillClusterer
from analyzer.dimension_reducer import DimensionReducer
from analyzer.report_generator import ReportGenerator
from analyzer.visualizations import VisualizationGenerator

# Initialize
app = typer.Typer(help="Labor Market Observatory CLI")
console = Console()
settings = get_settings()
logger = setup_logging(settings.log_level, settings.log_file)

@app.command()
def scrape(
    country: str = typer.Argument(..., help="Country code (CO, MX, AR)"),
    portal: str = typer.Argument(..., help="Portal name (computrabajo, bumeran, elempleo)"),
    pages: int = typer.Option(10, help="Number of pages to scrape")
):
    """Run web scraping for a specific portal and country."""
    console.print(f"[bold green]Starting scraper for {portal} in {country}[/bold green]")
    
    # Validate inputs
    if country not in settings.supported_countries:
        console.print(f"[red]Invalid country: {country}[/red]")
        raise typer.Exit(1)
    
    if portal not in settings.supported_portals:
        console.print(f"[red]Invalid portal: {portal}[/red]")
        raise typer.Exit(1)
    
    # Run appropriate spider
    try:
        from scrapy.crawler import CrawlerProcess
        from scrapy.utils.project import get_project_settings
        
        # Get scrapy settings
        scrapy_settings = get_project_settings()
        scrapy_settings.update({
            'LOG_LEVEL': 'INFO',
            'CLOSESPIDER_PAGECOUNT': pages
        })
        
        process = CrawlerProcess(scrapy_settings)
        
        # Select spider
        if portal == 'computrabajo':
            spider_class = ComputrabajoSpider
        elif portal == 'bumeran':
            spider_class = BumeranSpider
        elif portal == 'elempleo':
            spider_class = ElempleoSpider
        
        # Run spider
        process.crawl(spider_class, country=country)
        process.start()
        
        console.print("[bold green]Scraping completed![/bold green]")
        
    except Exception as e:
        console.print(f"[red]Scraping failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def extract(
    batch_size: int = typer.Option(100, help="Batch size for processing")
):
    """Extract skills from scraped job postings."""
    console.print("[bold green]Starting skill extraction...[/bold green]")
    
    try:
        pipeline = ExtractionPipeline()
        
        with console.status("[bold green]Extracting skills...") as status:
            stats = pipeline.process_batch(batch_size)
        
        # Display results
        table = Table(title="Extraction Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Jobs Processed", str(stats['jobs_processed']))
        table.add_row("Skills Extracted", str(stats['skills_extracted']))
        table.add_row("ESCO Matches", str(stats['esco_matches']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        table.add_row("Jobs/Second", f"{stats['jobs_per_second']:.2f}")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Extraction failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def enhance(
    batch_size: int = typer.Option(50, help="Batch size for LLM processing"),
    model: str = typer.Option("local", help="Model type (local or openai)")
):
    """Enhance extracted skills using LLM."""
    console.print(f"[bold green]Starting LLM enhancement with {model} model...[/bold green]")
    
    try:
        pipeline = LLMProcessingPipeline(model_type=model)
        
        with console.status("[bold green]Processing with LLM...") as status:
            stats = pipeline.process_batch(batch_size)
        
        # Display results
        table = Table(title="LLM Enhancement Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Jobs Processed", str(stats['jobs_processed']))
        table.add_row("Skills Enhanced", str(stats['skills_enhanced']))
        table.add_row("Implicit Skills Found", str(stats['implicit_skills_found']))
        table.add_row("Skills Normalized", str(stats['skills_normalized']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Enhancement failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def embed(
    model: Optional[str] = typer.Option(None, help="Embedding model name")
):
    """Generate embeddings for all skills."""
    console.print("[bold green]Starting embedding generation...[/bold green]")
    
    try:
        processor = BatchProcessor(model_name=model)
        
        with console.status("[bold green]Generating embeddings...") as status:
            stats = processor.process_all_skills()
        
        # Display results
        table = Table(title="Embedding Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Skills Processed", str(stats['skills_processed']))
        table.add_row("Embeddings Created", str(stats['embeddings_created']))
        table.add_row("Errors", str(stats['errors']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Embedding failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def analyze(
    method: str = typer.Option("hdbscan", help="Clustering method")
):
    """Run clustering analysis on skill embeddings."""
    console.print(f"[bold green]Starting clustering analysis with {method}...[/bold green]")
    
    try:
        clusterer = SkillClusterer()
        
        with console.status("[bold green]Running clustering...") as status:
            results = clusterer.run_clustering_pipeline()
        
        # Display results
        if results:
            table = Table(title="Clustering Results")
            table.add_column("Metric", style="cyan")
            table.add_column("Value", style="magenta")
            
            table.add_row("Number of Clusters", str(results['n_clusters']))
            table.add_row("Noise Points", str(results['n_noise']))
            table.add_row("Silhouette Score", f"{results['metrics'].get('silhouette_score', 0):.3f}")
            
            console.print(table)
            
            # Show top clusters
            console.print("\n[bold]Top 5 Clusters:[/bold]")
            for cluster in results['cluster_info'][:5]:
                console.print(f"  • {cluster['label']}: {cluster['size']} skills")
        
    except Exception as e:
        console.print(f"[red]Analysis failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def report(
    country: Optional[str] = typer.Option(None, help="Country code to filter"),
    format: str = typer.Option("pdf", help="Report format (pdf)")
):
    """Generate analysis report."""
    console.print("[bold green]Generating report...[/bold green]")
    
    try:
        generator = ReportGenerator()
        
        with console.status("[bold green]Creating report...") as status:
            filepath = generator.generate_full_report(
                country=country,
                include_visualizations=True
            )
        
        console.print(f"[bold green]Report generated: {filepath}[/bold green]")
        
    except Exception as e:
        console.print(f"[red]Report generation failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def pipeline(
    country: str = typer.Argument(..., help="Country code"),
    portal: str = typer.Argument(..., help="Portal name"),
    full: bool = typer.Option(False, help="Run full pipeline including scraping")
):
    """Run complete pipeline."""
    console.print("[bold green]Running complete pipeline...[/bold green]")
    
    steps = []
    
    if full:
        steps.append(("Scraping", lambda: scrape(country, portal, pages=5)))
    
    steps.extend([
        ("Extraction", lambda: extract(batch_size=100)),
        ("LLM Enhancement", lambda: enhance(batch_size=50)),
        ("Embedding", lambda: embed()),
        ("Clustering", lambda: analyze()),
        ("Report Generation", lambda: report(country=country))
    ])
    
    for step_name, step_func in track(steps, description="Processing..."):
        try:
            console.print(f"\n[bold cyan]Running: {step_name}[/bold cyan]")
            step_func()
            time.sleep(1)  # Brief pause between steps
        except Exception as e:
            console.print(f"[red]Step '{step_name}' failed: {e}[/red]")
            raise typer.Exit(1)
    
    console.print("\n[bold green]Pipeline completed successfully![/bold green]")

@app.command()
def status():
    """Show system status and statistics."""
    console.print("[bold green]Labor Market Observatory Status[/bold green]\n")
    
    try:
        db_ops = DatabaseOperations()
        
        # Get statistics
        stats = db_ops.get_skill_statistics()
        
        # Display overall stats
        table = Table(title="System Statistics")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Total Unique Skills", str(stats.get('total_unique_skills', 0)))
        table.add_row("Database Size", "N/A")  # Would need to implement
        
        console.print(table)
        
        # Top skills
        if stats.get('top_skills'):
            console.print("\n[bold]Top 10 Skills:[/bold]")
            for i, skill in enumerate(stats['top_skills'][:10], 1):
                console.print(f"  {i}. {skill['skill']} ({skill['count']} jobs)")
        
    except Exception as e:
        console.print(f"[red]Failed to get status: {e}[/red]")
        raise typer.Exit(1)

if __name__ == "__main__":
    app()

### src/utils/__init__.py
```python
from .validators import validate_country, validate_portal, validate_skill
from .cleaners import clean_text, normalize_text, remove_html
from .metrics import calculate_metrics, generate_statistics
from .logger import get_logger

__all__ = [
    'validate_country', 'validate_portal', 'validate_skill',
    'clean_text', 'normalize_text', 'remove_html',
    'calculate_metrics', 'generate_statistics',
    'get_logger'
]
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting job: {e}")
            raise
        finally:
            session.close()
    
    def get_unprocessed_jobs(self, limit: int = 100) -> List[RawJob]:
        """Get unprocessed job postings."""
        session = self.get_session()
        try:
            jobs = session.query(RawJob).filter(
                RawJob.is_processed == False
            ).limit(limit).all()
            return jobs
        finally:
            session.close()
    
    def mark_job_processed(self, job_id: str):
        """Mark a job as processed."""
        session = self.get_session()
        try:
            session.query(RawJob).filter(
                RawJob.job_id == job_id
            ).update({"is_processed": True})
            session.commit()
        finally:
            session.close()
    
    def insert_extracted_skills(self, job_id: str, skills: List[Dict[str, Any]]):
        """Insert extracted skills for a job."""
        session = self.get_session()
        try:
            for skill_data in skills:
                skill = ExtractedSkill(
                    job_id=job_id,
                    **skill_data
                )
                session.add(skill)
            session.commit()
            logger.info(f"Inserted {len(skills)} extracted skills for job {job_id}")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting extracted skills: {e}")
            raise
        finally:
            session.close()
    
    def get_extracted_skills_for_processing(self, limit: int = 100) -> List[Dict[str, Any]]:
        """Get jobs with extracted skills that need LLM processing."""
        session = self.get_session()
        try:
            # Get jobs that have extracted skills but no enhanced skills
            subquery = session.query(EnhancedSkill.job_id).distinct()
            
            jobs = session.query(RawJob).join(ExtractedSkill).filter(
                ~RawJob.job_id.in_(subquery)
            ).limit(limit).all()
            
            result = []
            for job in jobs:
                skills = session.query(ExtractedSkill).filter(
                    ExtractedSkill.job_id == job.job_id
                ).all()
                
                result.append({
                    'job_id': str(job.job_id),
                    'job_title': job.title,
                    'job_description': job.description,
                    'job_requirements': job.requirements,
                    'extracted_skills': [
                        {
                            'skill_text': skill.skill_text,
                            'extraction_method': skill.extraction_method,
                            'source_section': skill.source_section,
                            'confidence_score': skill.confidence_score
                        }
                        for skill in skills
                    ]
                })
            
            return result
        finally:
            session.close()
    
    def insert_enhanced_skills(self, job_id: str, skills: List[Dict[str, Any]]):
        """Insert enhanced skills for a job."""
        session = self.get_session()
        try:
            for skill_data in skills:
                skill = EnhancedSkill(
                    job_id=job_id,
                    **skill_data
                )
                session.add(skill)
            session.commit()
            logger.info(f"Inserted {len(skills)} enhanced skills for job {job_id}")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting enhanced skills: {e}")
            raise
        finally:
            session.close()
    
    def get_unique_skills_for_embedding(self) -> List[str]:
        """Get unique normalized skills that don't have embeddings yet."""
        session = self.get_session()
        try:
            # Get skills that don't have embeddings
            embedded_skills = session.query(SkillEmbedding.skill_text).distinct()
            
            unique_skills = session.query(
                EnhancedSkill.normalized_skill
            ).filter(
                EnhancedSkill.is_duplicate == False,
                ~EnhancedSkill.normalized_skill.in_(embedded_skills)
            ).distinct().all()
            
            return [skill[0] for skill in unique_skills]
        finally:
            session.close()
    
    def insert_skill_embeddings(self, embeddings: List[Dict[str, Any]]):
        """Insert skill embeddings."""
        session = self.get_session()
        try:
            for emb_data in embeddings:
                embedding = SkillEmbedding(**emb_data)
                session.add(embedding)
            session.commit()
            logger.info(f"Inserted {len(embeddings)} skill embeddings")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting embeddings: {e}")
            raise
        finally:
            session.close()
    
    def get_all_embeddings(self) -> List[Dict[str, Any]]:
        """Get all skill embeddings for clustering."""
        session = self.get_session()
        try:
            embeddings = session.query(SkillEmbedding).all()
            return [
                {
                    'skill_text': emb.skill_text,
                    'embedding': emb.embedding,
                    'embedding_id': str(emb.embedding_id)
                }
                for emb in embeddings
            ]
        finally:
            session.close()
    
    def save_analysis_results(self, analysis_type: str, results: Dict[str, Any], 
                            parameters: Dict[str, Any], country: Optional[str] = None):
        """Save analysis results."""
        session = self.get_session()
        try:
            analysis = AnalysisResult(
                analysis_type=analysis_type,
                country=country,
                parameters=parameters,
                results=results
            )
            session.add(analysis)
            session.commit()
            logger.info(f"Saved {analysis_type} analysis results")
        except Exception as e:
            session.rollback()
            logger.error(f"Error saving analysis results: {e}")
            raise
        finally:
            session.close()
    
    def get_skill_statistics(self, country: Optional[str] = None) -> Dict[str, Any]:
        """Get skill statistics by country."""
        session = self.get_session()
        try:
            query = session.query(
                EnhancedSkill.normalized_skill,
                func.count(func.distinct(EnhancedSkill.job_id)).label('job_count')
            ).join(RawJob).filter(
                EnhancedSkill.is_duplicate == False
            )
            
            if country:
                query = query.filter(RawJob.country == country)
            
            results = query.group_by(
                EnhancedSkill.normalized_skill
            ).order_by(
                func.count(func.distinct(EnhancedSkill.job_id)).desc()
            ).limit(50).all()
            
            return {
                'top_skills': [
                    {'skill': skill, 'count': count}
                    for skill, count in results
                ],
                'total_unique_skills': session.query(
                    func.count(func.distinct(EnhancedSkill.normalized_skill))
                ).filter(EnhancedSkill.is_duplicate == False).scalar()
            }
        finally:
            session.close()
```

---

## 3. Configuration Module

### src/config/__init__.py
```python
from .settings import Settings, get_settings
from .database import get_database_url
from .logging_config import setup_logging

__all__ = ['Settings', 'get_settings', 'get_database_url', 'setup_logging']
```

### src/config/settings.py
```python
from pydantic_settings import BaseSettings
from pydantic import Field, validator
from typing import Optional, List
import os
from functools import lru_cache

class Settings(BaseSettings):
    # Database
    database_url: str = Field(..., env='DATABASE_URL')
    database_pool_size: int = Field(20, env='DATABASE_POOL_SIZE')
    
    # Scraping
    scraper_user_agent: str = Field(..., env='SCRAPER_USER_AGENT')
    scraper_concurrent_requests: int = Field(16, env='SCRAPER_CONCURRENT_REQUESTS')
    scraper_download_delay: float = Field(1.0, env='SCRAPER_DOWNLOAD_DELAY')
    scraper_retry_times: int = Field(3, env='SCRAPER_RETRY_TIMES')
    
    # ESCO
    esco_api_url: str = Field('https://ec.europa.eu/esco/api', env='ESCO_API_URL')
    esco_version: str = Field('1.1.0', env='ESCO_VERSION')
    esco_language: str = Field('es', env='ESCO_LANGUAGE')
    
    # LLM
    llm_model_path: str = Field(..., env='LLM_MODEL_PATH')
    llm_context_length: int = Field(4096, env='LLM_CONTEXT_LENGTH')
    llm_max_tokens: int = Field(512, env='LLM_MAX_TOKENS')
    llm_temperature: float = Field(0.7, env='LLM_TEMPERATURE')
    llm_n_gpu_layers: int = Field(35, env='LLM_N_GPU_LAYERS')
    
    # OpenAI (Optional)
    openai_api_key: Optional[str] = Field(None, env='OPENAI_API_KEY')
    openai_model: str = Field('gpt-3.5-turbo', env='OPENAI_MODEL')
    
    # Embeddings
    embedding_model: str = Field('intfloat/multilingual-e5-base', env='EMBEDDING_MODEL')
    embedding_batch_size: int = Field(32, env='EMBEDDING_BATCH_SIZE')
    embedding_cache_dir: str = Field('./data/cache/embeddings', env='EMBEDDING_CACHE_DIR')
    
    # Analysis
    cluster_min_size: int = Field(5, env='CLUSTER_MIN_SIZE')
    cluster_min_samples: int = Field(3, env='CLUSTER_MIN_SAMPLES')
    umap_n_neighbors: int = Field(15, env='UMAP_N_NEIGHBORS')
    umap_min_dist: float = Field(0.1, env='UMAP_MIN_DIST')
    
    # Output
    output_dir: str = Field('./outputs', env='OUTPUT_DIR')
    report_format: str = Field('pdf', env='REPORT_FORMAT')
    log_level: str = Field('INFO', env='LOG_LEVEL')
    log_file: str = Field('./logs/labor_observatory.log', env='LOG_FILE')
    
    # Supported countries and portals
    supported_countries: List[str] = ['CO', 'MX', 'AR']
    supported_portals: List[str] = ['computrabajo', 'bumeran', 'elempleo']
    
    class Config:
        env_file = '.env'
        env_file_encoding = 'utf-8'
    
    @validator('output_dir', 'log_file', 'embedding_cache_dir')
    def create_directories(cls, v):
        os.makedirs(os.path.dirname(v) if os.path.dirname(v) else v, exist_ok=True)
        return v

@lru_cache()
def get_settings() -> Settings:
    """Get cached settings instance."""
    return Settings()
```

### src/config/database.py
```python
import os
from urllib.parse import urlparse

def get_database_url() -> str:
    """Get database URL from environment or construct from components."""
    if os.getenv('DATABASE_URL'):
        return os.getenv('DATABASE_URL')
    
    # Construct from individual components
    user = os.getenv('DB_USER', 'labor_user')
    password = os.getenv('DB_PASSWORD', 'password')
    host = os.getenv('DB_HOST', 'localhost')
    port = os.getenv('DB_PORT', '5432')
    name = os.getenv('DB_NAME', 'labor_observatory')
    
    return f"postgresql://{user}:{password}@{host}:{port}/{name}"

def get_database_config() -> dict:
    """Parse database URL into components."""
    url = get_database_url()
    parsed = urlparse(url)
    
    return {
        'host': parsed.hostname,
        'port': parsed.port or 5432,
        'user': parsed.username,
        'password': parsed.password,
        'database': parsed.path.lstrip('/')
    }
```

### src/config/logging_config.py
```python
import logging
import logging.handlers
import os
from datetime import datetime

def setup_logging(log_level: str = 'INFO', log_file: str = None):
    """Configure logging for the entire application."""
    
    # Create logs directory if needed
    if log_file:
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    # Create formatters
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
    )
    console_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )
    
    # Get root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, log_level.upper()))
    
    # Remove existing handlers
    root_logger.handlers.clear()
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)
    
    # File handler with rotation
    if log_file:
        file_handler = logging.handlers.RotatingFileHandler(
            log_file,
            maxBytes=10 * 1024 * 1024,  # 10MB
            backupCount=5
        )
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(file_formatter)
        root_logger.addHandler(file_handler)
    
    # Specific loggers configuration
    logging.getLogger('scrapy').setLevel(logging.WARNING)
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('transformers').setLevel(logging.WARNING)
    
    return root_logger
```

### config/esco_config.yaml
```yaml
# ESCO Configuration for Spanish Language
esco:
  version: "1.1.0"
  base_url: "https://ec.europa.eu/esco/api"
  
  # Endpoints
  endpoints:
    skills: "/resource/skill"
    occupations: "/resource/occupation"
    search: "/search"
  
  # Spanish language configuration
  language:
    primary: "es"
    fallback: "en"
  
  # Skill types to extract
  skill_types:
    - "skill/competence"
    - "knowledge"
    - "skill"
  
  # Search parameters
  search:
    limit: 100
    fields:
      - "preferredLabel"
      - "altLabels"
      - "description"
      - "broaderConcept"
    
  # Mapping rules for common tech terms
  tech_mappings:
    # Programming languages
    "python": "http://data.europa.eu/esco/skill/3897c3c6-556b-4b8a-bfeb-234b0f716950"
    "java": "http://data.europa.eu/esco/skill/b4b36f5a-d5e6-4d78-b4ed-5b0b0e4f7a8a"
    "javascript": "http://data.europa.eu/esco/skill/7c7c5c49-0122-4b2e-8f6e-4e6b3f3e5d5f"
    "react": "http://data.europa.eu/esco/skill/react-framework"
    "node.js": "http://data.europa.eu/esco/skill/nodejs-runtime"
    
    # Databases
    "sql": "http://data.europa.eu/esco/skill/sql-language"
    "mysql": "http://data.europa.eu/esco/skill/mysql-database"
    "postgresql": "http://data.europa.eu/esco/skill/postgresql-database"
    "mongodb": "http://data.europa.eu/esco/skill/mongodb-database"
    
    # Cloud platforms
    "aws": "http://data.europa.eu/esco/skill/amazon-web-services"
    "azure": "http://data.europa.eu/esco/skill/microsoft-azure"
    "gcp": "http://data.europa.eu/esco/skill/google-cloud-platform"
    
    # DevOps
    "docker": "http://data.europa.eu/esco/skill/docker-containerization"
    "kubernetes": "http://data.europa.eu/esco/skill/kubernetes-orchestration"
    "git": "http://data.europa.eu/esco/skill/git-version-control"
    
    # Soft skills (Spanish)
    "trabajo en equipo": "http://data.europa.eu/esco/skill/teamwork"
    "comunicación": "http://data.europa.eu/esco/skill/communication"
    "liderazgo": "http://data.europa.eu/esco/skill/leadership"
    "resolución de problemas": "http://data.europa.eu/esco/skill/problem-solving"
```

---

## 4. Scraper Module Files

### src/scraper/__init__.py
```python
from .spiders.computrabajo_spider import ComputrabajoSpider
from .spiders.bumeran_spider import BumeranSpider
from .spiders.elempleo_spider import ElempleoSpider

__all__ = ['ComputrabajoSpider', 'BumeranSpider', 'ElempleoSpider']
```

### src/scraper/scrapy.cfg
```ini
[settings]
default = scraper.settings

[deploy]
project = labor_observatory_scraper
```

### src/scraper/items.py
```python
import scrapy
from scrapy.item import Field
from datetime import datetime
import re

class JobItem(scrapy.Item):
    # Required fields
    portal = Field()
    country = Field()
    url = Field()
    title = Field()
    description = Field()
    
    # Optional fields
    company = Field()
    location = Field()
    requirements = Field()
    salary_raw = Field()
    contract_type = Field()
    remote_type = Field()
    posted_date = Field()
    raw_html = Field()
    
    # Metadata
    scraped_at = Field()
    
    def clean_text(self, text):
        """Clean and normalize text fields."""
        if not text:
            return None
        
        # Remove extra whitespace
        text = ' '.join(text.split())
        
        # Remove HTML entities
        text = re.sub(r'&[a-zA-Z]+;', ' ', text)
        
        return text.strip()
    
    def __setitem__(self, key, value):
        # Clean text fields
        if key in ['title', 'description', 'requirements', 'company', 'location'] and value:
            value = self.clean_text(value)
        
        # Set scraped_at automatically
        if key == 'scraped_at':
            value = datetime.now()
        
        super().__setitem__(key, value)
```

### src/scraper/pipelines.py
```python
import logging
from datetime import datetime
from typing import Optional
import re
from scrapy.exceptions import DropItem
from database.operations import DatabaseOperations

logger = logging.getLogger(__name__)

class ValidationPipeline:
    """Validate scraped items."""
    
    def process_item(self, item, spider):
        # Check required fields
        required_fields = ['portal', 'country', 'url', 'title', 'description']
        
        for field in required_fields:
            if not item.get(field):
                raise DropItem(f"Missing required field: {field}")
        
        # Validate country code
        if item['country'] not in ['CO', 'MX', 'AR']:
            raise DropItem(f"Invalid country code: {item['country']}")
        
        # Validate portal
        if item['portal'] not in ['computrabajo', 'bumeran', 'elempleo']:
            raise DropItem(f"Invalid portal: {item['portal']}")
        
        # Ensure minimum description length
        if len(item['description']) < 50:
            raise DropItem("Description too short")
        
        return item

class NormalizationPipeline:
    """Normalize item fields."""
    
    def process_item(self, item, spider):
        # Normalize contract type
        if item.get('contract_type'):
            item['contract_type'] = self.normalize_contract_type(item['contract_type'])
        
        # Normalize remote type
        if item.get('remote_type'):
            item['remote_type'] = self.normalize_remote_type(item['remote_type'])
        
        # Parse posted date
        if item.get('posted_date'):
            item['posted_date'] = self.parse_date(item['posted_date'])
        
        # Set scraped_at
        item['scraped_at'] = datetime.now()
        
        return item
    
    def normalize_contract_type(self, contract: str) -> str:
        """Normalize contract type to standard values."""
        contract_lower = contract.lower()
        
        if any(term in contract_lower for term in ['tiempo completo', 'full time', 'completo']):
            return 'full_time'
        elif any(term in contract_lower for term in ['medio tiempo', 'part time', 'parcial']):
            return 'part_time'
        elif any(term in contract_lower for term in ['freelance', 'independiente', 'autonomo']):
            return 'freelance'
        elif any(term in contract_lower for term in ['contrato', 'temporal', 'proyecto']):
            return 'contract'
        elif any(term in contract_lower for term in ['pasantia', 'practica', 'internship']):
            return 'internship'
        else:
            return 'other'
    
    def normalize_remote_type(self, remote: str) -> str:
        """Normalize remote work type."""
        remote_lower = remote.lower()
        
        if any(term in remote_lower for term in ['remoto', 'remote', 'teletrabajo']):
            return 'remote'
        elif any(term in remote_lower for term in ['hibrido', 'hybrid', 'mixto']):
            return 'hybrid'
        elif any(term in remote_lower for term in ['presencial', 'oficina', 'on-site']):
            return 'on_site'
        else:
            return 'not_specified'
    
    def parse_date(self, date_str: str) -> Optional[datetime]:
        """Parse various date formats."""
        # Common Spanish date patterns
        patterns = [
            r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})',  # DD/MM/YYYY or DD-MM-YYYY
            r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})',  # YYYY/MM/DD or YYYY-MM-DD
            r'hace (\d+) días?',  # "hace X días"
            r'(\d+) días? atrás',  # "X días atrás"
            r'hoy',  # "hoy"
            r'ayer',  # "ayer"
        ]
        
        # Try to match patterns
        for pattern in patterns:
            match = re.search(pattern, date_str, re.IGNORECASE)
            if match:
                if 'hace' in pattern or 'atrás' in pattern:
                    days_ago = int(match.group(1))
                    return datetime.now() - timedelta(days=days_ago)
                elif pattern == r'hoy':
                    return datetime.now().date()
                elif pattern == r'ayer':
                    return datetime.now() - timedelta(days=1)
                else:
                    # Handle date formats
                    try:
                        if len(match.groups()) == 3:
                            if match.group(1).isdigit() and len(match.group(1)) == 4:
                                # YYYY/MM/DD format
                                return datetime(
                                    int(match.group(1)),
                                    int(match.group(2)),
                                    int(match.group(3))
                                ).date()
                            else:
                                # DD/MM/YYYY format
                                return datetime(
                                    int(match.group(3)),
                                    int(match.group(2)),
                                    int(match.group(1))
                                ).date()
                    except ValueError:
                        pass
        
        return None

class DatabasePipeline:
    """Save items to PostgreSQL database."""
    
    def __init__(self):
        self.db_ops = None
    
    def open_spider(self, spider):
        self.db_ops = DatabaseOperations()
        logger.info(f"Database pipeline opened for spider: {spider.name}")
    
    def process_item(self, item, spider):
        try:
            # Convert item to dict
            job_data = dict(item)
            
            # Remove metadata fields
            job_data.pop('scraped_at', None)
            
            # Insert into database
            job_id = self.db_ops.insert_job(job_data)
            
            if job_id:
                logger.info(f"Saved job {job_id}: {item['title']}")
            else:
                logger.warning(f"Duplicate job skipped: {item['url']}")
            
            return item
            
        except Exception as e:
            logger.error(f"Error saving job to database: {e}")
            raise
```

### src/scraper/settings.py
```python
import os
from config.settings import get_settings

settings = get_settings()

BOT_NAME = 'labor_observatory_scraper'

SPIDER_MODULES = ['scraper.spiders']
NEWSPIDER_MODULE = 'scraper.spiders'

# Obey robots.txt rules
ROBOTSTXT_OBEY = True

# Configure maximum concurrent requests
CONCURRENT_REQUESTS = settings.scraper_concurrent_requests
CONCURRENT_REQUESTS_PER_DOMAIN = 8

# Configure delay
DOWNLOAD_DELAY = settings.scraper_download_delay
RANDOMIZE_DOWNLOAD_DELAY = True

# Disable cookies
COOKIES_ENABLED = False

# User agent
USER_AGENT = settings.scraper_user_agent

# Override default headers
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'Cache-Control': 'no-cache',
    'Pragma': 'no-cache',
}

# Retry configuration
RETRY_TIMES = settings.scraper_retry_times
RETRY_HTTP_CODES = [500, 502, 503, 504, 408, 429]

# Configure pipelines
ITEM_PIPELINES = {
    'scraper.pipelines.ValidationPipeline': 100,
    'scraper.pipelines.NormalizationPipeline': 200,
    'scraper.pipelines.DatabasePipeline': 300,
}

# AutoThrottle extension
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 1
AUTOTHROTTLE_MAX_DELAY = 10
AUTOTHROTTLE_TARGET_CONCURRENCY = 8.0
AUTOTHROTTLE_DEBUG = False

# Memory usage
MEMUSAGE_ENABLED = True
MEMUSAGE_LIMIT_MB = 2048
MEMUSAGE_WARNING_MB = 1536

# Logging
LOG_LEVEL = 'INFO'
LOG_FORMAT = '%(levelname)s: %(message)s'

# Cache
HTTPCACHE_ENABLED = False

# Download timeout
DOWNLOAD_TIMEOUT = 30

# Telnet Console (disabled for production)
TELNETCONSOLE_ENABLED = False

# Middleware settings
DOWNLOADER_MIDDLEWARES = {
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,
    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110,
}
```

### src/scraper/middlewares.py
```python
import random
import logging
from scrapy.downloadermiddlewares.retry import RetryMiddleware
from scrapy.utils.response import response_status_message
from fake_useragent import UserAgent

logger = logging.getLogger(__name__)

class RotateUserAgentMiddleware:
    """Rotate user agents for each request."""
    
    def __init__(self):
        self.ua = UserAgent()
        self.user_agent_list = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
        ]
    
    def process_request(self, request, spider):
        try:
            # Try to use fake-useragent
            ua = self.ua.random
        except:
            # Fallback to predefined list
            ua = random.choice(self.user_agent_list)
        
        request.headers['User-Agent'] = ua + ' Academic Research Bot'

class CustomRetryMiddleware(RetryMiddleware):
    """Custom retry middleware with exponential backoff."""
    
    def process_response(self, request, response, spider):
        if response.status in self.retry_http_codes:
            reason = response_status_message(response.status)
            
            # Log retry attempt
            retry_times = request.meta.get('retry_times', 0) + 1
            logger.warning(
                f"Retrying {request.url} (attempt {retry_times}): {reason}"
            )
            
            # Exponential backoff
            request.meta['download_delay'] = 2 ** retry_times
            
            return self._retry(request, reason, spider) or response
        
        return response
```

### src/scraper/spiders/__init__.py
```python
# Spider modules initialization
```

### src/scraper/spiders/base_spider.py
```python
import scrapy
from abc import ABC, abstractmethod
import logging
from datetime import datetime
from urllib.parse import urljoin

logger = logging.getLogger(__name__)

class BaseJobSpider(scrapy.Spider, ABC):
    """Base spider class for job portals."""
    
    def __init__(self, country=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.country = country
        self.total_scraped = 0
        self.start_time = datetime.now()
    
    @abstractmethod
    def parse_job(self, response):
        """Parse individual job posting. Must be implemented by subclasses."""
        pass
    
    def extract_text(self, selector, xpath_or_css, method='xpath'):
        """Safely extract text from selector."""
        try:
            if method == 'xpath':
                texts = selector.xpath(xpath_or_css).getall()
            else:
                texts = selector.css(xpath_or_css).getall()
            
            # Join and clean text
            text = ' '.join(texts)
            return ' '.join(text.split()) if text else None
        except Exception as e:
            logger.error(f"Error extracting text: {e}")
            return None
    
    def build_absolute_url(self, response, relative_url):
        """Build absolute URL from relative URL."""
        return urljoin(response.url, relative_url)
    
    def log_progress(self):
        """Log scraping progress."""
        elapsed = (datetime.now() - self.start_time).total_seconds()
        rate = self.total_scraped / elapsed if elapsed > 0 else 0
        
        logger.info(
            f"Spider {self.name} - Country: {self.country} - "
            f"Scraped: {self.total_scraped} - Rate: {rate:.2f} jobs/sec"
        )
```

### src/scraper/spiders/computrabajo_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import re
from urllib.parse import urlencode
import logging

logger = logging.getLogger(__name__)

class ComputrabajoSpider(BaseJobSpider):
    name = 'computrabajo'
    allowed_domains = ['computrabajo.com', 'computrabajo.com.co', 
                      'computrabajo.com.mx', 'computrabajo.com.ar']
    
    # URL patterns by country
    country_urls = {
        'CO': 'https://www.computrabajo.com.co',
        'MX': 'https://www.computrabajo.com.mx',
        'AR': 'https://www.computrabajo.com.ar'
    }
    
    # Tech-related search terms
    tech_keywords = [
        'desarrollador', 'developer', 'programador', 'software',
        'data', 'analyst', 'engineer', 'fullstack', 'frontend',
        'backend', 'devops', 'cloud', 'mobile', 'web'
    ]
    
    def start_requests(self):
        if not self.country or self.country not in self.country_urls:
            raise ValueError(f"Invalid country: {self.country}")
        
        base_url = self.country_urls[self.country]
        
        # Generate search URLs for tech keywords
        for keyword in self.tech_keywords:
            search_url = f"{base_url}/trabajo-de-{keyword}"
            yield Request(
                url=search_url,
                callback=self.parse_search_results,
                meta={'keyword': keyword, 'page': 1}
            )
    
    def parse_search_results(self, response):
        """Parse search results page."""
        # Extract job listings
        job_cards = response.css('article.js-o-card')
        
        for card in job_cards:
            # Extract job URL
            job_url = card.css('a.js-o-card__link::attr(href)').get()
            if job_url:
                absolute_url = self.build_absolute_url(response, job_url)
                yield Request(
                    url=absolute_url,
                    callback=self.parse_job,
                    meta={'search_keyword': response.meta.get('keyword')}
                )
        
        # Check for next page
        current_page = response.meta.get('page', 1)
        next_page_link = response.css('a[aria-label="Siguiente"]::attr(href)').get()
        
        if next_page_link and current_page < 10:  # Limit to 10 pages per keyword
            next_url = self.build_absolute_url(response, next_page_link)
            yield Request(
                url=next_url,
                callback=self.parse_search_results,
                meta={
                    'keyword': response.meta.get('keyword'),
                    'page': current_page + 1
                }
            )
    
    def parse_job(self, response):
        """Parse individual job posting."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'computrabajo'
        item['country'] = self.country
        item['url'] = response.url
        
        # Title
        item['title'] = self.extract_text(
            response, 
            '//h1[@class="fwB fs24"]//text()',
            'xpath'
        )
        
        # Company
        item['company'] = self.extract_text(
            response,
            '//a[@class="dIB fs16 js-o-link"]//text()',
            'xpath'
        )
        
        # Location
        location_parts = response.xpath(
            '//div[@class="fs16 fc_base mt5"]//span//text()'
        ).getall()
        item['location'] = ', '.join(location_parts) if location_parts else None
        
        # Description and requirements
        description_sections = response.xpath(
            '//div[@class="mbB"]//p//text() | //div[@class="mbB"]//li//text()'
        ).getall()
        
        full_text = ' '.join(description_sections)
        
        # Try to separate requirements
        req_pattern = r'(?:requisitos|requerimientos|requirements|experiencia|conocimientos):(.*?)(?:funciones|responsabilidades|beneficios|$)'
        req_match = re.search(req_pattern, full_text, re.IGNORECASE | re.DOTALL)
        
        if req_match:
            item['requirements'] = req_match.group(1).strip()
            item['description'] = full_text.replace(req_match.group(0), '').strip()
        else:
            item['description'] = full_text
            item['requirements'] = None
        
        # Salary
        salary_text = self.extract_text(
            response,
            '//span[@class="fs16 fc_aux"]//text()[contains(., "$")]',
            'xpath'
        )
        item['salary_raw'] = salary_text
        
        # Contract type
        contract_info = response.xpath(
            '//span[@class="fs13 fc_aux"]//text()'
        ).getall()
        
        for info in contract_info:
            info_lower = info.lower()
            if any(term in info_lower for term in ['tiempo completo', 'full time', 'part time']):
                item['contract_type'] = info
            elif any(term in info_lower for term in ['remoto', 'presencial', 'hibrido']):
                item['remote_type'] = info
        
        # Posted date
        date_text = self.extract_text(
            response,
            '//span[@class="fs13 fc_aux"][contains(text(), "Publicado")]//text()',
            'xpath'
        )
        if date_text:
            item['posted_date'] = date_text.replace('Publicado', '').strip()
        
        # Raw HTML (for debugging/reprocessing)
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

### src/scraper/spiders/bumeran_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import json
import re
import logging

logger = logging.getLogger(__name__)

class BumeranSpider(BaseJobSpider):
    name = 'bumeran'
    allowed_domains = ['bumeran.com', 'bumeran.com.mx', 'bumeran.com.ar']
    
    # URL patterns by country
    country_urls = {
        'MX': 'https://www.bumeran.com.mx',
        'AR': 'https://www.bumeran.com.ar'
    }
    
    # Tech categories
    tech_categories = [
        'informatica-telecomunicaciones',
        'tecnologia-sistemas',
        'desarrollo-programacion'
    ]
    
    def start_requests(self):
        if self.country not in self.country_urls:
            raise ValueError(f"Bumeran not available for country: {self.country}")
        
        base_url = self.country_urls[self.country]
        
        # Generate category URLs
        for category in self.tech_categories:
            category_url = f"{base_url}/empleos-{category}.html"
            yield Request(
                url=category_url,
                callback=self.parse_category,
                meta={'category': category, 'page': 1}
            )
    
    def parse_category(self, response):
        """Parse category listing page."""
        # Check if page uses React/JSON data
        scripts = response.xpath('//script[contains(text(), "__INITIAL_STATE__")]/text()').getall()
        
        if scripts:
            # Extract JSON data from script
            for script in scripts:
                match = re.search(r'__INITIAL_STATE__\s*=\s*({.*?});', script, re.DOTALL)
                if match:
                    try:
                        data = json.loads(match.group(1))
                        jobs = self.extract_jobs_from_json(data)
                        
                        for job in jobs:
                            yield Request(
                                url=job['url'],
                                callback=self.parse_job,
                                meta={'job_data': job}
                            )
                    except json.JSONDecodeError:
                        logger.error("Failed to parse JSON data")
        else:
            # Fallback to HTML parsing
            job_links = response.css('div.Card__CardContentWrapper a::attr(href)').getall()
            
            for link in job_links:
                absolute_url = self.build_absolute_url(response, link)
                yield Request(url=absolute_url, callback=self.parse_job)
        
        # Handle pagination
        current_page = response.meta.get('page', 1)
        if current_page < 10:  # Limit pages
            next_page = current_page + 1
            next_url = response.url.replace(
                f'.html', 
                f'-pagina-{next_page}.html'
            )
            yield Request(
                url=next_url,
                callback=self.parse_category,
                meta={
                    'category': response.meta.get('category'),
                    'page': next_page
                }
            )
    
    def extract_jobs_from_json(self, data):
        """Extract job data from JSON structure."""
        jobs = []
        
        # Navigate through possible JSON structures
        try:
            if 'results' in data:
                job_list = data['results'].get('jobs', [])
            elif 'jobs' in data:
                job_list = data['jobs']
            else:
                return jobs
            
            for job in job_list:
                job_info = {
                    'url': job.get('url', ''),
                    'title': job.get('title', ''),
                    'company': job.get('company', {}).get('name', ''),
                    'location': job.get('location', '')
                }
                if job_info['url']:
                    jobs.append(job_info)
        except Exception as e:
            logger.error(f"Error extracting jobs from JSON: {e}")
        
        return jobs
    
    def parse_job(self, response):
        """Parse individual job posting."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'bumeran'
        item['country'] = self.country
        item['url'] = response.url
        
        # Try to get pre-parsed data
        job_data = response.meta.get('job_data', {})
        
        # Title
        item['title'] = job_data.get('title') or self.extract_text(
            response,
            'h1[class*="Title"]::text',
            'css'
        )
        
        # Company
        item['company'] = job_data.get('company') or self.extract_text(
            response,
            'h2[class*="Company"]::text',
            'css'
        )
        
        # Location
        item['location'] = job_data.get('location') or self.extract_text(
            response,
            'span[class*="Location"]::text',
            'css'
        )
        
        # Description
        description_selectors = [
            'div[class*="Description"]',
            'div.detalle-aviso',
            'div#description'
        ]
        
        for selector in description_selectors:
            desc_elements = response.css(f'{selector} ::text').getall()
            if desc_elements:
                item['description'] = ' '.join(desc_elements)
                break
        
        # Requirements - often within description
        if item.get('description'):
            req_pattern = r'(?:requisitos|requerimientos|experiencia|competencias):(.*?)(?:beneficios|funciones|responsabilidades|$)'
            req_match = re.search(req_pattern, item['description'], re.IGNORECASE | re.DOTALL)
            
            if req_match:
                item['requirements'] = req_match.group(1).strip()
        
        # Salary
        salary_selectors = [
            'span[class*="Salary"]::text',
            'div[class*="salary"]::text',
            'span:contains("$")::text'
        ]
        
        for selector in salary_selectors:
            salary = response.css(selector).get()
            if salary and '$' in salary:
                item['salary_raw'] = salary
                break
        
        # Contract type and remote
        tags = response.css('span[class*="Tag"]::text').getall()
        for tag in tags:
            tag_lower = tag.lower()
            if any(term in tag_lower for term in ['tiempo completo', 'part time', 'freelance']):
                item['contract_type'] = tag
            elif any(term in tag_lower for term in ['remoto', 'presencial', 'hibrido']):
                item['remote_type'] = tag
        
        # Posted date
        date_text = response.css('span[class*="Date"]::text').get()
        if date_text:
            item['posted_date'] = date_text
        
        # Raw HTML
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

### src/scraper/spiders/elempleo_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import re
import logging
from urllib.parse import urljoin, urlparse, parse_qs

logger = logging.getLogger(__name__)

class ElempleoSpider(BaseJobSpider):
    name = 'elempleo'
    allowed_domains = ['elempleo.com']
    
    # Only available for Colombia
    country_urls = {
        'CO': 'https://www.elempleo.com'
    }
    
    # Tech-related categories in elempleo
    tech_categories = {
        'tecnologia': '1100',
        'sistemas': '1100',
        'informatica': '1100'
    }
    
    def start_requests(self):
        if self.country != 'CO':
            raise ValueError("elempleo.com is only available for Colombia (CO)")
        
        base_url = self.country_urls['CO']
        
        # Search URLs for technology jobs
        search_base = f"{base_url}/colombia/empleos"
        
        # Generate search requests
        for category_name, category_id in self.tech_categories.items():
            search_params = {
                'categoria': category_id,
                'pagina': 1
            }
            
            search_url = f"{search_base}?categoria={category_id}"
            
            yield Request(
                url=search_url,
                callback=self.parse_search_results,
                meta={
                    'category': category_name,
                    'page': 1
                }
            )
    
    def parse_search_results(self, response):
        """Parse search results from elempleo."""
        # Extract job cards
        job_cards = response.css('div.result-item')
        
        if not job_cards:
            # Try alternative selectors
            job_cards = response.css('article.js-offer')
        
        for card in job_cards:
            # Extract job URL
            job_link = card.css('a.js-offer-title::attr(href)').get()
            if not job_link:
                job_link = card.css('h2 a::attr(href)').get()
            
            if job_link:
                # Handle both relative and absolute URLs
                if not job_link.startswith('http'):
                    job_link = urljoin(response.url, job_link)
                
                # Extract basic info from card
                card_data = {
                    'title': card.css('h2 a::text').get(),
                    'company': card.css('span.info-company-name::text').get(),
                    'location': card.css('span.info-city::text').get()
                }
                
                yield Request(
                    url=job_link,
                    callback=self.parse_job,
                    meta={'card_data': card_data}
                )
        
        # Handle pagination
        current_page = response.meta.get('page', 1)
        next_page_link = response.css('a.js-btn-next::attr(href)').get()
        
        if not next_page_link:
            # Alternative pagination
            pagination_links = response.css('ul.pagination a::attr(href)').getall()
            for link in pagination_links:
                if f'pagina={current_page + 1}' in link:
                    next_page_link = link
                    break
        
        if next_page_link and current_page < 10:  # Limit to 10 pages
            next_url = urljoin(response.url, next_page_link)
            yield Request(
                url=next_url,
                callback=self.parse_search_results,
                meta={
                    'category': response.meta.get('category'),
                    'page': current_page + 1
                }
            )
    
    def parse_job(self, response):
        """Parse individual job posting from elempleo."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'elempleo'
        item['country'] = 'CO'
        item['url'] = response.url
        
        # Get card data if available
        card_data = response.meta.get('card_data', {})
        
        # Title
        item['title'] = self.extract_text(
            response,
            'h1.offer-title::text',
            'css'
        ) or card_data.get('title')
        
        # Company
        item['company'] = self.extract_text(
            response,
            'div.company-name a::text',
            'css'
        ) or self.extract_text(
            response,
            'span.offer-company::text',
            'css'
        ) or card_data.get('company')
        
        # Location
        location_parts = response.css('div.offer-location span::text').getall()
        if location_parts:
            item['location'] = ', '.join(location_parts)
        else:
            item['location'] = card_data.get('location')
        
        # Main content sections
        content_sections = response.css('div.offer-description')
        
        # Description
        description_html = content_sections.css('div#description').get()
        if description_html:
            # Clean HTML and extract text
            desc_text = re.sub(r'<[^>]+>', ' ', description_html)
            item['description'] = ' '.join(desc_text.split())
        
        # Requirements
        requirements_section = content_sections.css('div#requirements')
        if requirements_section:
            req_items = requirements_section.css('li::text').getall()
            if req_items:
                item['requirements'] = ' '.join(req_items)
            else:
                req_text = requirements_section.css('::text').getall()
                item['requirements'] = ' '.join(req_text)
        
        # If requirements not in separate section, try to extract from description
        if not item.get('requirements') and item.get('description'):
            req_pattern = r'(?:requisitos|perfil|experiencia|conocimientos):(.*?)(?:funciones|responsabilidades|ofrecemos|$)'
            req_match = re.search(req_pattern, item['description'], re.IGNORECASE | re.DOTALL)
            if req_match:
                item['requirements'] = req_match.group(1).strip()
        
        # Salary
        salary_element = response.css('span.offer-salary::text').get()
        if salary_element:
            item['salary_raw'] = salary_element
        else:
            # Look for salary in description
            salary_pattern = r'\$[\d.,]+ (?:millones|COP|pesos)'
            salary_match = re.search(salary_pattern, item.get('description', ''))
            if salary_match:
                item['salary_raw'] = salary_match.group(0)
        
        # Contract type
        contract_info = response.css('div.offer-info span::text').getall()
        for info in contract_info:
            info_lower = info.lower()
            if any(term in info_lower for term in ['contrato', 'tiempo completo', 'medio tiempo']):
                item['contract_type'] = info
            elif any(term in info_lower for term in ['remoto', 'presencial', 'teletrabajo']):
                item['remote_type'] = info
        
        # Posted date
        date_element = response.css('span.offer-date::text').get()
        if date_element:
            item['posted_date'] = date_element
        else:
            # Try to extract from meta tags
            date_meta = response.css('meta[property="article:published_time"]::attr(content)').get()
            if date_meta:
                item['posted_date'] = date_meta.split('T')[0]
        
        # Additional fields from structured data
        try:
            # Check for JSON-LD structured data
            json_ld = response.css('script[type="application/ld+json"]::text').get()
            if json_ld:
                import json
                data = json.loads(json_ld)
                
                # Extract additional info if available
                if isinstance(data, dict):
                    if 'title' in data and not item.get('title'):
                        item['title'] = data['title']
                    if 'hiringOrganization' in data and not item.get('company'):
                        item['company'] = data['hiringOrganization'].get('name')
                    if 'jobLocation' in data and not item.get('location'):
                        location = data['jobLocation']
                        if isinstance(location, dict):
                            item['location'] = location.get('address', {}).get('addressLocality')
        except Exception as e:
            logger.debug(f"Could not parse structured data: {e}")
        
        # Raw HTML
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

---

## 5. Extractor Module Files

### src/extractor/__init__.py
```python
from .pipeline import ExtractionPipeline
from .ner_extractor import NERExtractor
from .regex_patterns import RegexExtractor
from .esco_matcher import ESCOMatcher

__all__ = ['ExtractionPipeline', 'NERExtractor', 'RegexExtractor', 'ESCOMatcher']
```

### src/extractor/ner_extractor.py
```python
import spacy
from spacy.tokens import Doc, Span
from typing import List, Dict, Tuple, Optional
import logging
import os
from config.settings import get_settings

logger = logging.getLogger(__name__)

class NERExtractor:
    """Extract skills using Named Entity Recognition."""
    
    def __init__(self, model_path: Optional[str] = None):
        self.settings = get_settings()
        
        # Load spaCy model
        if model_path and os.path.exists(model_path):
            self.nlp = spacy.load(model_path)
            logger.info(f"Loaded custom NER model from {model_path}")
        else:
            # Load default Spanish model
            try:
                self.nlp = spacy.load("es_core_news_lg")
                logger.info("Loaded default Spanish model")
            except:
                logger.warning("Spanish model not found, downloading...")
                os.system("python -m spacy download es_core_news_lg")
                self.nlp = spacy.load("es_core_news_lg")
        
        # Add custom pipeline components
        self._add_tech_entity_ruler()
    
    def _add_tech_entity_ruler(self):
        """Add rule-based entity recognition for tech terms."""
        ruler = self.nlp.add_pipe("entity_ruler", before="ner")
        
        # Define patterns for common tech skills
        patterns = [
            # Programming languages
            {"label": "SKILL", "pattern": [{"LOWER": "python"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "java"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "javascript"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "typescript"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "c++"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "c#"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "php"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "ruby"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "go"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "golang"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "rust"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "kotlin"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "swift"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "r"}]},
            
            # Frameworks
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "react"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "react"}, {"LOWER": "native"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "angular"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "vue"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "vue"}, {"LOWER": "js"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "django"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "flask"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "spring"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "spring"}, {"LOWER": "boot"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "node"}, {"LOWER": "js"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "nodejs"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "express"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": ".net"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "laravel"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "rails"}]},
            
            # Databases
            {"label": "DATABASE", "pattern": [{"LOWER": "mysql"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "postgresql"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "postgres"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "mongodb"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "redis"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "elasticsearch"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "oracle"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "sql"}, {"LOWER": "server"}]},
            
            # Cloud & DevOps
            {"label": "PLATFORM", "pattern": [{"LOWER": "aws"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "amazon"}, {"LOWER": "web"}, {"LOWER": "services"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "azure"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "gcp"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "google"}, {"LOWER": "cloud"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "docker"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "kubernetes"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "jenkins"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "git"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "github"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "gitlab"}]},
            
            # Data & ML
            {"label": "SKILL", "pattern": [{"LOWER": "machine"}, {"LOWER": "learning"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "deep"}, {"LOWER": "learning"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "data"}, {"LOWER": "science"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "big"}, {"LOWER": "data"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "tensorflow"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "pytorch"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "scikit"}, {"LOWER": "learn"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "pandas"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "numpy"}]},
            
            # Methodologies
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "agile"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "scrum"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "kanban"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "devops"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "ci"}, {"LOWER": "cd"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "tdd"}]},
        ]
        
        # Add Spanish variations
        spanish_patterns = [
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "web"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "movil"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "móvil"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "base"}, {"LOWER": "de"}, {"LOWER": "datos"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "inteligencia"}, {"LOWER": "artificial"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "aprendizaje"}, {"LOWER": "automatico"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "aprendizaje"}, {"LOWER": "automático"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "ciencia"}, {"LOWER": "de"}, {"LOWER": "datos"}]},
        ]
        
        ruler.add_patterns(patterns + spanish_patterns)
    
    def extract(self, text: str, source_section: str = "unknown") -> List[Dict[str, any]]:
        """Extract skills from text using NER.
        
        Args:
            text: Text to extract skills from
            source_section: Section of job posting (title, description, requirements)
            
        Returns:
            List of extracted skills with metadata
        """
        if not text:
            return []
        
        # Process text with spaCy
        doc = self.nlp(text)
        
        extracted_skills = []
        seen_skills = set()
        
        # Extract entities
        for ent in doc.ents:
            if ent.label_ in ["SKILL", "FRAMEWORK", "DATABASE", "PLATFORM", "TOOL", "METHODOLOGY"]:
                skill_text = ent.text.lower().strip()
                
                # Skip if already seen
                if skill_text in seen_skills:
                    continue
                
                seen_skills.add(skill_text)
                
                extracted_skills.append({
                    "skill_text": skill_text,
                    "skill_type": "explicit",
                    "extraction_method": "ner",
                    "entity_label": ent.label_,
                    "confidence_score": 0.9,  # High confidence for NER
                    "source_section": source_section,
                    "span_start": ent.start_char,
                    "span_end": ent.end_char,
                    "context": text[max(0, ent.start_char-50):ent.end_char+50]
                })
        
        logger.debug(f"NER extracted {len(extracted_skills)} skills from {source_section}")
        
        return extracted_skills
    
    def extract_from_job(self, job_data: Dict[str, str]) -> List[Dict[str, any]]:
        """Extract skills from all sections of a job posting.
        
        Args:
            job_data: Dictionary with job fields (title, description, requirements)
            
        Returns:
            List of all extracted skills
        """
        all_skills = []
        
        # Extract from title
        if job_data.get('title'):
            title_skills = self.extract(job_data['title'], 'title')
            all_skills.extend(title_skills)
        
        # Extract from description
        if job_data.get('description'):
            desc_skills = self.extract(job_data['description'], 'description')
            all_skills.extend(desc_skills)
        
        # Extract from requirements
        if job_data.get('requirements'):
            req_skills = self.extract(job_data['requirements'], 'requirements')
            all_skills.extend(req_skills)
        
        return all_skills
```

### src/extractor/regex_patterns.py
```python
import re
from typing import List, Dict, Tuple
import logging

logger = logging.getLogger(__name__)

class RegexExtractor:
    """Extract skills using regular expressions."""
    
    def __init__(self):
        # Define regex patterns for skill extraction
        self.patterns = self._build_patterns()
    
    def _build_patterns(self) -> List[Tuple[str, re.Pattern, str]]:
        """Build regex patterns for skill extraction.
        
        Returns:
            List of tuples (pattern_name, compiled_regex, skill_type)
        """
        patterns = []
        
        # Experience patterns in Spanish
        experience_patterns = [
            (
                "experiencia_en",
                re.compile(
                    r"experiencia\s+(?:en|con)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "conocimientos_de",
                re.compile(
                    r"conocimientos?\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "manejo_de",
                re.compile(
                    r"manejo\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "tool"
            ),
            (
                "dominio_de",
                re.compile(
                    r"dominio\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "desarrollo_en",
                re.compile(
                    r"desarrollo\s+(?:en|con)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
        ]
        
        # Required skills patterns
        required_patterns = [
            (
                "requerimos",
                re.compile(
                    r"(?:requerimos|buscamos|necesitamos)\s+(?:personas?\s+con\s+)?(?:conocimientos?\s+en\s+)?([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+para\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "indispensable",
                re.compile(
                    r"(?:indispensable|fundamental|esencial)\s+(?:contar\s+con\s+)?(?:conocimientos?\s+en\s+)?([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
        ]
        
        # Technology stack patterns
        tech_stack_patterns = [
            (
                "tecnologias",
                re.compile(
                    r"(?:tecnologías?|herramientas?|lenguajes?)\s*:\s*([A-Za-z0-9\+\#\.\s,/\-\(\)]+?)(?:\n|$)",
                    re.IGNORECASE
                ),
                "mixed"
            ),
            (
                "stack_tecnologico",
                re.compile(
                    r"stack\s+(?:tecnológico|tech)\s*:\s*([A-Za-z0-9\+\#\.\s,/\-\(\)]+?)(?:\n|$)",
                    re.IGNORECASE
                ),
                "mixed"
            ),
        ]
        
        # List patterns (bullet points, numbered lists)
        list_patterns = [
            (
                "bullet_skills",
                re.compile(
                    r"(?:^|\n)\s*[\-\*\•]\s*([A-Za-z0-9\+\#\.\s]+?)(?:\n|$)",
                    re.MULTILINE
                ),
                "skill"
            ),
            (
                "numbered_skills",
                re.compile(
                    r"(?:^|\n)\s*\d+[\.\)]\s*([A-Za-z0-9\+\#\.\s]+?)(?:\n|$)",
                    re.MULTILINE
                ),
                "skill"
            ),
        ]
        
        # Certification patterns
        cert_patterns = [
            (
                "certificacion",
                re.compile(
                    r"(?:certificación|certificado)\s+(?:en\s+)?([A-Za-z0-9\+\#\.\s\-]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "certification"
            ),
        ]
        
        # Years of experience patterns
        experience_years_patterns = [
            (
                "años_experiencia",
                re.compile(
                    r"(\d+)\s*\+?\s*años?\s+(?:de\s+)?experiencia\s+(?:en\s+)?([A-Za-z0-9\+\#\.\s]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "skill_with_years"
            ),
        ]
        
        # Combine all patterns
        patterns.extend(experience_patterns)
        patterns.extend(required_patterns)
        patterns.extend(tech_stack_patterns)
        patterns.extend(list_patterns)
        patterns.extend(cert_patterns)
        patterns.extend(experience_years_patterns)
        
        return patterns
    
    def extract(self, text: str, source_section: str = "unknown") -> List[Dict[str, any]]:
        """Extract skills using regex patterns.
        
        Args:
            text: Text to extract skills from
            source_section: Section of job posting
            
        Returns:
            List of extracted skills with metadata
        """
        if not text:
            return []
        
        extracted_skills = []
        seen_skills = set()
        
        for pattern_name, regex, skill_type in self.patterns:
            matches = regex.finditer(text)
            
            for match in matches:
                if skill_type == "skill_with_years":
                    # Special handling for years of experience
                    years = match.group(1)
                    skill_text = match.group(2).strip().lower()
                    
                    if skill_text and skill_text not in seen_skills:
                        seen_skills.add(skill_text)
                        extracted_skills.append({
                            "skill_text": skill_text,
                            "skill_type": "explicit",
                            "extraction_method": "regex",
                            "pattern_name": pattern_name,
                            "confidence_score": 0.8,
                            "source_section": source_section,
                            "span_start": match.start(2),
                            "span_end": match.end(2),
                            "years_required": int(years),
                            "context": text[max(0, match.start()-30):match.end()+30]
                        })
                else:
                    # Normal skill extraction
                    skill_text = match.group(1).strip().lower()
                    
                    # Clean up extracted text
                    skill_text = self._clean_skill_text(skill_text)
                    
                    if skill_text and len(skill_text) > 1 and skill_text not in seen_skills:
                        # Additional validation
                        if self._is_valid_skill(skill_text):
                            seen_skills.add(skill_text)
                            
                            extracted_skills.append({
                                "skill_text": skill_text,
                                "skill_type": "explicit",
                                "extraction_method": "regex",
                                "pattern_name": pattern_name,
                                "confidence_score": 0.7,  # Lower than NER
                                "source_section": source_section,
                                "span_start": match.start(1),
                                "span_end": match.end(1),
                                "context": text[max(0, match.start()-30):match.end()+30]
                            })
        
        # Handle comma-separated lists within extracted skills
        expanded_skills = []
        for skill in extracted_skills:
            if ',' in skill['skill_text'] or ' y ' in skill['skill_text']:
                # Split and create individual skills
                parts = re.split(r'[,\s]+y\s+|,\s*', skill['skill_text'])
                for part in parts:
                    part = part.strip()
                    if part and self._is_valid_skill(part):
                        new_skill = skill.copy()
                        new_skill['skill_text'] = part
                        expanded_skills.append(new_skill)
            else:
                expanded_skills.append(skill)
        
        logger.debug(f"Regex extracted {len(expanded_skills)} skills from {source_section}")
        
        return expanded_skills
    
    def _clean_skill_text(self, text: str) -> str:
        """Clean extracted skill text.
        
        Args:
            text: Raw extracted text
            
        Returns:
            Cleaned skill text
        """
        # Remove common stop words at the beginning/end
        stop_words = [
            'el', 'la', 'los', 'las', 'un', 'una', 'de', 'del', 'al',
            'y', 'o', 'con', 'para', 'por', 'en', 'a'
        ]
        
        words = text.split()
        
        # Remove stop words from beginning
        while words and words[0].lower() in stop_words:
            words.pop(0)
        
        # Remove stop words from end
        while words and words[-1].lower() in stop_words:
            words.pop()
        
        cleaned = ' '.join(words)
        
        # Remove extra spaces and punctuation
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = re.sub(r'[^\w\s\+\#\.\-/]', '', cleaned)
        
        return cleaned.strip()
    
    def _is_valid_skill(self, skill_text: str) -> bool:
        """Validate if extracted text is likely a valid skill.
        
        Args:
            skill_text: Text to validate
            
        Returns:
            True if valid skill, False otherwise
        """
        # Check minimum length
        if len(skill_text) < 2:
            return False
        
        # Check if it's just numbers
        if skill_text.isdigit():
            return False
        
        # Check against blacklist of common false positives
        blacklist = [
            'años', 'año', 'experiencia', 'conocimiento', 'manejo',
            'desarrollo', 'persona', 'profesional', 'trabajo',
            'empresa', 'cliente', 'proyecto', 'equipo', 'area',
            'sistemas', 'tecnologia', 'informatica'  # Too generic
        ]
        
        if skill_text.lower() in blacklist:
            return False
        
        # Must contain at least one letter
        if not any(c.isalpha() for c in skill_text):
            return False
        
        return True
    
    def extract_from_job(self, job_data: Dict[str, str]) -> List[Dict[str, any]]:
        """Extract skills from all sections of a job posting.
        
        Args:
            job_data: Dictionary with job fields
            
        Returns:
            List of all extracted skills
        """
        all_skills = []
        
        # Process each section
        for section in ['title', 'description', 'requirements']:
            if job_data.get(section):
                section_skills = self.extract(job_data[section], section)
                all_skills.extend(section_skills)
        
        return all_skills
```

### src/extractor/esco_matcher.py
```python
import json
import logging
from typing import List, Dict, Optional, Set
from fuzzywuzzy import fuzz, process
import requests
from pathlib import Path
import yaml

logger = logging.getLogger(__name__)

class ESCOMatcher:
    """Match extracted skills to ESCO taxonomy."""
    
    def __init__(self, config_path: str = "config/esco_config.yaml"):
        self.config = self._load_config(config_path)
        self.skills_cache = {}
        self.local_mappings = self.config.get('tech_mappings', {})
        
        # Load local ESCO data if available
        self.local_esco_data = self._load_local_esco_data()
    
    def _load_config(self, config_path: str) -> dict:
        """Load ESCO configuration."""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config['esco']
        except Exception as e:
            logger.error(f"Failed to load ESCO config: {e}")
            return {}
    
    def _load_local_esco_data(self) -> Dict[str, Dict]:
        """Load local ESCO data files if available."""
        esco_data = {}
        
        # Try to load skills data
        skills_path = Path("data/esco/skills_es.csv")
        if skills_path.exists():
            try:
                import pandas as pd
                df = pd.read_csv(skills_path)
                
                for _, row in df.iterrows():
                    skill_uri = row.get('conceptUri', '')
                    esco_data[skill_uri] = {
                        'preferredLabel': row.get('preferredLabel', ''),
                        'altLabels': row.get('altLabels', '').split('|') if row.get('altLabels') else [],
                        'description': row.get('description', ''),
                        'skillType': row.get('skillType', '')
                    }
                
                logger.info(f"Loaded {len(esco_data)} ESCO skills from local file")
            except Exception as e:
                logger.error(f"Failed to load local ESCO data: {e}")
        
        return esco_data
    
    def match_skill(self, skill_text: str, threshold: float = 0.8) -> Optional[Dict[str, any]]:
        """Match a skill to ESCO taxonomy.
        
        Args:
            skill_text: Skill text to match
            threshold: Minimum similarity threshold (0-1)
            
        Returns:
            ESCO match data or None
        """
        skill_lower = skill_text.lower().strip()
        
        # Check direct mappings first
        if skill_lower in self.local_mappings:
            esco_uri = self.local_mappings[skill_lower]
            
            # Get details from local data or cache
            if esco_uri in self.local_esco_data:
                return {
                    'esco_uri': esco_uri,
                    'esco_preferred_label': self.local_esco_data[esco_uri]['preferredLabel'],
                    'match_type': 'direct',
                    'match_score': 1.0
                }
        
        # Try fuzzy matching against local data
        if self.local_esco_data:
            best_match = self._fuzzy_match_local(skill_text, threshold)
            if best_match:
                return best_match
        
        # Try API lookup (if configured and no local match)
        if self.config.get('base_url'):
            api_match = self._api_lookup(skill_text)
            if api_match:
                return api_match
        
        return None
    
    def _fuzzy_match_local(self, skill_text: str, threshold: float) -> Optional[Dict[str, any]]:
        """Fuzzy match against local ESCO data.
        
        Args:
            skill_text: Skill to match
            threshold: Minimum score threshold
            
        Returns:
            Best match or None
        """
        # Collect all labels for matching
        all_labels = []
        for uri, data in self.local_esco_data.items():
            # Add preferred label
            all_labels.append((data['preferredLabel'].lower(), uri, 'preferred'))
            
            # Add alternative labels
            for alt_label in data.get('altLabels', []):
                if alt_label:
                    all_labels.append((alt_label.lower(), uri, 'alternative'))
        
        # Find best match
        if all_labels:
            # Use token sort ratio for better matching of multi-word skills
            matches = process.extract(
                skill_text.lower(),
                [label[0] for label in all_labels],
                scorer=fuzz.token_sort_ratio,
                limit=3
            )
            
            for match_text, score in matches:
                if score >= threshold * 100:  # fuzzywuzzy uses 0-100 scale
                    # Find the corresponding URI
                    for label_text, uri, label_type in all_labels:
                        if label_text == match_text:
                            return {
                                'esco_uri': uri,
                                'esco_preferred_label': self.local_esco_data[uri]['preferredLabel'],
                                'match_type': f'fuzzy_{label_type}',
                                'match_score': score / 100.0,
                                'matched_text': match_text
                            }
        
        return None
    
    def _api_lookup(self, skill_text: str) -> Optional[Dict[str, any]]:
        """Look up skill using ESCO API.
        
        Args:
            skill_text: Skill to look up
            
        Returns:
            API match data or None
        """
        try:
            # Check cache first
            if skill_text in self.skills_cache:
                return self.skills_cache[skill_text]
            
            # Prepare API request
            api_url = f"{self.config['base_url']}{self.config['endpoints']['search']}"
            
            params = {
                'text': skill_text,
                'language': self.config['language']['primary'],
                'type': 'skill',
                'limit': 5
            }
            
            headers = {
                'Accept': 'application/json',
                'Accept-Language': self.config['language']['primary']
            }
            
            # Make request
            response = requests.get(api_url, params=params, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                if data.get('results'):
                    # Get first result
                    result = data['results'][0]
                    
                    match_data = {
                        'esco_uri': result.get('uri', ''),
                        'esco_preferred_label': result.get('preferredLabel', {}).get(
                            self.config['language']['primary'],
                            result.get('preferredLabel', {}).get('en', '')
                        ),
                        'match_type': 'api_search',
                        'match_score': result.get('score', 0.0)
                    }
                    
                    # Cache the result
                    self.skills_cache[skill_text] = match_data
                    
                    return match_data
            
        except Exception as e:
            logger.error(f"ESCO API lookup failed for '{skill_text}': {e}")
        
        return None
    
    def match_skills_batch(self, skills: List[str]) -> Dict[str, Optional[Dict]]:
        """Match multiple skills to ESCO taxonomy.
        
        Args:
            skills: List of skill texts
            
        Returns:
            Dictionary mapping skill text to ESCO match data
        """
        results = {}
        
        for skill in skills:
            match = self.match_skill(skill)
            results[skill] = match
        
        # Log statistics
        matched = sum(1 for v in results.values() if v is not None)
        logger.info(
            f"ESCO matching: {matched}/{len(skills)} skills matched "
            f"({matched/len(skills)*100:.1f}%)"
        )
        
        return results

### src/analyzer/dimension_reducer.py
```python
import logging
from typing import Tuple, Dict, Any, Optional
import numpy as np
from umap import UMAP
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import time

logger = logging.getLogger(__name__)

class DimensionReducer:
    """Reduce dimensionality of embeddings for visualization and clustering."""
    
    def __init__(self):
        self.reducers = {}
    
    def reduce_dimensions(self,
                         embeddings: np.ndarray,
                         method: str = 'umap',
                         n_components: int = 2,
                         **kwargs) -> Tuple[np.ndarray, Dict[str, Any]]:
        """Reduce dimensionality of embeddings.
        
        Args:
            embeddings: High-dimensional embeddings
            method: Reduction method ('umap', 'pca', 'tsne')
            n_components: Number of output dimensions
            **kwargs: Additional parameters for the method
            
        Returns:
            Tuple of (reduced embeddings, metadata)
        """
        logger.info(
            f"Reducing {embeddings.shape} to {n_components}D using {method}"
        )
        
        start_time = time.time()
        
        if method == 'umap':
            reduced, reducer = self._reduce_umap(embeddings, n_components, **kwargs)
        elif method == 'pca':
            reduced, reducer = self._reduce_pca(embeddings, n_components, **kwargs)
        elif method == 'tsne':
            reduced, reducer = self._reduce_tsne(embeddings, n_components, **kwargs)
        else:
            raise ValueError(f"Unknown reduction method: {method}")
        
        processing_time = time.time() - start_time
        
        # Store reducer for later use
        self.reducers[method] = reducer
        
        metadata = {
            'method': method,
            'n_components': n_components,
            'original_shape': embeddings.shape,
            'reduced_shape': reduced.shape,
            'processing_time': processing_time,
            'parameters': kwargs
        }
        
        logger.info(f"Dimension reduction complete in {processing_time:.2f}s")
        
        return reduced, metadata
    
    def _reduce_umap(self,
                    embeddings: np.ndarray,
                    n_components: int,
                    n_neighbors: int = 15,
                    min_dist: float = 0.1,
                    metric: str = 'cosine') -> Tuple[np.ndarray, UMAP]:
        """Reduce dimensions using UMAP.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            n_neighbors: UMAP n_neighbors parameter
            min_dist: UMAP min_dist parameter
            metric: Distance metric
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        reducer = UMAP(
            n_components=n_components,
            n_neighbors=n_neighbors,
            min_dist=min_dist,
            metric=metric,
            random_state=42,
            verbose=True
        )
        
        reduced = reducer.fit_transform(embeddings)
        
        return reduced, reducer
    
    def _reduce_pca(self,
                   embeddings: np.ndarray,
                   n_components: int) -> Tuple[np.ndarray, PCA]:
        """Reduce dimensions using PCA.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        reducer = PCA(n_components=n_components, random_state=42)
        reduced = reducer.fit_transform(embeddings)
        
        # Log explained variance
        if hasattr(reducer, 'explained_variance_ratio_'):
            total_variance = np.sum(reducer.explained_variance_ratio_)
            logger.info(f"PCA explained variance: {total_variance:.2%}")
        
        return reduced, reducer
    
    def _reduce_tsne(self,
                    embeddings: np.ndarray,
                    n_components: int,
                    perplexity: float = 30.0,
                    learning_rate: float = 200.0) -> Tuple[np.ndarray, TSNE]:
        """Reduce dimensions using t-SNE.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            perplexity: t-SNE perplexity parameter
            learning_rate: t-SNE learning rate
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        # For t-SNE, first reduce with PCA if dimensions > 50
        if embeddings.shape[1] > 50:
            logger.info("Pre-reducing with PCA for t-SNE")
            pca = PCA(n_components=50, random_state=42)
            embeddings = pca.fit_transform(embeddings)
        
        reducer = TSNE(
            n_components=n_components,
            perplexity=perplexity,
            learning_rate=learning_rate,
            random_state=42,
            verbose=1
        )
        
        reduced = reducer.fit_transform(embeddings)
        
        return reduced, reducer
    
    def transform_new_points(self,
                           embeddings: np.ndarray,
                           method: str = 'umap') -> np.ndarray:
        """Transform new points using existing reducer.
        
        Args:
            embeddings: New embeddings to transform
            method: Which reducer to use
            
        Returns:
            Transformed embeddings
        """
        if method not in self.reducers:
            raise ValueError(f"No {method} reducer available. Run reduce_dimensions first.")
        
        reducer = self.reducers[method]
        
        if method == 'tsne':
            logger.warning("t-SNE doesn't support transform. Returning original embeddings.")
            return embeddings
        
        return reducer.transform(embeddings)
    
    def create_embedding_map(self,
                           embeddings: np.ndarray,
                           labels: Optional[np.ndarray] = None,
                           skill_texts: Optional[list] = None) -> Dict[str, Any]:
        """Create a complete embedding map with 2D coordinates.
        
        Args:
            embeddings: Original embeddings
            labels: Cluster labels (optional)
            skill_texts: Skill names (optional)
            
        Returns:
            Dictionary with 2D coordinates and metadata
        """
        # Reduce to 2D
        coords_2d, metadata = self.reduce_dimensions(embeddings, method='umap', n_components=2)
        
        # Create map
        embedding_map = {
            'coordinates': coords_2d,
            'metadata': metadata
        }
        
        if labels is not None:
            embedding_map['labels'] = labels
        
        if skill_texts is not None:
            embedding_map['skills'] = skill_texts
        
        # Add statistics
        embedding_map['stats'] = {
            'x_range': (float(np.min(coords_2d[:, 0])), float(np.max(coords_2d[:, 0]))),
            'y_range': (float(np.min(coords_2d[:, 1])), float(np.max(coords_2d[:, 1]))),
            'center': (float(np.mean(coords_2d[:, 0])), float(np.mean(coords_2d[:, 1])))
        }
        
        return embedding_map

### src/analyzer/report_generator.py
```python
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import os
from pathlib import Path
from reportlab.lib import colors
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, PageBreak, Image
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from database.operations import DatabaseOperations

logger = logging.getLogger(__name__)

class ReportGenerator:
    """Generate PDF reports with analysis results."""
    
    def __init__(self, output_dir: str = None):
        self.output_dir = output_dir or "./outputs"
        os.makedirs(self.output_dir, exist_ok=True)
        self.db_ops = DatabaseOperations()
        self.styles = getSampleStyleSheet()
        self._add_custom_styles()
    
    def _add_custom_styles(self):
        """Add custom styles for the report."""
        self.styles.add(ParagraphStyle(
            name='CustomTitle',
            parent=self.styles['Title'],
            fontSize=24,
            textColor=colors.HexColor('#1a73e8'),
            spaceAfter=30
        ))
        
        self.styles.add(ParagraphStyle(
            name='SectionHeader',
            parent=self.styles['Heading1'],
            fontSize=16,
            textColor=colors.HexColor('#1a73e8'),
            spaceAfter=12
        ))
        
        self.styles.add(ParagraphStyle(
            name='SubsectionHeader',
            parent=self.styles['Heading2'],
            fontSize=14,
            textColor=colors.HexColor('#5f6368'),
            spaceAfter=10
        ))
    
    def generate_full_report(self, 
                           country: Optional[str] = None,
                           include_visualizations: bool = True) -> str:
        """Generate comprehensive analysis report.
        
        Args:
            country: Country code to filter by (optional)
            include_visualizations: Whether to include charts
            
        Returns:
            Path to generated report
        """
        # Create timestamp for filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        country_suffix = f"_{country}" if country else "_all"
        filename = f"labor_market_analysis{country_suffix}_{timestamp}.pdf"
        filepath = os.path.join(self.output_dir, filename)
        
        # Create document
        doc = SimpleDocTemplate(
            filepath,
            pagesize=A4,
            rightMargin=72,
            leftMargin=72,
            topMargin=72,
            bottomMargin=18
        )
        
        # Build content
        story = []
        
        # Title page
        story.extend(self._create_title_page(country))
        story.append(PageBreak())
        
        # Executive summary
        story.extend(self._create_executive_summary(country))
        story.append(PageBreak())
        
        # Skills analysis
        story.extend(self._create_skills_analysis(country, include_visualizations))
        story.append(PageBreak())
        
        # Clustering results
        story.extend(self._create_clustering_analysis(include_visualizations))
        story.append(PageBreak())
        
        # Temporal trends (if available)
        story.extend(self._create_temporal_analysis(country, include_visualizations))
        story.append(PageBreak())
        
        # Methodology
        story.extend(self._create_methodology_section())
        
        # Build PDF
        doc.build(story)
        
        logger.info(f"Report generated: {filepath}")
        return filepath
    
    def _create_title_page(self, country: Optional[str]) -> List:
        """Create title page elements."""
        elements = []
        
        # Title
        title_text = "Observatorio de Demanda Laboral Tecnológica"
        if country:
            country_names = {'CO': 'Colombia', 'MX': 'México', 'AR': 'Argentina'}
            title_text += f"\n{country_names.get(country, country)}"
        else:
            title_text += "\nAmérica Latina"
        
        elements.append(Paragraph(title_text, self.styles['CustomTitle']))
        elements.append(Spacer(1, 0.5*inch))
        
        # Subtitle
        subtitle = "Análisis Automatizado de Habilidades Técnicas"
        elements.append(Paragraph(subtitle, self.styles['Heading2']))
        elements.append(Spacer(1, 0.3*inch))
        
        # Date
        date_text = f"Fecha de generación: {datetime.now().strftime('%d de %B de %Y')}"
        elements.append(Paragraph(date_text, self.styles['Normal']))
        elements.append(Spacer(1, 2*inch))
        
        # Authors/Institution
        elements.append(Paragraph("Universidad XYZ", self.styles['Normal']))
        elements.append(Paragraph("Facultad de Ingeniería", self.styles['Normal']))
        
        return elements
    
    def _create_executive_summary(self, country: Optional[str]) -> List:
        """Create executive summary section."""
        elements = []
        
        elements.append(Paragraph("Resumen Ejecutivo", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get summary statistics
        stats = self.db_ops.get_skill_statistics(country)
        
        # Summary text
        summary_points = [
            f"Se analizaron un total de {stats.get('total_unique_skills', 0)} habilidades técnicas únicas.",
            f"Las 5 habilidades más demandadas son: {', '.join([s['skill'] for s in stats.get('top_skills', [])[:5]])}.",
            "El análisis revela una fuerte demanda de habilidades en desarrollo web, cloud computing y ciencia de datos.",
            "Se identificaron patrones emergentes en tecnologías de inteligencia artificial y DevOps."
        ]
        
        for point in summary_points:
            elements.append(Paragraph(f"• {point}", self.styles['Normal']))
            elements.append(Spacer(1, 0.1*inch))
        
        return elements
    
    def _create_skills_analysis(self, country: Optional[str], include_viz: bool) -> List:
        """Create skills analysis section."""
        elements = []
        
        elements.append(Paragraph("Análisis de Habilidades", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get skill statistics
        stats = self.db_ops.get_skill_statistics(country)
        
        # Top skills table
        elements.append(Paragraph("Top 20 Habilidades Más Demandadas", self.styles['SubsectionHeader']))
        
        if stats.get('top_skills'):
            # Create table data
            table_data = [['Posición', 'Habilidad', 'Frecuencia']]
            for i, skill_data in enumerate(stats['top_skills'][:20], 1):
                table_data.append([
                    str(i),
                    skill_data['skill'],
                    str(skill_data['count'])
                ])
            
            # Create table
            table = Table(table_data, colWidths=[1*inch, 3*inch, 1.5*inch])
            table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            
            elements.append(table)
            elements.append(Spacer(1, 0.3*inch))
        
        # Add visualization if requested
        if include_viz:
            viz_path = self._create_skill_frequency_chart(stats.get('top_skills', [])[:15])
            if viz_path:
                elements.append(Image(viz_path, width=6*inch, height=4*inch))
                elements.append(Spacer(1, 0.2*inch))
        
        return elements
    
    def _create_clustering_analysis(self, include_viz: bool) -> List:
        """Create clustering analysis section."""
        elements = []
        
        elements.append(Paragraph("Análisis de Clustering", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get latest clustering results
        # Note: This would need to be implemented in DatabaseOperations
        # For now, we'll use placeholder text
        
        elements.append(Paragraph(
            "El análisis de clustering identificó grupos coherentes de habilidades "
            "que típicamente aparecen juntas en las ofertas laborales:",
            self.styles['Normal']
        ))
        elements.append(Spacer(1, 0.1*inch))
        
        # Placeholder cluster descriptions
        clusters = [
            "Frontend Development: React, Vue.js, CSS, JavaScript, HTML5",
            "Backend Development: Node.js, Python, Django, Flask, API REST",
            "Data Science: Python, R, Machine Learning, SQL, Pandas",
            "DevOps: Docker, Kubernetes, AWS, CI/CD, Jenkins",
            "Mobile Development: React Native, Flutter, iOS, Android"
        ]
        
        for cluster in clusters:
            elements.append(Paragraph(f"• {cluster}", self.styles['Normal']))
            elements.append(Spacer(1, 0.05*inch))
        
        return elements
    
    def _create_temporal_analysis(self, country: Optional[str], include_viz: bool) -> List:
        """Create temporal trends analysis section."""
        elements = []
        
        elements.append(Paragraph("Tendencias Temporales", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        elements.append(Paragraph(
            "El análisis temporal permite identificar la evolución de la demanda "
            "de habilidades técnicas a lo largo del tiempo.",
            self.styles['Normal']
        ))
        
        # Placeholder for temporal analysis
        trends = [
            "Crecimiento sostenido en demanda de habilidades cloud (AWS, Azure)",
            "Aumento significativo en tecnologías de IA/ML en los últimos 6 meses",
            "Estabilidad en frameworks tradicionales (Spring, .NET)",
            "Emergencia de nuevas herramientas DevOps"
        ]
        
        elements.append(Spacer(1, 0.1*inch))
        for trend in trends:
            elements.append(Paragraph(f"• {trend}", self.styles['Normal']))
            elements.append(Spacer(1, 0.05*inch))
        
        return elements
    
    def _create_methodology_section(self) -> List:
        """Create methodology section."""
        elements = []
        
        elements.append(Paragraph("Metodología", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        methodology_text = """
        Este análisis se realizó mediante un pipeline automatizado que incluye:
        
        1. Web Scraping: Recolección automática de ofertas laborales de portales como 
           Computrabajo, Bumeran y elempleo.com.
        
        2. Extracción de Habilidades: Combinación de técnicas de NER (Named Entity Recognition) 
           y expresiones regulares para identificar menciones de habilidades técnicas.
        
        3. Enriquecimiento con LLM: Uso de modelos de lenguaje para identificar habilidades 
           implícitas y normalizar variaciones.
        
        4. Análisis Semántico: Generación de embeddings multilingües y clustering para 
           identificar grupos de habilidades relacionadas.
        
        5. Visualización: Generación de reportes estáticos con métricas agregadas y 
           visualizaciones interpretables.
        """
        
        elements.append(Paragraph(methodology_text, self.styles['Normal']))
        
        return elements
    
    def _create_skill_frequency_chart(self, top_skills: List[Dict[str, Any]]) -> Optional[str]:
        """Create skill frequency bar chart.
        
        Args:
            top_skills: List of top skills with counts
            
        Returns:
            Path to saved chart image
        """
        if not top_skills:
            return None
        
        try:
            # Prepare data
            skills = [s['skill'] for s in top_skills]
            counts = [s['count'] for s in top_skills]
            
            # Create figure
            plt.figure(figsize=(10, 6))
            
            # Create horizontal bar chart
            bars = plt.barh(skills, counts, color='#1a73e8')
            
            # Customize
            plt.xlabel('Número de Vacantes', fontsize=12)
            plt.title('Habilidades Más Demandadas', fontsize=14, fontweight='bold')
            plt.gca().invert_yaxis()  # Highest on top
            
            # Add value labels
            for i, bar in enumerate(bars):
                width = bar.get_width()
                plt.text(width + 1, bar.get_y() + bar.get_height()/2, 
                        f'{counts[i]}', 
                        ha='left', va='center')
            
            plt.tight_layout()
            
            # Save
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(self.output_dir, f"skill_frequency_{timestamp}.png")
            plt.savefig(filepath, dpi=300, bbox_inches='tight')
            plt.close()
            
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating skill frequency chart: {e}")
            return None
    
    def enrich_extracted_skills(self, extracted_skills: List[Dict]) -> List[Dict]:
        """Enrich extracted skills with ESCO matches.
        
        Args:
            extracted_skills: List of extracted skill dictionaries
            
        Returns:
            Enriched skill list
        """
        # Get unique skill texts
        unique_skills = list(set(skill['skill_text'] for skill in extracted_skills))
        
        # Match all unique skills
        esco_matches = self.match_skills_batch(unique_skills)
        
        # Enrich original skills
        enriched = []
        for skill in extracted_skills:
            enriched_skill = skill.copy()
            
            match = esco_matches.get(skill['skill_text'])
            if match:
                enriched_skill.update({
                    'esco_uri': match['esco_uri'],
                    'esco_preferred_label': match['esco_preferred_label'],
                    'esco_match_type': match['match_type'],
                    'esco_match_score': match['match_score']
                })
            
            enriched.append(enriched_skill)
        
        return enriched
```

### src/extractor/pipeline.py
```python
import logging
from typing import List, Dict, Optional
from database.operations import DatabaseOperations
from .ner_extractor import NERExtractor
from .regex_patterns import RegexExtractor
from .esco_matcher import ESCOMatcher
from config.settings import get_settings
import time

logger = logging.getLogger(__name__)

class ExtractionPipeline:
    """Main pipeline for skill extraction from job postings."""
    
    def __init__(self):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        
        # Initialize extractors
        logger.info("Initializing extraction components...")
        self.ner_extractor = NERExtractor()
        self.regex_extractor = RegexExtractor()
        self.esco_matcher = ESCOMatcher()
        
        logger.info("Extraction pipeline initialized")
    
    def process_batch(self, batch_size: int = 100) -> Dict[str, any]:
        """Process a batch of unprocessed jobs.
        
        Args:
            batch_size: Number of jobs to process
            
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'jobs_processed': 0,
            'skills_extracted': 0,
            'esco_matches': 0,
            'errors': 0
        }
        
        try:
            # Get unprocessed jobs
            jobs = self.db_ops.get_unprocessed_jobs(limit=batch_size)
            logger.info(f"Processing {len(jobs)} jobs")
            
            for job in jobs:
                try:
                    # Process individual job
                    skills = self.process_job(job)
                    
                    if skills:
                        # Save extracted skills
                        self.db_ops.insert_extracted_skills(
                            str(job.job_id),
                            skills
                        )
                        
                        # Mark job as processed
                        self.db_ops.mark_job_processed(str(job.job_id))
                        
                        # Update stats
                        stats['jobs_processed'] += 1
                        stats['skills_extracted'] += len(skills)
                        stats['esco_matches'] += sum(
                            1 for s in skills if s.get('esco_uri')
                        )
                    
                except Exception as e:
                    logger.error(f"Error processing job {job.job_id}: {e}")
                    stats['errors'] += 1
            
            # Calculate processing time
            stats['processing_time'] = time.time() - start_time
            stats['jobs_per_second'] = stats['jobs_processed'] / stats['processing_time'] if stats['processing_time'] > 0 else 0
            
            logger.info(
                f"Batch complete: {stats['jobs_processed']} jobs, "
                f"{stats['skills_extracted']} skills extracted, "
                f"{stats['esco_matches']} ESCO matches, "
                f"{stats['errors']} errors, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"Batch processing failed: {e}")
            raise
    
    def process_job(self, job) -> List[Dict[str, any]]:
        """Process a single job to extract skills.
        
        Args:
            job: Job object from database
            
        Returns:
            List of extracted and enriched skills
        """
        # Prepare job data
        job_data = {
            'title': job.title,
            'description': job.description,
            'requirements': job.requirements
        }
        
        # Extract skills using NER
        ner_skills = self.ner_extractor.extract_from_job(job_data)
        
        # Extract skills using regex
        regex_skills = self.regex_extractor.extract_from_job(job_data)
        
        # Combine and deduplicate
        all_skills = self._combine_skills(ner_skills, regex_skills)
        
        # Enrich with ESCO matches
        enriched_skills = self.esco_matcher.enrich_extracted_skills(all_skills)
        
        logger.debug(
            f"Job {job.job_id}: {len(ner_skills)} NER skills, "
            f"{len(regex_skills)} regex skills, "
            f"{len(enriched_skills)} total after deduplication"
        )
        
        return enriched_skills
    
    def _combine_skills(self, ner_skills: List[Dict], regex_skills: List[Dict]) -> List[Dict]:
        """Combine and deduplicate skills from different extractors.
        
        Args:
            ner_skills: Skills from NER
            regex_skills: Skills from regex
            
        Returns:
            Combined and deduplicated skill list
        """
        # Use skill text and source section as unique key
        seen_skills = {}
        
        # Process NER skills first (higher confidence)
        for skill in ner_skills:
            key = (skill['skill_text'], skill['source_section'])
            
            if key not in seen_skills:
                seen_skills[key] = skill
            else:
                # Update confidence if higher
                if skill['confidence_score'] > seen_skills[key]['confidence_score']:
                    seen_skills[key] = skill
        
        # Process regex skills
        for skill in regex_skills:
            key = (skill['skill_text'], skill['source_section'])
            
            if key not in seen_skills:
                seen_skills[key] = skill
            else:
                # Merge extraction methods
                existing = seen_skills[key]
                if existing['extraction_method'] != skill['extraction_method']:
                    existing['extraction_method'] = 'ner+regex'
                    existing['confidence_score'] = min(0.95, existing['confidence_score'] + 0.1)
        
        return list(seen_skills.values())
    
    def run_continuous(self, batch_size: int = 100, wait_time: int = 60):
        """Run extraction continuously.
        
        Args:
            batch_size: Jobs per batch
            wait_time: Seconds to wait between batches
        """
        logger.info(f"Starting continuous extraction (batch_size={batch_size}, wait={wait_time}s)")
        
        while True:
            try:
                stats = self.process_batch(batch_size)
                
                if stats['jobs_processed'] == 0:
                    logger.info(f"No jobs to process. Waiting {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    # Short pause between batches
                    time.sleep(5)
                    
            except KeyboardInterrupt:
                logger.info("Extraction stopped by user")
                break
            except Exception as e:
                logger.error(f"Extraction error: {e}")
                logger.info(f"Waiting {wait_time}s before retry...")
                time.sleep(wait_time)
```

---

## 6. LLM Processor Module Files

### src/llm_processor/__init__.py
```python
from .llm_handler import LLMHandler
from .prompts import PromptGenerator
from .esco_normalizer import ESCONormalizer
from .validator import SkillValidator

__all__ = ['LLMHandler', 'PromptGenerator', 'ESCONormalizer', 'SkillValidator']
```

### src/llm_processor/llm_handler.py
```python
import logging
from typing import List, Dict, Optional, Any
from llama_cpp import Llama
import openai
from config.settings import get_settings
import json
import time

logger = logging.getLogger(__name__)

class LLMHandler:
    """Handle LLM interactions for skill processing."""
    
    def __init__(self, model_type: str = "local"):
        self.settings = get_settings()
        self.model_type = model_type
        
        if model_type == "local":
            self._init_local_model()
        elif model_type == "openai":
            self._init_openai()
        else:
            raise ValueError(f"Unknown model type: {model_type}")
    
    def _init_local_model(self):
        """Initialize local LLaMA/Mistral model."""
        try:
            logger.info(f"Loading local model from {self.settings.llm_model_path}")
            
            self.model = Llama(
                model_path=self.settings.llm_model_path,
                n_ctx=self.settings.llm_context_length,
                n_gpu_layers=self.settings.llm_n_gpu_layers,
                verbose=False
            )
            
            logger.info("Local model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load local model: {e}")
            raise
    
    def _init_openai(self):
        """Initialize OpenAI API client."""
        if not self.settings.openai_api_key:
            raise ValueError("OpenAI API key not configured")
        
        openai.api_key = self.settings.openai_api_key
        self.model_name = self.settings.openai_model
        logger.info(f"OpenAI API initialized with model {self.model_name}")
    
    def process_skills(self, 
                      job_data: Dict[str, Any],
                      extracted_skills: List[Dict[str, Any]],
                      prompt_template: str) -> Dict[str, Any]:
        """Process skills using LLM.
        
        Args:
            job_data: Job information
            extracted_skills: Initially extracted skills
            prompt_template: Formatted prompt template
            
        Returns:
            LLM response with processed skills
        """
        start_time = time.time()
        
        try:
            if self.model_type == "local":
                response = self._process_local(prompt_template)
            else:
                response = self._process_openai(prompt_template)
            
            # Parse response
            result = self._parse_response(response)
            
            # Add metadata
            result['processing_time'] = time.time() - start_time
            result['model_type'] = self.model_type
            result['model_name'] = getattr(self, 'model_name', 'local_mistral')
            
            return result
            
        except Exception as e:
            logger.error(f"LLM processing failed: {e}")
            raise
    
    def _process_local(self, prompt: str) -> str:
        """Process using local model.
        
        Args:
            prompt: Complete prompt
            
        Returns:
            Model response
        """
        response = self.model(
            prompt,
            max_tokens=self.settings.llm_max_tokens,
            temperature=self.settings.llm_temperature,
            stop=["</response>", "\n\n\n"],
            echo=False
        )
        
        return response['choices'][0]['text']
    
    def _process_openai(self, prompt: str) -> str:
        """Process using OpenAI API.
        
        Args:
            prompt: Complete prompt
            
        Returns:
            Model response
        """
        response = openai.ChatCompletion.create(
            model=self.model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert in analyzing job postings and extracting technical skills. Respond in Spanish."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            max_tokens=self.settings.llm_max_tokens,
            temperature=self.settings.llm_temperature
        )
        
        return response.choices[0].message.content
    
    def _parse_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response.
        
        Args:
            response: Raw LLM response
            
        Returns:
            Parsed skills data
        """
        # Try to extract JSON if present
        if "```json" in response:
            # Extract JSON block
            start = response.find("```json") + 7
            end = response.find("```", start)
            json_str = response[start:end].strip()
            
            try:
                return json.loads(json_str)
            except json.JSONDecodeError:
                logger.warning("Failed to parse JSON from response")
        
        # Fallback: Parse structured text response
        result = {
            "explicit_skills": [],
            "implicit_skills": [],
            "normalized_skills": [],
            "deduplicated_skills": []
        }
        
        lines = response.strip().split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            
            # Detect sections
            if "habilidades explícitas" in line.lower():
                current_section = "explicit_skills"
            elif "habilidades implícitas" in line.lower():
                current_section = "implicit_skills"
            elif "habilidades normalizadas" in line.lower():
                current_section = "normalized_skills"
            elif "deduplicadas" in line.lower():
                current_section = "deduplicated_skills"
            elif line and current_section and (line.startswith('-') or line.startswith('*')):
                # Extract skill from bullet point
                skill_text = line.lstrip('-*').strip()
                
                # Parse skill with reasoning if present
                if ':' in skill_text:
                    skill, reasoning = skill_text.split(':', 1)
                    result[current_section].append({
                        "skill": skill.strip(),
                        "reasoning": reasoning.strip()
                    })
                else:
                    result[current_section].append({
                        "skill": skill_text
                    })
        
        return result
```

### src/llm_processor/prompts.py
```python
from typing import List, Dict, Any
import json

class PromptGenerator:
    """Generate prompts for LLM skill processing."""
    
    def __init__(self):
        self.templates = self._load_templates()
    
    def _load_templates(self) -> Dict[str, str]:
        """Load prompt templates."""
        return {
            "skill_processing": """Eres un experto en análisis de ofertas laborales tecnológicas en América Latina.

Datos de la vacante:
- Título: {job_title}
- Descripción: {job_description}
- Requisitos: {job_requirements}

Habilidades extraídas inicialmente:
{extracted_skills}

Por favor, realiza las siguientes tareas:

1. **Validación de habilidades explícitas**: Revisa las habilidades extraídas y confirma cuáles son realmente habilidades técnicas relevantes.

2. **Detección de habilidades implícitas**: Basándote en el contexto del puesto, identifica habilidades técnicas que serían necesarias pero no están mencionadas explícitamente. Por ejemplo:
   - Si menciona "desarrollo web full stack" → probablemente necesite Git, bases de datos, APIs REST
   - Si menciona "análisis de datos" → probablemente necesite SQL, Python/R, visualización
   - Si menciona "DevOps" → probablemente necesite CI/CD, contenedores, cloud

3. **Normalización con ESCO**: Para cada habilidad, proporciona la forma normalizada según estándares internacionales:
   - Usa nombres estándar (ej: "JS" → "JavaScript", "React.js" → "React")
   - Mantén el español cuando sea apropiado
   - Agrupa variaciones (ej: "MySQL/MariaDB" → "MySQL")

4. **Deduplicación**: Elimina habilidades duplicadas o redundantes:
   - Combina variaciones del mismo concepto
   - Elimina términos demasiado genéricos
   - Mantén el término más específico cuando haya jerarquía

Responde en el siguiente formato JSON:
```json
{{
  "explicit_skills": [
    {{"skill": "nombre", "confidence": 0.9, "original": "texto_original"}}
  ],
  "implicit_skills": [
    {{"skill": "nombre", "confidence": 0.7, "reasoning": "justificación"}}
  ],
  "normalized_skills": [
    {{"original": "skill_original", "normalized": "skill_normalizado", "esco_match": "posible_uri"}}
  ],
  "deduplicated_skills": [
    {{"skill": "nombre_final", "type": "explicit|implicit", "category": "programming|database|framework|tool|soft_skill"}}
  ]
}}
```""",

            "simple_inference": """Analiza esta oferta de trabajo y extrae SOLO las habilidades técnicas implícitas que no están mencionadas pero serían necesarias.

Título: {job_title}
Descripción resumida: {job_summary}
Habilidades ya identificadas: {known_skills}

Lista únicamente las habilidades técnicas implícitas con su justificación:
""",

            "normalization": """Normaliza las siguientes habilidades técnicas según estándares internacionales y la taxonomía ESCO:

Habilidades a normalizar:
{skills_list}

Para cada habilidad, proporciona:
- Forma normalizada
- Categoría (lenguaje/framework/base de datos/herramienta/metodología)
- Término ESCO equivalente si existe

Responde en formato de lista:
""",

            "deduplication": """Elimina duplicados y agrupa las siguientes habilidades:

Habilidades:
{skills_list}

Reglas:
- Combina variaciones del mismo concepto (ej: JS, JavaScript, javascript → JavaScript)
- Mantén el término más específico cuando haya jerarquía
- Elimina términos genéricos si hay específicos

Lista final sin duplicados:
"""
        }
    
    def generate_skill_processing_prompt(self, 
                                       job_data: Dict[str, Any],
                                       extracted_skills: List[Dict[str, Any]]) -> str:
        """Generate prompt for complete skill processing.
        
        Args:
            job_data: Job information
            extracted_skills: Initially extracted skills
            
        Returns:
            Formatted prompt
        """
        # Format extracted skills
        skills_text = self._format_extracted_skills(extracted_skills)
        
        prompt = self.templates["skill_processing"].format(
            job_title=job_data.get('title', 'No especificado'),
            job_description=job_data.get('description', 'No especificado'),
            job_requirements=job_data.get('requirements', 'No especificado'),
            extracted_skills=skills_text
        )
        
        return prompt
    
    def generate_inference_prompt(self,
                                job_title: str,
                                job_summary: str,
                                known_skills: List[str]) -> str:
        """Generate prompt for implicit skill inference.
        
        Args:
            job_title: Job title
            job_summary: Brief job description
            known_skills: Already identified skills
            
        Returns:
            Formatted prompt
        """
        skills_list = ", ".join(known_skills) if known_skills else "Ninguna"
        
        return self.templates["simple_inference"].format(
            job_title=job_title,
            job_summary=job_summary,
            known_skills=skills_list
        )
    
    def generate_normalization_prompt(self, skills: List[str]) -> str:
        """Generate prompt for skill normalization.
        
        Args:
            skills: List of skills to normalize
            
        Returns:
            Formatted prompt
        """
        skills_text = "\n".join(f"- {skill}" for skill in skills)
        
        return self.templates["normalization"].format(
            skills_list=skills_text
        )
    
    def generate_deduplication_prompt(self, skills: List[str]) -> str:
        """Generate prompt for skill deduplication.
        
        Args:
            skills: List of skills to deduplicate
            
        Returns:
            Formatted prompt
        """
        skills_text = "\n".join(f"- {skill}" for skill in skills)
        
        return self.templates["deduplication"].format(
            skills_list=skills_text
        )
    
    def _format_extracted_skills(self, skills: List[Dict[str, Any]]) -> str:
        """Format extracted skills for prompt.
        
        Args:
            skills: List of skill dictionaries
            
        Returns:
            Formatted text
        """
        formatted_skills = []
        
        # Group by source section
        by_section = {}
        for skill in skills:
            section = skill.get('source_section', 'unknown')
            if section not in by_section:
                by_section[section] = []
            by_section[section].append(skill)
        
        # Format each section
        for section, section_skills in by_section.items():
            formatted_skills.append(f"\nDe {section}:")
            for skill in section_skills:
                method = skill.get('extraction_method', 'unknown')
                confidence = skill.get('confidence_score', 0)
                text = skill.get('skill_text', '')
                
                formatted_skills.append(
                    f"  - {text} (método: {method}, confianza: {confidence:.2f})"
                )
        
        return "\n".join(formatted_skills)

### src/llm_processor/esco_normalizer.py
```python
import logging
from typing import List, Dict, Any, Optional
from fuzzywuzzy import fuzz
import yaml

logger = logging.getLogger(__name__)

class ESCONormalizer:
    """Normalize skills using ESCO taxonomy with LLM assistance."""
    
    def __init__(self, config_path: str = "config/esco_config.yaml"):
        self.config = self._load_config(config_path)
        self.normalization_rules = self._build_normalization_rules()
    
    def _load_config(self, config_path: str) -> dict:
        """Load ESCO configuration."""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config['esco']
        except Exception as e:
            logger.error(f"Failed to load ESCO config: {e}")
            return {}
    
    def _build_normalization_rules(self) -> Dict[str, str]:
        """Build normalization rules from config and common variations."""
        rules = {}
        
        # Load from config
        if 'tech_mappings' in self.config:
            rules.update(self.config['tech_mappings'])
        
        # Add common variations
        common_variations = {
            # JavaScript variations
            'js': 'JavaScript',
            'javascript': 'JavaScript',
            'java script': 'JavaScript',
            
            # Python variations
            'python3': 'Python',
            'python 3': 'Python',
            'python2': 'Python',
            
            # React variations
            'reactjs': 'React',
            'react.js': 'React',
            'react js': 'React',
            'react native': 'React Native',
            
            # Node variations
            'nodejs': 'Node.js',
            'node js': 'Node.js',
            'node': 'Node.js',
            
            # Database variations
            'postgres': 'PostgreSQL',
            'mysql': 'MySQL',
            'maria db': 'MariaDB',
            'mariadb': 'MariaDB',
            'mongo': 'MongoDB',
            'mongo db': 'MongoDB',
            
            # .NET variations
            'dotnet': '.NET',
            'dot net': '.NET',
            '.net core': '.NET Core',
            'asp.net': 'ASP.NET',
            
            # Other common variations
            'c++': 'C++',
            'c#': 'C#',
            'c sharp': 'C#',
            'objective c': 'Objective-C',
            'obj-c': 'Objective-C',
            
            # Spanish to English mappings
            'base de datos': 'Database',
            'desarrollo web': 'Web Development',
            'desarrollo móvil': 'Mobile Development',
            'aprendizaje automático': 'Machine Learning',
            'inteligencia artificial': 'Artificial Intelligence',
            'ciencia de datos': 'Data Science',
            'computación en la nube': 'Cloud Computing',
            'control de versiones': 'Version Control',
        }
        
        # Convert all keys to lowercase
        for key, value in common_variations.items():
            rules[key.lower()] = value
        
        return rules
    
    def normalize_skill(self, skill: str) -> Dict[str, Any]:
        """Normalize a single skill.
        
        Args:
            skill: Raw skill text
            
        Returns:
            Normalized skill data
        """
        skill_lower = skill.lower().strip()
        
        # Direct mapping
        if skill_lower in self.normalization_rules:
            return {
                'original': skill,
                'normalized': self.normalization_rules[skill_lower],
                'method': 'direct_mapping',
                'confidence': 1.0
            }
        
        # Try fuzzy matching
        best_match = None
        best_score = 0
        
        for pattern, normalized in self.normalization_rules.items():
            score = fuzz.ratio(skill_lower, pattern)
            if score > best_score and score >= 85:
                best_score = score
                best_match = normalized
        
        if best_match:
            return {
                'original': skill,
                'normalized': best_match,
                'method': 'fuzzy_mapping',
                'confidence': best_score / 100.0
            }
        
        # Category-based normalization
        normalized = self._category_normalization(skill)
        if normalized != skill:
            return {
                'original': skill,
                'normalized': normalized,
                'method': 'category_rules',
                'confidence': 0.8
            }
        
        # No normalization found
        return {
            'original': skill,
            'normalized': skill,
            'method': 'no_normalization',
            'confidence': 0.5
        }
    
    def _category_normalization(self, skill: str) -> str:
        """Apply category-based normalization rules.
        
        Args:
            skill: Skill to normalize
            
        Returns:
            Normalized skill
        """
        skill_lower = skill.lower()
        
        # Framework detection
        if 'framework' in skill_lower:
            skill = skill.replace('framework', '').replace('Framework', '').strip()
        
        # Version removal for certain technologies
        version_patterns = [
            (r'python\s*\d+\.?\d*', 'Python'),
            (r'java\s*\d+', 'Java'),
            (r'angular\s*\d+', 'Angular'),
            (r'vue\s*\d+', 'Vue.js'),
            (r'react\s*\d+', 'React'),
        ]
        
        import re
        for pattern, replacement in version_patterns:
            if re.search(pattern, skill_lower):
                return replacement
        
        # Capitalize properly
        # Special cases
        special_cases = {
            'mysql': 'MySQL',
            'postgresql': 'PostgreSQL',
            'mongodb': 'MongoDB',
            'javascript': 'JavaScript',
            'typescript': 'TypeScript',
            'graphql': 'GraphQL',
            'nodejs': 'Node.js',
            'reactjs': 'React',
            'vuejs': 'Vue.js',
        }
        
        if skill_lower in special_cases:
            return special_cases[skill_lower]
        
        # Default: capitalize first letter of each word
        return ' '.join(word.capitalize() for word in skill.split())
    
    def normalize_skills_batch(self, skills: List[str]) -> List[Dict[str, Any]]:
        """Normalize multiple skills.
        
        Args:
            skills: List of skills to normalize
            
        Returns:
            List of normalized skill data
        """
        normalized = []
        
        for skill in skills:
            result = self.normalize_skill(skill)
            normalized.append(result)
        
        return normalized
    
    def deduplicate_normalized_skills(self, normalized_skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Deduplicate normalized skills.
        
        Args:
            normalized_skills: List of normalized skill dictionaries
            
        Returns:
            Deduplicated list
        """
        # Group by normalized form
        skill_groups = {}
        
        for skill_data in normalized_skills:
            normalized = skill_data['normalized']
            
            if normalized not in skill_groups:
                skill_groups[normalized] = {
                    'normalized': normalized,
                    'originals': [],
                    'best_confidence': 0,
                    'methods': set()
                }
            
            skill_groups[normalized]['originals'].append(skill_data['original'])
            skill_groups[normalized]['methods'].add(skill_data['method'])
            skill_groups[normalized]['best_confidence'] = max(
                skill_groups[normalized]['best_confidence'],
                skill_data['confidence']
            )
        
        # Convert back to list
        deduplicated = []
        for normalized, group_data in skill_groups.items():
            deduplicated.append({
                'normalized': normalized,
                'original_variations': group_data['originals'],
                'confidence': group_data['best_confidence'],
                'methods': list(group_data['methods'])
            })
        
        return deduplicated

### src/llm_processor/validator.py
```python
import logging
from typing import List, Dict, Any, Set
import re

logger = logging.getLogger(__name__)

class SkillValidator:
    """Validate and filter skills."""
    
    def __init__(self):
        self.blacklist = self._build_blacklist()
        self.whitelist = self._build_whitelist()
        self.categories = self._build_categories()
    
    def _build_blacklist(self) -> Set[str]:
        """Build blacklist of non-skill terms."""
        return {
            # Generic terms
            'experiencia', 'años', 'año', 'conocimiento', 'conocimientos',
            'habilidad', 'habilidades', 'capacidad', 'competencia',
            'desarrollo', 'trabajo', 'empresa', 'cliente', 'proyecto',
            'equipo', 'persona', 'profesional', 'área', 'proceso',
            
            # Too generic tech terms
            'tecnología', 'tecnologías', 'sistema', 'sistemas',
            'informática', 'computación', 'software', 'hardware',
            'programación', 'desarrollo de software',
            
            # Common words
            'bueno', 'excelente', 'alto', 'nivel', 'manejo',
            'uso', 'gestión', 'administración', 'análisis',
            
            # Methodologies too generic
            'metodología', 'metodologías', 'mejores prácticas',
            
            # Soft skills (we focus on technical)
            'comunicación', 'liderazgo', 'trabajo en equipo',
            'responsabilidad', 'proactividad', 'creatividad'
        }
    
    def _build_whitelist(self) -> Set[str]:
        """Build whitelist of known valid skills."""
        return {
            # Programming languages
            'python', 'java', 'javascript', 'typescript', 'c++', 'c#',
            'php', 'ruby', 'go', 'golang', 'rust', 'kotlin', 'swift',
            'objective-c', 'r', 'scala', 'perl', 'matlab', 'julia',
            
            # Frameworks and libraries
            'react', 'angular', 'vue', 'django', 'flask', 'spring',
            'express', 'laravel', 'rails', 'fastapi', 'nextjs',
            '.net', 'asp.net', 'tensorflow', 'pytorch', 'keras',
            
            # Databases
            'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',
            'cassandra', 'dynamodb', 'oracle', 'sql server', 'sqlite',
            'neo4j', 'couchdb', 'firebase', 'supabase',
            
            # Cloud and DevOps
            'aws', 'azure', 'gcp', 'google cloud', 'docker', 'kubernetes',
            'jenkins', 'terraform', 'ansible', 'puppet', 'chef',
            'gitlab', 'github', 'bitbucket', 'circleci', 'travis',
            
            # Tools and platforms
            'git', 'jira', 'confluence', 'slack', 'linux', 'windows',
            'macos', 'ubuntu', 'centos', 'debian', 'nginx', 'apache',
            'grafana', 'prometheus', 'elasticsearch', 'kibana',
            
            # Data and ML
            'machine learning', 'deep learning', 'data science',
            'big data', 'spark', 'hadoop', 'kafka', 'airflow',
            'pandas', 'numpy', 'scikit-learn', 'matplotlib',
            
            # Mobile
            'android', 'ios', 'react native', 'flutter', 'xamarin',
            'swift', 'kotlin', 'objective-c', 'cordova', 'ionic',
            
            # Other
            'api', 'rest', 'graphql', 'websocket', 'microservices',
            'ci/cd', 'agile', 'scrum', 'kanban', 'tdd', 'bdd'
        }
    
    def _build_categories(self) -> Dict[str, Set[str]]:
        """Build skill categories."""
        return {
            'programming_language': {
                'python', 'java', 'javascript', 'typescript', 'c++', 'c#',
                'php', 'ruby', 'go', 'rust', 'kotlin', 'swift', 'r'
            },
            'framework': {
                'react', 'angular', 'vue', 'django', 'flask', 'spring',
                'express', 'laravel', 'rails', '.net', 'nextjs'
            },
            'database': {
                'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',
                'cassandra', 'dynamodb', 'oracle', 'sql server'
            },
            'cloud_platform': {
                'aws', 'azure', 'gcp', 'google cloud', 'heroku', 'digitalocean'
            },
            'devops_tool': {
                'docker', 'kubernetes', 'jenkins', 'terraform', 'ansible',
                'git', 'github', 'gitlab', 'ci/cd'
            },
            'data_ml': {
                'machine learning', 'deep learning', 'tensorflow', 'pytorch',
                'pandas', 'numpy', 'spark', 'hadoop'
            },
            'mobile': {
                'android', 'ios', 'react native', 'flutter', 'xamarin'
            },
            'methodology': {
                'agile', 'scrum', 'kanban', 'devops', 'tdd', 'bdd'
            }
        }
    
    def validate_skill(self, skill: str) -> Dict[str, Any]:
        """Validate a single skill.
        
        Args:
            skill: Skill to validate
            
        Returns:
            Validation result
        """
        skill_lower = skill.lower().strip()
        
        # Check blacklist
        if skill_lower in self.blacklist:
            return {
                'skill': skill,
                'valid': False,
                'reason': 'blacklisted',
                'confidence': 0.0
            }
        
        # Check whitelist
        if skill_lower in self.whitelist:
            category = self._categorize_skill(skill_lower)
            return {
                'skill': skill,
                'valid': True,
                'reason': 'whitelisted',
                'category': category,
                'confidence': 1.0
            }
        
        # Length check
        if len(skill_lower) < 2 or len(skill_lower) > 50:
            return {
                'skill': skill,
                'valid': False,
                'reason': 'invalid_length',
                'confidence': 0.0
            }
        
        # Pattern validation
        if not self._validate_pattern(skill_lower):
            return {
                'skill': skill,
                'valid': False,
                'reason': 'invalid_pattern',
                'confidence': 0.0
            }
        
        # Partial matches in whitelist
        for valid_skill in self.whitelist:
            if valid_skill in skill_lower or skill_lower in valid_skill:
                category = self._categorize_skill(valid_skill)
                return {
                    'skill': skill,
                    'valid': True,
                    'reason': 'partial_match',
                    'category': category,
                    'confidence': 0.7
                }
        
        # If not in whitelist but passes other checks
        return {
            'skill': skill,
            'valid': True,
            'reason': 'pattern_valid',
            'category': 'uncategorized',
            'confidence': 0.5
        }
    
    def _validate_pattern(self, skill: str) -> bool:
        """Validate skill pattern.
        
        Args:
            skill: Skill to validate
            
        Returns:
            True if pattern is valid
        """
        # Must contain at least one letter
        if not re.search(r'[a-zA-Z]', skill):
            return False
        
        # Check for suspicious patterns
        suspicious_patterns = [
            r'^\d+,  # Only numbers
            r'^[^a-zA-Z0-9\s\.\+\#\-/]+,  # Only special chars
            r'\b(años?|experiencia|conocimientos?)\b',  # Common non-skills
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, skill, re.IGNORECASE):
                return False
        
        return True
    
    def _categorize_skill(self, skill: str) -> str:
        """Categorize a skill.
        
        Args:
            skill: Skill to categorize
            
        Returns:
            Category name
        """
        skill_lower = skill.lower()
        
        for category, skills in self.categories.items():
            if skill_lower in skills:
                return category
        
        # Try partial matches
        for category, skills in self.categories.items():
            for cat_skill in skills:
                if cat_skill in skill_lower or skill_lower in cat_skill:
                    return category
        
        return 'uncategorized'
    
    def validate_skills_batch(self, skills: List[str]) -> List[Dict[str, Any]]:
        """Validate multiple skills.
        
        Args:
            skills: List of skills to validate
            
        Returns:
            List of validation results
        """
        results = []
        
        for skill in skills:
            result = self.validate_skill(skill)
            results.append(result)
        
        # Log statistics
        valid_count = sum(1 for r in results if r['valid'])
        logger.info(
            f"Skill validation: {valid_count}/{len(skills)} valid "
            f"({valid_count/len(skills)*100:.1f}%)"
        )
        
        return results
    
    def filter_valid_skills(self, skills: List[str], min_confidence: float = 0.5) -> List[str]:
        """Filter only valid skills.
        
        Args:
            skills: List of skills
            min_confidence: Minimum confidence threshold
            
        Returns:
            List of valid skills
        """
        results = self.validate_skills_batch(skills)
        
        valid_skills = [
            r['skill'] for r in results
            if r['valid'] and r['confidence'] >= min_confidence
        ]
        
        return valid_skills

### src/llm_processor/pipeline.py
```python
import logging
from typing import List, Dict, Any
from database.operations import DatabaseOperations
from .llm_handler import LLMHandler
from .prompts import PromptGenerator
from .esco_normalizer import ESCONormalizer
from .validator import SkillValidator
from config.settings import get_settings
import time

logger = logging.getLogger(__name__)

class LLMProcessingPipeline:
    """Main pipeline for LLM-based skill processing."""
    
    def __init__(self, model_type: str = "local"):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        
        logger.info("Initializing LLM processing components...")
        self.llm_handler = LLMHandler(model_type)
        self.prompt_generator = PromptGenerator()
        self.normalizer = ESCONormalizer()
        self.validator = SkillValidator()
        
        logger.info("LLM processing pipeline initialized")
    
    def process_batch(self, batch_size: int = 50) -> Dict[str, Any]:
        """Process a batch of jobs with extracted skills.
        
        Args:
            batch_size: Number of jobs to process
            
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'jobs_processed': 0,
            'skills_enhanced': 0,
            'implicit_skills_found': 0,
            'skills_normalized': 0,
            'errors': 0
        }
        
        try:
            # Get jobs with extracted skills needing LLM processing
            jobs_data = self.db_ops.get_extracted_skills_for_processing(limit=batch_size)
            logger.info(f"Processing {len(jobs_data)} jobs with LLM")
            
            for job_data in jobs_data:
                try:
                    # Process individual job
                    enhanced_skills = self.process_job(job_data)
                    
                    if enhanced_skills:
                        # Save enhanced skills
                        self.db_ops.insert_enhanced_skills(
                            job_data['job_id'],
                            enhanced_skills
                        )
                        
                        # Update statistics
                        stats['jobs_processed'] += 1
                        stats['skills_enhanced'] += len(enhanced_skills)
                        stats['implicit_skills_found'] += sum(
                            1 for s in enhanced_skills 
                            if s.get('skill_type') == 'implicit'
                        )
                        stats['skills_normalized'] += sum(
                            1 for s in enhanced_skills
                            if s.get('normalized_skill') != s.get('original_skill_text')
                        )
                    
                except Exception as e:
                    logger.error(f"Error processing job {job_data['job_id']}: {e}")
                    stats['errors'] += 1
            
            # Calculate processing time
            stats['processing_time'] = time.time() - start_time
            stats['jobs_per_minute'] = (stats['jobs_processed'] / stats['processing_time']) * 60 if stats['processing_time'] > 0 else 0
            
            logger.info(
                f"LLM batch complete: {stats['jobs_processed']} jobs, "
                f"{stats['skills_enhanced']} skills enhanced, "
                f"{stats['implicit_skills_found']} implicit skills found, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"LLM batch processing failed: {e}")
            raise
    
    def process_job(self, job_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Process a single job with LLM.
        
        Args:
            job_data: Job data with extracted skills
            
        Returns:
            List of enhanced skills
        """
        # Generate prompt
        prompt = self.prompt_generator.generate_skill_processing_prompt(
            {
                'title': job_data['job_title'],
                'description': job_data['job_description'],
                'requirements': job_data['job_requirements']
            },
            job_data['extracted_skills']
        )
        
        # Process with LLM
        llm_response = self.llm_handler.process_skills(
            job_data,
            job_data['extracted_skills'],
            prompt
        )
        
        # Process LLM response
        enhanced_skills = self._process_llm_response(
            llm_response,
            job_data['extracted_skills']
        )
        
        return enhanced_skills
    
    def _process_llm_response(self, 
                            llm_response: Dict[str, Any],
                            original_skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process LLM response into enhanced skills.
        
        Args:
            llm_response: Response from LLM
            original_skills: Original extracted skills
            
        Returns:
            List of enhanced skill records
        """
        enhanced_skills = []
        
        # Create a mapping of original skills
        original_map = {
            skill['skill_text'].lower(): skill 
            for skill in original_skills
        }
        
        # Process explicit skills (validated by LLM)
        for skill_data in llm_response.get('explicit_skills', []):
            skill_text = skill_data.get('skill', '').lower()
            
            # Validate skill
            validation = self.validator.validate_skill(skill_text)
            if not validation['valid']:
                continue
            
            # Normalize skill
            normalization = self.normalizer.normalize_skill(skill_text)
            
            enhanced_skills.append({
                'original_skill_text': skill_data.get('original', skill_text),
                'normalized_skill': normalization['normalized'],
                'skill_type': 'explicit',
                'esco_concept_uri': None,  # To be matched later
                'esco_preferred_label': None,
                'llm_confidence': skill_data.get('confidence', 0.8),
                'llm_reasoning': None,
                'is_duplicate': False,
                'llm_model': llm_response.get('model_name', 'unknown')
            })
        
        # Process implicit skills (inferred by LLM)
        for skill_data in llm_response.get('implicit_skills', []):
            skill_text = skill_data.get('skill', '').lower()
            
            # Validate skill
            validation = self.validator.validate_skill(skill_text)
            if not validation['valid']:
                continue
            
            # Normalize skill
            normalization = self.normalizer.normalize_skill(skill_text)
            
            enhanced_skills.append({
                'original_skill_text': None,
                'normalized_skill': normalization['normalized'],
                'skill_type': 'implicit',
                'esco_concept_uri': None,
                'esco_preferred_label': None,
                'llm_confidence': skill_data.get('confidence', 0.7),
                'llm_reasoning': skill_data.get('reasoning', ''),
                'is_duplicate': False,
                'llm_model': llm_response.get('model_name', 'unknown')
            })
        
        # Deduplicate skills
        enhanced_skills = self._deduplicate_skills(enhanced_skills)
        
        return enhanced_skills
    
    def _deduplicate_skills(self, skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Deduplicate enhanced skills.
        
        Args:
            skills: List of enhanced skills
            
        Returns:
            Deduplicated list with duplicates marked
        """
        seen_normalized = {}
        deduplicated = []
        
        # Sort by confidence (highest first)
        sorted_skills = sorted(
            skills, 
            key=lambda x: x.get('llm_confidence', 0),
            reverse=True
        )
        
        for skill in sorted_skills:
            normalized = skill['normalized_skill'].lower()
            
            if normalized not in seen_normalized:
                # First occurrence
                seen_normalized[normalized] = skill
                deduplicated.append(skill)
            else:
                # Duplicate found
                skill['is_duplicate'] = True
                skill['duplicate_of_id'] = id(seen_normalized[normalized])
                deduplicated.append(skill)
        
        return deduplicated
    
    def run_continuous(self, batch_size: int = 50, wait_time: int = 60):
        """Run LLM processing continuously.
        
        Args:
            batch_size: Jobs per batch
            wait_time: Seconds to wait between batches
        """
        logger.info(f"Starting continuous LLM processing (batch_size={batch_size}, wait={wait_time}s)")
        
        while True:
            try:
                stats = self.process_batch(batch_size)
                
                if stats['jobs_processed'] == 0:
                    logger.info(f"No jobs to process. Waiting {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    # Short pause between batches
                    time.sleep(10)
                    
            except KeyboardInterrupt:
                logger.info("LLM processing stopped by user")
                break
            except Exception as e:
                logger.error(f"LLM processing error: {e}")
                logger.info(f"Waiting {wait_time}s before retry...")
                time.sleep(wait_time)
```

---

## 7. Embedder Module Files

### src/embedder/__init__.py
```python
from .vectorizer import SkillVectorizer
from .model_loader import EmbeddingModelLoader
from .batch_processor import BatchProcessor

__all__ = ['SkillVectorizer', 'EmbeddingModelLoader', 'BatchProcessor']

### src/embedder/vectorizer.py
```python
import logging
from typing import List, Dict, Any, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import torch

logger = logging.getLogger(__name__)

class SkillVectorizer:
    """Generate embeddings for skills."""
    
    def __init__(self, model_name: str = None):
        self.model_name = model_name or "intfloat/multilingual-e5-base"
        self.model = None
        self.device = None
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialize the embedding model."""
        try:
            # Check for GPU availability
            if torch.cuda.is_available():
                self.device = 'cuda'
                logger.info(f"Using GPU for embeddings")
            else:
                self.device = 'cpu'
                logger.info(f"Using CPU for embeddings")
            
            # Load model
            logger.info(f"Loading embedding model: {self.model_name}")
            self.model = SentenceTransformer(self.model_name, device=self.device)
            
            # Get embedding dimension
            self.embedding_dim = self.model.get_sentence_embedding_dimension()
            logger.info(f"Model loaded. Embedding dimension: {self.embedding_dim}")
            
        except Exception as e:
            logger.error(f"Failed to initialize embedding model: {e}")
            raise
    
    def vectorize(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """Generate embeddings for a list of texts.
        
        Args:
            texts: List of texts to embed
            batch_size: Batch size for processing
            
        Returns:
            Array of embeddings
        """
        if not texts:
            return np.array([])
        
        try:
            # For E5 models, add instruction prefix
            if 'e5' in self.model_name.lower():
                texts = [f"query: {text}" for text in texts]
            
            # Generate embeddings
            embeddings = self.model.encode(
                texts,
                batch_size=batch_size,
                show_progress_bar=len(texts) > 100,
                convert_to_numpy=True,
                normalize_embeddings=True  # L2 normalization
            )
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Vectorization failed: {e}")
            raise
    
    def vectorize_single(self, text: str) -> np.ndarray:
        """Generate embedding for a single text.
        
        Args:
            text: Text to embed
            
        Returns:
            Embedding vector
        """
        return self.vectorize([text])[0]
    
    def compute_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """Compute cosine similarity between two embeddings.
        
        Args:
            embedding1: First embedding
            embedding2: Second embedding
            
        Returns:
            Cosine similarity score
        """
        # Assuming normalized embeddings, cosine similarity is just dot product
        return float(np.dot(embedding1, embedding2))
    
    def find_similar_skills(self, 
                          query_embedding: np.ndarray,
                          skill_embeddings: List[Dict[str, Any]],
                          top_k: int = 10,
                          threshold: float = 0.7) -> List[Dict[str, Any]]:
        """Find similar skills based on embeddings.
        
        Args:
            query_embedding: Query skill embedding
            skill_embeddings: List of skill embedding dictionaries
            top_k: Number of top results to return
            threshold: Minimum similarity threshold
            
        Returns:
            List of similar skills with scores
        """
        similarities = []
        
        for skill_data in skill_embeddings:
            embedding = skill_data['embedding']
            similarity = self.compute_similarity(query_embedding, embedding)
            
            if similarity >= threshold:
                similarities.append({
                    'skill': skill_data['skill_text'],
                    'similarity': similarity,
                    'embedding_id': skill_data.get('embedding_id')
                })
        
        # Sort by similarity
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        return similarities[:top_k]

### src/embedder/model_loader.py
```python
import logging
from typing import Dict, Any, Optional
import os
import json
from pathlib import Path
import torch
from transformers import AutoModel, AutoTokenizer
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)

class EmbeddingModelLoader:
    """Load and manage embedding models."""
    
    # Model configurations
    MODEL_CONFIGS = {
        'multilingual-e5-base': {
            'name': 'intfloat/multilingual-e5-base',
            'dimension': 768,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': 'query: '
        },
        'multilingual-e5-large': {
            'name': 'intfloat/multilingual-e5-large',
            'dimension': 1024,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': 'query: '
        },
        'beto': {
            'name': 'dccuchile/bert-base-spanish-wwm-cased',
            'dimension': 768,
            'max_length': 512,
            'type': 'transformers',
            'instruction_prefix': None
        },
        'labse': {
            'name': 'sentence-transformers/LaBSE',
            'dimension': 768,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': None
        },
        'multilingual-minilm': {
            'name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
            'dimension': 384,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': None
        }
    }
    
    def __init__(self, cache_dir: Optional[str] = None):
        self.cache_dir = cache_dir or "./data/cache/embeddings"
        os.makedirs(self.cache_dir, exist_ok=True)
        self.loaded_models = {}
    
    def load_model(self, model_key: str) -> Any:
        """Load an embedding model.
        
        Args:
            model_key: Key from MODEL_CONFIGS
            
        Returns:
            Loaded model
        """
        if model_key in self.loaded_models:
            logger.info(f"Model {model_key} already loaded")
            return self.loaded_models[model_key]
        
        if model_key not in self.MODEL_CONFIGS:
            raise ValueError(f"Unknown model key: {model_key}")
        
        config = self.MODEL_CONFIGS[model_key]
        
        try:
            if config['type'] == 'sentence-transformers':
                model = self._load_sentence_transformer(config)
            elif config['type'] == 'transformers':
                model = self._load_transformers_model(config)
            else:
                raise ValueError(f"Unknown model type: {config['type']}")
            
            self.loaded_models[model_key] = model
            logger.info(f"Successfully loaded model: {model_key}")
            
            return model
            
        except Exception as e:
            logger.error(f"Failed to load model {model_key}: {e}")
            raise
    
    def _load_sentence_transformer(self, config: Dict[str, Any]) -> SentenceTransformer:
        """Load a sentence-transformers model.
        
        Args:
            config: Model configuration
            
        Returns:
            Loaded model
        """
        model = SentenceTransformer(
            config['name'],
            cache_folder=self.cache_dir
        )
        
        # Verify dimension
        actual_dim = model.get_sentence_embedding_dimension()
        if actual_dim != config['dimension']:
            logger.warning(
                f"Model dimension mismatch: expected {config['dimension']}, "
                f"got {actual_dim}"
            )
        
        return model
    
    def _load_transformers_model(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Load a transformers model with tokenizer.
        
        Args:
            config: Model configuration
            
        Returns:
            Dictionary with model and tokenizer
        """
        model = AutoModel.from_pretrained(
            config['name'],
            cache_dir=self.cache_dir
        )
        
        tokenizer = AutoTokenizer.from_pretrained(
            config['name'],
            cache_dir=self.cache_dir
        )
        
        return {
            'model': model,
            'tokenizer': tokenizer,
            'config': config
        }
    
    def get_model_info(self, model_key: str) -> Dict[str, Any]:
        """Get information about a model.
        
        Args:
            model_key: Model key
            
        Returns:
            Model information
        """
        if model_key not in self.MODEL_CONFIGS:
            raise ValueError(f"Unknown model key: {model_key}")
        
        return self.MODEL_CONFIGS[model_key].copy()
    
    def list_available_models(self) -> Dict[str, Dict[str, Any]]:
        """List all available models.
        
        Returns:
            Dictionary of model configurations
        """
        return self.MODEL_CONFIGS.copy()
    
    def download_all_models(self):
        """Download all configured models."""
        logger.info("Downloading all configured embedding models...")
        
        for model_key in self.MODEL_CONFIGS:
            try:
                logger.info(f"Downloading {model_key}...")
                self.load_model(model_key)
            except Exception as e:
                logger.error(f"Failed to download {model_key}: {e}")
    
    def clear_cache(self):
        """Clear model cache."""
        import shutil
        
        if os.path.exists(self.cache_dir):
            shutil.rmtree(self.cache_dir)
            os.makedirs(self.cache_dir)
            logger.info("Model cache cleared")
        
        self.loaded_models.clear()

### src/embedder/batch_processor.py
```python
import logging
from typing import List, Dict, Any
import numpy as np
from database.operations import DatabaseOperations
from .vectorizer import SkillVectorizer
from config.settings import get_settings
import time
from tqdm import tqdm

logger = logging.getLogger(__name__)

class BatchProcessor:
    """Process skill embeddings in batches."""
    
    def __init__(self, model_name: str = None):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        self.vectorizer = SkillVectorizer(model_name)
        self.batch_size = self.settings.embedding_batch_size
    
    def process_all_skills(self) -> Dict[str, Any]:
        """Process all skills that need embeddings.
        
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'skills_processed': 0,
            'embeddings_created': 0,
            'errors': 0,
            'processing_time': 0
        }
        
        try:
            # Get skills without embeddings
            skills = self.db_ops.get_unique_skills_for_embedding()
            logger.info(f"Found {len(skills)} skills without embeddings")
            
            if not skills:
                return stats
            
            # Process in batches
            for i in tqdm(range(0, len(skills), self.batch_size), desc="Embedding skills"):
                batch = skills[i:i + self.batch_size]
                
                try:
                    # Generate embeddings
                    embeddings = self.vectorizer.vectorize(batch)
                    
                    # Prepare data for database
                    embedding_data = []
                    for skill_text, embedding in zip(batch, embeddings):
                        embedding_data.append({
                            'skill_text': skill_text,
                            'embedding': embedding.tolist(),  # Convert to list for pgvector
                            'model_name': self.vectorizer.model_name,
                            'model_version': '1.0'
                        })
                    
                    # Save to database
                    self.db_ops.insert_skill_embeddings(embedding_data)
                    
                    stats['skills_processed'] += len(batch)
                    stats['embeddings_created'] += len(embedding_data)
                    
                except Exception as e:
                    logger.error(f"Error processing batch {i//self.batch_size}: {e}")
                    stats['errors'] += 1
            
            stats['processing_time'] = time.time() - start_time
            
            logger.info(
                f"Embedding complete: {stats['skills_processed']} skills processed, "
                f"{stats['embeddings_created']} embeddings created, "
                f"{stats['errors']} errors, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"Embedding batch processing failed: {e}")
            raise
    
    def update_embeddings(self, force: bool = False) -> Dict[str, Any]:
        """Update embeddings for new or modified skills.
        
        Args:
            force: Force re-embedding of all skills
            
        Returns:
            Update statistics
        """
        if force:
            logger.warning("Force update requested - this will re-embed all skills")
            # Clear existing embeddings
            # Note: Implement this method in DatabaseOperations if needed
        
        return self.process_all_skills()
    
    def compute_similarity_matrix(self, skill_list: List[str] = None) -> np.ndarray:
        """Compute similarity matrix for skills.
        
        Args:
            skill_list: List of skills to compare (None for all)
            
        Returns:
            Similarity matrix
        """
        # Get embeddings from database
        all_embeddings = self.db_ops.get_all_embeddings()
        
        if skill_list:
            # Filter to requested skills
            embeddings_data = [
                e for e in all_embeddings 
                if e['skill_text'] in skill_list
            ]
        else:
            embeddings_data = all_embeddings
        
        if not embeddings_data:
            return np.array([])
        
        # Extract embedding vectors
        embeddings = np.array([e['embedding'] for e in embeddings_data])
        
        # Compute similarity matrix
        # Since embeddings are normalized, cosine similarity is just dot product
        similarity_matrix = np.dot(embeddings, embeddings.T)
        
        return similarity_matrix
    
    def find_duplicate_skills(self, threshold: float = 0.95) -> List[Dict[str, Any]]:
        """Find potential duplicate skills based on embedding similarity.
        
        Args:
            threshold: Similarity threshold for duplicates
            
        Returns:
            List of potential duplicates
        """
        all_embeddings = self.db_ops.get_all_embeddings()
        
        if len(all_embeddings) < 2:
            return []
        
        duplicates = []
        
        # Compare all pairs
        for i in range(len(all_embeddings)):
            for j in range(i + 1, len(all_embeddings)):
                skill1 = all_embeddings[i]
                skill2 = all_embeddings[j]
                
                similarity = self.vectorizer.compute_similarity(
                    np.array(skill1['embedding']),
                    np.array(skill2['embedding'])
                )
                
                if similarity >= threshold:
                    duplicates.append({
                        'skill1': skill1['skill_text'],
                        'skill2': skill2['skill_text'],
                        'similarity': similarity
                    })
        
        # Sort by similarity
        duplicates.sort(key=lambda x: x['similarity'], reverse=True)
        
        logger.info(f"Found {len(duplicates)} potential duplicate skill pairs")
        
        return duplicates
    
    def get_skill_recommendations(self, 
                                job_skills: List[str],
                                top_k: int = 10) -> List[Dict[str, Any]]:
        """Get skill recommendations based on current job skills.
        
        Args:
            job_skills: Current skills in job
            top_k: Number of recommendations
            
        Returns:
            List of recommended skills with scores
        """
        # Get embeddings for input skills
        input_embeddings = self.vectorizer.vectorize(job_skills)
        
        # Average the embeddings to get job profile
        job_profile = np.mean(input_embeddings, axis=0)
        
        # Get all skill embeddings
        all_embeddings = self.db_ops.get_all_embeddings()
        
        # Find similar skills
        recommendations = []
        
        for skill_data in all_embeddings:
            # Skip if already in job skills
            if skill_data['skill_text'] in job_skills:
                continue
            
            similarity = self.vectorizer.compute_similarity(
                job_profile,
                np.array(skill_data['embedding'])
            )
            
            recommendations.append({
                'skill': skill_data['skill_text'],
                'score': similarity
            })
        
        # Sort and return top K
        recommendations.sort(key=lambda x: x['score'], reverse=True)
        
        return recommendations[:top_k]

---

## 8. Analyzer Module Files

### src/analyzer/__init__.py
```python
from .clustering import SkillClusterer
from .dimension_reducer import DimensionReducer
from .report_generator import ReportGenerator
from .visualizations import VisualizationGenerator

__all__ = ['SkillClusterer', 'DimensionReducer', 'ReportGenerator', 'VisualizationGenerator']

### src/analyzer/clustering.py
```python
import logging
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from sklearn.cluster import HDBSCAN, KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score
import pandas as pd
from database.operations import DatabaseOperations
from config.settings import get_settings

logger = logging.getLogger(__name__)

class SkillClusterer:
    """Perform clustering on skill embeddings."""
    
    def __init__(self):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        self.clustering_method = 'hdbscan'
    
    def cluster_skills(self, 
                      embeddings: np.ndarray,
                      skill_texts: List[str],
                      method: str = 'hdbscan',
                      **kwargs) -> Dict[str, Any]:
        """Cluster skills based on embeddings.
        
        Args:
            embeddings: Skill embedding matrix
            skill_texts: List of skill texts
            method: Clustering method ('hdbscan' or 'kmeans')
            **kwargs: Additional parameters for clustering
            
        Returns:
            Clustering results
        """
        if len(embeddings) < 2:
            logger.warning("Not enough data points for clustering")
            return {}
        
        logger.info(f"Clustering {len(embeddings)} skills using {method}")
        
        if method == 'hdbscan':
            results = self._cluster_hdbscan(embeddings, skill_texts, **kwargs)
        elif method == 'kmeans':
            results = self._cluster_kmeans(embeddings, skill_texts, **kwargs)
        else:
            raise ValueError(f"Unknown clustering method: {method}")
        
        # Calculate metrics
        results['metrics'] = self._calculate_metrics(embeddings, results['labels'])
        
        # Characterize clusters
        results['cluster_info'] = self._characterize_clusters(
            results['labels'],
            skill_texts
        )
        
        return results
    
    def _cluster_hdbscan(self, 
                        embeddings: np.ndarray,
                        skill_texts: List[str],
                        min_cluster_size: int = None,
                        min_samples: int = None) -> Dict[str, Any]:
        """Perform HDBSCAN clustering.
        
        Args:
            embeddings: Embedding matrix
            skill_texts: Skill texts
            min_cluster_size: Minimum cluster size
            min_samples: Minimum samples for core points
            
        Returns:
            Clustering results
        """
        # Use settings if not provided
        min_cluster_size = min_cluster_size or self.settings.cluster_min_size
        min_samples = min_samples or self.settings.cluster_min_samples
        
        # Perform clustering
        clusterer = HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            metric='euclidean',
            cluster_selection_method='eom',
            prediction_data=True
        )
        
        labels = clusterer.fit_predict(embeddings)
        
        # Get cluster persistence (stability)
        cluster_persistence = clusterer.cluster_persistence_
        
        results = {
            'method': 'hdbscan',
            'labels': labels,
            'n_clusters': len(set(labels)) - (1 if -1 in labels else 0),
            'n_noise': list(labels).count(-1),
            'parameters': {
                'min_cluster_size': min_cluster_size,
                'min_samples': min_samples
            },
            'cluster_persistence': cluster_persistence,
            'clusterer': clusterer
        }
        
        logger.info(
            f"HDBSCAN found {results['n_clusters']} clusters "
            f"with {results['n_noise']} noise points"
        )
        
        return results
    
    def _cluster_kmeans(self,
                       embeddings: np.ndarray,
                       skill_texts: List[str],
                       n_clusters: int = 20) -> Dict[str, Any]:
        """Perform K-means clustering.
        
        Args:
            embeddings: Embedding matrix
            skill_texts: Skill texts
            n_clusters: Number of clusters
            
        Returns:
            Clustering results
        """
        # Perform clustering
        clusterer = KMeans(
            n_clusters=n_clusters,
            n_init=10,
            max_iter=300,
            random_state=42
        )
        
        labels = clusterer.fit_predict(embeddings)
        
        results = {
            'method': 'kmeans',
            'labels': labels,
            'n_clusters': n_clusters,
            'n_noise': 0,
            'parameters': {
                'n_clusters': n_clusters
            },
            'cluster_centers': clusterer.cluster_centers_,
            'inertia': clusterer.inertia_,
            'clusterer': clusterer
        }
        
        logger.info(f"K-means created {n_clusters} clusters")
        
        return results
    
    def _calculate_metrics(self, embeddings: np.ndarray, labels: np.ndarray) -> Dict[str, float]:
        """Calculate clustering quality metrics.
        
        Args:
            embeddings: Embedding matrix
            labels: Cluster labels
            
        Returns:
            Dictionary of metrics
        """
        metrics = {}
        
        # Remove noise points for metrics
        mask = labels >= 0
        if np.sum(mask) < 2:
            logger.warning("Not enough clustered points for metrics")
            return metrics
        
        try:
            # Silhouette score
            metrics['silhouette_score'] = silhouette_score(
                embeddings[mask],
                labels[mask]
            )
            
            # Davies-Bouldin index (lower is better)
            metrics['davies_bouldin_index'] = davies_bouldin_score(
                embeddings[mask],
                labels[mask]
            )
            
            # Cluster statistics
            unique_labels = np.unique(labels[labels >= 0])
            cluster_sizes = [np.sum(labels == label) for label in unique_labels]
            
            metrics['avg_cluster_size'] = np.mean(cluster_sizes)
            metrics['std_cluster_size'] = np.std(cluster_sizes)
            metrics['min_cluster_size'] = np.min(cluster_sizes)
            metrics['max_cluster_size'] = np.max(cluster_sizes)
            
        except Exception as e:
            logger.error(f"Error calculating metrics: {e}")
        
        return metrics
    
    def _characterize_clusters(self, 
                             labels: np.ndarray,
                             skill_texts: List[str]) -> List[Dict[str, Any]]:
        """Characterize each cluster.
        
        Args:
            labels: Cluster labels
            skill_texts: List of skill texts
            
        Returns:
            List of cluster characteristics
        """
        cluster_info = []
        
        # Create DataFrame for easier analysis
        df = pd.DataFrame({
            'skill': skill_texts,
            'cluster': labels
        })
        
        # Analyze each cluster
        unique_labels = sorted(set(labels))
        
        for label in unique_labels:
            if label == -1:  # Skip noise cluster
                continue
            
            cluster_skills = df[df['cluster'] == label]['skill'].tolist()
            
            # Get most common skills
            skill_counts = pd.Series(cluster_skills).value_counts()
            
            cluster_data = {
                'cluster_id': int(label),
                'size': len(cluster_skills),
                'top_skills': skill_counts.head(10).to_dict(),
                'all_skills': cluster_skills,
                'label': self._generate_cluster_label(skill_counts.head(5).index.tolist())
            }
            
            cluster_info.append(cluster_data)
        
        # Sort by size
        cluster_info.sort(key=lambda x: x['size'], reverse=True)
        
        return cluster_info
    
    def _generate_cluster_label(self, top_skills: List[str]) -> str:
        """Generate a descriptive label for a cluster.
        
        Args:
            top_skills: Top skills in cluster
            
        Returns:
            Cluster label
        """
        # Simple heuristic-based labeling
        skill_lower = [s.lower() for s in top_skills]
        
        if any('frontend' in s or 'react' in s or 'angular' in s or 'vue' in s for s in skill_lower):
            return "Frontend Development"
        elif any('backend' in s or 'node' in s or 'django' in s or 'spring' in s for s in skill_lower):
            return "Backend Development"
        elif any('data' in s or 'analytics' in s or 'sql' in s for s in skill_lower):
            return "Data & Analytics"
        elif any('machine learning' in s or 'ml' in s or 'ai' in s for s in skill_lower):
            return "Machine Learning & AI"
        elif any('devops' in s or 'docker' in s or 'kubernetes' in s for s in skill_lower):
            return "DevOps & Infrastructure"
        elif any('mobile' in s or 'android' in s or 'ios' in s for s in skill_lower):
            return "Mobile Development"
        elif any('cloud' in s or 'aws' in s or 'azure' in s for s in skill_lower):
            return "Cloud Computing"
        else:
            # Use most common skill as label
            return f"{top_skills[0]} & Related"
    
    def run_clustering_pipeline(self) -> Dict[str, Any]:
        """Run complete clustering pipeline on all skills.
        
        Returns:
            Complete clustering results
        """
        # Get all embeddings
        embeddings_data = self.db_ops.get_all_embeddings()
        
        if not embeddings_data:
            logger.warning("No embeddings found for clustering")
            return {}
        
        # Extract data
        skill_texts = [e['skill_text'] for e in embeddings_data]
        embeddings = np.array([e['embedding'] for e in embeddings_data])
        
        logger.info(f"Running clustering on {len(embeddings)} skills")
        
        # Run clustering
        results = self.cluster_skills(embeddings, skill_texts)
        
        # Save results to database
        self.db_ops.save_analysis_results(
            analysis_type='clustering',
            results={
                'n_clusters': results['n_clusters'],
                'n_noise': results['n_noise'],
                'metrics': results['metrics'],
                'cluster_info': results['cluster_info']
            },
            parameters=results['parameters']
        )
        
        return results, re.IGNORECASE)
    
    return url_pattern.match(url) is not None

def validate_date_format(date_str: str, format: str = '%Y-%m-%d') -> bool:
    """Validate date string format.
    
    Args:
        date_str: Date string to validate
        format: Expected date format
        
    Returns:
        True if valid format, False otherwise
    """
    from datetime import datetime
    
    try:
        datetime.strptime(date_str, format)
        return True
    except ValueError:
        return False

def validate_job_data(job_data: dict) -> List[str]:
    """Validate job data completeness.
    
    Args:
        job_data: Job data dictionary
        
    Returns:
        List of validation errors (empty if valid)
    """
    errors = []
    
    # Required fields
    required_fields = ['title', 'description', 'url', 'portal', 'country']
    
    for field in required_fields:
        if field not in job_data or not job_data[field]:
            errors.append(f"Missing required field: {field}")
    
    # Validate specific fields
    if 'country' in job_data and not validate_country(job_data['country']):
        errors.append(f"Invalid country code: {job_data['country']}")
    
    if 'portal' in job_data and not validate_portal(job_data['portal']):
        errors.append(f"Invalid portal: {job_data['portal']}")
    
    if 'url' in job_data and not validate_url(job_data['url']):
        errors.append(f"Invalid URL format: {job_data['url']}")
    
    # Validate text lengths
    if 'description' in job_data and len(job_data['description']) < 50:
        errors.append("Description too short (min 50 characters)")
    
    return errors

### src/utils/cleaners.py
```python
import re
import unicodedata
from typing import Optional
import html

def clean_text(text: str) -> str:
    """Clean text for general use.
    
    Args:
        text: Text to clean
        
    Returns:
        Cleaned text
    """
    if not text:
        return ""
    
    # Decode HTML entities
    text = html.unescape(text)
    
    # Remove HTML tags
    text = remove_html(text)
    
    # Normalize unicode
    text = unicodedata.normalize('NFKD', text)
    
    # Remove extra whitespace
    text = ' '.join(text.split())
    
    return text.strip()

def remove_html(text: str) -> str:
    """Remove HTML tags from text.
    
    Args:
        text: Text with potential HTML
        
    Returns:
        Text without HTML tags
    """
    if not text:
        return ""
    
    # Remove script and style elements
    text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.DOTALL | re.IGNORECASE)
    text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)
    
    # Remove all HTML tags
    text = re.sub(r'<[^>]+>', ' ', text)
    
    # Clean up whitespace
    text = ' '.join(text.split())
    
    return text

def normalize_text(text: str) -> str:
    """Normalize text for comparison.
    
    Args:
        text: Text to normalize
        
    Returns:
        Normalized text
    """
    if not text:
        return ""
    
    # Lowercase
    text = text.lower()
    
    # Remove accents
    text = unicodedata.normalize('NFD', text)
    text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')
    
    # Remove punctuation except for technical characters
    text = re.sub(r'[^\w\s\+\#\.\-/]', ' ', text)
    
    # Normalize whitespace
    text = ' '.join(text.split())
    
    return text.strip()

def clean_skill_text(text: str) -> str:
    """Clean skill text specifically.
    
    Args:
        text: Skill text to clean
        
    Returns:
        Cleaned skill text
    """
    if not text:
        return ""
    
    # Basic cleaning
    text = clean_text(text)
    
    # Remove common prefixes/suffixes
    prefixes = ['experiencia en', 'conocimientos de', 'manejo de', 'dominio de']
    for prefix in prefixes:
        if text.lower().startswith(prefix):
            text = text[len(prefix):].strip()
    
    # Remove parenthetical content
    text = re.sub(r'\([^)]*\)', '', text)
    
    # Remove version numbers (optional)
    # text = re.sub(r'\s*\d+(\.\d+)*\s*(#1-root-configuration-files)
2. [Database Setup Files](#2-database-setup-files)
3. [Configuration Module](#3-configuration-module)
4. [Scraper Module Files](#4-scraper-module-files)
5. [Extractor Module Files](#5-extractor-module-files)
6. [LLM Processor Module Files](#6-llm-processor-module-files)
7. [Embedder Module Files](#7-embedder-module-files)
8. [Analyzer Module Files](#8-analyzer-module-files)
9. [Orchestrator and Utilities](#9-orchestrator-and-utilities)
10. [Scripts](#10-scripts)

---

## 1. Root Configuration Files

### requirements.txt
```
# Core
python-dotenv==1.0.0
pydantic==2.5.3
typer==0.9.0
tqdm==4.66.1

# Web Scraping
scrapy==2.11.0
scrapy-selenium==0.0.7
beautifulsoup4==4.12.2
lxml==4.9.3
fake-useragent==1.4.0
requests==2.31.0

# Database
psycopg2-binary==2.9.9
sqlalchemy==2.0.23
pgvector==0.2.3
alembic==1.13.1

# NLP
spacy==3.7.2
langdetect==1.0.9
regex==2023.12.25

# Machine Learning
transformers==4.36.2
sentence-transformers==2.2.2
torch==2.1.2
llama-cpp-python==0.2.32
openai==1.6.1

# Data Processing
pandas==2.1.4
numpy==1.24.3
scikit-learn==1.3.2
umap-learn==0.5.5
hdbscan==0.8.33

# Visualization
matplotlib==3.8.2
seaborn==0.13.0
reportlab==4.0.8
pillow==10.1.0

# Testing
pytest==7.4.4
pytest-asyncio==0.23.3
pytest-cov==4.1.0

# Development
black==23.12.1
flake8==7.0.0
mypy==1.8.0
pre-commit==3.6.0
```

### .env.example
```bash
# Database Configuration
DATABASE_URL=postgresql://labor_user:your_password@localhost:5432/labor_observatory
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=0

# Scraping Configuration
SCRAPER_USER_AGENT="Mozilla/5.0 (Windows NT 10.0; Win64; x64) Academic Research Bot"
SCRAPER_CONCURRENT_REQUESTS=16
SCRAPER_DOWNLOAD_DELAY=1.0
SCRAPER_RETRY_TIMES=3

# ESCO API Configuration
ESCO_API_URL=https://ec.europa.eu/esco/api
ESCO_VERSION=1.1.0
ESCO_LANGUAGE=es

# LLM Configuration
LLM_MODEL_PATH=./data/models/mistral-7b-instruct.Q4_K_M.gguf
LLM_CONTEXT_LENGTH=4096
LLM_MAX_TOKENS=512
LLM_TEMPERATURE=0.7
LLM_N_GPU_LAYERS=35

# OpenAI Fallback (Optional)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-3.5-turbo

# Embedding Configuration
EMBEDDING_MODEL=intfloat/multilingual-e5-base
EMBEDDING_BATCH_SIZE=32
EMBEDDING_CACHE_DIR=./data/cache/embeddings

# Analysis Configuration
CLUSTER_MIN_SIZE=5
CLUSTER_MIN_SAMPLES=3
UMAP_N_NEIGHBORS=15
UMAP_MIN_DIST=0.1

# Output Configuration
OUTPUT_DIR=./outputs
REPORT_FORMAT=pdf
LOG_LEVEL=INFO
LOG_FILE=./logs/labor_observatory.log
```

### .gitignore
```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/
.venv

# IDE
.vscode/
.idea/
*.swp
*.swo

# Data and Models
data/models/*.gguf
data/models/*/
data/cache/
outputs/
logs/

# Database
*.db
*.sqlite3

# Environment
.env
.env.local

# OS
.DS_Store
Thumbs.db

# Testing
.coverage
htmlcov/
.pytest_cache/

# Scrapy
.scrapy/

# Notebooks
.ipynb_checkpoints/
```

### setup.py
```python
from setuptools import setup, find_packages

setup(
    name="labor-observatory",
    version="1.0.0",
    author="Your Team",
    description="Automated Labor Market Observatory for Latin America",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    python_requires=">=3.10",
    install_requires=[
        line.strip()
        for line in open("requirements.txt")
        if line.strip() and not line.startswith("#")
    ],
    entry_points={
        "console_scripts": [
            "labor-observatory=orchestrator:app",
        ],
    },
)
```

---

## 2. Database Setup Files

### src/database/migrations/001_initial_schema.sql
```sql
-- Create database
CREATE DATABASE IF NOT EXISTS labor_observatory
  WITH ENCODING 'UTF8'
  LC_COLLATE = 'en_US.UTF-8'
  LC_CTYPE = 'en_US.UTF-8';

\c labor_observatory;

-- Enable extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgvector";

-- Create tables
CREATE TABLE IF NOT EXISTS raw_jobs (
    job_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    portal VARCHAR(50) NOT NULL,
    country CHAR(2) NOT NULL,
    url TEXT NOT NULL,
    title TEXT NOT NULL,
    company TEXT,
    location TEXT,
    description TEXT NOT NULL,
    requirements TEXT,
    salary_raw TEXT,
    contract_type VARCHAR(50),
    remote_type VARCHAR(50),
    posted_date DATE,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    content_hash VARCHAR(64) UNIQUE,
    raw_html TEXT,
    is_processed BOOLEAN DEFAULT FALSE,
    
    CONSTRAINT chk_country CHECK (country IN ('CO', 'MX', 'AR')),
    CONSTRAINT chk_portal CHECK (portal IN ('computrabajo', 'bumeran', 'elempleo'))
);

CREATE INDEX idx_portal_country ON raw_jobs(portal, country);
CREATE INDEX idx_scraped_at ON raw_jobs(scraped_at);
CREATE INDEX idx_processed ON raw_jobs(is_processed);

CREATE TABLE IF NOT EXISTS extracted_skills (
    extraction_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    job_id UUID REFERENCES raw_jobs(job_id) ON DELETE CASCADE,
    skill_text TEXT NOT NULL,
    skill_type VARCHAR(50),
    extraction_method VARCHAR(50),
    confidence_score FLOAT,
    source_section VARCHAR(50),
    span_start INTEGER,
    span_end INTEGER,
    esco_uri TEXT,
    extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_job_skills ON extracted_skills(job_id);
CREATE INDEX idx_skill_text ON extracted_skills(skill_text);

CREATE TABLE IF NOT EXISTS enhanced_skills (
    enhancement_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    job_id UUID REFERENCES raw_jobs(job_id) ON DELETE CASCADE,
    original_skill_text TEXT,
    normalized_skill TEXT NOT NULL,
    skill_type VARCHAR(50),
    esco_concept_uri TEXT,
    esco_preferred_label TEXT,
    llm_confidence FLOAT,
    llm_reasoning TEXT,
    is_duplicate BOOLEAN DEFAULT FALSE,
    duplicate_of_id UUID,
    enhanced_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    llm_model VARCHAR(100)
);

CREATE INDEX idx_job_enhanced ON enhanced_skills(job_id);
CREATE INDEX idx_normalized ON enhanced_skills(normalized_skill);

CREATE TABLE IF NOT EXISTS skill_embeddings (
    embedding_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    skill_text TEXT UNIQUE NOT NULL,
    embedding vector(768) NOT NULL,
    model_name VARCHAR(100) NOT NULL,
    model_version VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_skill_lookup ON skill_embeddings(skill_text);
CREATE INDEX idx_embedding_similarity ON skill_embeddings 
  USING ivfflat (embedding vector_cosine_ops)
  WITH (lists = 100);

CREATE TABLE IF NOT EXISTS analysis_results (
    analysis_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    analysis_type VARCHAR(50),
    country CHAR(2),
    date_range_start DATE,
    date_range_end DATE,
    parameters JSONB,
    results JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_analysis_type ON analysis_results(analysis_type);
CREATE INDEX idx_analysis_date ON analysis_results(created_at);

-- Create views
CREATE VIEW skill_frequency AS
SELECT 
    es.normalized_skill,
    COUNT(DISTINCT es.job_id) as job_count,
    COUNT(*) as total_mentions,
    ARRAY_AGG(DISTINCT rj.country) as countries
FROM enhanced_skills es
JOIN raw_jobs rj ON es.job_id = rj.job_id
WHERE es.is_duplicate = FALSE
GROUP BY es.normalized_skill
ORDER BY job_count DESC;

CREATE VIEW country_skill_distribution AS
SELECT 
    rj.country,
    es.normalized_skill,
    COUNT(DISTINCT es.job_id) as job_count
FROM enhanced_skills es
JOIN raw_jobs rj ON es.job_id = rj.job_id
WHERE es.is_duplicate = FALSE
GROUP BY rj.country, es.normalized_skill
ORDER BY rj.country, job_count DESC;
```

### src/database/models.py
```python
from sqlalchemy import Column, String, Text, Boolean, Float, Integer, DateTime, Date, ForeignKey, JSON
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from pgvector.sqlalchemy import Vector
import uuid

Base = declarative_base()

class RawJob(Base):
    __tablename__ = 'raw_jobs'
    
    job_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    portal = Column(String(50), nullable=False)
    country = Column(String(2), nullable=False)
    url = Column(Text, nullable=False)
    title = Column(Text, nullable=False)
    company = Column(Text)
    location = Column(Text)
    description = Column(Text, nullable=False)
    requirements = Column(Text)
    salary_raw = Column(Text)
    contract_type = Column(String(50))
    remote_type = Column(String(50))
    posted_date = Column(Date)
    scraped_at = Column(DateTime, server_default=func.now())
    content_hash = Column(String(64), unique=True)
    raw_html = Column(Text)
    is_processed = Column(Boolean, default=False)
    
    # Relationships
    extracted_skills = relationship("ExtractedSkill", back_populates="job")
    enhanced_skills = relationship("EnhancedSkill", back_populates="job")

class ExtractedSkill(Base):
    __tablename__ = 'extracted_skills'
    
    extraction_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    job_id = Column(UUID(as_uuid=True), ForeignKey('raw_jobs.job_id'))
    skill_text = Column(Text, nullable=False)
    skill_type = Column(String(50))
    extraction_method = Column(String(50))
    confidence_score = Column(Float)
    source_section = Column(String(50))
    span_start = Column(Integer)
    span_end = Column(Integer)
    esco_uri = Column(Text)
    extracted_at = Column(DateTime, server_default=func.now())
    
    # Relationships
    job = relationship("RawJob", back_populates="extracted_skills")

class EnhancedSkill(Base):
    __tablename__ = 'enhanced_skills'
    
    enhancement_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    job_id = Column(UUID(as_uuid=True), ForeignKey('raw_jobs.job_id'))
    original_skill_text = Column(Text)
    normalized_skill = Column(Text, nullable=False)
    skill_type = Column(String(50))
    esco_concept_uri = Column(Text)
    esco_preferred_label = Column(Text)
    llm_confidence = Column(Float)
    llm_reasoning = Column(Text)
    is_duplicate = Column(Boolean, default=False)
    duplicate_of_id = Column(UUID(as_uuid=True))
    enhanced_at = Column(DateTime, server_default=func.now())
    llm_model = Column(String(100))
    
    # Relationships
    job = relationship("RawJob", back_populates="enhanced_skills")

class SkillEmbedding(Base):
    __tablename__ = 'skill_embeddings'
    
    embedding_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    skill_text = Column(Text, unique=True, nullable=False)
    embedding = Column(Vector(768), nullable=False)
    model_name = Column(String(100), nullable=False)
    model_version = Column(String(50))
    created_at = Column(DateTime, server_default=func.now())

class AnalysisResult(Base):
    __tablename__ = 'analysis_results'
    
    analysis_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    analysis_type = Column(String(50))
    country = Column(String(2))
    date_range_start = Column(Date)
    date_range_end = Column(Date)
    parameters = Column(JSONB)
    results = Column(JSONB)
    created_at = Column(DateTime, server_default=func.now())
```

### src/database/operations.py
```python
from typing import List, Optional, Dict, Any
from datetime import datetime
from sqlalchemy import create_engine, and_, or_, func
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.exc import IntegrityError
import os
from .models import Base, RawJob, ExtractedSkill, EnhancedSkill, SkillEmbedding, AnalysisResult
import hashlib
import logging

logger = logging.getLogger(__name__)

class DatabaseOperations:
    def __init__(self, database_url: Optional[str] = None):
        self.database_url = database_url or os.getenv('DATABASE_URL')
        self.engine = create_engine(
            self.database_url,
            pool_size=20,
            max_overflow=0,
            pool_pre_ping=True
        )
        Base.metadata.create_all(self.engine)
        self.SessionLocal = sessionmaker(bind=self.engine)
    
    def get_session(self) -> Session:
        """Get a new database session."""
        return self.SessionLocal()
    
    def insert_job(self, job_data: Dict[str, Any]) -> Optional[str]:
        """Insert a new job posting."""
        session = self.get_session()
        try:
            # Generate content hash
            content = f"{job_data['title']}{job_data['description']}{job_data.get('requirements', '')}"
            content_hash = hashlib.sha256(content.encode()).hexdigest()
            
            job = RawJob(
                **job_data,
                content_hash=content_hash
            )
            session.add(job)
            session.commit()
            
            job_id = str(job.job_id)
            logger.info(f"Inserted job {job_id}")
            return job_id
            
        except IntegrityError:
            session.rollback()
            logger.warning(f"Duplicate job detected: {job_data['url']}")
            return None

### src/analyzer/visualizations.py
```python
import logging
from typing import Dict, Any, List, Optional, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from datetime import datetime
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class VisualizationGenerator:
    """Generate static visualizations for analysis results."""
    
    def __init__(self, output_dir: str = None):
        self.output_dir = output_dir or "./outputs"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Set style
        plt.style.use('seaborn-v0_8-darkgrid')
        sns.set_palette("husl")
        
        # Set font for better Spanish support
        plt.rcParams['font.family'] = 'DejaVu Sans'
    
    def create_all_visualizations(self, 
                                analysis_data: Dict[str, Any],
                                country: Optional[str] = None) -> List[str]:
        """Create all standard visualizations.
        
        Args:
            analysis_data: Dictionary with analysis results
            country: Country code for filtering
            
        Returns:
            List of generated file paths
        """
        generated_files = []
        
        # Skill frequency chart
        if 'skill_statistics' in analysis_data:
            path = self.create_skill_frequency_chart(
                analysis_data['skill_statistics'],
                country
            )
            if path:
                generated_files.append(path)
        
        # Cluster visualization
        if 'clustering_results' in analysis_data:
            path = self.create_cluster_visualization(
                analysis_data['clustering_results']
            )
            if path:
                generated_files.append(path)
        
        # Geographic distribution
        if 'geographic_data' in analysis_data:
            path = self.create_geographic_distribution(
                analysis_data['geographic_data']
            )
            if path:
                generated_files.append(path)
        
        # Skill co-occurrence heatmap
        if 'skill_cooccurrence' in analysis_data:
            path = self.create_skill_cooccurrence_heatmap(
                analysis_data['skill_cooccurrence']
            )
            if path:
                generated_files.append(path)
        
        return generated_files
    
    def create_skill_frequency_chart(self,
                                   skill_stats: Dict[str, Any],
                                   country: Optional[str] = None,
                                   top_n: int = 20) -> Optional[str]:
        """Create horizontal bar chart of top skills.
        
        Args:
            skill_stats: Skill statistics data
            country: Country filter
            top_n: Number of top skills to show
            
        Returns:
            Path to saved chart
        """
        try:
            # Prepare data
            top_skills = skill_stats.get('top_skills', [])[:top_n]
            if not top_skills:
                logger.warning("No skill data for frequency chart")
                return None
            
            df = pd.DataFrame(top_skills)
            
            # Create figure
            fig, ax = plt.subplots(figsize=(12, 8))
            
            # Create horizontal bar chart
            bars = ax.barh(df['skill'], df['count'], 
                          color=plt.cm.viridis(np.linspace(0.3, 0.9, len(df))))
            
            # Customize
            ax.set_xlabel('Número de Vacantes', fontsize=14)
            ax.set_ylabel('Habilidad Técnica', fontsize=14)
            
            title = f'Top {top_n} Habilidades Más Demandadas'
            if country:
                country_names = {'CO': 'Colombia', 'MX': 'México', 'AR': 'Argentina'}
                title += f' - {country_names.get(country, country)}'
            ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
            
            # Add value labels
            for bar in bars:
                width = bar.get_width()
                ax.text(width + 1, bar.get_y() + bar.get_height()/2,
                       f'{int(width)}',
                       ha='left', va='center', fontsize=10)
            
            # Adjust layout
            plt.tight_layout()
            ax.invert_yaxis()  # Highest on top
            
            # Grid
            ax.grid(True, axis='x', alpha=0.3)
            ax.set_axisbelow(True)
            
            # Save
            filename = self._generate_filename('skill_frequency', country)
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created skill frequency chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating skill frequency chart: {e}")
            return None
    
    def create_cluster_visualization(self,
                                   clustering_results: Dict[str, Any]) -> Optional[str]:
        """Create 2D scatter plot of skill clusters.
        
        Args:
            clustering_results: Clustering analysis results
            
        Returns:
            Path to saved visualization
        """
        try:
            # Extract data
            coordinates = clustering_results.get('coordinates_2d')
            labels = clustering_results.get('labels')
            skills = clustering_results.get('skills', [])
            
            if coordinates is None or labels is None:
                logger.warning("Missing data for cluster visualization")
                return None
            
            # Create figure
            fig, ax = plt.subplots(figsize=(14, 10))
            
            # Get unique labels
            unique_labels = set(labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            
            # Color map
            colors = plt.cm.rainbow(np.linspace(0, 1, n_clusters))
            
            # Plot each cluster
            for k, col in zip(sorted(unique_labels), colors):
                if k == -1:
                    # Noise points in black
                    col = 'black'
                    label = 'Ruido'
                else:
                    label = f'Cluster {k}'
                
                class_member_mask = (labels == k)
                xy = coordinates[class_member_mask]
                
                ax.scatter(xy[:, 0], xy[:, 1], 
                         c=[col], 
                         label=label,
                         alpha=0.6,
                         s=30)
            
            # Add labels for some points (avoid overlap)
            if skills:
                # Sample points to label
                n_labels = min(30, len(skills))
                indices = np.random.choice(len(skills), n_labels, replace=False)
                
                for idx in indices:
                    ax.annotate(skills[idx], 
                              (coordinates[idx, 0], coordinates[idx, 1]),
                              fontsize=8,
                              alpha=0.7)
            
            # Customize
            ax.set_title('Visualización de Clusters de Habilidades', 
                        fontsize=16, fontweight='bold')
            ax.set_xlabel('UMAP Dimension 1', fontsize=12)
            ax.set_ylabel('UMAP Dimension 2', fontsize=12)
            
            # Legend
            ax.legend(loc='upper right', bbox_to_anchor=(1.15, 1))
            
            # Remove ticks
            ax.set_xticks([])
            ax.set_yticks([])
            
            # Save
            filename = self._generate_filename('skill_clusters')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created cluster visualization: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating cluster visualization: {e}")
            return None
    
    def create_geographic_distribution(self,
                                     geo_data: Dict[str, Any]) -> Optional[str]:
        """Create geographic distribution chart.
        
        Args:
            geo_data: Geographic distribution data
            
        Returns:
            Path to saved chart
        """
        try:
            # Create figure
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # Job distribution by country
            countries = ['Colombia', 'México', 'Argentina']
            job_counts = [
                geo_data.get('CO', {}).get('total_jobs', 0),
                geo_data.get('MX', {}).get('total_jobs', 0),
                geo_data.get('AR', {}).get('total_jobs', 0)
            ]
            
            # Pie chart
            colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
            wedges, texts, autotexts = ax1.pie(job_counts, 
                                              labels=countries,
                                              colors=colors,
                                              autopct='%1.1f%%',
                                              startangle=90)
            
            ax1.set_title('Distribución de Vacantes por País', 
                         fontsize=14, fontweight='bold')
            
            # Skills per country
            skill_counts = [
                geo_data.get('CO', {}).get('unique_skills', 0),
                geo_data.get('MX', {}).get('unique_skills', 0),
                geo_data.get('AR', {}).get('unique_skills', 0)
            ]
            
            bars = ax2.bar(countries, skill_counts, color=colors)
            ax2.set_title('Habilidades Únicas por País', 
                         fontsize=14, fontweight='bold')
            ax2.set_ylabel('Número de Habilidades', fontsize=12)
            
            # Add value labels
            for bar in bars:
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height,
                        f'{int(height)}',
                        ha='center', va='bottom')
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('geographic_distribution')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created geographic distribution chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating geographic distribution: {e}")
            return None
    
    def create_skill_cooccurrence_heatmap(self,
                                        cooccurrence_data: Dict[str, Any],
                                        top_n: int = 15) -> Optional[str]:
        """Create heatmap of skill co-occurrences.
        
        Args:
            cooccurrence_data: Skill co-occurrence matrix
            top_n: Number of top skills to include
            
        Returns:
            Path to saved heatmap
        """
        try:
            # Convert to DataFrame
            df = pd.DataFrame(cooccurrence_data.get('matrix', []))
            skills = cooccurrence_data.get('skills', [])
            
            if df.empty or not skills:
                logger.warning("No co-occurrence data available")
                return None
            
            # Select top skills
            if len(skills) > top_n:
                # Sum co-occurrences for each skill
                skill_importance = df.sum(axis=0) + df.sum(axis=1)
                top_indices = skill_importance.nlargest(top_n).index
                df = df.loc[top_indices, top_indices]
                skills = [skills[i] for i in top_indices]
            
            # Create figure
            fig, ax = plt.subplots(figsize=(12, 10))
            
            # Create heatmap
            sns.heatmap(df, 
                       xticklabels=skills,
                       yticklabels=skills,
                       cmap='YlOrRd',
                       cbar_kws={'label': 'Co-ocurrencias'},
                       square=True,
                       linewidths=0.5,
                       ax=ax)
            
            # Customize
            ax.set_title('Matriz de Co-ocurrencia de Habilidades', 
                        fontsize=16, fontweight='bold', pad=20)
            
            # Rotate labels
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('skill_cooccurrence')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created co-occurrence heatmap: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating co-occurrence heatmap: {e}")
            return None
    
    def create_temporal_trends(self,
                             temporal_data: Dict[str, Any],
                             skills: List[str] = None) -> Optional[str]:
        """Create temporal trend visualization.
        
        Args:
            temporal_data: Temporal trend data
            skills: List of skills to plot (default: top 5)
            
        Returns:
            Path to saved chart
        """
        try:
            # Prepare data
            df = pd.DataFrame(temporal_data.get('trends', []))
            
            if df.empty:
                logger.warning("No temporal data available")
                return None
            
            # Convert date column
            df['date'] = pd.to_datetime(df['date'])
            
            # Select skills to plot
            if not skills:
                # Get top 5 skills by total mentions
                skill_totals = df.groupby('skill')['count'].sum()
                skills = skill_totals.nlargest(5).index.tolist()
            
            # Create figure
            fig, ax = plt.subplots(figsize=(14, 8))
            
            # Plot each skill
            for skill in skills:
                skill_data = df[df['skill'] == skill]
                ax.plot(skill_data['date'], skill_data['count'], 
                       marker='o', label=skill, linewidth=2)
            
            # Customize
            ax.set_title('Tendencias Temporales de Habilidades', 
                        fontsize=16, fontweight='bold')
            ax.set_xlabel('Fecha', fontsize=12)
            ax.set_ylabel('Número de Menciones', fontsize=12)
            
            # Format x-axis
            import matplotlib.dates as mdates
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
            ax.xaxis.set_major_locator(mdates.MonthLocator())
            plt.xticks(rotation=45)
            
            # Legend
            ax.legend(loc='upper left', bbox_to_anchor=(1, 1))
            
            # Grid
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('temporal_trends')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created temporal trends chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating temporal trends: {e}")
            return None
    
    def _generate_filename(self, chart_type: str, country: Optional[str] = None) -> str:
        """Generate filename with timestamp.
        
        Args:
            chart_type: Type of chart
            country: Country code (optional)
            
        Returns:
            Generated filename
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        country_suffix = f"_{country}" if country else ""
        return f"{chart_type}{country_suffix}_{timestamp}.png"

---

## 9. Orchestrator and Utilities

### src/orchestrator.py
```python
#!/usr/bin/env python3
"""
Main orchestrator for the Labor Market Observatory pipeline.
"""

import logging
import sys
from typing import Optional
import typer
from rich.console import Console
from rich.table import Table
from rich.progress import track
import time

from config.settings import get_settings
from config.logging_config import setup_logging
from database.operations import DatabaseOperations
from scraper.spiders.computrabajo_spider import ComputrabajoSpider
from scraper.spiders.bumeran_spider import BumeranSpider
from scraper.spiders.elempleo_spider import ElempleoSpider
from extractor.pipeline import ExtractionPipeline
from llm_processor.pipeline import LLMProcessingPipeline
from embedder.batch_processor import BatchProcessor
from analyzer.clustering import SkillClusterer
from analyzer.dimension_reducer import DimensionReducer
from analyzer.report_generator import ReportGenerator
from analyzer.visualizations import VisualizationGenerator

# Initialize
app = typer.Typer(help="Labor Market Observatory CLI")
console = Console()
settings = get_settings()
logger = setup_logging(settings.log_level, settings.log_file)

@app.command()
def scrape(
    country: str = typer.Argument(..., help="Country code (CO, MX, AR)"),
    portal: str = typer.Argument(..., help="Portal name (computrabajo, bumeran, elempleo)"),
    pages: int = typer.Option(10, help="Number of pages to scrape")
):
    """Run web scraping for a specific portal and country."""
    console.print(f"[bold green]Starting scraper for {portal} in {country}[/bold green]")
    
    # Validate inputs
    if country not in settings.supported_countries:
        console.print(f"[red]Invalid country: {country}[/red]")
        raise typer.Exit(1)
    
    if portal not in settings.supported_portals:
        console.print(f"[red]Invalid portal: {portal}[/red]")
        raise typer.Exit(1)
    
    # Run appropriate spider
    try:
        from scrapy.crawler import CrawlerProcess
        from scrapy.utils.project import get_project_settings
        
        # Get scrapy settings
        scrapy_settings = get_project_settings()
        scrapy_settings.update({
            'LOG_LEVEL': 'INFO',
            'CLOSESPIDER_PAGECOUNT': pages
        })
        
        process = CrawlerProcess(scrapy_settings)
        
        # Select spider
        if portal == 'computrabajo':
            spider_class = ComputrabajoSpider
        elif portal == 'bumeran':
            spider_class = BumeranSpider
        elif portal == 'elempleo':
            spider_class = ElempleoSpider
        
        # Run spider
        process.crawl(spider_class, country=country)
        process.start()
        
        console.print("[bold green]Scraping completed![/bold green]")
        
    except Exception as e:
        console.print(f"[red]Scraping failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def extract(
    batch_size: int = typer.Option(100, help="Batch size for processing")
):
    """Extract skills from scraped job postings."""
    console.print("[bold green]Starting skill extraction...[/bold green]")
    
    try:
        pipeline = ExtractionPipeline()
        
        with console.status("[bold green]Extracting skills...") as status:
            stats = pipeline.process_batch(batch_size)
        
        # Display results
        table = Table(title="Extraction Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Jobs Processed", str(stats['jobs_processed']))
        table.add_row("Skills Extracted", str(stats['skills_extracted']))
        table.add_row("ESCO Matches", str(stats['esco_matches']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        table.add_row("Jobs/Second", f"{stats['jobs_per_second']:.2f}")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Extraction failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def enhance(
    batch_size: int = typer.Option(50, help="Batch size for LLM processing"),
    model: str = typer.Option("local", help="Model type (local or openai)")
):
    """Enhance extracted skills using LLM."""
    console.print(f"[bold green]Starting LLM enhancement with {model} model...[/bold green]")
    
    try:
        pipeline = LLMProcessingPipeline(model_type=model)
        
        with console.status("[bold green]Processing with LLM...") as status:
            stats = pipeline.process_batch(batch_size)
        
        # Display results
        table = Table(title="LLM Enhancement Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Jobs Processed", str(stats['jobs_processed']))
        table.add_row("Skills Enhanced", str(stats['skills_enhanced']))
        table.add_row("Implicit Skills Found", str(stats['implicit_skills_found']))
        table.add_row("Skills Normalized", str(stats['skills_normalized']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Enhancement failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def embed(
    model: Optional[str] = typer.Option(None, help="Embedding model name")
):
    """Generate embeddings for all skills."""
    console.print("[bold green]Starting embedding generation...[/bold green]")
    
    try:
        processor = BatchProcessor(model_name=model)
        
        with console.status("[bold green]Generating embeddings...") as status:
            stats = processor.process_all_skills()
        
        # Display results
        table = Table(title="Embedding Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Skills Processed", str(stats['skills_processed']))
        table.add_row("Embeddings Created", str(stats['embeddings_created']))
        table.add_row("Errors", str(stats['errors']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Embedding failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def analyze(
    method: str = typer.Option("hdbscan", help="Clustering method")
):
    """Run clustering analysis on skill embeddings."""
    console.print(f"[bold green]Starting clustering analysis with {method}...[/bold green]")
    
    try:
        clusterer = SkillClusterer()
        
        with console.status("[bold green]Running clustering...") as status:
            results = clusterer.run_clustering_pipeline()
        
        # Display results
        if results:
            table = Table(title="Clustering Results")
            table.add_column("Metric", style="cyan")
            table.add_column("Value", style="magenta")
            
            table.add_row("Number of Clusters", str(results['n_clusters']))
            table.add_row("Noise Points", str(results['n_noise']))
            table.add_row("Silhouette Score", f"{results['metrics'].get('silhouette_score', 0):.3f}")
            
            console.print(table)
            
            # Show top clusters
            console.print("\n[bold]Top 5 Clusters:[/bold]")
            for cluster in results['cluster_info'][:5]:
                console.print(f"  • {cluster['label']}: {cluster['size']} skills")
        
    except Exception as e:
        console.print(f"[red]Analysis failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def report(
    country: Optional[str] = typer.Option(None, help="Country code to filter"),
    format: str = typer.Option("pdf", help="Report format (pdf)")
):
    """Generate analysis report."""
    console.print("[bold green]Generating report...[/bold green]")
    
    try:
        generator = ReportGenerator()
        
        with console.status("[bold green]Creating report...") as status:
            filepath = generator.generate_full_report(
                country=country,
                include_visualizations=True
            )
        
        console.print(f"[bold green]Report generated: {filepath}[/bold green]")
        
    except Exception as e:
        console.print(f"[red]Report generation failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def pipeline(
    country: str = typer.Argument(..., help="Country code"),
    portal: str = typer.Argument(..., help="Portal name"),
    full: bool = typer.Option(False, help="Run full pipeline including scraping")
):
    """Run complete pipeline."""
    console.print("[bold green]Running complete pipeline...[/bold green]")
    
    steps = []
    
    if full:
        steps.append(("Scraping", lambda: scrape(country, portal, pages=5)))
    
    steps.extend([
        ("Extraction", lambda: extract(batch_size=100)),
        ("LLM Enhancement", lambda: enhance(batch_size=50)),
        ("Embedding", lambda: embed()),
        ("Clustering", lambda: analyze()),
        ("Report Generation", lambda: report(country=country))
    ])
    
    for step_name, step_func in track(steps, description="Processing..."):
        try:
            console.print(f"\n[bold cyan]Running: {step_name}[/bold cyan]")
            step_func()
            time.sleep(1)  # Brief pause between steps
        except Exception as e:
            console.print(f"[red]Step '{step_name}' failed: {e}[/red]")
            raise typer.Exit(1)
    
    console.print("\n[bold green]Pipeline completed successfully![/bold green]")

@app.command()
def status():
    """Show system status and statistics."""
    console.print("[bold green]Labor Market Observatory Status[/bold green]\n")
    
    try:
        db_ops = DatabaseOperations()
        
        # Get statistics
        stats = db_ops.get_skill_statistics()
        
        # Display overall stats
        table = Table(title="System Statistics")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Total Unique Skills", str(stats.get('total_unique_skills', 0)))
        table.add_row("Database Size", "N/A")  # Would need to implement
        
        console.print(table)
        
        # Top skills
        if stats.get('top_skills'):
            console.print("\n[bold]Top 10 Skills:[/bold]")
            for i, skill in enumerate(stats['top_skills'][:10], 1):
                console.print(f"  {i}. {skill['skill']} ({skill['count']} jobs)")
        
    except Exception as e:
        console.print(f"[red]Failed to get status: {e}[/red]")
        raise typer.Exit(1)

if __name__ == "__main__":
    app()

### src/utils/__init__.py
```python
from .validators import validate_country, validate_portal, validate_skill
from .cleaners import clean_text, normalize_text, remove_html
from .metrics import calculate_metrics, generate_statistics
from .logger import get_logger

__all__ = [
    'validate_country', 'validate_portal', 'validate_skill',
    'clean_text', 'normalize_text', 'remove_html',
    'calculate_metrics', 'generate_statistics',
    'get_logger'
]
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting job: {e}")
            raise
        finally:
            session.close()
    
    def get_unprocessed_jobs(self, limit: int = 100) -> List[RawJob]:
        """Get unprocessed job postings."""
        session = self.get_session()
        try:
            jobs = session.query(RawJob).filter(
                RawJob.is_processed == False
            ).limit(limit).all()
            return jobs
        finally:
            session.close()
    
    def mark_job_processed(self, job_id: str):
        """Mark a job as processed."""
        session = self.get_session()
        try:
            session.query(RawJob).filter(
                RawJob.job_id == job_id
            ).update({"is_processed": True})
            session.commit()
        finally:
            session.close()
    
    def insert_extracted_skills(self, job_id: str, skills: List[Dict[str, Any]]):
        """Insert extracted skills for a job."""
        session = self.get_session()
        try:
            for skill_data in skills:
                skill = ExtractedSkill(
                    job_id=job_id,
                    **skill_data
                )
                session.add(skill)
            session.commit()
            logger.info(f"Inserted {len(skills)} extracted skills for job {job_id}")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting extracted skills: {e}")
            raise
        finally:
            session.close()
    
    def get_extracted_skills_for_processing(self, limit: int = 100) -> List[Dict[str, Any]]:
        """Get jobs with extracted skills that need LLM processing."""
        session = self.get_session()
        try:
            # Get jobs that have extracted skills but no enhanced skills
            subquery = session.query(EnhancedSkill.job_id).distinct()
            
            jobs = session.query(RawJob).join(ExtractedSkill).filter(
                ~RawJob.job_id.in_(subquery)
            ).limit(limit).all()
            
            result = []
            for job in jobs:
                skills = session.query(ExtractedSkill).filter(
                    ExtractedSkill.job_id == job.job_id
                ).all()
                
                result.append({
                    'job_id': str(job.job_id),
                    'job_title': job.title,
                    'job_description': job.description,
                    'job_requirements': job.requirements,
                    'extracted_skills': [
                        {
                            'skill_text': skill.skill_text,
                            'extraction_method': skill.extraction_method,
                            'source_section': skill.source_section,
                            'confidence_score': skill.confidence_score
                        }
                        for skill in skills
                    ]
                })
            
            return result
        finally:
            session.close()
    
    def insert_enhanced_skills(self, job_id: str, skills: List[Dict[str, Any]]):
        """Insert enhanced skills for a job."""
        session = self.get_session()
        try:
            for skill_data in skills:
                skill = EnhancedSkill(
                    job_id=job_id,
                    **skill_data
                )
                session.add(skill)
            session.commit()
            logger.info(f"Inserted {len(skills)} enhanced skills for job {job_id}")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting enhanced skills: {e}")
            raise
        finally:
            session.close()
    
    def get_unique_skills_for_embedding(self) -> List[str]:
        """Get unique normalized skills that don't have embeddings yet."""
        session = self.get_session()
        try:
            # Get skills that don't have embeddings
            embedded_skills = session.query(SkillEmbedding.skill_text).distinct()
            
            unique_skills = session.query(
                EnhancedSkill.normalized_skill
            ).filter(
                EnhancedSkill.is_duplicate == False,
                ~EnhancedSkill.normalized_skill.in_(embedded_skills)
            ).distinct().all()
            
            return [skill[0] for skill in unique_skills]
        finally:
            session.close()
    
    def insert_skill_embeddings(self, embeddings: List[Dict[str, Any]]):
        """Insert skill embeddings."""
        session = self.get_session()
        try:
            for emb_data in embeddings:
                embedding = SkillEmbedding(**emb_data)
                session.add(embedding)
            session.commit()
            logger.info(f"Inserted {len(embeddings)} skill embeddings")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting embeddings: {e}")
            raise
        finally:
            session.close()
    
    def get_all_embeddings(self) -> List[Dict[str, Any]]:
        """Get all skill embeddings for clustering."""
        session = self.get_session()
        try:
            embeddings = session.query(SkillEmbedding).all()
            return [
                {
                    'skill_text': emb.skill_text,
                    'embedding': emb.embedding,
                    'embedding_id': str(emb.embedding_id)
                }
                for emb in embeddings
            ]
        finally:
            session.close()
    
    def save_analysis_results(self, analysis_type: str, results: Dict[str, Any], 
                            parameters: Dict[str, Any], country: Optional[str] = None):
        """Save analysis results."""
        session = self.get_session()
        try:
            analysis = AnalysisResult(
                analysis_type=analysis_type,
                country=country,
                parameters=parameters,
                results=results
            )
            session.add(analysis)
            session.commit()
            logger.info(f"Saved {analysis_type} analysis results")
        except Exception as e:
            session.rollback()
            logger.error(f"Error saving analysis results: {e}")
            raise
        finally:
            session.close()
    
    def get_skill_statistics(self, country: Optional[str] = None) -> Dict[str, Any]:
        """Get skill statistics by country."""
        session = self.get_session()
        try:
            query = session.query(
                EnhancedSkill.normalized_skill,
                func.count(func.distinct(EnhancedSkill.job_id)).label('job_count')
            ).join(RawJob).filter(
                EnhancedSkill.is_duplicate == False
            )
            
            if country:
                query = query.filter(RawJob.country == country)
            
            results = query.group_by(
                EnhancedSkill.normalized_skill
            ).order_by(
                func.count(func.distinct(EnhancedSkill.job_id)).desc()
            ).limit(50).all()
            
            return {
                'top_skills': [
                    {'skill': skill, 'count': count}
                    for skill, count in results
                ],
                'total_unique_skills': session.query(
                    func.count(func.distinct(EnhancedSkill.normalized_skill))
                ).filter(EnhancedSkill.is_duplicate == False).scalar()
            }
        finally:
            session.close()
```

---

## 3. Configuration Module

### src/config/__init__.py
```python
from .settings import Settings, get_settings
from .database import get_database_url
from .logging_config import setup_logging

__all__ = ['Settings', 'get_settings', 'get_database_url', 'setup_logging']
```

### src/config/settings.py
```python
from pydantic_settings import BaseSettings
from pydantic import Field, validator
from typing import Optional, List
import os
from functools import lru_cache

class Settings(BaseSettings):
    # Database
    database_url: str = Field(..., env='DATABASE_URL')
    database_pool_size: int = Field(20, env='DATABASE_POOL_SIZE')
    
    # Scraping
    scraper_user_agent: str = Field(..., env='SCRAPER_USER_AGENT')
    scraper_concurrent_requests: int = Field(16, env='SCRAPER_CONCURRENT_REQUESTS')
    scraper_download_delay: float = Field(1.0, env='SCRAPER_DOWNLOAD_DELAY')
    scraper_retry_times: int = Field(3, env='SCRAPER_RETRY_TIMES')
    
    # ESCO
    esco_api_url: str = Field('https://ec.europa.eu/esco/api', env='ESCO_API_URL')
    esco_version: str = Field('1.1.0', env='ESCO_VERSION')
    esco_language: str = Field('es', env='ESCO_LANGUAGE')
    
    # LLM
    llm_model_path: str = Field(..., env='LLM_MODEL_PATH')
    llm_context_length: int = Field(4096, env='LLM_CONTEXT_LENGTH')
    llm_max_tokens: int = Field(512, env='LLM_MAX_TOKENS')
    llm_temperature: float = Field(0.7, env='LLM_TEMPERATURE')
    llm_n_gpu_layers: int = Field(35, env='LLM_N_GPU_LAYERS')
    
    # OpenAI (Optional)
    openai_api_key: Optional[str] = Field(None, env='OPENAI_API_KEY')
    openai_model: str = Field('gpt-3.5-turbo', env='OPENAI_MODEL')
    
    # Embeddings
    embedding_model: str = Field('intfloat/multilingual-e5-base', env='EMBEDDING_MODEL')
    embedding_batch_size: int = Field(32, env='EMBEDDING_BATCH_SIZE')
    embedding_cache_dir: str = Field('./data/cache/embeddings', env='EMBEDDING_CACHE_DIR')
    
    # Analysis
    cluster_min_size: int = Field(5, env='CLUSTER_MIN_SIZE')
    cluster_min_samples: int = Field(3, env='CLUSTER_MIN_SAMPLES')
    umap_n_neighbors: int = Field(15, env='UMAP_N_NEIGHBORS')
    umap_min_dist: float = Field(0.1, env='UMAP_MIN_DIST')
    
    # Output
    output_dir: str = Field('./outputs', env='OUTPUT_DIR')
    report_format: str = Field('pdf', env='REPORT_FORMAT')
    log_level: str = Field('INFO', env='LOG_LEVEL')
    log_file: str = Field('./logs/labor_observatory.log', env='LOG_FILE')
    
    # Supported countries and portals
    supported_countries: List[str] = ['CO', 'MX', 'AR']
    supported_portals: List[str] = ['computrabajo', 'bumeran', 'elempleo']
    
    class Config:
        env_file = '.env'
        env_file_encoding = 'utf-8'
    
    @validator('output_dir', 'log_file', 'embedding_cache_dir')
    def create_directories(cls, v):
        os.makedirs(os.path.dirname(v) if os.path.dirname(v) else v, exist_ok=True)
        return v

@lru_cache()
def get_settings() -> Settings:
    """Get cached settings instance."""
    return Settings()
```

### src/config/database.py
```python
import os
from urllib.parse import urlparse

def get_database_url() -> str:
    """Get database URL from environment or construct from components."""
    if os.getenv('DATABASE_URL'):
        return os.getenv('DATABASE_URL')
    
    # Construct from individual components
    user = os.getenv('DB_USER', 'labor_user')
    password = os.getenv('DB_PASSWORD', 'password')
    host = os.getenv('DB_HOST', 'localhost')
    port = os.getenv('DB_PORT', '5432')
    name = os.getenv('DB_NAME', 'labor_observatory')
    
    return f"postgresql://{user}:{password}@{host}:{port}/{name}"

def get_database_config() -> dict:
    """Parse database URL into components."""
    url = get_database_url()
    parsed = urlparse(url)
    
    return {
        'host': parsed.hostname,
        'port': parsed.port or 5432,
        'user': parsed.username,
        'password': parsed.password,
        'database': parsed.path.lstrip('/')
    }
```

### src/config/logging_config.py
```python
import logging
import logging.handlers
import os
from datetime import datetime

def setup_logging(log_level: str = 'INFO', log_file: str = None):
    """Configure logging for the entire application."""
    
    # Create logs directory if needed
    if log_file:
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    # Create formatters
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
    )
    console_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )
    
    # Get root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, log_level.upper()))
    
    # Remove existing handlers
    root_logger.handlers.clear()
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)
    
    # File handler with rotation
    if log_file:
        file_handler = logging.handlers.RotatingFileHandler(
            log_file,
            maxBytes=10 * 1024 * 1024,  # 10MB
            backupCount=5
        )
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(file_formatter)
        root_logger.addHandler(file_handler)
    
    # Specific loggers configuration
    logging.getLogger('scrapy').setLevel(logging.WARNING)
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('transformers').setLevel(logging.WARNING)
    
    return root_logger
```

### config/esco_config.yaml
```yaml
# ESCO Configuration for Spanish Language
esco:
  version: "1.1.0"
  base_url: "https://ec.europa.eu/esco/api"
  
  # Endpoints
  endpoints:
    skills: "/resource/skill"
    occupations: "/resource/occupation"
    search: "/search"
  
  # Spanish language configuration
  language:
    primary: "es"
    fallback: "en"
  
  # Skill types to extract
  skill_types:
    - "skill/competence"
    - "knowledge"
    - "skill"
  
  # Search parameters
  search:
    limit: 100
    fields:
      - "preferredLabel"
      - "altLabels"
      - "description"
      - "broaderConcept"
    
  # Mapping rules for common tech terms
  tech_mappings:
    # Programming languages
    "python": "http://data.europa.eu/esco/skill/3897c3c6-556b-4b8a-bfeb-234b0f716950"
    "java": "http://data.europa.eu/esco/skill/b4b36f5a-d5e6-4d78-b4ed-5b0b0e4f7a8a"
    "javascript": "http://data.europa.eu/esco/skill/7c7c5c49-0122-4b2e-8f6e-4e6b3f3e5d5f"
    "react": "http://data.europa.eu/esco/skill/react-framework"
    "node.js": "http://data.europa.eu/esco/skill/nodejs-runtime"
    
    # Databases
    "sql": "http://data.europa.eu/esco/skill/sql-language"
    "mysql": "http://data.europa.eu/esco/skill/mysql-database"
    "postgresql": "http://data.europa.eu/esco/skill/postgresql-database"
    "mongodb": "http://data.europa.eu/esco/skill/mongodb-database"
    
    # Cloud platforms
    "aws": "http://data.europa.eu/esco/skill/amazon-web-services"
    "azure": "http://data.europa.eu/esco/skill/microsoft-azure"
    "gcp": "http://data.europa.eu/esco/skill/google-cloud-platform"
    
    # DevOps
    "docker": "http://data.europa.eu/esco/skill/docker-containerization"
    "kubernetes": "http://data.europa.eu/esco/skill/kubernetes-orchestration"
    "git": "http://data.europa.eu/esco/skill/git-version-control"
    
    # Soft skills (Spanish)
    "trabajo en equipo": "http://data.europa.eu/esco/skill/teamwork"
    "comunicación": "http://data.europa.eu/esco/skill/communication"
    "liderazgo": "http://data.europa.eu/esco/skill/leadership"
    "resolución de problemas": "http://data.europa.eu/esco/skill/problem-solving"
```

---

## 4. Scraper Module Files

### src/scraper/__init__.py
```python
from .spiders.computrabajo_spider import ComputrabajoSpider
from .spiders.bumeran_spider import BumeranSpider
from .spiders.elempleo_spider import ElempleoSpider

__all__ = ['ComputrabajoSpider', 'BumeranSpider', 'ElempleoSpider']
```

### src/scraper/scrapy.cfg
```ini
[settings]
default = scraper.settings

[deploy]
project = labor_observatory_scraper
```

### src/scraper/items.py
```python
import scrapy
from scrapy.item import Field
from datetime import datetime
import re

class JobItem(scrapy.Item):
    # Required fields
    portal = Field()
    country = Field()
    url = Field()
    title = Field()
    description = Field()
    
    # Optional fields
    company = Field()
    location = Field()
    requirements = Field()
    salary_raw = Field()
    contract_type = Field()
    remote_type = Field()
    posted_date = Field()
    raw_html = Field()
    
    # Metadata
    scraped_at = Field()
    
    def clean_text(self, text):
        """Clean and normalize text fields."""
        if not text:
            return None
        
        # Remove extra whitespace
        text = ' '.join(text.split())
        
        # Remove HTML entities
        text = re.sub(r'&[a-zA-Z]+;', ' ', text)
        
        return text.strip()
    
    def __setitem__(self, key, value):
        # Clean text fields
        if key in ['title', 'description', 'requirements', 'company', 'location'] and value:
            value = self.clean_text(value)
        
        # Set scraped_at automatically
        if key == 'scraped_at':
            value = datetime.now()
        
        super().__setitem__(key, value)
```

### src/scraper/pipelines.py
```python
import logging
from datetime import datetime
from typing import Optional
import re
from scrapy.exceptions import DropItem
from database.operations import DatabaseOperations

logger = logging.getLogger(__name__)

class ValidationPipeline:
    """Validate scraped items."""
    
    def process_item(self, item, spider):
        # Check required fields
        required_fields = ['portal', 'country', 'url', 'title', 'description']
        
        for field in required_fields:
            if not item.get(field):
                raise DropItem(f"Missing required field: {field}")
        
        # Validate country code
        if item['country'] not in ['CO', 'MX', 'AR']:
            raise DropItem(f"Invalid country code: {item['country']}")
        
        # Validate portal
        if item['portal'] not in ['computrabajo', 'bumeran', 'elempleo']:
            raise DropItem(f"Invalid portal: {item['portal']}")
        
        # Ensure minimum description length
        if len(item['description']) < 50:
            raise DropItem("Description too short")
        
        return item

class NormalizationPipeline:
    """Normalize item fields."""
    
    def process_item(self, item, spider):
        # Normalize contract type
        if item.get('contract_type'):
            item['contract_type'] = self.normalize_contract_type(item['contract_type'])
        
        # Normalize remote type
        if item.get('remote_type'):
            item['remote_type'] = self.normalize_remote_type(item['remote_type'])
        
        # Parse posted date
        if item.get('posted_date'):
            item['posted_date'] = self.parse_date(item['posted_date'])
        
        # Set scraped_at
        item['scraped_at'] = datetime.now()
        
        return item
    
    def normalize_contract_type(self, contract: str) -> str:
        """Normalize contract type to standard values."""
        contract_lower = contract.lower()
        
        if any(term in contract_lower for term in ['tiempo completo', 'full time', 'completo']):
            return 'full_time'
        elif any(term in contract_lower for term in ['medio tiempo', 'part time', 'parcial']):
            return 'part_time'
        elif any(term in contract_lower for term in ['freelance', 'independiente', 'autonomo']):
            return 'freelance'
        elif any(term in contract_lower for term in ['contrato', 'temporal', 'proyecto']):
            return 'contract'
        elif any(term in contract_lower for term in ['pasantia', 'practica', 'internship']):
            return 'internship'
        else:
            return 'other'
    
    def normalize_remote_type(self, remote: str) -> str:
        """Normalize remote work type."""
        remote_lower = remote.lower()
        
        if any(term in remote_lower for term in ['remoto', 'remote', 'teletrabajo']):
            return 'remote'
        elif any(term in remote_lower for term in ['hibrido', 'hybrid', 'mixto']):
            return 'hybrid'
        elif any(term in remote_lower for term in ['presencial', 'oficina', 'on-site']):
            return 'on_site'
        else:
            return 'not_specified'
    
    def parse_date(self, date_str: str) -> Optional[datetime]:
        """Parse various date formats."""
        # Common Spanish date patterns
        patterns = [
            r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})',  # DD/MM/YYYY or DD-MM-YYYY
            r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})',  # YYYY/MM/DD or YYYY-MM-DD
            r'hace (\d+) días?',  # "hace X días"
            r'(\d+) días? atrás',  # "X días atrás"
            r'hoy',  # "hoy"
            r'ayer',  # "ayer"
        ]
        
        # Try to match patterns
        for pattern in patterns:
            match = re.search(pattern, date_str, re.IGNORECASE)
            if match:
                if 'hace' in pattern or 'atrás' in pattern:
                    days_ago = int(match.group(1))
                    return datetime.now() - timedelta(days=days_ago)
                elif pattern == r'hoy':
                    return datetime.now().date()
                elif pattern == r'ayer':
                    return datetime.now() - timedelta(days=1)
                else:
                    # Handle date formats
                    try:
                        if len(match.groups()) == 3:
                            if match.group(1).isdigit() and len(match.group(1)) == 4:
                                # YYYY/MM/DD format
                                return datetime(
                                    int(match.group(1)),
                                    int(match.group(2)),
                                    int(match.group(3))
                                ).date()
                            else:
                                # DD/MM/YYYY format
                                return datetime(
                                    int(match.group(3)),
                                    int(match.group(2)),
                                    int(match.group(1))
                                ).date()
                    except ValueError:
                        pass
        
        return None

class DatabasePipeline:
    """Save items to PostgreSQL database."""
    
    def __init__(self):
        self.db_ops = None
    
    def open_spider(self, spider):
        self.db_ops = DatabaseOperations()
        logger.info(f"Database pipeline opened for spider: {spider.name}")
    
    def process_item(self, item, spider):
        try:
            # Convert item to dict
            job_data = dict(item)
            
            # Remove metadata fields
            job_data.pop('scraped_at', None)
            
            # Insert into database
            job_id = self.db_ops.insert_job(job_data)
            
            if job_id:
                logger.info(f"Saved job {job_id}: {item['title']}")
            else:
                logger.warning(f"Duplicate job skipped: {item['url']}")
            
            return item
            
        except Exception as e:
            logger.error(f"Error saving job to database: {e}")
            raise
```

### src/scraper/settings.py
```python
import os
from config.settings import get_settings

settings = get_settings()

BOT_NAME = 'labor_observatory_scraper'

SPIDER_MODULES = ['scraper.spiders']
NEWSPIDER_MODULE = 'scraper.spiders'

# Obey robots.txt rules
ROBOTSTXT_OBEY = True

# Configure maximum concurrent requests
CONCURRENT_REQUESTS = settings.scraper_concurrent_requests
CONCURRENT_REQUESTS_PER_DOMAIN = 8

# Configure delay
DOWNLOAD_DELAY = settings.scraper_download_delay
RANDOMIZE_DOWNLOAD_DELAY = True

# Disable cookies
COOKIES_ENABLED = False

# User agent
USER_AGENT = settings.scraper_user_agent

# Override default headers
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'Cache-Control': 'no-cache',
    'Pragma': 'no-cache',
}

# Retry configuration
RETRY_TIMES = settings.scraper_retry_times
RETRY_HTTP_CODES = [500, 502, 503, 504, 408, 429]

# Configure pipelines
ITEM_PIPELINES = {
    'scraper.pipelines.ValidationPipeline': 100,
    'scraper.pipelines.NormalizationPipeline': 200,
    'scraper.pipelines.DatabasePipeline': 300,
}

# AutoThrottle extension
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 1
AUTOTHROTTLE_MAX_DELAY = 10
AUTOTHROTTLE_TARGET_CONCURRENCY = 8.0
AUTOTHROTTLE_DEBUG = False

# Memory usage
MEMUSAGE_ENABLED = True
MEMUSAGE_LIMIT_MB = 2048
MEMUSAGE_WARNING_MB = 1536

# Logging
LOG_LEVEL = 'INFO'
LOG_FORMAT = '%(levelname)s: %(message)s'

# Cache
HTTPCACHE_ENABLED = False

# Download timeout
DOWNLOAD_TIMEOUT = 30

# Telnet Console (disabled for production)
TELNETCONSOLE_ENABLED = False

# Middleware settings
DOWNLOADER_MIDDLEWARES = {
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,
    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110,
}
```

### src/scraper/middlewares.py
```python
import random
import logging
from scrapy.downloadermiddlewares.retry import RetryMiddleware
from scrapy.utils.response import response_status_message
from fake_useragent import UserAgent

logger = logging.getLogger(__name__)

class RotateUserAgentMiddleware:
    """Rotate user agents for each request."""
    
    def __init__(self):
        self.ua = UserAgent()
        self.user_agent_list = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
        ]
    
    def process_request(self, request, spider):
        try:
            # Try to use fake-useragent
            ua = self.ua.random
        except:
            # Fallback to predefined list
            ua = random.choice(self.user_agent_list)
        
        request.headers['User-Agent'] = ua + ' Academic Research Bot'

class CustomRetryMiddleware(RetryMiddleware):
    """Custom retry middleware with exponential backoff."""
    
    def process_response(self, request, response, spider):
        if response.status in self.retry_http_codes:
            reason = response_status_message(response.status)
            
            # Log retry attempt
            retry_times = request.meta.get('retry_times', 0) + 1
            logger.warning(
                f"Retrying {request.url} (attempt {retry_times}): {reason}"
            )
            
            # Exponential backoff
            request.meta['download_delay'] = 2 ** retry_times
            
            return self._retry(request, reason, spider) or response
        
        return response
```

### src/scraper/spiders/__init__.py
```python
# Spider modules initialization
```

### src/scraper/spiders/base_spider.py
```python
import scrapy
from abc import ABC, abstractmethod
import logging
from datetime import datetime
from urllib.parse import urljoin

logger = logging.getLogger(__name__)

class BaseJobSpider(scrapy.Spider, ABC):
    """Base spider class for job portals."""
    
    def __init__(self, country=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.country = country
        self.total_scraped = 0
        self.start_time = datetime.now()
    
    @abstractmethod
    def parse_job(self, response):
        """Parse individual job posting. Must be implemented by subclasses."""
        pass
    
    def extract_text(self, selector, xpath_or_css, method='xpath'):
        """Safely extract text from selector."""
        try:
            if method == 'xpath':
                texts = selector.xpath(xpath_or_css).getall()
            else:
                texts = selector.css(xpath_or_css).getall()
            
            # Join and clean text
            text = ' '.join(texts)
            return ' '.join(text.split()) if text else None
        except Exception as e:
            logger.error(f"Error extracting text: {e}")
            return None
    
    def build_absolute_url(self, response, relative_url):
        """Build absolute URL from relative URL."""
        return urljoin(response.url, relative_url)
    
    def log_progress(self):
        """Log scraping progress."""
        elapsed = (datetime.now() - self.start_time).total_seconds()
        rate = self.total_scraped / elapsed if elapsed > 0 else 0
        
        logger.info(
            f"Spider {self.name} - Country: {self.country} - "
            f"Scraped: {self.total_scraped} - Rate: {rate:.2f} jobs/sec"
        )
```

### src/scraper/spiders/computrabajo_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import re
from urllib.parse import urlencode
import logging

logger = logging.getLogger(__name__)

class ComputrabajoSpider(BaseJobSpider):
    name = 'computrabajo'
    allowed_domains = ['computrabajo.com', 'computrabajo.com.co', 
                      'computrabajo.com.mx', 'computrabajo.com.ar']
    
    # URL patterns by country
    country_urls = {
        'CO': 'https://www.computrabajo.com.co',
        'MX': 'https://www.computrabajo.com.mx',
        'AR': 'https://www.computrabajo.com.ar'
    }
    
    # Tech-related search terms
    tech_keywords = [
        'desarrollador', 'developer', 'programador', 'software',
        'data', 'analyst', 'engineer', 'fullstack', 'frontend',
        'backend', 'devops', 'cloud', 'mobile', 'web'
    ]
    
    def start_requests(self):
        if not self.country or self.country not in self.country_urls:
            raise ValueError(f"Invalid country: {self.country}")
        
        base_url = self.country_urls[self.country]
        
        # Generate search URLs for tech keywords
        for keyword in self.tech_keywords:
            search_url = f"{base_url}/trabajo-de-{keyword}"
            yield Request(
                url=search_url,
                callback=self.parse_search_results,
                meta={'keyword': keyword, 'page': 1}
            )
    
    def parse_search_results(self, response):
        """Parse search results page."""
        # Extract job listings
        job_cards = response.css('article.js-o-card')
        
        for card in job_cards:
            # Extract job URL
            job_url = card.css('a.js-o-card__link::attr(href)').get()
            if job_url:
                absolute_url = self.build_absolute_url(response, job_url)
                yield Request(
                    url=absolute_url,
                    callback=self.parse_job,
                    meta={'search_keyword': response.meta.get('keyword')}
                )
        
        # Check for next page
        current_page = response.meta.get('page', 1)
        next_page_link = response.css('a[aria-label="Siguiente"]::attr(href)').get()
        
        if next_page_link and current_page < 10:  # Limit to 10 pages per keyword
            next_url = self.build_absolute_url(response, next_page_link)
            yield Request(
                url=next_url,
                callback=self.parse_search_results,
                meta={
                    'keyword': response.meta.get('keyword'),
                    'page': current_page + 1
                }
            )
    
    def parse_job(self, response):
        """Parse individual job posting."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'computrabajo'
        item['country'] = self.country
        item['url'] = response.url
        
        # Title
        item['title'] = self.extract_text(
            response, 
            '//h1[@class="fwB fs24"]//text()',
            'xpath'
        )
        
        # Company
        item['company'] = self.extract_text(
            response,
            '//a[@class="dIB fs16 js-o-link"]//text()',
            'xpath'
        )
        
        # Location
        location_parts = response.xpath(
            '//div[@class="fs16 fc_base mt5"]//span//text()'
        ).getall()
        item['location'] = ', '.join(location_parts) if location_parts else None
        
        # Description and requirements
        description_sections = response.xpath(
            '//div[@class="mbB"]//p//text() | //div[@class="mbB"]//li//text()'
        ).getall()
        
        full_text = ' '.join(description_sections)
        
        # Try to separate requirements
        req_pattern = r'(?:requisitos|requerimientos|requirements|experiencia|conocimientos):(.*?)(?:funciones|responsabilidades|beneficios|$)'
        req_match = re.search(req_pattern, full_text, re.IGNORECASE | re.DOTALL)
        
        if req_match:
            item['requirements'] = req_match.group(1).strip()
            item['description'] = full_text.replace(req_match.group(0), '').strip()
        else:
            item['description'] = full_text
            item['requirements'] = None
        
        # Salary
        salary_text = self.extract_text(
            response,
            '//span[@class="fs16 fc_aux"]//text()[contains(., "$")]',
            'xpath'
        )
        item['salary_raw'] = salary_text
        
        # Contract type
        contract_info = response.xpath(
            '//span[@class="fs13 fc_aux"]//text()'
        ).getall()
        
        for info in contract_info:
            info_lower = info.lower()
            if any(term in info_lower for term in ['tiempo completo', 'full time', 'part time']):
                item['contract_type'] = info
            elif any(term in info_lower for term in ['remoto', 'presencial', 'hibrido']):
                item['remote_type'] = info
        
        # Posted date
        date_text = self.extract_text(
            response,
            '//span[@class="fs13 fc_aux"][contains(text(), "Publicado")]//text()',
            'xpath'
        )
        if date_text:
            item['posted_date'] = date_text.replace('Publicado', '').strip()
        
        # Raw HTML (for debugging/reprocessing)
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

### src/scraper/spiders/bumeran_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import json
import re
import logging

logger = logging.getLogger(__name__)

class BumeranSpider(BaseJobSpider):
    name = 'bumeran'
    allowed_domains = ['bumeran.com', 'bumeran.com.mx', 'bumeran.com.ar']
    
    # URL patterns by country
    country_urls = {
        'MX': 'https://www.bumeran.com.mx',
        'AR': 'https://www.bumeran.com.ar'
    }
    
    # Tech categories
    tech_categories = [
        'informatica-telecomunicaciones',
        'tecnologia-sistemas',
        'desarrollo-programacion'
    ]
    
    def start_requests(self):
        if self.country not in self.country_urls:
            raise ValueError(f"Bumeran not available for country: {self.country}")
        
        base_url = self.country_urls[self.country]
        
        # Generate category URLs
        for category in self.tech_categories:
            category_url = f"{base_url}/empleos-{category}.html"
            yield Request(
                url=category_url,
                callback=self.parse_category,
                meta={'category': category, 'page': 1}
            )
    
    def parse_category(self, response):
        """Parse category listing page."""
        # Check if page uses React/JSON data
        scripts = response.xpath('//script[contains(text(), "__INITIAL_STATE__")]/text()').getall()
        
        if scripts:
            # Extract JSON data from script
            for script in scripts:
                match = re.search(r'__INITIAL_STATE__\s*=\s*({.*?});', script, re.DOTALL)
                if match:
                    try:
                        data = json.loads(match.group(1))
                        jobs = self.extract_jobs_from_json(data)
                        
                        for job in jobs:
                            yield Request(
                                url=job['url'],
                                callback=self.parse_job,
                                meta={'job_data': job}
                            )
                    except json.JSONDecodeError:
                        logger.error("Failed to parse JSON data")
        else:
            # Fallback to HTML parsing
            job_links = response.css('div.Card__CardContentWrapper a::attr(href)').getall()
            
            for link in job_links:
                absolute_url = self.build_absolute_url(response, link)
                yield Request(url=absolute_url, callback=self.parse_job)
        
        # Handle pagination
        current_page = response.meta.get('page', 1)
        if current_page < 10:  # Limit pages
            next_page = current_page + 1
            next_url = response.url.replace(
                f'.html', 
                f'-pagina-{next_page}.html'
            )
            yield Request(
                url=next_url,
                callback=self.parse_category,
                meta={
                    'category': response.meta.get('category'),
                    'page': next_page
                }
            )
    
    def extract_jobs_from_json(self, data):
        """Extract job data from JSON structure."""
        jobs = []
        
        # Navigate through possible JSON structures
        try:
            if 'results' in data:
                job_list = data['results'].get('jobs', [])
            elif 'jobs' in data:
                job_list = data['jobs']
            else:
                return jobs
            
            for job in job_list:
                job_info = {
                    'url': job.get('url', ''),
                    'title': job.get('title', ''),
                    'company': job.get('company', {}).get('name', ''),
                    'location': job.get('location', '')
                }
                if job_info['url']:
                    jobs.append(job_info)
        except Exception as e:
            logger.error(f"Error extracting jobs from JSON: {e}")
        
        return jobs
    
    def parse_job(self, response):
        """Parse individual job posting."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'bumeran'
        item['country'] = self.country
        item['url'] = response.url
        
        # Try to get pre-parsed data
        job_data = response.meta.get('job_data', {})
        
        # Title
        item['title'] = job_data.get('title') or self.extract_text(
            response,
            'h1[class*="Title"]::text',
            'css'
        )
        
        # Company
        item['company'] = job_data.get('company') or self.extract_text(
            response,
            'h2[class*="Company"]::text',
            'css'
        )
        
        # Location
        item['location'] = job_data.get('location') or self.extract_text(
            response,
            'span[class*="Location"]::text',
            'css'
        )
        
        # Description
        description_selectors = [
            'div[class*="Description"]',
            'div.detalle-aviso',
            'div#description'
        ]
        
        for selector in description_selectors:
            desc_elements = response.css(f'{selector} ::text').getall()
            if desc_elements:
                item['description'] = ' '.join(desc_elements)
                break
        
        # Requirements - often within description
        if item.get('description'):
            req_pattern = r'(?:requisitos|requerimientos|experiencia|competencias):(.*?)(?:beneficios|funciones|responsabilidades|$)'
            req_match = re.search(req_pattern, item['description'], re.IGNORECASE | re.DOTALL)
            
            if req_match:
                item['requirements'] = req_match.group(1).strip()
        
        # Salary
        salary_selectors = [
            'span[class*="Salary"]::text',
            'div[class*="salary"]::text',
            'span:contains("$")::text'
        ]
        
        for selector in salary_selectors:
            salary = response.css(selector).get()
            if salary and '$' in salary:
                item['salary_raw'] = salary
                break
        
        # Contract type and remote
        tags = response.css('span[class*="Tag"]::text').getall()
        for tag in tags:
            tag_lower = tag.lower()
            if any(term in tag_lower for term in ['tiempo completo', 'part time', 'freelance']):
                item['contract_type'] = tag
            elif any(term in tag_lower for term in ['remoto', 'presencial', 'hibrido']):
                item['remote_type'] = tag
        
        # Posted date
        date_text = response.css('span[class*="Date"]::text').get()
        if date_text:
            item['posted_date'] = date_text
        
        # Raw HTML
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

### src/scraper/spiders/elempleo_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import re
import logging
from urllib.parse import urljoin, urlparse, parse_qs

logger = logging.getLogger(__name__)

class ElempleoSpider(BaseJobSpider):
    name = 'elempleo'
    allowed_domains = ['elempleo.com']
    
    # Only available for Colombia
    country_urls = {
        'CO': 'https://www.elempleo.com'
    }
    
    # Tech-related categories in elempleo
    tech_categories = {
        'tecnologia': '1100',
        'sistemas': '1100',
        'informatica': '1100'
    }
    
    def start_requests(self):
        if self.country != 'CO':
            raise ValueError("elempleo.com is only available for Colombia (CO)")
        
        base_url = self.country_urls['CO']
        
        # Search URLs for technology jobs
        search_base = f"{base_url}/colombia/empleos"
        
        # Generate search requests
        for category_name, category_id in self.tech_categories.items():
            search_params = {
                'categoria': category_id,
                'pagina': 1
            }
            
            search_url = f"{search_base}?categoria={category_id}"
            
            yield Request(
                url=search_url,
                callback=self.parse_search_results,
                meta={
                    'category': category_name,
                    'page': 1
                }
            )
    
    def parse_search_results(self, response):
        """Parse search results from elempleo."""
        # Extract job cards
        job_cards = response.css('div.result-item')
        
        if not job_cards:
            # Try alternative selectors
            job_cards = response.css('article.js-offer')
        
        for card in job_cards:
            # Extract job URL
            job_link = card.css('a.js-offer-title::attr(href)').get()
            if not job_link:
                job_link = card.css('h2 a::attr(href)').get()
            
            if job_link:
                # Handle both relative and absolute URLs
                if not job_link.startswith('http'):
                    job_link = urljoin(response.url, job_link)
                
                # Extract basic info from card
                card_data = {
                    'title': card.css('h2 a::text').get(),
                    'company': card.css('span.info-company-name::text').get(),
                    'location': card.css('span.info-city::text').get()
                }
                
                yield Request(
                    url=job_link,
                    callback=self.parse_job,
                    meta={'card_data': card_data}
                )
        
        # Handle pagination
        current_page = response.meta.get('page', 1)
        next_page_link = response.css('a.js-btn-next::attr(href)').get()
        
        if not next_page_link:
            # Alternative pagination
            pagination_links = response.css('ul.pagination a::attr(href)').getall()
            for link in pagination_links:
                if f'pagina={current_page + 1}' in link:
                    next_page_link = link
                    break
        
        if next_page_link and current_page < 10:  # Limit to 10 pages
            next_url = urljoin(response.url, next_page_link)
            yield Request(
                url=next_url,
                callback=self.parse_search_results,
                meta={
                    'category': response.meta.get('category'),
                    'page': current_page + 1
                }
            )
    
    def parse_job(self, response):
        """Parse individual job posting from elempleo."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'elempleo'
        item['country'] = 'CO'
        item['url'] = response.url
        
        # Get card data if available
        card_data = response.meta.get('card_data', {})
        
        # Title
        item['title'] = self.extract_text(
            response,
            'h1.offer-title::text',
            'css'
        ) or card_data.get('title')
        
        # Company
        item['company'] = self.extract_text(
            response,
            'div.company-name a::text',
            'css'
        ) or self.extract_text(
            response,
            'span.offer-company::text',
            'css'
        ) or card_data.get('company')
        
        # Location
        location_parts = response.css('div.offer-location span::text').getall()
        if location_parts:
            item['location'] = ', '.join(location_parts)
        else:
            item['location'] = card_data.get('location')
        
        # Main content sections
        content_sections = response.css('div.offer-description')
        
        # Description
        description_html = content_sections.css('div#description').get()
        if description_html:
            # Clean HTML and extract text
            desc_text = re.sub(r'<[^>]+>', ' ', description_html)
            item['description'] = ' '.join(desc_text.split())
        
        # Requirements
        requirements_section = content_sections.css('div#requirements')
        if requirements_section:
            req_items = requirements_section.css('li::text').getall()
            if req_items:
                item['requirements'] = ' '.join(req_items)
            else:
                req_text = requirements_section.css('::text').getall()
                item['requirements'] = ' '.join(req_text)
        
        # If requirements not in separate section, try to extract from description
        if not item.get('requirements') and item.get('description'):
            req_pattern = r'(?:requisitos|perfil|experiencia|conocimientos):(.*?)(?:funciones|responsabilidades|ofrecemos|$)'
            req_match = re.search(req_pattern, item['description'], re.IGNORECASE | re.DOTALL)
            if req_match:
                item['requirements'] = req_match.group(1).strip()
        
        # Salary
        salary_element = response.css('span.offer-salary::text').get()
        if salary_element:
            item['salary_raw'] = salary_element
        else:
            # Look for salary in description
            salary_pattern = r'\$[\d.,]+ (?:millones|COP|pesos)'
            salary_match = re.search(salary_pattern, item.get('description', ''))
            if salary_match:
                item['salary_raw'] = salary_match.group(0)
        
        # Contract type
        contract_info = response.css('div.offer-info span::text').getall()
        for info in contract_info:
            info_lower = info.lower()
            if any(term in info_lower for term in ['contrato', 'tiempo completo', 'medio tiempo']):
                item['contract_type'] = info
            elif any(term in info_lower for term in ['remoto', 'presencial', 'teletrabajo']):
                item['remote_type'] = info
        
        # Posted date
        date_element = response.css('span.offer-date::text').get()
        if date_element:
            item['posted_date'] = date_element
        else:
            # Try to extract from meta tags
            date_meta = response.css('meta[property="article:published_time"]::attr(content)').get()
            if date_meta:
                item['posted_date'] = date_meta.split('T')[0]
        
        # Additional fields from structured data
        try:
            # Check for JSON-LD structured data
            json_ld = response.css('script[type="application/ld+json"]::text').get()
            if json_ld:
                import json
                data = json.loads(json_ld)
                
                # Extract additional info if available
                if isinstance(data, dict):
                    if 'title' in data and not item.get('title'):
                        item['title'] = data['title']
                    if 'hiringOrganization' in data and not item.get('company'):
                        item['company'] = data['hiringOrganization'].get('name')
                    if 'jobLocation' in data and not item.get('location'):
                        location = data['jobLocation']
                        if isinstance(location, dict):
                            item['location'] = location.get('address', {}).get('addressLocality')
        except Exception as e:
            logger.debug(f"Could not parse structured data: {e}")
        
        # Raw HTML
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

---

## 5. Extractor Module Files

### src/extractor/__init__.py
```python
from .pipeline import ExtractionPipeline
from .ner_extractor import NERExtractor
from .regex_patterns import RegexExtractor
from .esco_matcher import ESCOMatcher

__all__ = ['ExtractionPipeline', 'NERExtractor', 'RegexExtractor', 'ESCOMatcher']
```

### src/extractor/ner_extractor.py
```python
import spacy
from spacy.tokens import Doc, Span
from typing import List, Dict, Tuple, Optional
import logging
import os
from config.settings import get_settings

logger = logging.getLogger(__name__)

class NERExtractor:
    """Extract skills using Named Entity Recognition."""
    
    def __init__(self, model_path: Optional[str] = None):
        self.settings = get_settings()
        
        # Load spaCy model
        if model_path and os.path.exists(model_path):
            self.nlp = spacy.load(model_path)
            logger.info(f"Loaded custom NER model from {model_path}")
        else:
            # Load default Spanish model
            try:
                self.nlp = spacy.load("es_core_news_lg")
                logger.info("Loaded default Spanish model")
            except:
                logger.warning("Spanish model not found, downloading...")
                os.system("python -m spacy download es_core_news_lg")
                self.nlp = spacy.load("es_core_news_lg")
        
        # Add custom pipeline components
        self._add_tech_entity_ruler()
    
    def _add_tech_entity_ruler(self):
        """Add rule-based entity recognition for tech terms."""
        ruler = self.nlp.add_pipe("entity_ruler", before="ner")
        
        # Define patterns for common tech skills
        patterns = [
            # Programming languages
            {"label": "SKILL", "pattern": [{"LOWER": "python"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "java"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "javascript"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "typescript"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "c++"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "c#"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "php"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "ruby"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "go"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "golang"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "rust"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "kotlin"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "swift"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "r"}]},
            
            # Frameworks
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "react"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "react"}, {"LOWER": "native"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "angular"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "vue"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "vue"}, {"LOWER": "js"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "django"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "flask"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "spring"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "spring"}, {"LOWER": "boot"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "node"}, {"LOWER": "js"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "nodejs"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "express"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": ".net"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "laravel"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "rails"}]},
            
            # Databases
            {"label": "DATABASE", "pattern": [{"LOWER": "mysql"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "postgresql"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "postgres"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "mongodb"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "redis"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "elasticsearch"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "oracle"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "sql"}, {"LOWER": "server"}]},
            
            # Cloud & DevOps
            {"label": "PLATFORM", "pattern": [{"LOWER": "aws"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "amazon"}, {"LOWER": "web"}, {"LOWER": "services"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "azure"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "gcp"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "google"}, {"LOWER": "cloud"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "docker"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "kubernetes"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "jenkins"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "git"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "github"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "gitlab"}]},
            
            # Data & ML
            {"label": "SKILL", "pattern": [{"LOWER": "machine"}, {"LOWER": "learning"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "deep"}, {"LOWER": "learning"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "data"}, {"LOWER": "science"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "big"}, {"LOWER": "data"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "tensorflow"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "pytorch"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "scikit"}, {"LOWER": "learn"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "pandas"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "numpy"}]},
            
            # Methodologies
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "agile"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "scrum"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "kanban"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "devops"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "ci"}, {"LOWER": "cd"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "tdd"}]},
        ]
        
        # Add Spanish variations
        spanish_patterns = [
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "web"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "movil"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "móvil"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "base"}, {"LOWER": "de"}, {"LOWER": "datos"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "inteligencia"}, {"LOWER": "artificial"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "aprendizaje"}, {"LOWER": "automatico"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "aprendizaje"}, {"LOWER": "automático"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "ciencia"}, {"LOWER": "de"}, {"LOWER": "datos"}]},
        ]
        
        ruler.add_patterns(patterns + spanish_patterns)
    
    def extract(self, text: str, source_section: str = "unknown") -> List[Dict[str, any]]:
        """Extract skills from text using NER.
        
        Args:
            text: Text to extract skills from
            source_section: Section of job posting (title, description, requirements)
            
        Returns:
            List of extracted skills with metadata
        """
        if not text:
            return []
        
        # Process text with spaCy
        doc = self.nlp(text)
        
        extracted_skills = []
        seen_skills = set()
        
        # Extract entities
        for ent in doc.ents:
            if ent.label_ in ["SKILL", "FRAMEWORK", "DATABASE", "PLATFORM", "TOOL", "METHODOLOGY"]:
                skill_text = ent.text.lower().strip()
                
                # Skip if already seen
                if skill_text in seen_skills:
                    continue
                
                seen_skills.add(skill_text)
                
                extracted_skills.append({
                    "skill_text": skill_text,
                    "skill_type": "explicit",
                    "extraction_method": "ner",
                    "entity_label": ent.label_,
                    "confidence_score": 0.9,  # High confidence for NER
                    "source_section": source_section,
                    "span_start": ent.start_char,
                    "span_end": ent.end_char,
                    "context": text[max(0, ent.start_char-50):ent.end_char+50]
                })
        
        logger.debug(f"NER extracted {len(extracted_skills)} skills from {source_section}")
        
        return extracted_skills
    
    def extract_from_job(self, job_data: Dict[str, str]) -> List[Dict[str, any]]:
        """Extract skills from all sections of a job posting.
        
        Args:
            job_data: Dictionary with job fields (title, description, requirements)
            
        Returns:
            List of all extracted skills
        """
        all_skills = []
        
        # Extract from title
        if job_data.get('title'):
            title_skills = self.extract(job_data['title'], 'title')
            all_skills.extend(title_skills)
        
        # Extract from description
        if job_data.get('description'):
            desc_skills = self.extract(job_data['description'], 'description')
            all_skills.extend(desc_skills)
        
        # Extract from requirements
        if job_data.get('requirements'):
            req_skills = self.extract(job_data['requirements'], 'requirements')
            all_skills.extend(req_skills)
        
        return all_skills
```

### src/extractor/regex_patterns.py
```python
import re
from typing import List, Dict, Tuple
import logging

logger = logging.getLogger(__name__)

class RegexExtractor:
    """Extract skills using regular expressions."""
    
    def __init__(self):
        # Define regex patterns for skill extraction
        self.patterns = self._build_patterns()
    
    def _build_patterns(self) -> List[Tuple[str, re.Pattern, str]]:
        """Build regex patterns for skill extraction.
        
        Returns:
            List of tuples (pattern_name, compiled_regex, skill_type)
        """
        patterns = []
        
        # Experience patterns in Spanish
        experience_patterns = [
            (
                "experiencia_en",
                re.compile(
                    r"experiencia\s+(?:en|con)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "conocimientos_de",
                re.compile(
                    r"conocimientos?\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "manejo_de",
                re.compile(
                    r"manejo\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "tool"
            ),
            (
                "dominio_de",
                re.compile(
                    r"dominio\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "desarrollo_en",
                re.compile(
                    r"desarrollo\s+(?:en|con)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
        ]
        
        # Required skills patterns
        required_patterns = [
            (
                "requerimos",
                re.compile(
                    r"(?:requerimos|buscamos|necesitamos)\s+(?:personas?\s+con\s+)?(?:conocimientos?\s+en\s+)?([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+para\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "indispensable",
                re.compile(
                    r"(?:indispensable|fundamental|esencial)\s+(?:contar\s+con\s+)?(?:conocimientos?\s+en\s+)?([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
        ]
        
        # Technology stack patterns
        tech_stack_patterns = [
            (
                "tecnologias",
                re.compile(
                    r"(?:tecnologías?|herramientas?|lenguajes?)\s*:\s*([A-Za-z0-9\+\#\.\s,/\-\(\)]+?)(?:\n|$)",
                    re.IGNORECASE
                ),
                "mixed"
            ),
            (
                "stack_tecnologico",
                re.compile(
                    r"stack\s+(?:tecnológico|tech)\s*:\s*([A-Za-z0-9\+\#\.\s,/\-\(\)]+?)(?:\n|$)",
                    re.IGNORECASE
                ),
                "mixed"
            ),
        ]
        
        # List patterns (bullet points, numbered lists)
        list_patterns = [
            (
                "bullet_skills",
                re.compile(
                    r"(?:^|\n)\s*[\-\*\•]\s*([A-Za-z0-9\+\#\.\s]+?)(?:\n|$)",
                    re.MULTILINE
                ),
                "skill"
            ),
            (
                "numbered_skills",
                re.compile(
                    r"(?:^|\n)\s*\d+[\.\)]\s*([A-Za-z0-9\+\#\.\s]+?)(?:\n|$)",
                    re.MULTILINE
                ),
                "skill"
            ),
        ]
        
        # Certification patterns
        cert_patterns = [
            (
                "certificacion",
                re.compile(
                    r"(?:certificación|certificado)\s+(?:en\s+)?([A-Za-z0-9\+\#\.\s\-]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "certification"
            ),
        ]
        
        # Years of experience patterns
        experience_years_patterns = [
            (
                "años_experiencia",
                re.compile(
                    r"(\d+)\s*\+?\s*años?\s+(?:de\s+)?experiencia\s+(?:en\s+)?([A-Za-z0-9\+\#\.\s]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "skill_with_years"
            ),
        ]
        
        # Combine all patterns
        patterns.extend(experience_patterns)
        patterns.extend(required_patterns)
        patterns.extend(tech_stack_patterns)
        patterns.extend(list_patterns)
        patterns.extend(cert_patterns)
        patterns.extend(experience_years_patterns)
        
        return patterns
    
    def extract(self, text: str, source_section: str = "unknown") -> List[Dict[str, any]]:
        """Extract skills using regex patterns.
        
        Args:
            text: Text to extract skills from
            source_section: Section of job posting
            
        Returns:
            List of extracted skills with metadata
        """
        if not text:
            return []
        
        extracted_skills = []
        seen_skills = set()
        
        for pattern_name, regex, skill_type in self.patterns:
            matches = regex.finditer(text)
            
            for match in matches:
                if skill_type == "skill_with_years":
                    # Special handling for years of experience
                    years = match.group(1)
                    skill_text = match.group(2).strip().lower()
                    
                    if skill_text and skill_text not in seen_skills:
                        seen_skills.add(skill_text)
                        extracted_skills.append({
                            "skill_text": skill_text,
                            "skill_type": "explicit",
                            "extraction_method": "regex",
                            "pattern_name": pattern_name,
                            "confidence_score": 0.8,
                            "source_section": source_section,
                            "span_start": match.start(2),
                            "span_end": match.end(2),
                            "years_required": int(years),
                            "context": text[max(0, match.start()-30):match.end()+30]
                        })
                else:
                    # Normal skill extraction
                    skill_text = match.group(1).strip().lower()
                    
                    # Clean up extracted text
                    skill_text = self._clean_skill_text(skill_text)
                    
                    if skill_text and len(skill_text) > 1 and skill_text not in seen_skills:
                        # Additional validation
                        if self._is_valid_skill(skill_text):
                            seen_skills.add(skill_text)
                            
                            extracted_skills.append({
                                "skill_text": skill_text,
                                "skill_type": "explicit",
                                "extraction_method": "regex",
                                "pattern_name": pattern_name,
                                "confidence_score": 0.7,  # Lower than NER
                                "source_section": source_section,
                                "span_start": match.start(1),
                                "span_end": match.end(1),
                                "context": text[max(0, match.start()-30):match.end()+30]
                            })
        
        # Handle comma-separated lists within extracted skills
        expanded_skills = []
        for skill in extracted_skills:
            if ',' in skill['skill_text'] or ' y ' in skill['skill_text']:
                # Split and create individual skills
                parts = re.split(r'[,\s]+y\s+|,\s*', skill['skill_text'])
                for part in parts:
                    part = part.strip()
                    if part and self._is_valid_skill(part):
                        new_skill = skill.copy()
                        new_skill['skill_text'] = part
                        expanded_skills.append(new_skill)
            else:
                expanded_skills.append(skill)
        
        logger.debug(f"Regex extracted {len(expanded_skills)} skills from {source_section}")
        
        return expanded_skills
    
    def _clean_skill_text(self, text: str) -> str:
        """Clean extracted skill text.
        
        Args:
            text: Raw extracted text
            
        Returns:
            Cleaned skill text
        """
        # Remove common stop words at the beginning/end
        stop_words = [
            'el', 'la', 'los', 'las', 'un', 'una', 'de', 'del', 'al',
            'y', 'o', 'con', 'para', 'por', 'en', 'a'
        ]
        
        words = text.split()
        
        # Remove stop words from beginning
        while words and words[0].lower() in stop_words:
            words.pop(0)
        
        # Remove stop words from end
        while words and words[-1].lower() in stop_words:
            words.pop()
        
        cleaned = ' '.join(words)
        
        # Remove extra spaces and punctuation
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = re.sub(r'[^\w\s\+\#\.\-/]', '', cleaned)
        
        return cleaned.strip()
    
    def _is_valid_skill(self, skill_text: str) -> bool:
        """Validate if extracted text is likely a valid skill.
        
        Args:
            skill_text: Text to validate
            
        Returns:
            True if valid skill, False otherwise
        """
        # Check minimum length
        if len(skill_text) < 2:
            return False
        
        # Check if it's just numbers
        if skill_text.isdigit():
            return False
        
        # Check against blacklist of common false positives
        blacklist = [
            'años', 'año', 'experiencia', 'conocimiento', 'manejo',
            'desarrollo', 'persona', 'profesional', 'trabajo',
            'empresa', 'cliente', 'proyecto', 'equipo', 'area',
            'sistemas', 'tecnologia', 'informatica'  # Too generic
        ]
        
        if skill_text.lower() in blacklist:
            return False
        
        # Must contain at least one letter
        if not any(c.isalpha() for c in skill_text):
            return False
        
        return True
    
    def extract_from_job(self, job_data: Dict[str, str]) -> List[Dict[str, any]]:
        """Extract skills from all sections of a job posting.
        
        Args:
            job_data: Dictionary with job fields
            
        Returns:
            List of all extracted skills
        """
        all_skills = []
        
        # Process each section
        for section in ['title', 'description', 'requirements']:
            if job_data.get(section):
                section_skills = self.extract(job_data[section], section)
                all_skills.extend(section_skills)
        
        return all_skills
```

### src/extractor/esco_matcher.py
```python
import json
import logging
from typing import List, Dict, Optional, Set
from fuzzywuzzy import fuzz, process
import requests
from pathlib import Path
import yaml

logger = logging.getLogger(__name__)

class ESCOMatcher:
    """Match extracted skills to ESCO taxonomy."""
    
    def __init__(self, config_path: str = "config/esco_config.yaml"):
        self.config = self._load_config(config_path)
        self.skills_cache = {}
        self.local_mappings = self.config.get('tech_mappings', {})
        
        # Load local ESCO data if available
        self.local_esco_data = self._load_local_esco_data()
    
    def _load_config(self, config_path: str) -> dict:
        """Load ESCO configuration."""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config['esco']
        except Exception as e:
            logger.error(f"Failed to load ESCO config: {e}")
            return {}
    
    def _load_local_esco_data(self) -> Dict[str, Dict]:
        """Load local ESCO data files if available."""
        esco_data = {}
        
        # Try to load skills data
        skills_path = Path("data/esco/skills_es.csv")
        if skills_path.exists():
            try:
                import pandas as pd
                df = pd.read_csv(skills_path)
                
                for _, row in df.iterrows():
                    skill_uri = row.get('conceptUri', '')
                    esco_data[skill_uri] = {
                        'preferredLabel': row.get('preferredLabel', ''),
                        'altLabels': row.get('altLabels', '').split('|') if row.get('altLabels') else [],
                        'description': row.get('description', ''),
                        'skillType': row.get('skillType', '')
                    }
                
                logger.info(f"Loaded {len(esco_data)} ESCO skills from local file")
            except Exception as e:
                logger.error(f"Failed to load local ESCO data: {e}")
        
        return esco_data
    
    def match_skill(self, skill_text: str, threshold: float = 0.8) -> Optional[Dict[str, any]]:
        """Match a skill to ESCO taxonomy.
        
        Args:
            skill_text: Skill text to match
            threshold: Minimum similarity threshold (0-1)
            
        Returns:
            ESCO match data or None
        """
        skill_lower = skill_text.lower().strip()
        
        # Check direct mappings first
        if skill_lower in self.local_mappings:
            esco_uri = self.local_mappings[skill_lower]
            
            # Get details from local data or cache
            if esco_uri in self.local_esco_data:
                return {
                    'esco_uri': esco_uri,
                    'esco_preferred_label': self.local_esco_data[esco_uri]['preferredLabel'],
                    'match_type': 'direct',
                    'match_score': 1.0
                }
        
        # Try fuzzy matching against local data
        if self.local_esco_data:
            best_match = self._fuzzy_match_local(skill_text, threshold)
            if best_match:
                return best_match
        
        # Try API lookup (if configured and no local match)
        if self.config.get('base_url'):
            api_match = self._api_lookup(skill_text)
            if api_match:
                return api_match
        
        return None
    
    def _fuzzy_match_local(self, skill_text: str, threshold: float) -> Optional[Dict[str, any]]:
        """Fuzzy match against local ESCO data.
        
        Args:
            skill_text: Skill to match
            threshold: Minimum score threshold
            
        Returns:
            Best match or None
        """
        # Collect all labels for matching
        all_labels = []
        for uri, data in self.local_esco_data.items():
            # Add preferred label
            all_labels.append((data['preferredLabel'].lower(), uri, 'preferred'))
            
            # Add alternative labels
            for alt_label in data.get('altLabels', []):
                if alt_label:
                    all_labels.append((alt_label.lower(), uri, 'alternative'))
        
        # Find best match
        if all_labels:
            # Use token sort ratio for better matching of multi-word skills
            matches = process.extract(
                skill_text.lower(),
                [label[0] for label in all_labels],
                scorer=fuzz.token_sort_ratio,
                limit=3
            )
            
            for match_text, score in matches:
                if score >= threshold * 100:  # fuzzywuzzy uses 0-100 scale
                    # Find the corresponding URI
                    for label_text, uri, label_type in all_labels:
                        if label_text == match_text:
                            return {
                                'esco_uri': uri,
                                'esco_preferred_label': self.local_esco_data[uri]['preferredLabel'],
                                'match_type': f'fuzzy_{label_type}',
                                'match_score': score / 100.0,
                                'matched_text': match_text
                            }
        
        return None
    
    def _api_lookup(self, skill_text: str) -> Optional[Dict[str, any]]:
        """Look up skill using ESCO API.
        
        Args:
            skill_text: Skill to look up
            
        Returns:
            API match data or None
        """
        try:
            # Check cache first
            if skill_text in self.skills_cache:
                return self.skills_cache[skill_text]
            
            # Prepare API request
            api_url = f"{self.config['base_url']}{self.config['endpoints']['search']}"
            
            params = {
                'text': skill_text,
                'language': self.config['language']['primary'],
                'type': 'skill',
                'limit': 5
            }
            
            headers = {
                'Accept': 'application/json',
                'Accept-Language': self.config['language']['primary']
            }
            
            # Make request
            response = requests.get(api_url, params=params, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                if data.get('results'):
                    # Get first result
                    result = data['results'][0]
                    
                    match_data = {
                        'esco_uri': result.get('uri', ''),
                        'esco_preferred_label': result.get('preferredLabel', {}).get(
                            self.config['language']['primary'],
                            result.get('preferredLabel', {}).get('en', '')
                        ),
                        'match_type': 'api_search',
                        'match_score': result.get('score', 0.0)
                    }
                    
                    # Cache the result
                    self.skills_cache[skill_text] = match_data
                    
                    return match_data
            
        except Exception as e:
            logger.error(f"ESCO API lookup failed for '{skill_text}': {e}")
        
        return None
    
    def match_skills_batch(self, skills: List[str]) -> Dict[str, Optional[Dict]]:
        """Match multiple skills to ESCO taxonomy.
        
        Args:
            skills: List of skill texts
            
        Returns:
            Dictionary mapping skill text to ESCO match data
        """
        results = {}
        
        for skill in skills:
            match = self.match_skill(skill)
            results[skill] = match
        
        # Log statistics
        matched = sum(1 for v in results.values() if v is not None)
        logger.info(
            f"ESCO matching: {matched}/{len(skills)} skills matched "
            f"({matched/len(skills)*100:.1f}%)"
        )
        
        return results

### src/analyzer/dimension_reducer.py
```python
import logging
from typing import Tuple, Dict, Any, Optional
import numpy as np
from umap import UMAP
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import time

logger = logging.getLogger(__name__)

class DimensionReducer:
    """Reduce dimensionality of embeddings for visualization and clustering."""
    
    def __init__(self):
        self.reducers = {}
    
    def reduce_dimensions(self,
                         embeddings: np.ndarray,
                         method: str = 'umap',
                         n_components: int = 2,
                         **kwargs) -> Tuple[np.ndarray, Dict[str, Any]]:
        """Reduce dimensionality of embeddings.
        
        Args:
            embeddings: High-dimensional embeddings
            method: Reduction method ('umap', 'pca', 'tsne')
            n_components: Number of output dimensions
            **kwargs: Additional parameters for the method
            
        Returns:
            Tuple of (reduced embeddings, metadata)
        """
        logger.info(
            f"Reducing {embeddings.shape} to {n_components}D using {method}"
        )
        
        start_time = time.time()
        
        if method == 'umap':
            reduced, reducer = self._reduce_umap(embeddings, n_components, **kwargs)
        elif method == 'pca':
            reduced, reducer = self._reduce_pca(embeddings, n_components, **kwargs)
        elif method == 'tsne':
            reduced, reducer = self._reduce_tsne(embeddings, n_components, **kwargs)
        else:
            raise ValueError(f"Unknown reduction method: {method}")
        
        processing_time = time.time() - start_time
        
        # Store reducer for later use
        self.reducers[method] = reducer
        
        metadata = {
            'method': method,
            'n_components': n_components,
            'original_shape': embeddings.shape,
            'reduced_shape': reduced.shape,
            'processing_time': processing_time,
            'parameters': kwargs
        }
        
        logger.info(f"Dimension reduction complete in {processing_time:.2f}s")
        
        return reduced, metadata
    
    def _reduce_umap(self,
                    embeddings: np.ndarray,
                    n_components: int,
                    n_neighbors: int = 15,
                    min_dist: float = 0.1,
                    metric: str = 'cosine') -> Tuple[np.ndarray, UMAP]:
        """Reduce dimensions using UMAP.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            n_neighbors: UMAP n_neighbors parameter
            min_dist: UMAP min_dist parameter
            metric: Distance metric
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        reducer = UMAP(
            n_components=n_components,
            n_neighbors=n_neighbors,
            min_dist=min_dist,
            metric=metric,
            random_state=42,
            verbose=True
        )
        
        reduced = reducer.fit_transform(embeddings)
        
        return reduced, reducer
    
    def _reduce_pca(self,
                   embeddings: np.ndarray,
                   n_components: int) -> Tuple[np.ndarray, PCA]:
        """Reduce dimensions using PCA.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        reducer = PCA(n_components=n_components, random_state=42)
        reduced = reducer.fit_transform(embeddings)
        
        # Log explained variance
        if hasattr(reducer, 'explained_variance_ratio_'):
            total_variance = np.sum(reducer.explained_variance_ratio_)
            logger.info(f"PCA explained variance: {total_variance:.2%}")
        
        return reduced, reducer
    
    def _reduce_tsne(self,
                    embeddings: np.ndarray,
                    n_components: int,
                    perplexity: float = 30.0,
                    learning_rate: float = 200.0) -> Tuple[np.ndarray, TSNE]:
        """Reduce dimensions using t-SNE.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            perplexity: t-SNE perplexity parameter
            learning_rate: t-SNE learning rate
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        # For t-SNE, first reduce with PCA if dimensions > 50
        if embeddings.shape[1] > 50:
            logger.info("Pre-reducing with PCA for t-SNE")
            pca = PCA(n_components=50, random_state=42)
            embeddings = pca.fit_transform(embeddings)
        
        reducer = TSNE(
            n_components=n_components,
            perplexity=perplexity,
            learning_rate=learning_rate,
            random_state=42,
            verbose=1
        )
        
        reduced = reducer.fit_transform(embeddings)
        
        return reduced, reducer
    
    def transform_new_points(self,
                           embeddings: np.ndarray,
                           method: str = 'umap') -> np.ndarray:
        """Transform new points using existing reducer.
        
        Args:
            embeddings: New embeddings to transform
            method: Which reducer to use
            
        Returns:
            Transformed embeddings
        """
        if method not in self.reducers:
            raise ValueError(f"No {method} reducer available. Run reduce_dimensions first.")
        
        reducer = self.reducers[method]
        
        if method == 'tsne':
            logger.warning("t-SNE doesn't support transform. Returning original embeddings.")
            return embeddings
        
        return reducer.transform(embeddings)
    
    def create_embedding_map(self,
                           embeddings: np.ndarray,
                           labels: Optional[np.ndarray] = None,
                           skill_texts: Optional[list] = None) -> Dict[str, Any]:
        """Create a complete embedding map with 2D coordinates.
        
        Args:
            embeddings: Original embeddings
            labels: Cluster labels (optional)
            skill_texts: Skill names (optional)
            
        Returns:
            Dictionary with 2D coordinates and metadata
        """
        # Reduce to 2D
        coords_2d, metadata = self.reduce_dimensions(embeddings, method='umap', n_components=2)
        
        # Create map
        embedding_map = {
            'coordinates': coords_2d,
            'metadata': metadata
        }
        
        if labels is not None:
            embedding_map['labels'] = labels
        
        if skill_texts is not None:
            embedding_map['skills'] = skill_texts
        
        # Add statistics
        embedding_map['stats'] = {
            'x_range': (float(np.min(coords_2d[:, 0])), float(np.max(coords_2d[:, 0]))),
            'y_range': (float(np.min(coords_2d[:, 1])), float(np.max(coords_2d[:, 1]))),
            'center': (float(np.mean(coords_2d[:, 0])), float(np.mean(coords_2d[:, 1])))
        }
        
        return embedding_map

### src/analyzer/report_generator.py
```python
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import os
from pathlib import Path
from reportlab.lib import colors
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, PageBreak, Image
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from database.operations import DatabaseOperations

logger = logging.getLogger(__name__)

class ReportGenerator:
    """Generate PDF reports with analysis results."""
    
    def __init__(self, output_dir: str = None):
        self.output_dir = output_dir or "./outputs"
        os.makedirs(self.output_dir, exist_ok=True)
        self.db_ops = DatabaseOperations()
        self.styles = getSampleStyleSheet()
        self._add_custom_styles()
    
    def _add_custom_styles(self):
        """Add custom styles for the report."""
        self.styles.add(ParagraphStyle(
            name='CustomTitle',
            parent=self.styles['Title'],
            fontSize=24,
            textColor=colors.HexColor('#1a73e8'),
            spaceAfter=30
        ))
        
        self.styles.add(ParagraphStyle(
            name='SectionHeader',
            parent=self.styles['Heading1'],
            fontSize=16,
            textColor=colors.HexColor('#1a73e8'),
            spaceAfter=12
        ))
        
        self.styles.add(ParagraphStyle(
            name='SubsectionHeader',
            parent=self.styles['Heading2'],
            fontSize=14,
            textColor=colors.HexColor('#5f6368'),
            spaceAfter=10
        ))
    
    def generate_full_report(self, 
                           country: Optional[str] = None,
                           include_visualizations: bool = True) -> str:
        """Generate comprehensive analysis report.
        
        Args:
            country: Country code to filter by (optional)
            include_visualizations: Whether to include charts
            
        Returns:
            Path to generated report
        """
        # Create timestamp for filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        country_suffix = f"_{country}" if country else "_all"
        filename = f"labor_market_analysis{country_suffix}_{timestamp}.pdf"
        filepath = os.path.join(self.output_dir, filename)
        
        # Create document
        doc = SimpleDocTemplate(
            filepath,
            pagesize=A4,
            rightMargin=72,
            leftMargin=72,
            topMargin=72,
            bottomMargin=18
        )
        
        # Build content
        story = []
        
        # Title page
        story.extend(self._create_title_page(country))
        story.append(PageBreak())
        
        # Executive summary
        story.extend(self._create_executive_summary(country))
        story.append(PageBreak())
        
        # Skills analysis
        story.extend(self._create_skills_analysis(country, include_visualizations))
        story.append(PageBreak())
        
        # Clustering results
        story.extend(self._create_clustering_analysis(include_visualizations))
        story.append(PageBreak())
        
        # Temporal trends (if available)
        story.extend(self._create_temporal_analysis(country, include_visualizations))
        story.append(PageBreak())
        
        # Methodology
        story.extend(self._create_methodology_section())
        
        # Build PDF
        doc.build(story)
        
        logger.info(f"Report generated: {filepath}")
        return filepath
    
    def _create_title_page(self, country: Optional[str]) -> List:
        """Create title page elements."""
        elements = []
        
        # Title
        title_text = "Observatorio de Demanda Laboral Tecnológica"
        if country:
            country_names = {'CO': 'Colombia', 'MX': 'México', 'AR': 'Argentina'}
            title_text += f"\n{country_names.get(country, country)}"
        else:
            title_text += "\nAmérica Latina"
        
        elements.append(Paragraph(title_text, self.styles['CustomTitle']))
        elements.append(Spacer(1, 0.5*inch))
        
        # Subtitle
        subtitle = "Análisis Automatizado de Habilidades Técnicas"
        elements.append(Paragraph(subtitle, self.styles['Heading2']))
        elements.append(Spacer(1, 0.3*inch))
        
        # Date
        date_text = f"Fecha de generación: {datetime.now().strftime('%d de %B de %Y')}"
        elements.append(Paragraph(date_text, self.styles['Normal']))
        elements.append(Spacer(1, 2*inch))
        
        # Authors/Institution
        elements.append(Paragraph("Universidad XYZ", self.styles['Normal']))
        elements.append(Paragraph("Facultad de Ingeniería", self.styles['Normal']))
        
        return elements
    
    def _create_executive_summary(self, country: Optional[str]) -> List:
        """Create executive summary section."""
        elements = []
        
        elements.append(Paragraph("Resumen Ejecutivo", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get summary statistics
        stats = self.db_ops.get_skill_statistics(country)
        
        # Summary text
        summary_points = [
            f"Se analizaron un total de {stats.get('total_unique_skills', 0)} habilidades técnicas únicas.",
            f"Las 5 habilidades más demandadas son: {', '.join([s['skill'] for s in stats.get('top_skills', [])[:5]])}.",
            "El análisis revela una fuerte demanda de habilidades en desarrollo web, cloud computing y ciencia de datos.",
            "Se identificaron patrones emergentes en tecnologías de inteligencia artificial y DevOps."
        ]
        
        for point in summary_points:
            elements.append(Paragraph(f"• {point}", self.styles['Normal']))
            elements.append(Spacer(1, 0.1*inch))
        
        return elements
    
    def _create_skills_analysis(self, country: Optional[str], include_viz: bool) -> List:
        """Create skills analysis section."""
        elements = []
        
        elements.append(Paragraph("Análisis de Habilidades", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get skill statistics
        stats = self.db_ops.get_skill_statistics(country)
        
        # Top skills table
        elements.append(Paragraph("Top 20 Habilidades Más Demandadas", self.styles['SubsectionHeader']))
        
        if stats.get('top_skills'):
            # Create table data
            table_data = [['Posición', 'Habilidad', 'Frecuencia']]
            for i, skill_data in enumerate(stats['top_skills'][:20], 1):
                table_data.append([
                    str(i),
                    skill_data['skill'],
                    str(skill_data['count'])
                ])
            
            # Create table
            table = Table(table_data, colWidths=[1*inch, 3*inch, 1.5*inch])
            table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            
            elements.append(table)
            elements.append(Spacer(1, 0.3*inch))
        
        # Add visualization if requested
        if include_viz:
            viz_path = self._create_skill_frequency_chart(stats.get('top_skills', [])[:15])
            if viz_path:
                elements.append(Image(viz_path, width=6*inch, height=4*inch))
                elements.append(Spacer(1, 0.2*inch))
        
        return elements
    
    def _create_clustering_analysis(self, include_viz: bool) -> List:
        """Create clustering analysis section."""
        elements = []
        
        elements.append(Paragraph("Análisis de Clustering", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get latest clustering results
        # Note: This would need to be implemented in DatabaseOperations
        # For now, we'll use placeholder text
        
        elements.append(Paragraph(
            "El análisis de clustering identificó grupos coherentes de habilidades "
            "que típicamente aparecen juntas en las ofertas laborales:",
            self.styles['Normal']
        ))
        elements.append(Spacer(1, 0.1*inch))
        
        # Placeholder cluster descriptions
        clusters = [
            "Frontend Development: React, Vue.js, CSS, JavaScript, HTML5",
            "Backend Development: Node.js, Python, Django, Flask, API REST",
            "Data Science: Python, R, Machine Learning, SQL, Pandas",
            "DevOps: Docker, Kubernetes, AWS, CI/CD, Jenkins",
            "Mobile Development: React Native, Flutter, iOS, Android"
        ]
        
        for cluster in clusters:
            elements.append(Paragraph(f"• {cluster}", self.styles['Normal']))
            elements.append(Spacer(1, 0.05*inch))
        
        return elements
    
    def _create_temporal_analysis(self, country: Optional[str], include_viz: bool) -> List:
        """Create temporal trends analysis section."""
        elements = []
        
        elements.append(Paragraph("Tendencias Temporales", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        elements.append(Paragraph(
            "El análisis temporal permite identificar la evolución de la demanda "
            "de habilidades técnicas a lo largo del tiempo.",
            self.styles['Normal']
        ))
        
        # Placeholder for temporal analysis
        trends = [
            "Crecimiento sostenido en demanda de habilidades cloud (AWS, Azure)",
            "Aumento significativo en tecnologías de IA/ML en los últimos 6 meses",
            "Estabilidad en frameworks tradicionales (Spring, .NET)",
            "Emergencia de nuevas herramientas DevOps"
        ]
        
        elements.append(Spacer(1, 0.1*inch))
        for trend in trends:
            elements.append(Paragraph(f"• {trend}", self.styles['Normal']))
            elements.append(Spacer(1, 0.05*inch))
        
        return elements
    
    def _create_methodology_section(self) -> List:
        """Create methodology section."""
        elements = []
        
        elements.append(Paragraph("Metodología", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        methodology_text = """
        Este análisis se realizó mediante un pipeline automatizado que incluye:
        
        1. Web Scraping: Recolección automática de ofertas laborales de portales como 
           Computrabajo, Bumeran y elempleo.com.
        
        2. Extracción de Habilidades: Combinación de técnicas de NER (Named Entity Recognition) 
           y expresiones regulares para identificar menciones de habilidades técnicas.
        
        3. Enriquecimiento con LLM: Uso de modelos de lenguaje para identificar habilidades 
           implícitas y normalizar variaciones.
        
        4. Análisis Semántico: Generación de embeddings multilingües y clustering para 
           identificar grupos de habilidades relacionadas.
        
        5. Visualización: Generación de reportes estáticos con métricas agregadas y 
           visualizaciones interpretables.
        """
        
        elements.append(Paragraph(methodology_text, self.styles['Normal']))
        
        return elements
    
    def _create_skill_frequency_chart(self, top_skills: List[Dict[str, Any]]) -> Optional[str]:
        """Create skill frequency bar chart.
        
        Args:
            top_skills: List of top skills with counts
            
        Returns:
            Path to saved chart image
        """
        if not top_skills:
            return None
        
        try:
            # Prepare data
            skills = [s['skill'] for s in top_skills]
            counts = [s['count'] for s in top_skills]
            
            # Create figure
            plt.figure(figsize=(10, 6))
            
            # Create horizontal bar chart
            bars = plt.barh(skills, counts, color='#1a73e8')
            
            # Customize
            plt.xlabel('Número de Vacantes', fontsize=12)
            plt.title('Habilidades Más Demandadas', fontsize=14, fontweight='bold')
            plt.gca().invert_yaxis()  # Highest on top
            
            # Add value labels
            for i, bar in enumerate(bars):
                width = bar.get_width()
                plt.text(width + 1, bar.get_y() + bar.get_height()/2, 
                        f'{counts[i]}', 
                        ha='left', va='center')
            
            plt.tight_layout()
            
            # Save
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(self.output_dir, f"skill_frequency_{timestamp}.png")
            plt.savefig(filepath, dpi=300, bbox_inches='tight')
            plt.close()
            
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating skill frequency chart: {e}")
            return None
    
    def enrich_extracted_skills(self, extracted_skills: List[Dict]) -> List[Dict]:
        """Enrich extracted skills with ESCO matches.
        
        Args:
            extracted_skills: List of extracted skill dictionaries
            
        Returns:
            Enriched skill list
        """
        # Get unique skill texts
        unique_skills = list(set(skill['skill_text'] for skill in extracted_skills))
        
        # Match all unique skills
        esco_matches = self.match_skills_batch(unique_skills)
        
        # Enrich original skills
        enriched = []
        for skill in extracted_skills:
            enriched_skill = skill.copy()
            
            match = esco_matches.get(skill['skill_text'])
            if match:
                enriched_skill.update({
                    'esco_uri': match['esco_uri'],
                    'esco_preferred_label': match['esco_preferred_label'],
                    'esco_match_type': match['match_type'],
                    'esco_match_score': match['match_score']
                })
            
            enriched.append(enriched_skill)
        
        return enriched
```

### src/extractor/pipeline.py
```python
import logging
from typing import List, Dict, Optional
from database.operations import DatabaseOperations
from .ner_extractor import NERExtractor
from .regex_patterns import RegexExtractor
from .esco_matcher import ESCOMatcher
from config.settings import get_settings
import time

logger = logging.getLogger(__name__)

class ExtractionPipeline:
    """Main pipeline for skill extraction from job postings."""
    
    def __init__(self):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        
        # Initialize extractors
        logger.info("Initializing extraction components...")
        self.ner_extractor = NERExtractor()
        self.regex_extractor = RegexExtractor()
        self.esco_matcher = ESCOMatcher()
        
        logger.info("Extraction pipeline initialized")
    
    def process_batch(self, batch_size: int = 100) -> Dict[str, any]:
        """Process a batch of unprocessed jobs.
        
        Args:
            batch_size: Number of jobs to process
            
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'jobs_processed': 0,
            'skills_extracted': 0,
            'esco_matches': 0,
            'errors': 0
        }
        
        try:
            # Get unprocessed jobs
            jobs = self.db_ops.get_unprocessed_jobs(limit=batch_size)
            logger.info(f"Processing {len(jobs)} jobs")
            
            for job in jobs:
                try:
                    # Process individual job
                    skills = self.process_job(job)
                    
                    if skills:
                        # Save extracted skills
                        self.db_ops.insert_extracted_skills(
                            str(job.job_id),
                            skills
                        )
                        
                        # Mark job as processed
                        self.db_ops.mark_job_processed(str(job.job_id))
                        
                        # Update stats
                        stats['jobs_processed'] += 1
                        stats['skills_extracted'] += len(skills)
                        stats['esco_matches'] += sum(
                            1 for s in skills if s.get('esco_uri')
                        )
                    
                except Exception as e:
                    logger.error(f"Error processing job {job.job_id}: {e}")
                    stats['errors'] += 1
            
            # Calculate processing time
            stats['processing_time'] = time.time() - start_time
            stats['jobs_per_second'] = stats['jobs_processed'] / stats['processing_time'] if stats['processing_time'] > 0 else 0
            
            logger.info(
                f"Batch complete: {stats['jobs_processed']} jobs, "
                f"{stats['skills_extracted']} skills extracted, "
                f"{stats['esco_matches']} ESCO matches, "
                f"{stats['errors']} errors, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"Batch processing failed: {e}")
            raise
    
    def process_job(self, job) -> List[Dict[str, any]]:
        """Process a single job to extract skills.
        
        Args:
            job: Job object from database
            
        Returns:
            List of extracted and enriched skills
        """
        # Prepare job data
        job_data = {
            'title': job.title,
            'description': job.description,
            'requirements': job.requirements
        }
        
        # Extract skills using NER
        ner_skills = self.ner_extractor.extract_from_job(job_data)
        
        # Extract skills using regex
        regex_skills = self.regex_extractor.extract_from_job(job_data)
        
        # Combine and deduplicate
        all_skills = self._combine_skills(ner_skills, regex_skills)
        
        # Enrich with ESCO matches
        enriched_skills = self.esco_matcher.enrich_extracted_skills(all_skills)
        
        logger.debug(
            f"Job {job.job_id}: {len(ner_skills)} NER skills, "
            f"{len(regex_skills)} regex skills, "
            f"{len(enriched_skills)} total after deduplication"
        )
        
        return enriched_skills
    
    def _combine_skills(self, ner_skills: List[Dict], regex_skills: List[Dict]) -> List[Dict]:
        """Combine and deduplicate skills from different extractors.
        
        Args:
            ner_skills: Skills from NER
            regex_skills: Skills from regex
            
        Returns:
            Combined and deduplicated skill list
        """
        # Use skill text and source section as unique key
        seen_skills = {}
        
        # Process NER skills first (higher confidence)
        for skill in ner_skills:
            key = (skill['skill_text'], skill['source_section'])
            
            if key not in seen_skills:
                seen_skills[key] = skill
            else:
                # Update confidence if higher
                if skill['confidence_score'] > seen_skills[key]['confidence_score']:
                    seen_skills[key] = skill
        
        # Process regex skills
        for skill in regex_skills:
            key = (skill['skill_text'], skill['source_section'])
            
            if key not in seen_skills:
                seen_skills[key] = skill
            else:
                # Merge extraction methods
                existing = seen_skills[key]
                if existing['extraction_method'] != skill['extraction_method']:
                    existing['extraction_method'] = 'ner+regex'
                    existing['confidence_score'] = min(0.95, existing['confidence_score'] + 0.1)
        
        return list(seen_skills.values())
    
    def run_continuous(self, batch_size: int = 100, wait_time: int = 60):
        """Run extraction continuously.
        
        Args:
            batch_size: Jobs per batch
            wait_time: Seconds to wait between batches
        """
        logger.info(f"Starting continuous extraction (batch_size={batch_size}, wait={wait_time}s)")
        
        while True:
            try:
                stats = self.process_batch(batch_size)
                
                if stats['jobs_processed'] == 0:
                    logger.info(f"No jobs to process. Waiting {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    # Short pause between batches
                    time.sleep(5)
                    
            except KeyboardInterrupt:
                logger.info("Extraction stopped by user")
                break
            except Exception as e:
                logger.error(f"Extraction error: {e}")
                logger.info(f"Waiting {wait_time}s before retry...")
                time.sleep(wait_time)
```

---

## 6. LLM Processor Module Files

### src/llm_processor/__init__.py
```python
from .llm_handler import LLMHandler
from .prompts import PromptGenerator
from .esco_normalizer import ESCONormalizer
from .validator import SkillValidator

__all__ = ['LLMHandler', 'PromptGenerator', 'ESCONormalizer', 'SkillValidator']
```

### src/llm_processor/llm_handler.py
```python
import logging
from typing import List, Dict, Optional, Any
from llama_cpp import Llama
import openai
from config.settings import get_settings
import json
import time

logger = logging.getLogger(__name__)

class LLMHandler:
    """Handle LLM interactions for skill processing."""
    
    def __init__(self, model_type: str = "local"):
        self.settings = get_settings()
        self.model_type = model_type
        
        if model_type == "local":
            self._init_local_model()
        elif model_type == "openai":
            self._init_openai()
        else:
            raise ValueError(f"Unknown model type: {model_type}")
    
    def _init_local_model(self):
        """Initialize local LLaMA/Mistral model."""
        try:
            logger.info(f"Loading local model from {self.settings.llm_model_path}")
            
            self.model = Llama(
                model_path=self.settings.llm_model_path,
                n_ctx=self.settings.llm_context_length,
                n_gpu_layers=self.settings.llm_n_gpu_layers,
                verbose=False
            )
            
            logger.info("Local model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load local model: {e}")
            raise
    
    def _init_openai(self):
        """Initialize OpenAI API client."""
        if not self.settings.openai_api_key:
            raise ValueError("OpenAI API key not configured")
        
        openai.api_key = self.settings.openai_api_key
        self.model_name = self.settings.openai_model
        logger.info(f"OpenAI API initialized with model {self.model_name}")
    
    def process_skills(self, 
                      job_data: Dict[str, Any],
                      extracted_skills: List[Dict[str, Any]],
                      prompt_template: str) -> Dict[str, Any]:
        """Process skills using LLM.
        
        Args:
            job_data: Job information
            extracted_skills: Initially extracted skills
            prompt_template: Formatted prompt template
            
        Returns:
            LLM response with processed skills
        """
        start_time = time.time()
        
        try:
            if self.model_type == "local":
                response = self._process_local(prompt_template)
            else:
                response = self._process_openai(prompt_template)
            
            # Parse response
            result = self._parse_response(response)
            
            # Add metadata
            result['processing_time'] = time.time() - start_time
            result['model_type'] = self.model_type
            result['model_name'] = getattr(self, 'model_name', 'local_mistral')
            
            return result
            
        except Exception as e:
            logger.error(f"LLM processing failed: {e}")
            raise
    
    def _process_local(self, prompt: str) -> str:
        """Process using local model.
        
        Args:
            prompt: Complete prompt
            
        Returns:
            Model response
        """
        response = self.model(
            prompt,
            max_tokens=self.settings.llm_max_tokens,
            temperature=self.settings.llm_temperature,
            stop=["</response>", "\n\n\n"],
            echo=False
        )
        
        return response['choices'][0]['text']
    
    def _process_openai(self, prompt: str) -> str:
        """Process using OpenAI API.
        
        Args:
            prompt: Complete prompt
            
        Returns:
            Model response
        """
        response = openai.ChatCompletion.create(
            model=self.model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert in analyzing job postings and extracting technical skills. Respond in Spanish."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            max_tokens=self.settings.llm_max_tokens,
            temperature=self.settings.llm_temperature
        )
        
        return response.choices[0].message.content
    
    def _parse_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response.
        
        Args:
            response: Raw LLM response
            
        Returns:
            Parsed skills data
        """
        # Try to extract JSON if present
        if "```json" in response:
            # Extract JSON block
            start = response.find("```json") + 7
            end = response.find("```", start)
            json_str = response[start:end].strip()
            
            try:
                return json.loads(json_str)
            except json.JSONDecodeError:
                logger.warning("Failed to parse JSON from response")
        
        # Fallback: Parse structured text response
        result = {
            "explicit_skills": [],
            "implicit_skills": [],
            "normalized_skills": [],
            "deduplicated_skills": []
        }
        
        lines = response.strip().split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            
            # Detect sections
            if "habilidades explícitas" in line.lower():
                current_section = "explicit_skills"
            elif "habilidades implícitas" in line.lower():
                current_section = "implicit_skills"
            elif "habilidades normalizadas" in line.lower():
                current_section = "normalized_skills"
            elif "deduplicadas" in line.lower():
                current_section = "deduplicated_skills"
            elif line and current_section and (line.startswith('-') or line.startswith('*')):
                # Extract skill from bullet point
                skill_text = line.lstrip('-*').strip()
                
                # Parse skill with reasoning if present
                if ':' in skill_text:
                    skill, reasoning = skill_text.split(':', 1)
                    result[current_section].append({
                        "skill": skill.strip(),
                        "reasoning": reasoning.strip()
                    })
                else:
                    result[current_section].append({
                        "skill": skill_text
                    })
        
        return result
```

### src/llm_processor/prompts.py
```python
from typing import List, Dict, Any
import json

class PromptGenerator:
    """Generate prompts for LLM skill processing."""
    
    def __init__(self):
        self.templates = self._load_templates()
    
    def _load_templates(self) -> Dict[str, str]:
        """Load prompt templates."""
        return {
            "skill_processing": """Eres un experto en análisis de ofertas laborales tecnológicas en América Latina.

Datos de la vacante:
- Título: {job_title}
- Descripción: {job_description}
- Requisitos: {job_requirements}

Habilidades extraídas inicialmente:
{extracted_skills}

Por favor, realiza las siguientes tareas:

1. **Validación de habilidades explícitas**: Revisa las habilidades extraídas y confirma cuáles son realmente habilidades técnicas relevantes.

2. **Detección de habilidades implícitas**: Basándote en el contexto del puesto, identifica habilidades técnicas que serían necesarias pero no están mencionadas explícitamente. Por ejemplo:
   - Si menciona "desarrollo web full stack" → probablemente necesite Git, bases de datos, APIs REST
   - Si menciona "análisis de datos" → probablemente necesite SQL, Python/R, visualización
   - Si menciona "DevOps" → probablemente necesite CI/CD, contenedores, cloud

3. **Normalización con ESCO**: Para cada habilidad, proporciona la forma normalizada según estándares internacionales:
   - Usa nombres estándar (ej: "JS" → "JavaScript", "React.js" → "React")
   - Mantén el español cuando sea apropiado
   - Agrupa variaciones (ej: "MySQL/MariaDB" → "MySQL")

4. **Deduplicación**: Elimina habilidades duplicadas o redundantes:
   - Combina variaciones del mismo concepto
   - Elimina términos demasiado genéricos
   - Mantén el término más específico cuando haya jerarquía

Responde en el siguiente formato JSON:
```json
{{
  "explicit_skills": [
    {{"skill": "nombre", "confidence": 0.9, "original": "texto_original"}}
  ],
  "implicit_skills": [
    {{"skill": "nombre", "confidence": 0.7, "reasoning": "justificación"}}
  ],
  "normalized_skills": [
    {{"original": "skill_original", "normalized": "skill_normalizado", "esco_match": "posible_uri"}}
  ],
  "deduplicated_skills": [
    {{"skill": "nombre_final", "type": "explicit|implicit", "category": "programming|database|framework|tool|soft_skill"}}
  ]
}}
```""",

            "simple_inference": """Analiza esta oferta de trabajo y extrae SOLO las habilidades técnicas implícitas que no están mencionadas pero serían necesarias.

Título: {job_title}
Descripción resumida: {job_summary}
Habilidades ya identificadas: {known_skills}

Lista únicamente las habilidades técnicas implícitas con su justificación:
""",

            "normalization": """Normaliza las siguientes habilidades técnicas según estándares internacionales y la taxonomía ESCO:

Habilidades a normalizar:
{skills_list}

Para cada habilidad, proporciona:
- Forma normalizada
- Categoría (lenguaje/framework/base de datos/herramienta/metodología)
- Término ESCO equivalente si existe

Responde en formato de lista:
""",

            "deduplication": """Elimina duplicados y agrupa las siguientes habilidades:

Habilidades:
{skills_list}

Reglas:
- Combina variaciones del mismo concepto (ej: JS, JavaScript, javascript → JavaScript)
- Mantén el término más específico cuando haya jerarquía
- Elimina términos genéricos si hay específicos

Lista final sin duplicados:
"""
        }
    
    def generate_skill_processing_prompt(self, 
                                       job_data: Dict[str, Any],
                                       extracted_skills: List[Dict[str, Any]]) -> str:
        """Generate prompt for complete skill processing.
        
        Args:
            job_data: Job information
            extracted_skills: Initially extracted skills
            
        Returns:
            Formatted prompt
        """
        # Format extracted skills
        skills_text = self._format_extracted_skills(extracted_skills)
        
        prompt = self.templates["skill_processing"].format(
            job_title=job_data.get('title', 'No especificado'),
            job_description=job_data.get('description', 'No especificado'),
            job_requirements=job_data.get('requirements', 'No especificado'),
            extracted_skills=skills_text
        )
        
        return prompt
    
    def generate_inference_prompt(self,
                                job_title: str,
                                job_summary: str,
                                known_skills: List[str]) -> str:
        """Generate prompt for implicit skill inference.
        
        Args:
            job_title: Job title
            job_summary: Brief job description
            known_skills: Already identified skills
            
        Returns:
            Formatted prompt
        """
        skills_list = ", ".join(known_skills) if known_skills else "Ninguna"
        
        return self.templates["simple_inference"].format(
            job_title=job_title,
            job_summary=job_summary,
            known_skills=skills_list
        )
    
    def generate_normalization_prompt(self, skills: List[str]) -> str:
        """Generate prompt for skill normalization.
        
        Args:
            skills: List of skills to normalize
            
        Returns:
            Formatted prompt
        """
        skills_text = "\n".join(f"- {skill}" for skill in skills)
        
        return self.templates["normalization"].format(
            skills_list=skills_text
        )
    
    def generate_deduplication_prompt(self, skills: List[str]) -> str:
        """Generate prompt for skill deduplication.
        
        Args:
            skills: List of skills to deduplicate
            
        Returns:
            Formatted prompt
        """
        skills_text = "\n".join(f"- {skill}" for skill in skills)
        
        return self.templates["deduplication"].format(
            skills_list=skills_text
        )
    
    def _format_extracted_skills(self, skills: List[Dict[str, Any]]) -> str:
        """Format extracted skills for prompt.
        
        Args:
            skills: List of skill dictionaries
            
        Returns:
            Formatted text
        """
        formatted_skills = []
        
        # Group by source section
        by_section = {}
        for skill in skills:
            section = skill.get('source_section', 'unknown')
            if section not in by_section:
                by_section[section] = []
            by_section[section].append(skill)
        
        # Format each section
        for section, section_skills in by_section.items():
            formatted_skills.append(f"\nDe {section}:")
            for skill in section_skills:
                method = skill.get('extraction_method', 'unknown')
                confidence = skill.get('confidence_score', 0)
                text = skill.get('skill_text', '')
                
                formatted_skills.append(
                    f"  - {text} (método: {method}, confianza: {confidence:.2f})"
                )
        
        return "\n".join(formatted_skills)

### src/llm_processor/esco_normalizer.py
```python
import logging
from typing import List, Dict, Any, Optional
from fuzzywuzzy import fuzz
import yaml

logger = logging.getLogger(__name__)

class ESCONormalizer:
    """Normalize skills using ESCO taxonomy with LLM assistance."""
    
    def __init__(self, config_path: str = "config/esco_config.yaml"):
        self.config = self._load_config(config_path)
        self.normalization_rules = self._build_normalization_rules()
    
    def _load_config(self, config_path: str) -> dict:
        """Load ESCO configuration."""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config['esco']
        except Exception as e:
            logger.error(f"Failed to load ESCO config: {e}")
            return {}
    
    def _build_normalization_rules(self) -> Dict[str, str]:
        """Build normalization rules from config and common variations."""
        rules = {}
        
        # Load from config
        if 'tech_mappings' in self.config:
            rules.update(self.config['tech_mappings'])
        
        # Add common variations
        common_variations = {
            # JavaScript variations
            'js': 'JavaScript',
            'javascript': 'JavaScript',
            'java script': 'JavaScript',
            
            # Python variations
            'python3': 'Python',
            'python 3': 'Python',
            'python2': 'Python',
            
            # React variations
            'reactjs': 'React',
            'react.js': 'React',
            'react js': 'React',
            'react native': 'React Native',
            
            # Node variations
            'nodejs': 'Node.js',
            'node js': 'Node.js',
            'node': 'Node.js',
            
            # Database variations
            'postgres': 'PostgreSQL',
            'mysql': 'MySQL',
            'maria db': 'MariaDB',
            'mariadb': 'MariaDB',
            'mongo': 'MongoDB',
            'mongo db': 'MongoDB',
            
            # .NET variations
            'dotnet': '.NET',
            'dot net': '.NET',
            '.net core': '.NET Core',
            'asp.net': 'ASP.NET',
            
            # Other common variations
            'c++': 'C++',
            'c#': 'C#',
            'c sharp': 'C#',
            'objective c': 'Objective-C',
            'obj-c': 'Objective-C',
            
            # Spanish to English mappings
            'base de datos': 'Database',
            'desarrollo web': 'Web Development',
            'desarrollo móvil': 'Mobile Development',
            'aprendizaje automático': 'Machine Learning',
            'inteligencia artificial': 'Artificial Intelligence',
            'ciencia de datos': 'Data Science',
            'computación en la nube': 'Cloud Computing',
            'control de versiones': 'Version Control',
        }
        
        # Convert all keys to lowercase
        for key, value in common_variations.items():
            rules[key.lower()] = value
        
        return rules
    
    def normalize_skill(self, skill: str) -> Dict[str, Any]:
        """Normalize a single skill.
        
        Args:
            skill: Raw skill text
            
        Returns:
            Normalized skill data
        """
        skill_lower = skill.lower().strip()
        
        # Direct mapping
        if skill_lower in self.normalization_rules:
            return {
                'original': skill,
                'normalized': self.normalization_rules[skill_lower],
                'method': 'direct_mapping',
                'confidence': 1.0
            }
        
        # Try fuzzy matching
        best_match = None
        best_score = 0
        
        for pattern, normalized in self.normalization_rules.items():
            score = fuzz.ratio(skill_lower, pattern)
            if score > best_score and score >= 85:
                best_score = score
                best_match = normalized
        
        if best_match:
            return {
                'original': skill,
                'normalized': best_match,
                'method': 'fuzzy_mapping',
                'confidence': best_score / 100.0
            }
        
        # Category-based normalization
        normalized = self._category_normalization(skill)
        if normalized != skill:
            return {
                'original': skill,
                'normalized': normalized,
                'method': 'category_rules',
                'confidence': 0.8
            }
        
        # No normalization found
        return {
            'original': skill,
            'normalized': skill,
            'method': 'no_normalization',
            'confidence': 0.5
        }
    
    def _category_normalization(self, skill: str) -> str:
        """Apply category-based normalization rules.
        
        Args:
            skill: Skill to normalize
            
        Returns:
            Normalized skill
        """
        skill_lower = skill.lower()
        
        # Framework detection
        if 'framework' in skill_lower:
            skill = skill.replace('framework', '').replace('Framework', '').strip()
        
        # Version removal for certain technologies
        version_patterns = [
            (r'python\s*\d+\.?\d*', 'Python'),
            (r'java\s*\d+', 'Java'),
            (r'angular\s*\d+', 'Angular'),
            (r'vue\s*\d+', 'Vue.js'),
            (r'react\s*\d+', 'React'),
        ]
        
        import re
        for pattern, replacement in version_patterns:
            if re.search(pattern, skill_lower):
                return replacement
        
        # Capitalize properly
        # Special cases
        special_cases = {
            'mysql': 'MySQL',
            'postgresql': 'PostgreSQL',
            'mongodb': 'MongoDB',
            'javascript': 'JavaScript',
            'typescript': 'TypeScript',
            'graphql': 'GraphQL',
            'nodejs': 'Node.js',
            'reactjs': 'React',
            'vuejs': 'Vue.js',
        }
        
        if skill_lower in special_cases:
            return special_cases[skill_lower]
        
        # Default: capitalize first letter of each word
        return ' '.join(word.capitalize() for word in skill.split())
    
    def normalize_skills_batch(self, skills: List[str]) -> List[Dict[str, Any]]:
        """Normalize multiple skills.
        
        Args:
            skills: List of skills to normalize
            
        Returns:
            List of normalized skill data
        """
        normalized = []
        
        for skill in skills:
            result = self.normalize_skill(skill)
            normalized.append(result)
        
        return normalized
    
    def deduplicate_normalized_skills(self, normalized_skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Deduplicate normalized skills.
        
        Args:
            normalized_skills: List of normalized skill dictionaries
            
        Returns:
            Deduplicated list
        """
        # Group by normalized form
        skill_groups = {}
        
        for skill_data in normalized_skills:
            normalized = skill_data['normalized']
            
            if normalized not in skill_groups:
                skill_groups[normalized] = {
                    'normalized': normalized,
                    'originals': [],
                    'best_confidence': 0,
                    'methods': set()
                }
            
            skill_groups[normalized]['originals'].append(skill_data['original'])
            skill_groups[normalized]['methods'].add(skill_data['method'])
            skill_groups[normalized]['best_confidence'] = max(
                skill_groups[normalized]['best_confidence'],
                skill_data['confidence']
            )
        
        # Convert back to list
        deduplicated = []
        for normalized, group_data in skill_groups.items():
            deduplicated.append({
                'normalized': normalized,
                'original_variations': group_data['originals'],
                'confidence': group_data['best_confidence'],
                'methods': list(group_data['methods'])
            })
        
        return deduplicated

### src/llm_processor/validator.py
```python
import logging
from typing import List, Dict, Any, Set
import re

logger = logging.getLogger(__name__)

class SkillValidator:
    """Validate and filter skills."""
    
    def __init__(self):
        self.blacklist = self._build_blacklist()
        self.whitelist = self._build_whitelist()
        self.categories = self._build_categories()
    
    def _build_blacklist(self) -> Set[str]:
        """Build blacklist of non-skill terms."""
        return {
            # Generic terms
            'experiencia', 'años', 'año', 'conocimiento', 'conocimientos',
            'habilidad', 'habilidades', 'capacidad', 'competencia',
            'desarrollo', 'trabajo', 'empresa', 'cliente', 'proyecto',
            'equipo', 'persona', 'profesional', 'área', 'proceso',
            
            # Too generic tech terms
            'tecnología', 'tecnologías', 'sistema', 'sistemas',
            'informática', 'computación', 'software', 'hardware',
            'programación', 'desarrollo de software',
            
            # Common words
            'bueno', 'excelente', 'alto', 'nivel', 'manejo',
            'uso', 'gestión', 'administración', 'análisis',
            
            # Methodologies too generic
            'metodología', 'metodologías', 'mejores prácticas',
            
            # Soft skills (we focus on technical)
            'comunicación', 'liderazgo', 'trabajo en equipo',
            'responsabilidad', 'proactividad', 'creatividad'
        }
    
    def _build_whitelist(self) -> Set[str]:
        """Build whitelist of known valid skills."""
        return {
            # Programming languages
            'python', 'java', 'javascript', 'typescript', 'c++', 'c#',
            'php', 'ruby', 'go', 'golang', 'rust', 'kotlin', 'swift',
            'objective-c', 'r', 'scala', 'perl', 'matlab', 'julia',
            
            # Frameworks and libraries
            'react', 'angular', 'vue', 'django', 'flask', 'spring',
            'express', 'laravel', 'rails', 'fastapi', 'nextjs',
            '.net', 'asp.net', 'tensorflow', 'pytorch', 'keras',
            
            # Databases
            'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',
            'cassandra', 'dynamodb', 'oracle', 'sql server', 'sqlite',
            'neo4j', 'couchdb', 'firebase', 'supabase',
            
            # Cloud and DevOps
            'aws', 'azure', 'gcp', 'google cloud', 'docker', 'kubernetes',
            'jenkins', 'terraform', 'ansible', 'puppet', 'chef',
            'gitlab', 'github', 'bitbucket', 'circleci', 'travis',
            
            # Tools and platforms
            'git', 'jira', 'confluence', 'slack', 'linux', 'windows',
            'macos', 'ubuntu', 'centos', 'debian', 'nginx', 'apache',
            'grafana', 'prometheus', 'elasticsearch', 'kibana',
            
            # Data and ML
            'machine learning', 'deep learning', 'data science',
            'big data', 'spark', 'hadoop', 'kafka', 'airflow',
            'pandas', 'numpy', 'scikit-learn', 'matplotlib',
            
            # Mobile
            'android', 'ios', 'react native', 'flutter', 'xamarin',
            'swift', 'kotlin', 'objective-c', 'cordova', 'ionic',
            
            # Other
            'api', 'rest', 'graphql', 'websocket', 'microservices',
            'ci/cd', 'agile', 'scrum', 'kanban', 'tdd', 'bdd'
        }
    
    def _build_categories(self) -> Dict[str, Set[str]]:
        """Build skill categories."""
        return {
            'programming_language': {
                'python', 'java', 'javascript', 'typescript', 'c++', 'c#',
                'php', 'ruby', 'go', 'rust', 'kotlin', 'swift', 'r'
            },
            'framework': {
                'react', 'angular', 'vue', 'django', 'flask', 'spring',
                'express', 'laravel', 'rails', '.net', 'nextjs'
            },
            'database': {
                'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',
                'cassandra', 'dynamodb', 'oracle', 'sql server'
            },
            'cloud_platform': {
                'aws', 'azure', 'gcp', 'google cloud', 'heroku', 'digitalocean'
            },
            'devops_tool': {
                'docker', 'kubernetes', 'jenkins', 'terraform', 'ansible',
                'git', 'github', 'gitlab', 'ci/cd'
            },
            'data_ml': {
                'machine learning', 'deep learning', 'tensorflow', 'pytorch',
                'pandas', 'numpy', 'spark', 'hadoop'
            },
            'mobile': {
                'android', 'ios', 'react native', 'flutter', 'xamarin'
            },
            'methodology': {
                'agile', 'scrum', 'kanban', 'devops', 'tdd', 'bdd'
            }
        }
    
    def validate_skill(self, skill: str) -> Dict[str, Any]:
        """Validate a single skill.
        
        Args:
            skill: Skill to validate
            
        Returns:
            Validation result
        """
        skill_lower = skill.lower().strip()
        
        # Check blacklist
        if skill_lower in self.blacklist:
            return {
                'skill': skill,
                'valid': False,
                'reason': 'blacklisted',
                'confidence': 0.0
            }
        
        # Check whitelist
        if skill_lower in self.whitelist:
            category = self._categorize_skill(skill_lower)
            return {
                'skill': skill,
                'valid': True,
                'reason': 'whitelisted',
                'category': category,
                'confidence': 1.0
            }
        
        # Length check
        if len(skill_lower) < 2 or len(skill_lower) > 50:
            return {
                'skill': skill,
                'valid': False,
                'reason': 'invalid_length',
                'confidence': 0.0
            }
        
        # Pattern validation
        if not self._validate_pattern(skill_lower):
            return {
                'skill': skill,
                'valid': False,
                'reason': 'invalid_pattern',
                'confidence': 0.0
            }
        
        # Partial matches in whitelist
        for valid_skill in self.whitelist:
            if valid_skill in skill_lower or skill_lower in valid_skill:
                category = self._categorize_skill(valid_skill)
                return {
                    'skill': skill,
                    'valid': True,
                    'reason': 'partial_match',
                    'category': category,
                    'confidence': 0.7
                }
        
        # If not in whitelist but passes other checks
        return {
            'skill': skill,
            'valid': True,
            'reason': 'pattern_valid',
            'category': 'uncategorized',
            'confidence': 0.5
        }
    
    def _validate_pattern(self, skill: str) -> bool:
        """Validate skill pattern.
        
        Args:
            skill: Skill to validate
            
        Returns:
            True if pattern is valid
        """
        # Must contain at least one letter
        if not re.search(r'[a-zA-Z]', skill):
            return False
        
        # Check for suspicious patterns
        suspicious_patterns = [
            r'^\d+,  # Only numbers
            r'^[^a-zA-Z0-9\s\.\+\#\-/]+,  # Only special chars
            r'\b(años?|experiencia|conocimientos?)\b',  # Common non-skills
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, skill, re.IGNORECASE):
                return False
        
        return True
    
    def _categorize_skill(self, skill: str) -> str:
        """Categorize a skill.
        
        Args:
            skill: Skill to categorize
            
        Returns:
            Category name
        """
        skill_lower = skill.lower()
        
        for category, skills in self.categories.items():
            if skill_lower in skills:
                return category
        
        # Try partial matches
        for category, skills in self.categories.items():
            for cat_skill in skills:
                if cat_skill in skill_lower or skill_lower in cat_skill:
                    return category
        
        return 'uncategorized'
    
    def validate_skills_batch(self, skills: List[str]) -> List[Dict[str, Any]]:
        """Validate multiple skills.
        
        Args:
            skills: List of skills to validate
            
        Returns:
            List of validation results
        """
        results = []
        
        for skill in skills:
            result = self.validate_skill(skill)
            results.append(result)
        
        # Log statistics
        valid_count = sum(1 for r in results if r['valid'])
        logger.info(
            f"Skill validation: {valid_count}/{len(skills)} valid "
            f"({valid_count/len(skills)*100:.1f}%)"
        )
        
        return results
    
    def filter_valid_skills(self, skills: List[str], min_confidence: float = 0.5) -> List[str]:
        """Filter only valid skills.
        
        Args:
            skills: List of skills
            min_confidence: Minimum confidence threshold
            
        Returns:
            List of valid skills
        """
        results = self.validate_skills_batch(skills)
        
        valid_skills = [
            r['skill'] for r in results
            if r['valid'] and r['confidence'] >= min_confidence
        ]
        
        return valid_skills

### src/llm_processor/pipeline.py
```python
import logging
from typing import List, Dict, Any
from database.operations import DatabaseOperations
from .llm_handler import LLMHandler
from .prompts import PromptGenerator
from .esco_normalizer import ESCONormalizer
from .validator import SkillValidator
from config.settings import get_settings
import time

logger = logging.getLogger(__name__)

class LLMProcessingPipeline:
    """Main pipeline for LLM-based skill processing."""
    
    def __init__(self, model_type: str = "local"):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        
        logger.info("Initializing LLM processing components...")
        self.llm_handler = LLMHandler(model_type)
        self.prompt_generator = PromptGenerator()
        self.normalizer = ESCONormalizer()
        self.validator = SkillValidator()
        
        logger.info("LLM processing pipeline initialized")
    
    def process_batch(self, batch_size: int = 50) -> Dict[str, Any]:
        """Process a batch of jobs with extracted skills.
        
        Args:
            batch_size: Number of jobs to process
            
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'jobs_processed': 0,
            'skills_enhanced': 0,
            'implicit_skills_found': 0,
            'skills_normalized': 0,
            'errors': 0
        }
        
        try:
            # Get jobs with extracted skills needing LLM processing
            jobs_data = self.db_ops.get_extracted_skills_for_processing(limit=batch_size)
            logger.info(f"Processing {len(jobs_data)} jobs with LLM")
            
            for job_data in jobs_data:
                try:
                    # Process individual job
                    enhanced_skills = self.process_job(job_data)
                    
                    if enhanced_skills:
                        # Save enhanced skills
                        self.db_ops.insert_enhanced_skills(
                            job_data['job_id'],
                            enhanced_skills
                        )
                        
                        # Update statistics
                        stats['jobs_processed'] += 1
                        stats['skills_enhanced'] += len(enhanced_skills)
                        stats['implicit_skills_found'] += sum(
                            1 for s in enhanced_skills 
                            if s.get('skill_type') == 'implicit'
                        )
                        stats['skills_normalized'] += sum(
                            1 for s in enhanced_skills
                            if s.get('normalized_skill') != s.get('original_skill_text')
                        )
                    
                except Exception as e:
                    logger.error(f"Error processing job {job_data['job_id']}: {e}")
                    stats['errors'] += 1
            
            # Calculate processing time
            stats['processing_time'] = time.time() - start_time
            stats['jobs_per_minute'] = (stats['jobs_processed'] / stats['processing_time']) * 60 if stats['processing_time'] > 0 else 0
            
            logger.info(
                f"LLM batch complete: {stats['jobs_processed']} jobs, "
                f"{stats['skills_enhanced']} skills enhanced, "
                f"{stats['implicit_skills_found']} implicit skills found, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"LLM batch processing failed: {e}")
            raise
    
    def process_job(self, job_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Process a single job with LLM.
        
        Args:
            job_data: Job data with extracted skills
            
        Returns:
            List of enhanced skills
        """
        # Generate prompt
        prompt = self.prompt_generator.generate_skill_processing_prompt(
            {
                'title': job_data['job_title'],
                'description': job_data['job_description'],
                'requirements': job_data['job_requirements']
            },
            job_data['extracted_skills']
        )
        
        # Process with LLM
        llm_response = self.llm_handler.process_skills(
            job_data,
            job_data['extracted_skills'],
            prompt
        )
        
        # Process LLM response
        enhanced_skills = self._process_llm_response(
            llm_response,
            job_data['extracted_skills']
        )
        
        return enhanced_skills
    
    def _process_llm_response(self, 
                            llm_response: Dict[str, Any],
                            original_skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process LLM response into enhanced skills.
        
        Args:
            llm_response: Response from LLM
            original_skills: Original extracted skills
            
        Returns:
            List of enhanced skill records
        """
        enhanced_skills = []
        
        # Create a mapping of original skills
        original_map = {
            skill['skill_text'].lower(): skill 
            for skill in original_skills
        }
        
        # Process explicit skills (validated by LLM)
        for skill_data in llm_response.get('explicit_skills', []):
            skill_text = skill_data.get('skill', '').lower()
            
            # Validate skill
            validation = self.validator.validate_skill(skill_text)
            if not validation['valid']:
                continue
            
            # Normalize skill
            normalization = self.normalizer.normalize_skill(skill_text)
            
            enhanced_skills.append({
                'original_skill_text': skill_data.get('original', skill_text),
                'normalized_skill': normalization['normalized'],
                'skill_type': 'explicit',
                'esco_concept_uri': None,  # To be matched later
                'esco_preferred_label': None,
                'llm_confidence': skill_data.get('confidence', 0.8),
                'llm_reasoning': None,
                'is_duplicate': False,
                'llm_model': llm_response.get('model_name', 'unknown')
            })
        
        # Process implicit skills (inferred by LLM)
        for skill_data in llm_response.get('implicit_skills', []):
            skill_text = skill_data.get('skill', '').lower()
            
            # Validate skill
            validation = self.validator.validate_skill(skill_text)
            if not validation['valid']:
                continue
            
            # Normalize skill
            normalization = self.normalizer.normalize_skill(skill_text)
            
            enhanced_skills.append({
                'original_skill_text': None,
                'normalized_skill': normalization['normalized'],
                'skill_type': 'implicit',
                'esco_concept_uri': None,
                'esco_preferred_label': None,
                'llm_confidence': skill_data.get('confidence', 0.7),
                'llm_reasoning': skill_data.get('reasoning', ''),
                'is_duplicate': False,
                'llm_model': llm_response.get('model_name', 'unknown')
            })
        
        # Deduplicate skills
        enhanced_skills = self._deduplicate_skills(enhanced_skills)
        
        return enhanced_skills
    
    def _deduplicate_skills(self, skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Deduplicate enhanced skills.
        
        Args:
            skills: List of enhanced skills
            
        Returns:
            Deduplicated list with duplicates marked
        """
        seen_normalized = {}
        deduplicated = []
        
        # Sort by confidence (highest first)
        sorted_skills = sorted(
            skills, 
            key=lambda x: x.get('llm_confidence', 0),
            reverse=True
        )
        
        for skill in sorted_skills:
            normalized = skill['normalized_skill'].lower()
            
            if normalized not in seen_normalized:
                # First occurrence
                seen_normalized[normalized] = skill
                deduplicated.append(skill)
            else:
                # Duplicate found
                skill['is_duplicate'] = True
                skill['duplicate_of_id'] = id(seen_normalized[normalized])
                deduplicated.append(skill)
        
        return deduplicated
    
    def run_continuous(self, batch_size: int = 50, wait_time: int = 60):
        """Run LLM processing continuously.
        
        Args:
            batch_size: Jobs per batch
            wait_time: Seconds to wait between batches
        """
        logger.info(f"Starting continuous LLM processing (batch_size={batch_size}, wait={wait_time}s)")
        
        while True:
            try:
                stats = self.process_batch(batch_size)
                
                if stats['jobs_processed'] == 0:
                    logger.info(f"No jobs to process. Waiting {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    # Short pause between batches
                    time.sleep(10)
                    
            except KeyboardInterrupt:
                logger.info("LLM processing stopped by user")
                break
            except Exception as e:
                logger.error(f"LLM processing error: {e}")
                logger.info(f"Waiting {wait_time}s before retry...")
                time.sleep(wait_time)
```

---

## 7. Embedder Module Files

### src/embedder/__init__.py
```python
from .vectorizer import SkillVectorizer
from .model_loader import EmbeddingModelLoader
from .batch_processor import BatchProcessor

__all__ = ['SkillVectorizer', 'EmbeddingModelLoader', 'BatchProcessor']

### src/embedder/vectorizer.py
```python
import logging
from typing import List, Dict, Any, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import torch

logger = logging.getLogger(__name__)

class SkillVectorizer:
    """Generate embeddings for skills."""
    
    def __init__(self, model_name: str = None):
        self.model_name = model_name or "intfloat/multilingual-e5-base"
        self.model = None
        self.device = None
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialize the embedding model."""
        try:
            # Check for GPU availability
            if torch.cuda.is_available():
                self.device = 'cuda'
                logger.info(f"Using GPU for embeddings")
            else:
                self.device = 'cpu'
                logger.info(f"Using CPU for embeddings")
            
            # Load model
            logger.info(f"Loading embedding model: {self.model_name}")
            self.model = SentenceTransformer(self.model_name, device=self.device)
            
            # Get embedding dimension
            self.embedding_dim = self.model.get_sentence_embedding_dimension()
            logger.info(f"Model loaded. Embedding dimension: {self.embedding_dim}")
            
        except Exception as e:
            logger.error(f"Failed to initialize embedding model: {e}")
            raise
    
    def vectorize(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """Generate embeddings for a list of texts.
        
        Args:
            texts: List of texts to embed
            batch_size: Batch size for processing
            
        Returns:
            Array of embeddings
        """
        if not texts:
            return np.array([])
        
        try:
            # For E5 models, add instruction prefix
            if 'e5' in self.model_name.lower():
                texts = [f"query: {text}" for text in texts]
            
            # Generate embeddings
            embeddings = self.model.encode(
                texts,
                batch_size=batch_size,
                show_progress_bar=len(texts) > 100,
                convert_to_numpy=True,
                normalize_embeddings=True  # L2 normalization
            )
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Vectorization failed: {e}")
            raise
    
    def vectorize_single(self, text: str) -> np.ndarray:
        """Generate embedding for a single text.
        
        Args:
            text: Text to embed
            
        Returns:
            Embedding vector
        """
        return self.vectorize([text])[0]
    
    def compute_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """Compute cosine similarity between two embeddings.
        
        Args:
            embedding1: First embedding
            embedding2: Second embedding
            
        Returns:
            Cosine similarity score
        """
        # Assuming normalized embeddings, cosine similarity is just dot product
        return float(np.dot(embedding1, embedding2))
    
    def find_similar_skills(self, 
                          query_embedding: np.ndarray,
                          skill_embeddings: List[Dict[str, Any]],
                          top_k: int = 10,
                          threshold: float = 0.7) -> List[Dict[str, Any]]:
        """Find similar skills based on embeddings.
        
        Args:
            query_embedding: Query skill embedding
            skill_embeddings: List of skill embedding dictionaries
            top_k: Number of top results to return
            threshold: Minimum similarity threshold
            
        Returns:
            List of similar skills with scores
        """
        similarities = []
        
        for skill_data in skill_embeddings:
            embedding = skill_data['embedding']
            similarity = self.compute_similarity(query_embedding, embedding)
            
            if similarity >= threshold:
                similarities.append({
                    'skill': skill_data['skill_text'],
                    'similarity': similarity,
                    'embedding_id': skill_data.get('embedding_id')
                })
        
        # Sort by similarity
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        return similarities[:top_k]

### src/embedder/model_loader.py
```python
import logging
from typing import Dict, Any, Optional
import os
import json
from pathlib import Path
import torch
from transformers import AutoModel, AutoTokenizer
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)

class EmbeddingModelLoader:
    """Load and manage embedding models."""
    
    # Model configurations
    MODEL_CONFIGS = {
        'multilingual-e5-base': {
            'name': 'intfloat/multilingual-e5-base',
            'dimension': 768,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': 'query: '
        },
        'multilingual-e5-large': {
            'name': 'intfloat/multilingual-e5-large',
            'dimension': 1024,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': 'query: '
        },
        'beto': {
            'name': 'dccuchile/bert-base-spanish-wwm-cased',
            'dimension': 768,
            'max_length': 512,
            'type': 'transformers',
            'instruction_prefix': None
        },
        'labse': {
            'name': 'sentence-transformers/LaBSE',
            'dimension': 768,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': None
        },
        'multilingual-minilm': {
            'name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
            'dimension': 384,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': None
        }
    }
    
    def __init__(self, cache_dir: Optional[str] = None):
        self.cache_dir = cache_dir or "./data/cache/embeddings"
        os.makedirs(self.cache_dir, exist_ok=True)
        self.loaded_models = {}
    
    def load_model(self, model_key: str) -> Any:
        """Load an embedding model.
        
        Args:
            model_key: Key from MODEL_CONFIGS
            
        Returns:
            Loaded model
        """
        if model_key in self.loaded_models:
            logger.info(f"Model {model_key} already loaded")
            return self.loaded_models[model_key]
        
        if model_key not in self.MODEL_CONFIGS:
            raise ValueError(f"Unknown model key: {model_key}")
        
        config = self.MODEL_CONFIGS[model_key]
        
        try:
            if config['type'] == 'sentence-transformers':
                model = self._load_sentence_transformer(config)
            elif config['type'] == 'transformers':
                model = self._load_transformers_model(config)
            else:
                raise ValueError(f"Unknown model type: {config['type']}")
            
            self.loaded_models[model_key] = model
            logger.info(f"Successfully loaded model: {model_key}")
            
            return model
            
        except Exception as e:
            logger.error(f"Failed to load model {model_key}: {e}")
            raise
    
    def _load_sentence_transformer(self, config: Dict[str, Any]) -> SentenceTransformer:
        """Load a sentence-transformers model.
        
        Args:
            config: Model configuration
            
        Returns:
            Loaded model
        """
        model = SentenceTransformer(
            config['name'],
            cache_folder=self.cache_dir
        )
        
        # Verify dimension
        actual_dim = model.get_sentence_embedding_dimension()
        if actual_dim != config['dimension']:
            logger.warning(
                f"Model dimension mismatch: expected {config['dimension']}, "
                f"got {actual_dim}"
            )
        
        return model
    
    def _load_transformers_model(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Load a transformers model with tokenizer.
        
        Args:
            config: Model configuration
            
        Returns:
            Dictionary with model and tokenizer
        """
        model = AutoModel.from_pretrained(
            config['name'],
            cache_dir=self.cache_dir
        )
        
        tokenizer = AutoTokenizer.from_pretrained(
            config['name'],
            cache_dir=self.cache_dir
        )
        
        return {
            'model': model,
            'tokenizer': tokenizer,
            'config': config
        }
    
    def get_model_info(self, model_key: str) -> Dict[str, Any]:
        """Get information about a model.
        
        Args:
            model_key: Model key
            
        Returns:
            Model information
        """
        if model_key not in self.MODEL_CONFIGS:
            raise ValueError(f"Unknown model key: {model_key}")
        
        return self.MODEL_CONFIGS[model_key].copy()
    
    def list_available_models(self) -> Dict[str, Dict[str, Any]]:
        """List all available models.
        
        Returns:
            Dictionary of model configurations
        """
        return self.MODEL_CONFIGS.copy()
    
    def download_all_models(self):
        """Download all configured models."""
        logger.info("Downloading all configured embedding models...")
        
        for model_key in self.MODEL_CONFIGS:
            try:
                logger.info(f"Downloading {model_key}...")
                self.load_model(model_key)
            except Exception as e:
                logger.error(f"Failed to download {model_key}: {e}")
    
    def clear_cache(self):
        """Clear model cache."""
        import shutil
        
        if os.path.exists(self.cache_dir):
            shutil.rmtree(self.cache_dir)
            os.makedirs(self.cache_dir)
            logger.info("Model cache cleared")
        
        self.loaded_models.clear()

### src/embedder/batch_processor.py
```python
import logging
from typing import List, Dict, Any
import numpy as np
from database.operations import DatabaseOperations
from .vectorizer import SkillVectorizer
from config.settings import get_settings
import time
from tqdm import tqdm

logger = logging.getLogger(__name__)

class BatchProcessor:
    """Process skill embeddings in batches."""
    
    def __init__(self, model_name: str = None):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        self.vectorizer = SkillVectorizer(model_name)
        self.batch_size = self.settings.embedding_batch_size
    
    def process_all_skills(self) -> Dict[str, Any]:
        """Process all skills that need embeddings.
        
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'skills_processed': 0,
            'embeddings_created': 0,
            'errors': 0,
            'processing_time': 0
        }
        
        try:
            # Get skills without embeddings
            skills = self.db_ops.get_unique_skills_for_embedding()
            logger.info(f"Found {len(skills)} skills without embeddings")
            
            if not skills:
                return stats
            
            # Process in batches
            for i in tqdm(range(0, len(skills), self.batch_size), desc="Embedding skills"):
                batch = skills[i:i + self.batch_size]
                
                try:
                    # Generate embeddings
                    embeddings = self.vectorizer.vectorize(batch)
                    
                    # Prepare data for database
                    embedding_data = []
                    for skill_text, embedding in zip(batch, embeddings):
                        embedding_data.append({
                            'skill_text': skill_text,
                            'embedding': embedding.tolist(),  # Convert to list for pgvector
                            'model_name': self.vectorizer.model_name,
                            'model_version': '1.0'
                        })
                    
                    # Save to database
                    self.db_ops.insert_skill_embeddings(embedding_data)
                    
                    stats['skills_processed'] += len(batch)
                    stats['embeddings_created'] += len(embedding_data)
                    
                except Exception as e:
                    logger.error(f"Error processing batch {i//self.batch_size}: {e}")
                    stats['errors'] += 1
            
            stats['processing_time'] = time.time() - start_time
            
            logger.info(
                f"Embedding complete: {stats['skills_processed']} skills processed, "
                f"{stats['embeddings_created']} embeddings created, "
                f"{stats['errors']} errors, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"Embedding batch processing failed: {e}")
            raise
    
    def update_embeddings(self, force: bool = False) -> Dict[str, Any]:
        """Update embeddings for new or modified skills.
        
        Args:
            force: Force re-embedding of all skills
            
        Returns:
            Update statistics
        """
        if force:
            logger.warning("Force update requested - this will re-embed all skills")
            # Clear existing embeddings
            # Note: Implement this method in DatabaseOperations if needed
        
        return self.process_all_skills()
    
    def compute_similarity_matrix(self, skill_list: List[str] = None) -> np.ndarray:
        """Compute similarity matrix for skills.
        
        Args:
            skill_list: List of skills to compare (None for all)
            
        Returns:
            Similarity matrix
        """
        # Get embeddings from database
        all_embeddings = self.db_ops.get_all_embeddings()
        
        if skill_list:
            # Filter to requested skills
            embeddings_data = [
                e for e in all_embeddings 
                if e['skill_text'] in skill_list
            ]
        else:
            embeddings_data = all_embeddings
        
        if not embeddings_data:
            return np.array([])
        
        # Extract embedding vectors
        embeddings = np.array([e['embedding'] for e in embeddings_data])
        
        # Compute similarity matrix
        # Since embeddings are normalized, cosine similarity is just dot product
        similarity_matrix = np.dot(embeddings, embeddings.T)
        
        return similarity_matrix
    
    def find_duplicate_skills(self, threshold: float = 0.95) -> List[Dict[str, Any]]:
        """Find potential duplicate skills based on embedding similarity.
        
        Args:
            threshold: Similarity threshold for duplicates
            
        Returns:
            List of potential duplicates
        """
        all_embeddings = self.db_ops.get_all_embeddings()
        
        if len(all_embeddings) < 2:
            return []
        
        duplicates = []
        
        # Compare all pairs
        for i in range(len(all_embeddings)):
            for j in range(i + 1, len(all_embeddings)):
                skill1 = all_embeddings[i]
                skill2 = all_embeddings[j]
                
                similarity = self.vectorizer.compute_similarity(
                    np.array(skill1['embedding']),
                    np.array(skill2['embedding'])
                )
                
                if similarity >= threshold:
                    duplicates.append({
                        'skill1': skill1['skill_text'],
                        'skill2': skill2['skill_text'],
                        'similarity': similarity
                    })
        
        # Sort by similarity
        duplicates.sort(key=lambda x: x['similarity'], reverse=True)
        
        logger.info(f"Found {len(duplicates)} potential duplicate skill pairs")
        
        return duplicates
    
    def get_skill_recommendations(self, 
                                job_skills: List[str],
                                top_k: int = 10) -> List[Dict[str, Any]]:
        """Get skill recommendations based on current job skills.
        
        Args:
            job_skills: Current skills in job
            top_k: Number of recommendations
            
        Returns:
            List of recommended skills with scores
        """
        # Get embeddings for input skills
        input_embeddings = self.vectorizer.vectorize(job_skills)
        
        # Average the embeddings to get job profile
        job_profile = np.mean(input_embeddings, axis=0)
        
        # Get all skill embeddings
        all_embeddings = self.db_ops.get_all_embeddings()
        
        # Find similar skills
        recommendations = []
        
        for skill_data in all_embeddings:
            # Skip if already in job skills
            if skill_data['skill_text'] in job_skills:
                continue
            
            similarity = self.vectorizer.compute_similarity(
                job_profile,
                np.array(skill_data['embedding'])
            )
            
            recommendations.append({
                'skill': skill_data['skill_text'],
                'score': similarity
            })
        
        # Sort and return top K
        recommendations.sort(key=lambda x: x['score'], reverse=True)
        
        return recommendations[:top_k]

---

## 8. Analyzer Module Files

### src/analyzer/__init__.py
```python
from .clustering import SkillClusterer
from .dimension_reducer import DimensionReducer
from .report_generator import ReportGenerator
from .visualizations import VisualizationGenerator

__all__ = ['SkillClusterer', 'DimensionReducer', 'ReportGenerator', 'VisualizationGenerator']

### src/analyzer/clustering.py
```python
import logging
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from sklearn.cluster import HDBSCAN, KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score
import pandas as pd
from database.operations import DatabaseOperations
from config.settings import get_settings

logger = logging.getLogger(__name__)

class SkillClusterer:
    """Perform clustering on skill embeddings."""
    
    def __init__(self):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        self.clustering_method = 'hdbscan'
    
    def cluster_skills(self, 
                      embeddings: np.ndarray,
                      skill_texts: List[str],
                      method: str = 'hdbscan',
                      **kwargs) -> Dict[str, Any]:
        """Cluster skills based on embeddings.
        
        Args:
            embeddings: Skill embedding matrix
            skill_texts: List of skill texts
            method: Clustering method ('hdbscan' or 'kmeans')
            **kwargs: Additional parameters for clustering
            
        Returns:
            Clustering results
        """
        if len(embeddings) < 2:
            logger.warning("Not enough data points for clustering")
            return {}
        
        logger.info(f"Clustering {len(embeddings)} skills using {method}")
        
        if method == 'hdbscan':
            results = self._cluster_hdbscan(embeddings, skill_texts, **kwargs)
        elif method == 'kmeans':
            results = self._cluster_kmeans(embeddings, skill_texts, **kwargs)
        else:
            raise ValueError(f"Unknown clustering method: {method}")
        
        # Calculate metrics
        results['metrics'] = self._calculate_metrics(embeddings, results['labels'])
        
        # Characterize clusters
        results['cluster_info'] = self._characterize_clusters(
            results['labels'],
            skill_texts
        )
        
        return results
    
    def _cluster_hdbscan(self, 
                        embeddings: np.ndarray,
                        skill_texts: List[str],
                        min_cluster_size: int = None,
                        min_samples: int = None) -> Dict[str, Any]:
        """Perform HDBSCAN clustering.
        
        Args:
            embeddings: Embedding matrix
            skill_texts: Skill texts
            min_cluster_size: Minimum cluster size
            min_samples: Minimum samples for core points
            
        Returns:
            Clustering results
        """
        # Use settings if not provided
        min_cluster_size = min_cluster_size or self.settings.cluster_min_size
        min_samples = min_samples or self.settings.cluster_min_samples
        
        # Perform clustering
        clusterer = HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            metric='euclidean',
            cluster_selection_method='eom',
            prediction_data=True
        )
        
        labels = clusterer.fit_predict(embeddings)
        
        # Get cluster persistence (stability)
        cluster_persistence = clusterer.cluster_persistence_
        
        results = {
            'method': 'hdbscan',
            'labels': labels,
            'n_clusters': len(set(labels)) - (1 if -1 in labels else 0),
            'n_noise': list(labels).count(-1),
            'parameters': {
                'min_cluster_size': min_cluster_size,
                'min_samples': min_samples
            },
            'cluster_persistence': cluster_persistence,
            'clusterer': clusterer
        }
        
        logger.info(
            f"HDBSCAN found {results['n_clusters']} clusters "
            f"with {results['n_noise']} noise points"
        )
        
        return results
    
    def _cluster_kmeans(self,
                       embeddings: np.ndarray,
                       skill_texts: List[str],
                       n_clusters: int = 20) -> Dict[str, Any]:
        """Perform K-means clustering.
        
        Args:
            embeddings: Embedding matrix
            skill_texts: Skill texts
            n_clusters: Number of clusters
            
        Returns:
            Clustering results
        """
        # Perform clustering
        clusterer = KMeans(
            n_clusters=n_clusters,
            n_init=10,
            max_iter=300,
            random_state=42
        )
        
        labels = clusterer.fit_predict(embeddings)
        
        results = {
            'method': 'kmeans',
            'labels': labels,
            'n_clusters': n_clusters,
            'n_noise': 0,
            'parameters': {
                'n_clusters': n_clusters
            },
            'cluster_centers': clusterer.cluster_centers_,
            'inertia': clusterer.inertia_,
            'clusterer': clusterer
        }
        
        logger.info(f"K-means created {n_clusters} clusters")
        
        return results
    
    def _calculate_metrics(self, embeddings: np.ndarray, labels: np.ndarray) -> Dict[str, float]:
        """Calculate clustering quality metrics.
        
        Args:
            embeddings: Embedding matrix
            labels: Cluster labels
            
        Returns:
            Dictionary of metrics
        """
        metrics = {}
        
        # Remove noise points for metrics
        mask = labels >= 0
        if np.sum(mask) < 2:
            logger.warning("Not enough clustered points for metrics")
            return metrics
        
        try:
            # Silhouette score
            metrics['silhouette_score'] = silhouette_score(
                embeddings[mask],
                labels[mask]
            )
            
            # Davies-Bouldin index (lower is better)
            metrics['davies_bouldin_index'] = davies_bouldin_score(
                embeddings[mask],
                labels[mask]
            )
            
            # Cluster statistics
            unique_labels = np.unique(labels[labels >= 0])
            cluster_sizes = [np.sum(labels == label) for label in unique_labels]
            
            metrics['avg_cluster_size'] = np.mean(cluster_sizes)
            metrics['std_cluster_size'] = np.std(cluster_sizes)
            metrics['min_cluster_size'] = np.min(cluster_sizes)
            metrics['max_cluster_size'] = np.max(cluster_sizes)
            
        except Exception as e:
            logger.error(f"Error calculating metrics: {e}")
        
        return metrics
    
    def _characterize_clusters(self, 
                             labels: np.ndarray,
                             skill_texts: List[str]) -> List[Dict[str, Any]]:
        """Characterize each cluster.
        
        Args:
            labels: Cluster labels
            skill_texts: List of skill texts
            
        Returns:
            List of cluster characteristics
        """
        cluster_info = []
        
        # Create DataFrame for easier analysis
        df = pd.DataFrame({
            'skill': skill_texts,
            'cluster': labels
        })
        
        # Analyze each cluster
        unique_labels = sorted(set(labels))
        
        for label in unique_labels:
            if label == -1:  # Skip noise cluster
                continue
            
            cluster_skills = df[df['cluster'] == label]['skill'].tolist()
            
            # Get most common skills
            skill_counts = pd.Series(cluster_skills).value_counts()
            
            cluster_data = {
                'cluster_id': int(label),
                'size': len(cluster_skills),
                'top_skills': skill_counts.head(10).to_dict(),
                'all_skills': cluster_skills,
                'label': self._generate_cluster_label(skill_counts.head(5).index.tolist())
            }
            
            cluster_info.append(cluster_data)
        
        # Sort by size
        cluster_info.sort(key=lambda x: x['size'], reverse=True)
        
        return cluster_info
    
    def _generate_cluster_label(self, top_skills: List[str]) -> str:
        """Generate a descriptive label for a cluster.
        
        Args:
            top_skills: Top skills in cluster
            
        Returns:
            Cluster label
        """
        # Simple heuristic-based labeling
        skill_lower = [s.lower() for s in top_skills]
        
        if any('frontend' in s or 'react' in s or 'angular' in s or 'vue' in s for s in skill_lower):
            return "Frontend Development"
        elif any('backend' in s or 'node' in s or 'django' in s or 'spring' in s for s in skill_lower):
            return "Backend Development"
        elif any('data' in s or 'analytics' in s or 'sql' in s for s in skill_lower):
            return "Data & Analytics"
        elif any('machine learning' in s or 'ml' in s or 'ai' in s for s in skill_lower):
            return "Machine Learning & AI"
        elif any('devops' in s or 'docker' in s or 'kubernetes' in s for s in skill_lower):
            return "DevOps & Infrastructure"
        elif any('mobile' in s or 'android' in s or 'ios' in s for s in skill_lower):
            return "Mobile Development"
        elif any('cloud' in s or 'aws' in s or 'azure' in s for s in skill_lower):
            return "Cloud Computing"
        else:
            # Use most common skill as label
            return f"{top_skills[0]} & Related"
    
    def run_clustering_pipeline(self) -> Dict[str, Any]:
        """Run complete clustering pipeline on all skills.
        
        Returns:
            Complete clustering results
        """
        # Get all embeddings
        embeddings_data = self.db_ops.get_all_embeddings()
        
        if not embeddings_data:
            logger.warning("No embeddings found for clustering")
            return {}
        
        # Extract data
        skill_texts = [e['skill_text'] for e in embeddings_data]
        embeddings = np.array([e['embedding'] for e in embeddings_data])
        
        logger.info(f"Running clustering on {len(embeddings)} skills")
        
        # Run clustering
        results = self.cluster_skills(embeddings, skill_texts)
        
        # Save results to database
        self.db_ops.save_analysis_results(
            analysis_type='clustering',
            results={
                'n_clusters': results['n_clusters'],
                'n_noise': results['n_noise'],
                'metrics': results['metrics'],
                'cluster_info': results['cluster_info']
            },
            parameters=results['parameters']
        )
        
        return results, '', text)
    
    return text.strip()

def clean_company_name(name: str) -> str:
    """Clean company name.
    
    Args:
        name: Company name to clean
        
    Returns:
        Cleaned company name
    """
    if not name:
        return ""
    
    # Basic cleaning
    name = clean_text(name)
    
    # Remove common suffixes
    suffixes = ['S.A.', 'S.A', 'SA', 'S.A.S', 'SAS', 'S.R.L', 'SRL', 
                'S.L', 'SL', 'Inc.', 'Inc', 'LLC', 'Ltd', 'Corp']
    
    for suffix in suffixes:
        pattern = rf'\s*{re.escape(suffix)}\.?\s*(#1-root-configuration-files)
2. [Database Setup Files](#2-database-setup-files)
3. [Configuration Module](#3-configuration-module)
4. [Scraper Module Files](#4-scraper-module-files)
5. [Extractor Module Files](#5-extractor-module-files)
6. [LLM Processor Module Files](#6-llm-processor-module-files)
7. [Embedder Module Files](#7-embedder-module-files)
8. [Analyzer Module Files](#8-analyzer-module-files)
9. [Orchestrator and Utilities](#9-orchestrator-and-utilities)
10. [Scripts](#10-scripts)

---

## 1. Root Configuration Files

### requirements.txt
```
# Core
python-dotenv==1.0.0
pydantic==2.5.3
typer==0.9.0
tqdm==4.66.1

# Web Scraping
scrapy==2.11.0
scrapy-selenium==0.0.7
beautifulsoup4==4.12.2
lxml==4.9.3
fake-useragent==1.4.0
requests==2.31.0

# Database
psycopg2-binary==2.9.9
sqlalchemy==2.0.23
pgvector==0.2.3
alembic==1.13.1

# NLP
spacy==3.7.2
langdetect==1.0.9
regex==2023.12.25

# Machine Learning
transformers==4.36.2
sentence-transformers==2.2.2
torch==2.1.2
llama-cpp-python==0.2.32
openai==1.6.1

# Data Processing
pandas==2.1.4
numpy==1.24.3
scikit-learn==1.3.2
umap-learn==0.5.5
hdbscan==0.8.33

# Visualization
matplotlib==3.8.2
seaborn==0.13.0
reportlab==4.0.8
pillow==10.1.0

# Testing
pytest==7.4.4
pytest-asyncio==0.23.3
pytest-cov==4.1.0

# Development
black==23.12.1
flake8==7.0.0
mypy==1.8.0
pre-commit==3.6.0
```

### .env.example
```bash
# Database Configuration
DATABASE_URL=postgresql://labor_user:your_password@localhost:5432/labor_observatory
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=0

# Scraping Configuration
SCRAPER_USER_AGENT="Mozilla/5.0 (Windows NT 10.0; Win64; x64) Academic Research Bot"
SCRAPER_CONCURRENT_REQUESTS=16
SCRAPER_DOWNLOAD_DELAY=1.0
SCRAPER_RETRY_TIMES=3

# ESCO API Configuration
ESCO_API_URL=https://ec.europa.eu/esco/api
ESCO_VERSION=1.1.0
ESCO_LANGUAGE=es

# LLM Configuration
LLM_MODEL_PATH=./data/models/mistral-7b-instruct.Q4_K_M.gguf
LLM_CONTEXT_LENGTH=4096
LLM_MAX_TOKENS=512
LLM_TEMPERATURE=0.7
LLM_N_GPU_LAYERS=35

# OpenAI Fallback (Optional)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-3.5-turbo

# Embedding Configuration
EMBEDDING_MODEL=intfloat/multilingual-e5-base
EMBEDDING_BATCH_SIZE=32
EMBEDDING_CACHE_DIR=./data/cache/embeddings

# Analysis Configuration
CLUSTER_MIN_SIZE=5
CLUSTER_MIN_SAMPLES=3
UMAP_N_NEIGHBORS=15
UMAP_MIN_DIST=0.1

# Output Configuration
OUTPUT_DIR=./outputs
REPORT_FORMAT=pdf
LOG_LEVEL=INFO
LOG_FILE=./logs/labor_observatory.log
```

### .gitignore
```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/
.venv

# IDE
.vscode/
.idea/
*.swp
*.swo

# Data and Models
data/models/*.gguf
data/models/*/
data/cache/
outputs/
logs/

# Database
*.db
*.sqlite3

# Environment
.env
.env.local

# OS
.DS_Store
Thumbs.db

# Testing
.coverage
htmlcov/
.pytest_cache/

# Scrapy
.scrapy/

# Notebooks
.ipynb_checkpoints/
```

### setup.py
```python
from setuptools import setup, find_packages

setup(
    name="labor-observatory",
    version="1.0.0",
    author="Your Team",
    description="Automated Labor Market Observatory for Latin America",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    python_requires=">=3.10",
    install_requires=[
        line.strip()
        for line in open("requirements.txt")
        if line.strip() and not line.startswith("#")
    ],
    entry_points={
        "console_scripts": [
            "labor-observatory=orchestrator:app",
        ],
    },
)
```

---

## 2. Database Setup Files

### src/database/migrations/001_initial_schema.sql
```sql
-- Create database
CREATE DATABASE IF NOT EXISTS labor_observatory
  WITH ENCODING 'UTF8'
  LC_COLLATE = 'en_US.UTF-8'
  LC_CTYPE = 'en_US.UTF-8';

\c labor_observatory;

-- Enable extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgvector";

-- Create tables
CREATE TABLE IF NOT EXISTS raw_jobs (
    job_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    portal VARCHAR(50) NOT NULL,
    country CHAR(2) NOT NULL,
    url TEXT NOT NULL,
    title TEXT NOT NULL,
    company TEXT,
    location TEXT,
    description TEXT NOT NULL,
    requirements TEXT,
    salary_raw TEXT,
    contract_type VARCHAR(50),
    remote_type VARCHAR(50),
    posted_date DATE,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    content_hash VARCHAR(64) UNIQUE,
    raw_html TEXT,
    is_processed BOOLEAN DEFAULT FALSE,
    
    CONSTRAINT chk_country CHECK (country IN ('CO', 'MX', 'AR')),
    CONSTRAINT chk_portal CHECK (portal IN ('computrabajo', 'bumeran', 'elempleo'))
);

CREATE INDEX idx_portal_country ON raw_jobs(portal, country);
CREATE INDEX idx_scraped_at ON raw_jobs(scraped_at);
CREATE INDEX idx_processed ON raw_jobs(is_processed);

CREATE TABLE IF NOT EXISTS extracted_skills (
    extraction_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    job_id UUID REFERENCES raw_jobs(job_id) ON DELETE CASCADE,
    skill_text TEXT NOT NULL,
    skill_type VARCHAR(50),
    extraction_method VARCHAR(50),
    confidence_score FLOAT,
    source_section VARCHAR(50),
    span_start INTEGER,
    span_end INTEGER,
    esco_uri TEXT,
    extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_job_skills ON extracted_skills(job_id);
CREATE INDEX idx_skill_text ON extracted_skills(skill_text);

CREATE TABLE IF NOT EXISTS enhanced_skills (
    enhancement_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    job_id UUID REFERENCES raw_jobs(job_id) ON DELETE CASCADE,
    original_skill_text TEXT,
    normalized_skill TEXT NOT NULL,
    skill_type VARCHAR(50),
    esco_concept_uri TEXT,
    esco_preferred_label TEXT,
    llm_confidence FLOAT,
    llm_reasoning TEXT,
    is_duplicate BOOLEAN DEFAULT FALSE,
    duplicate_of_id UUID,
    enhanced_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    llm_model VARCHAR(100)
);

CREATE INDEX idx_job_enhanced ON enhanced_skills(job_id);
CREATE INDEX idx_normalized ON enhanced_skills(normalized_skill);

CREATE TABLE IF NOT EXISTS skill_embeddings (
    embedding_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    skill_text TEXT UNIQUE NOT NULL,
    embedding vector(768) NOT NULL,
    model_name VARCHAR(100) NOT NULL,
    model_version VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_skill_lookup ON skill_embeddings(skill_text);
CREATE INDEX idx_embedding_similarity ON skill_embeddings 
  USING ivfflat (embedding vector_cosine_ops)
  WITH (lists = 100);

CREATE TABLE IF NOT EXISTS analysis_results (
    analysis_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    analysis_type VARCHAR(50),
    country CHAR(2),
    date_range_start DATE,
    date_range_end DATE,
    parameters JSONB,
    results JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_analysis_type ON analysis_results(analysis_type);
CREATE INDEX idx_analysis_date ON analysis_results(created_at);

-- Create views
CREATE VIEW skill_frequency AS
SELECT 
    es.normalized_skill,
    COUNT(DISTINCT es.job_id) as job_count,
    COUNT(*) as total_mentions,
    ARRAY_AGG(DISTINCT rj.country) as countries
FROM enhanced_skills es
JOIN raw_jobs rj ON es.job_id = rj.job_id
WHERE es.is_duplicate = FALSE
GROUP BY es.normalized_skill
ORDER BY job_count DESC;

CREATE VIEW country_skill_distribution AS
SELECT 
    rj.country,
    es.normalized_skill,
    COUNT(DISTINCT es.job_id) as job_count
FROM enhanced_skills es
JOIN raw_jobs rj ON es.job_id = rj.job_id
WHERE es.is_duplicate = FALSE
GROUP BY rj.country, es.normalized_skill
ORDER BY rj.country, job_count DESC;
```

### src/database/models.py
```python
from sqlalchemy import Column, String, Text, Boolean, Float, Integer, DateTime, Date, ForeignKey, JSON
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from pgvector.sqlalchemy import Vector
import uuid

Base = declarative_base()

class RawJob(Base):
    __tablename__ = 'raw_jobs'
    
    job_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    portal = Column(String(50), nullable=False)
    country = Column(String(2), nullable=False)
    url = Column(Text, nullable=False)
    title = Column(Text, nullable=False)
    company = Column(Text)
    location = Column(Text)
    description = Column(Text, nullable=False)
    requirements = Column(Text)
    salary_raw = Column(Text)
    contract_type = Column(String(50))
    remote_type = Column(String(50))
    posted_date = Column(Date)
    scraped_at = Column(DateTime, server_default=func.now())
    content_hash = Column(String(64), unique=True)
    raw_html = Column(Text)
    is_processed = Column(Boolean, default=False)
    
    # Relationships
    extracted_skills = relationship("ExtractedSkill", back_populates="job")
    enhanced_skills = relationship("EnhancedSkill", back_populates="job")

class ExtractedSkill(Base):
    __tablename__ = 'extracted_skills'
    
    extraction_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    job_id = Column(UUID(as_uuid=True), ForeignKey('raw_jobs.job_id'))
    skill_text = Column(Text, nullable=False)
    skill_type = Column(String(50))
    extraction_method = Column(String(50))
    confidence_score = Column(Float)
    source_section = Column(String(50))
    span_start = Column(Integer)
    span_end = Column(Integer)
    esco_uri = Column(Text)
    extracted_at = Column(DateTime, server_default=func.now())
    
    # Relationships
    job = relationship("RawJob", back_populates="extracted_skills")

class EnhancedSkill(Base):
    __tablename__ = 'enhanced_skills'
    
    enhancement_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    job_id = Column(UUID(as_uuid=True), ForeignKey('raw_jobs.job_id'))
    original_skill_text = Column(Text)
    normalized_skill = Column(Text, nullable=False)
    skill_type = Column(String(50))
    esco_concept_uri = Column(Text)
    esco_preferred_label = Column(Text)
    llm_confidence = Column(Float)
    llm_reasoning = Column(Text)
    is_duplicate = Column(Boolean, default=False)
    duplicate_of_id = Column(UUID(as_uuid=True))
    enhanced_at = Column(DateTime, server_default=func.now())
    llm_model = Column(String(100))
    
    # Relationships
    job = relationship("RawJob", back_populates="enhanced_skills")

class SkillEmbedding(Base):
    __tablename__ = 'skill_embeddings'
    
    embedding_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    skill_text = Column(Text, unique=True, nullable=False)
    embedding = Column(Vector(768), nullable=False)
    model_name = Column(String(100), nullable=False)
    model_version = Column(String(50))
    created_at = Column(DateTime, server_default=func.now())

class AnalysisResult(Base):
    __tablename__ = 'analysis_results'
    
    analysis_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    analysis_type = Column(String(50))
    country = Column(String(2))
    date_range_start = Column(Date)
    date_range_end = Column(Date)
    parameters = Column(JSONB)
    results = Column(JSONB)
    created_at = Column(DateTime, server_default=func.now())
```

### src/database/operations.py
```python
from typing import List, Optional, Dict, Any
from datetime import datetime
from sqlalchemy import create_engine, and_, or_, func
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.exc import IntegrityError
import os
from .models import Base, RawJob, ExtractedSkill, EnhancedSkill, SkillEmbedding, AnalysisResult
import hashlib
import logging

logger = logging.getLogger(__name__)

class DatabaseOperations:
    def __init__(self, database_url: Optional[str] = None):
        self.database_url = database_url or os.getenv('DATABASE_URL')
        self.engine = create_engine(
            self.database_url,
            pool_size=20,
            max_overflow=0,
            pool_pre_ping=True
        )
        Base.metadata.create_all(self.engine)
        self.SessionLocal = sessionmaker(bind=self.engine)
    
    def get_session(self) -> Session:
        """Get a new database session."""
        return self.SessionLocal()
    
    def insert_job(self, job_data: Dict[str, Any]) -> Optional[str]:
        """Insert a new job posting."""
        session = self.get_session()
        try:
            # Generate content hash
            content = f"{job_data['title']}{job_data['description']}{job_data.get('requirements', '')}"
            content_hash = hashlib.sha256(content.encode()).hexdigest()
            
            job = RawJob(
                **job_data,
                content_hash=content_hash
            )
            session.add(job)
            session.commit()
            
            job_id = str(job.job_id)
            logger.info(f"Inserted job {job_id}")
            return job_id
            
        except IntegrityError:
            session.rollback()
            logger.warning(f"Duplicate job detected: {job_data['url']}")
            return None

### src/analyzer/visualizations.py
```python
import logging
from typing import Dict, Any, List, Optional, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from datetime import datetime
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class VisualizationGenerator:
    """Generate static visualizations for analysis results."""
    
    def __init__(self, output_dir: str = None):
        self.output_dir = output_dir or "./outputs"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Set style
        plt.style.use('seaborn-v0_8-darkgrid')
        sns.set_palette("husl")
        
        # Set font for better Spanish support
        plt.rcParams['font.family'] = 'DejaVu Sans'
    
    def create_all_visualizations(self, 
                                analysis_data: Dict[str, Any],
                                country: Optional[str] = None) -> List[str]:
        """Create all standard visualizations.
        
        Args:
            analysis_data: Dictionary with analysis results
            country: Country code for filtering
            
        Returns:
            List of generated file paths
        """
        generated_files = []
        
        # Skill frequency chart
        if 'skill_statistics' in analysis_data:
            path = self.create_skill_frequency_chart(
                analysis_data['skill_statistics'],
                country
            )
            if path:
                generated_files.append(path)
        
        # Cluster visualization
        if 'clustering_results' in analysis_data:
            path = self.create_cluster_visualization(
                analysis_data['clustering_results']
            )
            if path:
                generated_files.append(path)
        
        # Geographic distribution
        if 'geographic_data' in analysis_data:
            path = self.create_geographic_distribution(
                analysis_data['geographic_data']
            )
            if path:
                generated_files.append(path)
        
        # Skill co-occurrence heatmap
        if 'skill_cooccurrence' in analysis_data:
            path = self.create_skill_cooccurrence_heatmap(
                analysis_data['skill_cooccurrence']
            )
            if path:
                generated_files.append(path)
        
        return generated_files
    
    def create_skill_frequency_chart(self,
                                   skill_stats: Dict[str, Any],
                                   country: Optional[str] = None,
                                   top_n: int = 20) -> Optional[str]:
        """Create horizontal bar chart of top skills.
        
        Args:
            skill_stats: Skill statistics data
            country: Country filter
            top_n: Number of top skills to show
            
        Returns:
            Path to saved chart
        """
        try:
            # Prepare data
            top_skills = skill_stats.get('top_skills', [])[:top_n]
            if not top_skills:
                logger.warning("No skill data for frequency chart")
                return None
            
            df = pd.DataFrame(top_skills)
            
            # Create figure
            fig, ax = plt.subplots(figsize=(12, 8))
            
            # Create horizontal bar chart
            bars = ax.barh(df['skill'], df['count'], 
                          color=plt.cm.viridis(np.linspace(0.3, 0.9, len(df))))
            
            # Customize
            ax.set_xlabel('Número de Vacantes', fontsize=14)
            ax.set_ylabel('Habilidad Técnica', fontsize=14)
            
            title = f'Top {top_n} Habilidades Más Demandadas'
            if country:
                country_names = {'CO': 'Colombia', 'MX': 'México', 'AR': 'Argentina'}
                title += f' - {country_names.get(country, country)}'
            ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
            
            # Add value labels
            for bar in bars:
                width = bar.get_width()
                ax.text(width + 1, bar.get_y() + bar.get_height()/2,
                       f'{int(width)}',
                       ha='left', va='center', fontsize=10)
            
            # Adjust layout
            plt.tight_layout()
            ax.invert_yaxis()  # Highest on top
            
            # Grid
            ax.grid(True, axis='x', alpha=0.3)
            ax.set_axisbelow(True)
            
            # Save
            filename = self._generate_filename('skill_frequency', country)
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created skill frequency chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating skill frequency chart: {e}")
            return None
    
    def create_cluster_visualization(self,
                                   clustering_results: Dict[str, Any]) -> Optional[str]:
        """Create 2D scatter plot of skill clusters.
        
        Args:
            clustering_results: Clustering analysis results
            
        Returns:
            Path to saved visualization
        """
        try:
            # Extract data
            coordinates = clustering_results.get('coordinates_2d')
            labels = clustering_results.get('labels')
            skills = clustering_results.get('skills', [])
            
            if coordinates is None or labels is None:
                logger.warning("Missing data for cluster visualization")
                return None
            
            # Create figure
            fig, ax = plt.subplots(figsize=(14, 10))
            
            # Get unique labels
            unique_labels = set(labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            
            # Color map
            colors = plt.cm.rainbow(np.linspace(0, 1, n_clusters))
            
            # Plot each cluster
            for k, col in zip(sorted(unique_labels), colors):
                if k == -1:
                    # Noise points in black
                    col = 'black'
                    label = 'Ruido'
                else:
                    label = f'Cluster {k}'
                
                class_member_mask = (labels == k)
                xy = coordinates[class_member_mask]
                
                ax.scatter(xy[:, 0], xy[:, 1], 
                         c=[col], 
                         label=label,
                         alpha=0.6,
                         s=30)
            
            # Add labels for some points (avoid overlap)
            if skills:
                # Sample points to label
                n_labels = min(30, len(skills))
                indices = np.random.choice(len(skills), n_labels, replace=False)
                
                for idx in indices:
                    ax.annotate(skills[idx], 
                              (coordinates[idx, 0], coordinates[idx, 1]),
                              fontsize=8,
                              alpha=0.7)
            
            # Customize
            ax.set_title('Visualización de Clusters de Habilidades', 
                        fontsize=16, fontweight='bold')
            ax.set_xlabel('UMAP Dimension 1', fontsize=12)
            ax.set_ylabel('UMAP Dimension 2', fontsize=12)
            
            # Legend
            ax.legend(loc='upper right', bbox_to_anchor=(1.15, 1))
            
            # Remove ticks
            ax.set_xticks([])
            ax.set_yticks([])
            
            # Save
            filename = self._generate_filename('skill_clusters')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created cluster visualization: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating cluster visualization: {e}")
            return None
    
    def create_geographic_distribution(self,
                                     geo_data: Dict[str, Any]) -> Optional[str]:
        """Create geographic distribution chart.
        
        Args:
            geo_data: Geographic distribution data
            
        Returns:
            Path to saved chart
        """
        try:
            # Create figure
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # Job distribution by country
            countries = ['Colombia', 'México', 'Argentina']
            job_counts = [
                geo_data.get('CO', {}).get('total_jobs', 0),
                geo_data.get('MX', {}).get('total_jobs', 0),
                geo_data.get('AR', {}).get('total_jobs', 0)
            ]
            
            # Pie chart
            colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
            wedges, texts, autotexts = ax1.pie(job_counts, 
                                              labels=countries,
                                              colors=colors,
                                              autopct='%1.1f%%',
                                              startangle=90)
            
            ax1.set_title('Distribución de Vacantes por País', 
                         fontsize=14, fontweight='bold')
            
            # Skills per country
            skill_counts = [
                geo_data.get('CO', {}).get('unique_skills', 0),
                geo_data.get('MX', {}).get('unique_skills', 0),
                geo_data.get('AR', {}).get('unique_skills', 0)
            ]
            
            bars = ax2.bar(countries, skill_counts, color=colors)
            ax2.set_title('Habilidades Únicas por País', 
                         fontsize=14, fontweight='bold')
            ax2.set_ylabel('Número de Habilidades', fontsize=12)
            
            # Add value labels
            for bar in bars:
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height,
                        f'{int(height)}',
                        ha='center', va='bottom')
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('geographic_distribution')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created geographic distribution chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating geographic distribution: {e}")
            return None
    
    def create_skill_cooccurrence_heatmap(self,
                                        cooccurrence_data: Dict[str, Any],
                                        top_n: int = 15) -> Optional[str]:
        """Create heatmap of skill co-occurrences.
        
        Args:
            cooccurrence_data: Skill co-occurrence matrix
            top_n: Number of top skills to include
            
        Returns:
            Path to saved heatmap
        """
        try:
            # Convert to DataFrame
            df = pd.DataFrame(cooccurrence_data.get('matrix', []))
            skills = cooccurrence_data.get('skills', [])
            
            if df.empty or not skills:
                logger.warning("No co-occurrence data available")
                return None
            
            # Select top skills
            if len(skills) > top_n:
                # Sum co-occurrences for each skill
                skill_importance = df.sum(axis=0) + df.sum(axis=1)
                top_indices = skill_importance.nlargest(top_n).index
                df = df.loc[top_indices, top_indices]
                skills = [skills[i] for i in top_indices]
            
            # Create figure
            fig, ax = plt.subplots(figsize=(12, 10))
            
            # Create heatmap
            sns.heatmap(df, 
                       xticklabels=skills,
                       yticklabels=skills,
                       cmap='YlOrRd',
                       cbar_kws={'label': 'Co-ocurrencias'},
                       square=True,
                       linewidths=0.5,
                       ax=ax)
            
            # Customize
            ax.set_title('Matriz de Co-ocurrencia de Habilidades', 
                        fontsize=16, fontweight='bold', pad=20)
            
            # Rotate labels
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('skill_cooccurrence')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created co-occurrence heatmap: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating co-occurrence heatmap: {e}")
            return None
    
    def create_temporal_trends(self,
                             temporal_data: Dict[str, Any],
                             skills: List[str] = None) -> Optional[str]:
        """Create temporal trend visualization.
        
        Args:
            temporal_data: Temporal trend data
            skills: List of skills to plot (default: top 5)
            
        Returns:
            Path to saved chart
        """
        try:
            # Prepare data
            df = pd.DataFrame(temporal_data.get('trends', []))
            
            if df.empty:
                logger.warning("No temporal data available")
                return None
            
            # Convert date column
            df['date'] = pd.to_datetime(df['date'])
            
            # Select skills to plot
            if not skills:
                # Get top 5 skills by total mentions
                skill_totals = df.groupby('skill')['count'].sum()
                skills = skill_totals.nlargest(5).index.tolist()
            
            # Create figure
            fig, ax = plt.subplots(figsize=(14, 8))
            
            # Plot each skill
            for skill in skills:
                skill_data = df[df['skill'] == skill]
                ax.plot(skill_data['date'], skill_data['count'], 
                       marker='o', label=skill, linewidth=2)
            
            # Customize
            ax.set_title('Tendencias Temporales de Habilidades', 
                        fontsize=16, fontweight='bold')
            ax.set_xlabel('Fecha', fontsize=12)
            ax.set_ylabel('Número de Menciones', fontsize=12)
            
            # Format x-axis
            import matplotlib.dates as mdates
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
            ax.xaxis.set_major_locator(mdates.MonthLocator())
            plt.xticks(rotation=45)
            
            # Legend
            ax.legend(loc='upper left', bbox_to_anchor=(1, 1))
            
            # Grid
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('temporal_trends')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created temporal trends chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating temporal trends: {e}")
            return None
    
    def _generate_filename(self, chart_type: str, country: Optional[str] = None) -> str:
        """Generate filename with timestamp.
        
        Args:
            chart_type: Type of chart
            country: Country code (optional)
            
        Returns:
            Generated filename
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        country_suffix = f"_{country}" if country else ""
        return f"{chart_type}{country_suffix}_{timestamp}.png"

---

## 9. Orchestrator and Utilities

### src/orchestrator.py
```python
#!/usr/bin/env python3
"""
Main orchestrator for the Labor Market Observatory pipeline.
"""

import logging
import sys
from typing import Optional
import typer
from rich.console import Console
from rich.table import Table
from rich.progress import track
import time

from config.settings import get_settings
from config.logging_config import setup_logging
from database.operations import DatabaseOperations
from scraper.spiders.computrabajo_spider import ComputrabajoSpider
from scraper.spiders.bumeran_spider import BumeranSpider
from scraper.spiders.elempleo_spider import ElempleoSpider
from extractor.pipeline import ExtractionPipeline
from llm_processor.pipeline import LLMProcessingPipeline
from embedder.batch_processor import BatchProcessor
from analyzer.clustering import SkillClusterer
from analyzer.dimension_reducer import DimensionReducer
from analyzer.report_generator import ReportGenerator
from analyzer.visualizations import VisualizationGenerator

# Initialize
app = typer.Typer(help="Labor Market Observatory CLI")
console = Console()
settings = get_settings()
logger = setup_logging(settings.log_level, settings.log_file)

@app.command()
def scrape(
    country: str = typer.Argument(..., help="Country code (CO, MX, AR)"),
    portal: str = typer.Argument(..., help="Portal name (computrabajo, bumeran, elempleo)"),
    pages: int = typer.Option(10, help="Number of pages to scrape")
):
    """Run web scraping for a specific portal and country."""
    console.print(f"[bold green]Starting scraper for {portal} in {country}[/bold green]")
    
    # Validate inputs
    if country not in settings.supported_countries:
        console.print(f"[red]Invalid country: {country}[/red]")
        raise typer.Exit(1)
    
    if portal not in settings.supported_portals:
        console.print(f"[red]Invalid portal: {portal}[/red]")
        raise typer.Exit(1)
    
    # Run appropriate spider
    try:
        from scrapy.crawler import CrawlerProcess
        from scrapy.utils.project import get_project_settings
        
        # Get scrapy settings
        scrapy_settings = get_project_settings()
        scrapy_settings.update({
            'LOG_LEVEL': 'INFO',
            'CLOSESPIDER_PAGECOUNT': pages
        })
        
        process = CrawlerProcess(scrapy_settings)
        
        # Select spider
        if portal == 'computrabajo':
            spider_class = ComputrabajoSpider
        elif portal == 'bumeran':
            spider_class = BumeranSpider
        elif portal == 'elempleo':
            spider_class = ElempleoSpider
        
        # Run spider
        process.crawl(spider_class, country=country)
        process.start()
        
        console.print("[bold green]Scraping completed![/bold green]")
        
    except Exception as e:
        console.print(f"[red]Scraping failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def extract(
    batch_size: int = typer.Option(100, help="Batch size for processing")
):
    """Extract skills from scraped job postings."""
    console.print("[bold green]Starting skill extraction...[/bold green]")
    
    try:
        pipeline = ExtractionPipeline()
        
        with console.status("[bold green]Extracting skills...") as status:
            stats = pipeline.process_batch(batch_size)
        
        # Display results
        table = Table(title="Extraction Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Jobs Processed", str(stats['jobs_processed']))
        table.add_row("Skills Extracted", str(stats['skills_extracted']))
        table.add_row("ESCO Matches", str(stats['esco_matches']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        table.add_row("Jobs/Second", f"{stats['jobs_per_second']:.2f}")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Extraction failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def enhance(
    batch_size: int = typer.Option(50, help="Batch size for LLM processing"),
    model: str = typer.Option("local", help="Model type (local or openai)")
):
    """Enhance extracted skills using LLM."""
    console.print(f"[bold green]Starting LLM enhancement with {model} model...[/bold green]")
    
    try:
        pipeline = LLMProcessingPipeline(model_type=model)
        
        with console.status("[bold green]Processing with LLM...") as status:
            stats = pipeline.process_batch(batch_size)
        
        # Display results
        table = Table(title="LLM Enhancement Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Jobs Processed", str(stats['jobs_processed']))
        table.add_row("Skills Enhanced", str(stats['skills_enhanced']))
        table.add_row("Implicit Skills Found", str(stats['implicit_skills_found']))
        table.add_row("Skills Normalized", str(stats['skills_normalized']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Enhancement failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def embed(
    model: Optional[str] = typer.Option(None, help="Embedding model name")
):
    """Generate embeddings for all skills."""
    console.print("[bold green]Starting embedding generation...[/bold green]")
    
    try:
        processor = BatchProcessor(model_name=model)
        
        with console.status("[bold green]Generating embeddings...") as status:
            stats = processor.process_all_skills()
        
        # Display results
        table = Table(title="Embedding Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Skills Processed", str(stats['skills_processed']))
        table.add_row("Embeddings Created", str(stats['embeddings_created']))
        table.add_row("Errors", str(stats['errors']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Embedding failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def analyze(
    method: str = typer.Option("hdbscan", help="Clustering method")
):
    """Run clustering analysis on skill embeddings."""
    console.print(f"[bold green]Starting clustering analysis with {method}...[/bold green]")
    
    try:
        clusterer = SkillClusterer()
        
        with console.status("[bold green]Running clustering...") as status:
            results = clusterer.run_clustering_pipeline()
        
        # Display results
        if results:
            table = Table(title="Clustering Results")
            table.add_column("Metric", style="cyan")
            table.add_column("Value", style="magenta")
            
            table.add_row("Number of Clusters", str(results['n_clusters']))
            table.add_row("Noise Points", str(results['n_noise']))
            table.add_row("Silhouette Score", f"{results['metrics'].get('silhouette_score', 0):.3f}")
            
            console.print(table)
            
            # Show top clusters
            console.print("\n[bold]Top 5 Clusters:[/bold]")
            for cluster in results['cluster_info'][:5]:
                console.print(f"  • {cluster['label']}: {cluster['size']} skills")
        
    except Exception as e:
        console.print(f"[red]Analysis failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def report(
    country: Optional[str] = typer.Option(None, help="Country code to filter"),
    format: str = typer.Option("pdf", help="Report format (pdf)")
):
    """Generate analysis report."""
    console.print("[bold green]Generating report...[/bold green]")
    
    try:
        generator = ReportGenerator()
        
        with console.status("[bold green]Creating report...") as status:
            filepath = generator.generate_full_report(
                country=country,
                include_visualizations=True
            )
        
        console.print(f"[bold green]Report generated: {filepath}[/bold green]")
        
    except Exception as e:
        console.print(f"[red]Report generation failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def pipeline(
    country: str = typer.Argument(..., help="Country code"),
    portal: str = typer.Argument(..., help="Portal name"),
    full: bool = typer.Option(False, help="Run full pipeline including scraping")
):
    """Run complete pipeline."""
    console.print("[bold green]Running complete pipeline...[/bold green]")
    
    steps = []
    
    if full:
        steps.append(("Scraping", lambda: scrape(country, portal, pages=5)))
    
    steps.extend([
        ("Extraction", lambda: extract(batch_size=100)),
        ("LLM Enhancement", lambda: enhance(batch_size=50)),
        ("Embedding", lambda: embed()),
        ("Clustering", lambda: analyze()),
        ("Report Generation", lambda: report(country=country))
    ])
    
    for step_name, step_func in track(steps, description="Processing..."):
        try:
            console.print(f"\n[bold cyan]Running: {step_name}[/bold cyan]")
            step_func()
            time.sleep(1)  # Brief pause between steps
        except Exception as e:
            console.print(f"[red]Step '{step_name}' failed: {e}[/red]")
            raise typer.Exit(1)
    
    console.print("\n[bold green]Pipeline completed successfully![/bold green]")

@app.command()
def status():
    """Show system status and statistics."""
    console.print("[bold green]Labor Market Observatory Status[/bold green]\n")
    
    try:
        db_ops = DatabaseOperations()
        
        # Get statistics
        stats = db_ops.get_skill_statistics()
        
        # Display overall stats
        table = Table(title="System Statistics")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Total Unique Skills", str(stats.get('total_unique_skills', 0)))
        table.add_row("Database Size", "N/A")  # Would need to implement
        
        console.print(table)
        
        # Top skills
        if stats.get('top_skills'):
            console.print("\n[bold]Top 10 Skills:[/bold]")
            for i, skill in enumerate(stats['top_skills'][:10], 1):
                console.print(f"  {i}. {skill['skill']} ({skill['count']} jobs)")
        
    except Exception as e:
        console.print(f"[red]Failed to get status: {e}[/red]")
        raise typer.Exit(1)

if __name__ == "__main__":
    app()

### src/utils/__init__.py
```python
from .validators import validate_country, validate_portal, validate_skill
from .cleaners import clean_text, normalize_text, remove_html
from .metrics import calculate_metrics, generate_statistics
from .logger import get_logger

__all__ = [
    'validate_country', 'validate_portal', 'validate_skill',
    'clean_text', 'normalize_text', 'remove_html',
    'calculate_metrics', 'generate_statistics',
    'get_logger'
]
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting job: {e}")
            raise
        finally:
            session.close()
    
    def get_unprocessed_jobs(self, limit: int = 100) -> List[RawJob]:
        """Get unprocessed job postings."""
        session = self.get_session()
        try:
            jobs = session.query(RawJob).filter(
                RawJob.is_processed == False
            ).limit(limit).all()
            return jobs
        finally:
            session.close()
    
    def mark_job_processed(self, job_id: str):
        """Mark a job as processed."""
        session = self.get_session()
        try:
            session.query(RawJob).filter(
                RawJob.job_id == job_id
            ).update({"is_processed": True})
            session.commit()
        finally:
            session.close()
    
    def insert_extracted_skills(self, job_id: str, skills: List[Dict[str, Any]]):
        """Insert extracted skills for a job."""
        session = self.get_session()
        try:
            for skill_data in skills:
                skill = ExtractedSkill(
                    job_id=job_id,
                    **skill_data
                )
                session.add(skill)
            session.commit()
            logger.info(f"Inserted {len(skills)} extracted skills for job {job_id}")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting extracted skills: {e}")
            raise
        finally:
            session.close()
    
    def get_extracted_skills_for_processing(self, limit: int = 100) -> List[Dict[str, Any]]:
        """Get jobs with extracted skills that need LLM processing."""
        session = self.get_session()
        try:
            # Get jobs that have extracted skills but no enhanced skills
            subquery = session.query(EnhancedSkill.job_id).distinct()
            
            jobs = session.query(RawJob).join(ExtractedSkill).filter(
                ~RawJob.job_id.in_(subquery)
            ).limit(limit).all()
            
            result = []
            for job in jobs:
                skills = session.query(ExtractedSkill).filter(
                    ExtractedSkill.job_id == job.job_id
                ).all()
                
                result.append({
                    'job_id': str(job.job_id),
                    'job_title': job.title,
                    'job_description': job.description,
                    'job_requirements': job.requirements,
                    'extracted_skills': [
                        {
                            'skill_text': skill.skill_text,
                            'extraction_method': skill.extraction_method,
                            'source_section': skill.source_section,
                            'confidence_score': skill.confidence_score
                        }
                        for skill in skills
                    ]
                })
            
            return result
        finally:
            session.close()
    
    def insert_enhanced_skills(self, job_id: str, skills: List[Dict[str, Any]]):
        """Insert enhanced skills for a job."""
        session = self.get_session()
        try:
            for skill_data in skills:
                skill = EnhancedSkill(
                    job_id=job_id,
                    **skill_data
                )
                session.add(skill)
            session.commit()
            logger.info(f"Inserted {len(skills)} enhanced skills for job {job_id}")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting enhanced skills: {e}")
            raise
        finally:
            session.close()
    
    def get_unique_skills_for_embedding(self) -> List[str]:
        """Get unique normalized skills that don't have embeddings yet."""
        session = self.get_session()
        try:
            # Get skills that don't have embeddings
            embedded_skills = session.query(SkillEmbedding.skill_text).distinct()
            
            unique_skills = session.query(
                EnhancedSkill.normalized_skill
            ).filter(
                EnhancedSkill.is_duplicate == False,
                ~EnhancedSkill.normalized_skill.in_(embedded_skills)
            ).distinct().all()
            
            return [skill[0] for skill in unique_skills]
        finally:
            session.close()
    
    def insert_skill_embeddings(self, embeddings: List[Dict[str, Any]]):
        """Insert skill embeddings."""
        session = self.get_session()
        try:
            for emb_data in embeddings:
                embedding = SkillEmbedding(**emb_data)
                session.add(embedding)
            session.commit()
            logger.info(f"Inserted {len(embeddings)} skill embeddings")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting embeddings: {e}")
            raise
        finally:
            session.close()
    
    def get_all_embeddings(self) -> List[Dict[str, Any]]:
        """Get all skill embeddings for clustering."""
        session = self.get_session()
        try:
            embeddings = session.query(SkillEmbedding).all()
            return [
                {
                    'skill_text': emb.skill_text,
                    'embedding': emb.embedding,
                    'embedding_id': str(emb.embedding_id)
                }
                for emb in embeddings
            ]
        finally:
            session.close()
    
    def save_analysis_results(self, analysis_type: str, results: Dict[str, Any], 
                            parameters: Dict[str, Any], country: Optional[str] = None):
        """Save analysis results."""
        session = self.get_session()
        try:
            analysis = AnalysisResult(
                analysis_type=analysis_type,
                country=country,
                parameters=parameters,
                results=results
            )
            session.add(analysis)
            session.commit()
            logger.info(f"Saved {analysis_type} analysis results")
        except Exception as e:
            session.rollback()
            logger.error(f"Error saving analysis results: {e}")
            raise
        finally:
            session.close()
    
    def get_skill_statistics(self, country: Optional[str] = None) -> Dict[str, Any]:
        """Get skill statistics by country."""
        session = self.get_session()
        try:
            query = session.query(
                EnhancedSkill.normalized_skill,
                func.count(func.distinct(EnhancedSkill.job_id)).label('job_count')
            ).join(RawJob).filter(
                EnhancedSkill.is_duplicate == False
            )
            
            if country:
                query = query.filter(RawJob.country == country)
            
            results = query.group_by(
                EnhancedSkill.normalized_skill
            ).order_by(
                func.count(func.distinct(EnhancedSkill.job_id)).desc()
            ).limit(50).all()
            
            return {
                'top_skills': [
                    {'skill': skill, 'count': count}
                    for skill, count in results
                ],
                'total_unique_skills': session.query(
                    func.count(func.distinct(EnhancedSkill.normalized_skill))
                ).filter(EnhancedSkill.is_duplicate == False).scalar()
            }
        finally:
            session.close()
```

---

## 3. Configuration Module

### src/config/__init__.py
```python
from .settings import Settings, get_settings
from .database import get_database_url
from .logging_config import setup_logging

__all__ = ['Settings', 'get_settings', 'get_database_url', 'setup_logging']
```

### src/config/settings.py
```python
from pydantic_settings import BaseSettings
from pydantic import Field, validator
from typing import Optional, List
import os
from functools import lru_cache

class Settings(BaseSettings):
    # Database
    database_url: str = Field(..., env='DATABASE_URL')
    database_pool_size: int = Field(20, env='DATABASE_POOL_SIZE')
    
    # Scraping
    scraper_user_agent: str = Field(..., env='SCRAPER_USER_AGENT')
    scraper_concurrent_requests: int = Field(16, env='SCRAPER_CONCURRENT_REQUESTS')
    scraper_download_delay: float = Field(1.0, env='SCRAPER_DOWNLOAD_DELAY')
    scraper_retry_times: int = Field(3, env='SCRAPER_RETRY_TIMES')
    
    # ESCO
    esco_api_url: str = Field('https://ec.europa.eu/esco/api', env='ESCO_API_URL')
    esco_version: str = Field('1.1.0', env='ESCO_VERSION')
    esco_language: str = Field('es', env='ESCO_LANGUAGE')
    
    # LLM
    llm_model_path: str = Field(..., env='LLM_MODEL_PATH')
    llm_context_length: int = Field(4096, env='LLM_CONTEXT_LENGTH')
    llm_max_tokens: int = Field(512, env='LLM_MAX_TOKENS')
    llm_temperature: float = Field(0.7, env='LLM_TEMPERATURE')
    llm_n_gpu_layers: int = Field(35, env='LLM_N_GPU_LAYERS')
    
    # OpenAI (Optional)
    openai_api_key: Optional[str] = Field(None, env='OPENAI_API_KEY')
    openai_model: str = Field('gpt-3.5-turbo', env='OPENAI_MODEL')
    
    # Embeddings
    embedding_model: str = Field('intfloat/multilingual-e5-base', env='EMBEDDING_MODEL')
    embedding_batch_size: int = Field(32, env='EMBEDDING_BATCH_SIZE')
    embedding_cache_dir: str = Field('./data/cache/embeddings', env='EMBEDDING_CACHE_DIR')
    
    # Analysis
    cluster_min_size: int = Field(5, env='CLUSTER_MIN_SIZE')
    cluster_min_samples: int = Field(3, env='CLUSTER_MIN_SAMPLES')
    umap_n_neighbors: int = Field(15, env='UMAP_N_NEIGHBORS')
    umap_min_dist: float = Field(0.1, env='UMAP_MIN_DIST')
    
    # Output
    output_dir: str = Field('./outputs', env='OUTPUT_DIR')
    report_format: str = Field('pdf', env='REPORT_FORMAT')
    log_level: str = Field('INFO', env='LOG_LEVEL')
    log_file: str = Field('./logs/labor_observatory.log', env='LOG_FILE')
    
    # Supported countries and portals
    supported_countries: List[str] = ['CO', 'MX', 'AR']
    supported_portals: List[str] = ['computrabajo', 'bumeran', 'elempleo']
    
    class Config:
        env_file = '.env'
        env_file_encoding = 'utf-8'
    
    @validator('output_dir', 'log_file', 'embedding_cache_dir')
    def create_directories(cls, v):
        os.makedirs(os.path.dirname(v) if os.path.dirname(v) else v, exist_ok=True)
        return v

@lru_cache()
def get_settings() -> Settings:
    """Get cached settings instance."""
    return Settings()
```

### src/config/database.py
```python
import os
from urllib.parse import urlparse

def get_database_url() -> str:
    """Get database URL from environment or construct from components."""
    if os.getenv('DATABASE_URL'):
        return os.getenv('DATABASE_URL')
    
    # Construct from individual components
    user = os.getenv('DB_USER', 'labor_user')
    password = os.getenv('DB_PASSWORD', 'password')
    host = os.getenv('DB_HOST', 'localhost')
    port = os.getenv('DB_PORT', '5432')
    name = os.getenv('DB_NAME', 'labor_observatory')
    
    return f"postgresql://{user}:{password}@{host}:{port}/{name}"

def get_database_config() -> dict:
    """Parse database URL into components."""
    url = get_database_url()
    parsed = urlparse(url)
    
    return {
        'host': parsed.hostname,
        'port': parsed.port or 5432,
        'user': parsed.username,
        'password': parsed.password,
        'database': parsed.path.lstrip('/')
    }
```

### src/config/logging_config.py
```python
import logging
import logging.handlers
import os
from datetime import datetime

def setup_logging(log_level: str = 'INFO', log_file: str = None):
    """Configure logging for the entire application."""
    
    # Create logs directory if needed
    if log_file:
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    # Create formatters
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
    )
    console_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )
    
    # Get root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, log_level.upper()))
    
    # Remove existing handlers
    root_logger.handlers.clear()
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)
    
    # File handler with rotation
    if log_file:
        file_handler = logging.handlers.RotatingFileHandler(
            log_file,
            maxBytes=10 * 1024 * 1024,  # 10MB
            backupCount=5
        )
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(file_formatter)
        root_logger.addHandler(file_handler)
    
    # Specific loggers configuration
    logging.getLogger('scrapy').setLevel(logging.WARNING)
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('transformers').setLevel(logging.WARNING)
    
    return root_logger
```

### config/esco_config.yaml
```yaml
# ESCO Configuration for Spanish Language
esco:
  version: "1.1.0"
  base_url: "https://ec.europa.eu/esco/api"
  
  # Endpoints
  endpoints:
    skills: "/resource/skill"
    occupations: "/resource/occupation"
    search: "/search"
  
  # Spanish language configuration
  language:
    primary: "es"
    fallback: "en"
  
  # Skill types to extract
  skill_types:
    - "skill/competence"
    - "knowledge"
    - "skill"
  
  # Search parameters
  search:
    limit: 100
    fields:
      - "preferredLabel"
      - "altLabels"
      - "description"
      - "broaderConcept"
    
  # Mapping rules for common tech terms
  tech_mappings:
    # Programming languages
    "python": "http://data.europa.eu/esco/skill/3897c3c6-556b-4b8a-bfeb-234b0f716950"
    "java": "http://data.europa.eu/esco/skill/b4b36f5a-d5e6-4d78-b4ed-5b0b0e4f7a8a"
    "javascript": "http://data.europa.eu/esco/skill/7c7c5c49-0122-4b2e-8f6e-4e6b3f3e5d5f"
    "react": "http://data.europa.eu/esco/skill/react-framework"
    "node.js": "http://data.europa.eu/esco/skill/nodejs-runtime"
    
    # Databases
    "sql": "http://data.europa.eu/esco/skill/sql-language"
    "mysql": "http://data.europa.eu/esco/skill/mysql-database"
    "postgresql": "http://data.europa.eu/esco/skill/postgresql-database"
    "mongodb": "http://data.europa.eu/esco/skill/mongodb-database"
    
    # Cloud platforms
    "aws": "http://data.europa.eu/esco/skill/amazon-web-services"
    "azure": "http://data.europa.eu/esco/skill/microsoft-azure"
    "gcp": "http://data.europa.eu/esco/skill/google-cloud-platform"
    
    # DevOps
    "docker": "http://data.europa.eu/esco/skill/docker-containerization"
    "kubernetes": "http://data.europa.eu/esco/skill/kubernetes-orchestration"
    "git": "http://data.europa.eu/esco/skill/git-version-control"
    
    # Soft skills (Spanish)
    "trabajo en equipo": "http://data.europa.eu/esco/skill/teamwork"
    "comunicación": "http://data.europa.eu/esco/skill/communication"
    "liderazgo": "http://data.europa.eu/esco/skill/leadership"
    "resolución de problemas": "http://data.europa.eu/esco/skill/problem-solving"
```

---

## 4. Scraper Module Files

### src/scraper/__init__.py
```python
from .spiders.computrabajo_spider import ComputrabajoSpider
from .spiders.bumeran_spider import BumeranSpider
from .spiders.elempleo_spider import ElempleoSpider

__all__ = ['ComputrabajoSpider', 'BumeranSpider', 'ElempleoSpider']
```

### src/scraper/scrapy.cfg
```ini
[settings]
default = scraper.settings

[deploy]
project = labor_observatory_scraper
```

### src/scraper/items.py
```python
import scrapy
from scrapy.item import Field
from datetime import datetime
import re

class JobItem(scrapy.Item):
    # Required fields
    portal = Field()
    country = Field()
    url = Field()
    title = Field()
    description = Field()
    
    # Optional fields
    company = Field()
    location = Field()
    requirements = Field()
    salary_raw = Field()
    contract_type = Field()
    remote_type = Field()
    posted_date = Field()
    raw_html = Field()
    
    # Metadata
    scraped_at = Field()
    
    def clean_text(self, text):
        """Clean and normalize text fields."""
        if not text:
            return None
        
        # Remove extra whitespace
        text = ' '.join(text.split())
        
        # Remove HTML entities
        text = re.sub(r'&[a-zA-Z]+;', ' ', text)
        
        return text.strip()
    
    def __setitem__(self, key, value):
        # Clean text fields
        if key in ['title', 'description', 'requirements', 'company', 'location'] and value:
            value = self.clean_text(value)
        
        # Set scraped_at automatically
        if key == 'scraped_at':
            value = datetime.now()
        
        super().__setitem__(key, value)
```

### src/scraper/pipelines.py
```python
import logging
from datetime import datetime
from typing import Optional
import re
from scrapy.exceptions import DropItem
from database.operations import DatabaseOperations

logger = logging.getLogger(__name__)

class ValidationPipeline:
    """Validate scraped items."""
    
    def process_item(self, item, spider):
        # Check required fields
        required_fields = ['portal', 'country', 'url', 'title', 'description']
        
        for field in required_fields:
            if not item.get(field):
                raise DropItem(f"Missing required field: {field}")
        
        # Validate country code
        if item['country'] not in ['CO', 'MX', 'AR']:
            raise DropItem(f"Invalid country code: {item['country']}")
        
        # Validate portal
        if item['portal'] not in ['computrabajo', 'bumeran', 'elempleo']:
            raise DropItem(f"Invalid portal: {item['portal']}")
        
        # Ensure minimum description length
        if len(item['description']) < 50:
            raise DropItem("Description too short")
        
        return item

class NormalizationPipeline:
    """Normalize item fields."""
    
    def process_item(self, item, spider):
        # Normalize contract type
        if item.get('contract_type'):
            item['contract_type'] = self.normalize_contract_type(item['contract_type'])
        
        # Normalize remote type
        if item.get('remote_type'):
            item['remote_type'] = self.normalize_remote_type(item['remote_type'])
        
        # Parse posted date
        if item.get('posted_date'):
            item['posted_date'] = self.parse_date(item['posted_date'])
        
        # Set scraped_at
        item['scraped_at'] = datetime.now()
        
        return item
    
    def normalize_contract_type(self, contract: str) -> str:
        """Normalize contract type to standard values."""
        contract_lower = contract.lower()
        
        if any(term in contract_lower for term in ['tiempo completo', 'full time', 'completo']):
            return 'full_time'
        elif any(term in contract_lower for term in ['medio tiempo', 'part time', 'parcial']):
            return 'part_time'
        elif any(term in contract_lower for term in ['freelance', 'independiente', 'autonomo']):
            return 'freelance'
        elif any(term in contract_lower for term in ['contrato', 'temporal', 'proyecto']):
            return 'contract'
        elif any(term in contract_lower for term in ['pasantia', 'practica', 'internship']):
            return 'internship'
        else:
            return 'other'
    
    def normalize_remote_type(self, remote: str) -> str:
        """Normalize remote work type."""
        remote_lower = remote.lower()
        
        if any(term in remote_lower for term in ['remoto', 'remote', 'teletrabajo']):
            return 'remote'
        elif any(term in remote_lower for term in ['hibrido', 'hybrid', 'mixto']):
            return 'hybrid'
        elif any(term in remote_lower for term in ['presencial', 'oficina', 'on-site']):
            return 'on_site'
        else:
            return 'not_specified'
    
    def parse_date(self, date_str: str) -> Optional[datetime]:
        """Parse various date formats."""
        # Common Spanish date patterns
        patterns = [
            r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})',  # DD/MM/YYYY or DD-MM-YYYY
            r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})',  # YYYY/MM/DD or YYYY-MM-DD
            r'hace (\d+) días?',  # "hace X días"
            r'(\d+) días? atrás',  # "X días atrás"
            r'hoy',  # "hoy"
            r'ayer',  # "ayer"
        ]
        
        # Try to match patterns
        for pattern in patterns:
            match = re.search(pattern, date_str, re.IGNORECASE)
            if match:
                if 'hace' in pattern or 'atrás' in pattern:
                    days_ago = int(match.group(1))
                    return datetime.now() - timedelta(days=days_ago)
                elif pattern == r'hoy':
                    return datetime.now().date()
                elif pattern == r'ayer':
                    return datetime.now() - timedelta(days=1)
                else:
                    # Handle date formats
                    try:
                        if len(match.groups()) == 3:
                            if match.group(1).isdigit() and len(match.group(1)) == 4:
                                # YYYY/MM/DD format
                                return datetime(
                                    int(match.group(1)),
                                    int(match.group(2)),
                                    int(match.group(3))
                                ).date()
                            else:
                                # DD/MM/YYYY format
                                return datetime(
                                    int(match.group(3)),
                                    int(match.group(2)),
                                    int(match.group(1))
                                ).date()
                    except ValueError:
                        pass
        
        return None

class DatabasePipeline:
    """Save items to PostgreSQL database."""
    
    def __init__(self):
        self.db_ops = None
    
    def open_spider(self, spider):
        self.db_ops = DatabaseOperations()
        logger.info(f"Database pipeline opened for spider: {spider.name}")
    
    def process_item(self, item, spider):
        try:
            # Convert item to dict
            job_data = dict(item)
            
            # Remove metadata fields
            job_data.pop('scraped_at', None)
            
            # Insert into database
            job_id = self.db_ops.insert_job(job_data)
            
            if job_id:
                logger.info(f"Saved job {job_id}: {item['title']}")
            else:
                logger.warning(f"Duplicate job skipped: {item['url']}")
            
            return item
            
        except Exception as e:
            logger.error(f"Error saving job to database: {e}")
            raise
```

### src/scraper/settings.py
```python
import os
from config.settings import get_settings

settings = get_settings()

BOT_NAME = 'labor_observatory_scraper'

SPIDER_MODULES = ['scraper.spiders']
NEWSPIDER_MODULE = 'scraper.spiders'

# Obey robots.txt rules
ROBOTSTXT_OBEY = True

# Configure maximum concurrent requests
CONCURRENT_REQUESTS = settings.scraper_concurrent_requests
CONCURRENT_REQUESTS_PER_DOMAIN = 8

# Configure delay
DOWNLOAD_DELAY = settings.scraper_download_delay
RANDOMIZE_DOWNLOAD_DELAY = True

# Disable cookies
COOKIES_ENABLED = False

# User agent
USER_AGENT = settings.scraper_user_agent

# Override default headers
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'Cache-Control': 'no-cache',
    'Pragma': 'no-cache',
}

# Retry configuration
RETRY_TIMES = settings.scraper_retry_times
RETRY_HTTP_CODES = [500, 502, 503, 504, 408, 429]

# Configure pipelines
ITEM_PIPELINES = {
    'scraper.pipelines.ValidationPipeline': 100,
    'scraper.pipelines.NormalizationPipeline': 200,
    'scraper.pipelines.DatabasePipeline': 300,
}

# AutoThrottle extension
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 1
AUTOTHROTTLE_MAX_DELAY = 10
AUTOTHROTTLE_TARGET_CONCURRENCY = 8.0
AUTOTHROTTLE_DEBUG = False

# Memory usage
MEMUSAGE_ENABLED = True
MEMUSAGE_LIMIT_MB = 2048
MEMUSAGE_WARNING_MB = 1536

# Logging
LOG_LEVEL = 'INFO'
LOG_FORMAT = '%(levelname)s: %(message)s'

# Cache
HTTPCACHE_ENABLED = False

# Download timeout
DOWNLOAD_TIMEOUT = 30

# Telnet Console (disabled for production)
TELNETCONSOLE_ENABLED = False

# Middleware settings
DOWNLOADER_MIDDLEWARES = {
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,
    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110,
}
```

### src/scraper/middlewares.py
```python
import random
import logging
from scrapy.downloadermiddlewares.retry import RetryMiddleware
from scrapy.utils.response import response_status_message
from fake_useragent import UserAgent

logger = logging.getLogger(__name__)

class RotateUserAgentMiddleware:
    """Rotate user agents for each request."""
    
    def __init__(self):
        self.ua = UserAgent()
        self.user_agent_list = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
        ]
    
    def process_request(self, request, spider):
        try:
            # Try to use fake-useragent
            ua = self.ua.random
        except:
            # Fallback to predefined list
            ua = random.choice(self.user_agent_list)
        
        request.headers['User-Agent'] = ua + ' Academic Research Bot'

class CustomRetryMiddleware(RetryMiddleware):
    """Custom retry middleware with exponential backoff."""
    
    def process_response(self, request, response, spider):
        if response.status in self.retry_http_codes:
            reason = response_status_message(response.status)
            
            # Log retry attempt
            retry_times = request.meta.get('retry_times', 0) + 1
            logger.warning(
                f"Retrying {request.url} (attempt {retry_times}): {reason}"
            )
            
            # Exponential backoff
            request.meta['download_delay'] = 2 ** retry_times
            
            return self._retry(request, reason, spider) or response
        
        return response
```

### src/scraper/spiders/__init__.py
```python
# Spider modules initialization
```

### src/scraper/spiders/base_spider.py
```python
import scrapy
from abc import ABC, abstractmethod
import logging
from datetime import datetime
from urllib.parse import urljoin

logger = logging.getLogger(__name__)

class BaseJobSpider(scrapy.Spider, ABC):
    """Base spider class for job portals."""
    
    def __init__(self, country=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.country = country
        self.total_scraped = 0
        self.start_time = datetime.now()
    
    @abstractmethod
    def parse_job(self, response):
        """Parse individual job posting. Must be implemented by subclasses."""
        pass
    
    def extract_text(self, selector, xpath_or_css, method='xpath'):
        """Safely extract text from selector."""
        try:
            if method == 'xpath':
                texts = selector.xpath(xpath_or_css).getall()
            else:
                texts = selector.css(xpath_or_css).getall()
            
            # Join and clean text
            text = ' '.join(texts)
            return ' '.join(text.split()) if text else None
        except Exception as e:
            logger.error(f"Error extracting text: {e}")
            return None
    
    def build_absolute_url(self, response, relative_url):
        """Build absolute URL from relative URL."""
        return urljoin(response.url, relative_url)
    
    def log_progress(self):
        """Log scraping progress."""
        elapsed = (datetime.now() - self.start_time).total_seconds()
        rate = self.total_scraped / elapsed if elapsed > 0 else 0
        
        logger.info(
            f"Spider {self.name} - Country: {self.country} - "
            f"Scraped: {self.total_scraped} - Rate: {rate:.2f} jobs/sec"
        )
```

### src/scraper/spiders/computrabajo_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import re
from urllib.parse import urlencode
import logging

logger = logging.getLogger(__name__)

class ComputrabajoSpider(BaseJobSpider):
    name = 'computrabajo'
    allowed_domains = ['computrabajo.com', 'computrabajo.com.co', 
                      'computrabajo.com.mx', 'computrabajo.com.ar']
    
    # URL patterns by country
    country_urls = {
        'CO': 'https://www.computrabajo.com.co',
        'MX': 'https://www.computrabajo.com.mx',
        'AR': 'https://www.computrabajo.com.ar'
    }
    
    # Tech-related search terms
    tech_keywords = [
        'desarrollador', 'developer', 'programador', 'software',
        'data', 'analyst', 'engineer', 'fullstack', 'frontend',
        'backend', 'devops', 'cloud', 'mobile', 'web'
    ]
    
    def start_requests(self):
        if not self.country or self.country not in self.country_urls:
            raise ValueError(f"Invalid country: {self.country}")
        
        base_url = self.country_urls[self.country]
        
        # Generate search URLs for tech keywords
        for keyword in self.tech_keywords:
            search_url = f"{base_url}/trabajo-de-{keyword}"
            yield Request(
                url=search_url,
                callback=self.parse_search_results,
                meta={'keyword': keyword, 'page': 1}
            )
    
    def parse_search_results(self, response):
        """Parse search results page."""
        # Extract job listings
        job_cards = response.css('article.js-o-card')
        
        for card in job_cards:
            # Extract job URL
            job_url = card.css('a.js-o-card__link::attr(href)').get()
            if job_url:
                absolute_url = self.build_absolute_url(response, job_url)
                yield Request(
                    url=absolute_url,
                    callback=self.parse_job,
                    meta={'search_keyword': response.meta.get('keyword')}
                )
        
        # Check for next page
        current_page = response.meta.get('page', 1)
        next_page_link = response.css('a[aria-label="Siguiente"]::attr(href)').get()
        
        if next_page_link and current_page < 10:  # Limit to 10 pages per keyword
            next_url = self.build_absolute_url(response, next_page_link)
            yield Request(
                url=next_url,
                callback=self.parse_search_results,
                meta={
                    'keyword': response.meta.get('keyword'),
                    'page': current_page + 1
                }
            )
    
    def parse_job(self, response):
        """Parse individual job posting."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'computrabajo'
        item['country'] = self.country
        item['url'] = response.url
        
        # Title
        item['title'] = self.extract_text(
            response, 
            '//h1[@class="fwB fs24"]//text()',
            'xpath'
        )
        
        # Company
        item['company'] = self.extract_text(
            response,
            '//a[@class="dIB fs16 js-o-link"]//text()',
            'xpath'
        )
        
        # Location
        location_parts = response.xpath(
            '//div[@class="fs16 fc_base mt5"]//span//text()'
        ).getall()
        item['location'] = ', '.join(location_parts) if location_parts else None
        
        # Description and requirements
        description_sections = response.xpath(
            '//div[@class="mbB"]//p//text() | //div[@class="mbB"]//li//text()'
        ).getall()
        
        full_text = ' '.join(description_sections)
        
        # Try to separate requirements
        req_pattern = r'(?:requisitos|requerimientos|requirements|experiencia|conocimientos):(.*?)(?:funciones|responsabilidades|beneficios|$)'
        req_match = re.search(req_pattern, full_text, re.IGNORECASE | re.DOTALL)
        
        if req_match:
            item['requirements'] = req_match.group(1).strip()
            item['description'] = full_text.replace(req_match.group(0), '').strip()
        else:
            item['description'] = full_text
            item['requirements'] = None
        
        # Salary
        salary_text = self.extract_text(
            response,
            '//span[@class="fs16 fc_aux"]//text()[contains(., "$")]',
            'xpath'
        )
        item['salary_raw'] = salary_text
        
        # Contract type
        contract_info = response.xpath(
            '//span[@class="fs13 fc_aux"]//text()'
        ).getall()
        
        for info in contract_info:
            info_lower = info.lower()
            if any(term in info_lower for term in ['tiempo completo', 'full time', 'part time']):
                item['contract_type'] = info
            elif any(term in info_lower for term in ['remoto', 'presencial', 'hibrido']):
                item['remote_type'] = info
        
        # Posted date
        date_text = self.extract_text(
            response,
            '//span[@class="fs13 fc_aux"][contains(text(), "Publicado")]//text()',
            'xpath'
        )
        if date_text:
            item['posted_date'] = date_text.replace('Publicado', '').strip()
        
        # Raw HTML (for debugging/reprocessing)
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

### src/scraper/spiders/bumeran_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import json
import re
import logging

logger = logging.getLogger(__name__)

class BumeranSpider(BaseJobSpider):
    name = 'bumeran'
    allowed_domains = ['bumeran.com', 'bumeran.com.mx', 'bumeran.com.ar']
    
    # URL patterns by country
    country_urls = {
        'MX': 'https://www.bumeran.com.mx',
        'AR': 'https://www.bumeran.com.ar'
    }
    
    # Tech categories
    tech_categories = [
        'informatica-telecomunicaciones',
        'tecnologia-sistemas',
        'desarrollo-programacion'
    ]
    
    def start_requests(self):
        if self.country not in self.country_urls:
            raise ValueError(f"Bumeran not available for country: {self.country}")
        
        base_url = self.country_urls[self.country]
        
        # Generate category URLs
        for category in self.tech_categories:
            category_url = f"{base_url}/empleos-{category}.html"
            yield Request(
                url=category_url,
                callback=self.parse_category,
                meta={'category': category, 'page': 1}
            )
    
    def parse_category(self, response):
        """Parse category listing page."""
        # Check if page uses React/JSON data
        scripts = response.xpath('//script[contains(text(), "__INITIAL_STATE__")]/text()').getall()
        
        if scripts:
            # Extract JSON data from script
            for script in scripts:
                match = re.search(r'__INITIAL_STATE__\s*=\s*({.*?});', script, re.DOTALL)
                if match:
                    try:
                        data = json.loads(match.group(1))
                        jobs = self.extract_jobs_from_json(data)
                        
                        for job in jobs:
                            yield Request(
                                url=job['url'],
                                callback=self.parse_job,
                                meta={'job_data': job}
                            )
                    except json.JSONDecodeError:
                        logger.error("Failed to parse JSON data")
        else:
            # Fallback to HTML parsing
            job_links = response.css('div.Card__CardContentWrapper a::attr(href)').getall()
            
            for link in job_links:
                absolute_url = self.build_absolute_url(response, link)
                yield Request(url=absolute_url, callback=self.parse_job)
        
        # Handle pagination
        current_page = response.meta.get('page', 1)
        if current_page < 10:  # Limit pages
            next_page = current_page + 1
            next_url = response.url.replace(
                f'.html', 
                f'-pagina-{next_page}.html'
            )
            yield Request(
                url=next_url,
                callback=self.parse_category,
                meta={
                    'category': response.meta.get('category'),
                    'page': next_page
                }
            )
    
    def extract_jobs_from_json(self, data):
        """Extract job data from JSON structure."""
        jobs = []
        
        # Navigate through possible JSON structures
        try:
            if 'results' in data:
                job_list = data['results'].get('jobs', [])
            elif 'jobs' in data:
                job_list = data['jobs']
            else:
                return jobs
            
            for job in job_list:
                job_info = {
                    'url': job.get('url', ''),
                    'title': job.get('title', ''),
                    'company': job.get('company', {}).get('name', ''),
                    'location': job.get('location', '')
                }
                if job_info['url']:
                    jobs.append(job_info)
        except Exception as e:
            logger.error(f"Error extracting jobs from JSON: {e}")
        
        return jobs
    
    def parse_job(self, response):
        """Parse individual job posting."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'bumeran'
        item['country'] = self.country
        item['url'] = response.url
        
        # Try to get pre-parsed data
        job_data = response.meta.get('job_data', {})
        
        # Title
        item['title'] = job_data.get('title') or self.extract_text(
            response,
            'h1[class*="Title"]::text',
            'css'
        )
        
        # Company
        item['company'] = job_data.get('company') or self.extract_text(
            response,
            'h2[class*="Company"]::text',
            'css'
        )
        
        # Location
        item['location'] = job_data.get('location') or self.extract_text(
            response,
            'span[class*="Location"]::text',
            'css'
        )
        
        # Description
        description_selectors = [
            'div[class*="Description"]',
            'div.detalle-aviso',
            'div#description'
        ]
        
        for selector in description_selectors:
            desc_elements = response.css(f'{selector} ::text').getall()
            if desc_elements:
                item['description'] = ' '.join(desc_elements)
                break
        
        # Requirements - often within description
        if item.get('description'):
            req_pattern = r'(?:requisitos|requerimientos|experiencia|competencias):(.*?)(?:beneficios|funciones|responsabilidades|$)'
            req_match = re.search(req_pattern, item['description'], re.IGNORECASE | re.DOTALL)
            
            if req_match:
                item['requirements'] = req_match.group(1).strip()
        
        # Salary
        salary_selectors = [
            'span[class*="Salary"]::text',
            'div[class*="salary"]::text',
            'span:contains("$")::text'
        ]
        
        for selector in salary_selectors:
            salary = response.css(selector).get()
            if salary and '$' in salary:
                item['salary_raw'] = salary
                break
        
        # Contract type and remote
        tags = response.css('span[class*="Tag"]::text').getall()
        for tag in tags:
            tag_lower = tag.lower()
            if any(term in tag_lower for term in ['tiempo completo', 'part time', 'freelance']):
                item['contract_type'] = tag
            elif any(term in tag_lower for term in ['remoto', 'presencial', 'hibrido']):
                item['remote_type'] = tag
        
        # Posted date
        date_text = response.css('span[class*="Date"]::text').get()
        if date_text:
            item['posted_date'] = date_text
        
        # Raw HTML
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

### src/scraper/spiders/elempleo_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import re
import logging
from urllib.parse import urljoin, urlparse, parse_qs

logger = logging.getLogger(__name__)

class ElempleoSpider(BaseJobSpider):
    name = 'elempleo'
    allowed_domains = ['elempleo.com']
    
    # Only available for Colombia
    country_urls = {
        'CO': 'https://www.elempleo.com'
    }
    
    # Tech-related categories in elempleo
    tech_categories = {
        'tecnologia': '1100',
        'sistemas': '1100',
        'informatica': '1100'
    }
    
    def start_requests(self):
        if self.country != 'CO':
            raise ValueError("elempleo.com is only available for Colombia (CO)")
        
        base_url = self.country_urls['CO']
        
        # Search URLs for technology jobs
        search_base = f"{base_url}/colombia/empleos"
        
        # Generate search requests
        for category_name, category_id in self.tech_categories.items():
            search_params = {
                'categoria': category_id,
                'pagina': 1
            }
            
            search_url = f"{search_base}?categoria={category_id}"
            
            yield Request(
                url=search_url,
                callback=self.parse_search_results,
                meta={
                    'category': category_name,
                    'page': 1
                }
            )
    
    def parse_search_results(self, response):
        """Parse search results from elempleo."""
        # Extract job cards
        job_cards = response.css('div.result-item')
        
        if not job_cards:
            # Try alternative selectors
            job_cards = response.css('article.js-offer')
        
        for card in job_cards:
            # Extract job URL
            job_link = card.css('a.js-offer-title::attr(href)').get()
            if not job_link:
                job_link = card.css('h2 a::attr(href)').get()
            
            if job_link:
                # Handle both relative and absolute URLs
                if not job_link.startswith('http'):
                    job_link = urljoin(response.url, job_link)
                
                # Extract basic info from card
                card_data = {
                    'title': card.css('h2 a::text').get(),
                    'company': card.css('span.info-company-name::text').get(),
                    'location': card.css('span.info-city::text').get()
                }
                
                yield Request(
                    url=job_link,
                    callback=self.parse_job,
                    meta={'card_data': card_data}
                )
        
        # Handle pagination
        current_page = response.meta.get('page', 1)
        next_page_link = response.css('a.js-btn-next::attr(href)').get()
        
        if not next_page_link:
            # Alternative pagination
            pagination_links = response.css('ul.pagination a::attr(href)').getall()
            for link in pagination_links:
                if f'pagina={current_page + 1}' in link:
                    next_page_link = link
                    break
        
        if next_page_link and current_page < 10:  # Limit to 10 pages
            next_url = urljoin(response.url, next_page_link)
            yield Request(
                url=next_url,
                callback=self.parse_search_results,
                meta={
                    'category': response.meta.get('category'),
                    'page': current_page + 1
                }
            )
    
    def parse_job(self, response):
        """Parse individual job posting from elempleo."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'elempleo'
        item['country'] = 'CO'
        item['url'] = response.url
        
        # Get card data if available
        card_data = response.meta.get('card_data', {})
        
        # Title
        item['title'] = self.extract_text(
            response,
            'h1.offer-title::text',
            'css'
        ) or card_data.get('title')
        
        # Company
        item['company'] = self.extract_text(
            response,
            'div.company-name a::text',
            'css'
        ) or self.extract_text(
            response,
            'span.offer-company::text',
            'css'
        ) or card_data.get('company')
        
        # Location
        location_parts = response.css('div.offer-location span::text').getall()
        if location_parts:
            item['location'] = ', '.join(location_parts)
        else:
            item['location'] = card_data.get('location')
        
        # Main content sections
        content_sections = response.css('div.offer-description')
        
        # Description
        description_html = content_sections.css('div#description').get()
        if description_html:
            # Clean HTML and extract text
            desc_text = re.sub(r'<[^>]+>', ' ', description_html)
            item['description'] = ' '.join(desc_text.split())
        
        # Requirements
        requirements_section = content_sections.css('div#requirements')
        if requirements_section:
            req_items = requirements_section.css('li::text').getall()
            if req_items:
                item['requirements'] = ' '.join(req_items)
            else:
                req_text = requirements_section.css('::text').getall()
                item['requirements'] = ' '.join(req_text)
        
        # If requirements not in separate section, try to extract from description
        if not item.get('requirements') and item.get('description'):
            req_pattern = r'(?:requisitos|perfil|experiencia|conocimientos):(.*?)(?:funciones|responsabilidades|ofrecemos|$)'
            req_match = re.search(req_pattern, item['description'], re.IGNORECASE | re.DOTALL)
            if req_match:
                item['requirements'] = req_match.group(1).strip()
        
        # Salary
        salary_element = response.css('span.offer-salary::text').get()
        if salary_element:
            item['salary_raw'] = salary_element
        else:
            # Look for salary in description
            salary_pattern = r'\$[\d.,]+ (?:millones|COP|pesos)'
            salary_match = re.search(salary_pattern, item.get('description', ''))
            if salary_match:
                item['salary_raw'] = salary_match.group(0)
        
        # Contract type
        contract_info = response.css('div.offer-info span::text').getall()
        for info in contract_info:
            info_lower = info.lower()
            if any(term in info_lower for term in ['contrato', 'tiempo completo', 'medio tiempo']):
                item['contract_type'] = info
            elif any(term in info_lower for term in ['remoto', 'presencial', 'teletrabajo']):
                item['remote_type'] = info
        
        # Posted date
        date_element = response.css('span.offer-date::text').get()
        if date_element:
            item['posted_date'] = date_element
        else:
            # Try to extract from meta tags
            date_meta = response.css('meta[property="article:published_time"]::attr(content)').get()
            if date_meta:
                item['posted_date'] = date_meta.split('T')[0]
        
        # Additional fields from structured data
        try:
            # Check for JSON-LD structured data
            json_ld = response.css('script[type="application/ld+json"]::text').get()
            if json_ld:
                import json
                data = json.loads(json_ld)
                
                # Extract additional info if available
                if isinstance(data, dict):
                    if 'title' in data and not item.get('title'):
                        item['title'] = data['title']
                    if 'hiringOrganization' in data and not item.get('company'):
                        item['company'] = data['hiringOrganization'].get('name')
                    if 'jobLocation' in data and not item.get('location'):
                        location = data['jobLocation']
                        if isinstance(location, dict):
                            item['location'] = location.get('address', {}).get('addressLocality')
        except Exception as e:
            logger.debug(f"Could not parse structured data: {e}")
        
        # Raw HTML
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

---

## 5. Extractor Module Files

### src/extractor/__init__.py
```python
from .pipeline import ExtractionPipeline
from .ner_extractor import NERExtractor
from .regex_patterns import RegexExtractor
from .esco_matcher import ESCOMatcher

__all__ = ['ExtractionPipeline', 'NERExtractor', 'RegexExtractor', 'ESCOMatcher']
```

### src/extractor/ner_extractor.py
```python
import spacy
from spacy.tokens import Doc, Span
from typing import List, Dict, Tuple, Optional
import logging
import os
from config.settings import get_settings

logger = logging.getLogger(__name__)

class NERExtractor:
    """Extract skills using Named Entity Recognition."""
    
    def __init__(self, model_path: Optional[str] = None):
        self.settings = get_settings()
        
        # Load spaCy model
        if model_path and os.path.exists(model_path):
            self.nlp = spacy.load(model_path)
            logger.info(f"Loaded custom NER model from {model_path}")
        else:
            # Load default Spanish model
            try:
                self.nlp = spacy.load("es_core_news_lg")
                logger.info("Loaded default Spanish model")
            except:
                logger.warning("Spanish model not found, downloading...")
                os.system("python -m spacy download es_core_news_lg")
                self.nlp = spacy.load("es_core_news_lg")
        
        # Add custom pipeline components
        self._add_tech_entity_ruler()
    
    def _add_tech_entity_ruler(self):
        """Add rule-based entity recognition for tech terms."""
        ruler = self.nlp.add_pipe("entity_ruler", before="ner")
        
        # Define patterns for common tech skills
        patterns = [
            # Programming languages
            {"label": "SKILL", "pattern": [{"LOWER": "python"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "java"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "javascript"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "typescript"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "c++"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "c#"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "php"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "ruby"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "go"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "golang"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "rust"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "kotlin"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "swift"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "r"}]},
            
            # Frameworks
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "react"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "react"}, {"LOWER": "native"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "angular"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "vue"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "vue"}, {"LOWER": "js"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "django"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "flask"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "spring"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "spring"}, {"LOWER": "boot"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "node"}, {"LOWER": "js"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "nodejs"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "express"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": ".net"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "laravel"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "rails"}]},
            
            # Databases
            {"label": "DATABASE", "pattern": [{"LOWER": "mysql"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "postgresql"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "postgres"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "mongodb"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "redis"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "elasticsearch"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "oracle"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "sql"}, {"LOWER": "server"}]},
            
            # Cloud & DevOps
            {"label": "PLATFORM", "pattern": [{"LOWER": "aws"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "amazon"}, {"LOWER": "web"}, {"LOWER": "services"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "azure"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "gcp"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "google"}, {"LOWER": "cloud"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "docker"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "kubernetes"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "jenkins"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "git"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "github"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "gitlab"}]},
            
            # Data & ML
            {"label": "SKILL", "pattern": [{"LOWER": "machine"}, {"LOWER": "learning"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "deep"}, {"LOWER": "learning"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "data"}, {"LOWER": "science"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "big"}, {"LOWER": "data"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "tensorflow"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "pytorch"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "scikit"}, {"LOWER": "learn"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "pandas"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "numpy"}]},
            
            # Methodologies
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "agile"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "scrum"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "kanban"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "devops"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "ci"}, {"LOWER": "cd"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "tdd"}]},
        ]
        
        # Add Spanish variations
        spanish_patterns = [
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "web"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "movil"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "móvil"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "base"}, {"LOWER": "de"}, {"LOWER": "datos"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "inteligencia"}, {"LOWER": "artificial"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "aprendizaje"}, {"LOWER": "automatico"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "aprendizaje"}, {"LOWER": "automático"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "ciencia"}, {"LOWER": "de"}, {"LOWER": "datos"}]},
        ]
        
        ruler.add_patterns(patterns + spanish_patterns)
    
    def extract(self, text: str, source_section: str = "unknown") -> List[Dict[str, any]]:
        """Extract skills from text using NER.
        
        Args:
            text: Text to extract skills from
            source_section: Section of job posting (title, description, requirements)
            
        Returns:
            List of extracted skills with metadata
        """
        if not text:
            return []
        
        # Process text with spaCy
        doc = self.nlp(text)
        
        extracted_skills = []
        seen_skills = set()
        
        # Extract entities
        for ent in doc.ents:
            if ent.label_ in ["SKILL", "FRAMEWORK", "DATABASE", "PLATFORM", "TOOL", "METHODOLOGY"]:
                skill_text = ent.text.lower().strip()
                
                # Skip if already seen
                if skill_text in seen_skills:
                    continue
                
                seen_skills.add(skill_text)
                
                extracted_skills.append({
                    "skill_text": skill_text,
                    "skill_type": "explicit",
                    "extraction_method": "ner",
                    "entity_label": ent.label_,
                    "confidence_score": 0.9,  # High confidence for NER
                    "source_section": source_section,
                    "span_start": ent.start_char,
                    "span_end": ent.end_char,
                    "context": text[max(0, ent.start_char-50):ent.end_char+50]
                })
        
        logger.debug(f"NER extracted {len(extracted_skills)} skills from {source_section}")
        
        return extracted_skills
    
    def extract_from_job(self, job_data: Dict[str, str]) -> List[Dict[str, any]]:
        """Extract skills from all sections of a job posting.
        
        Args:
            job_data: Dictionary with job fields (title, description, requirements)
            
        Returns:
            List of all extracted skills
        """
        all_skills = []
        
        # Extract from title
        if job_data.get('title'):
            title_skills = self.extract(job_data['title'], 'title')
            all_skills.extend(title_skills)
        
        # Extract from description
        if job_data.get('description'):
            desc_skills = self.extract(job_data['description'], 'description')
            all_skills.extend(desc_skills)
        
        # Extract from requirements
        if job_data.get('requirements'):
            req_skills = self.extract(job_data['requirements'], 'requirements')
            all_skills.extend(req_skills)
        
        return all_skills
```

### src/extractor/regex_patterns.py
```python
import re
from typing import List, Dict, Tuple
import logging

logger = logging.getLogger(__name__)

class RegexExtractor:
    """Extract skills using regular expressions."""
    
    def __init__(self):
        # Define regex patterns for skill extraction
        self.patterns = self._build_patterns()
    
    def _build_patterns(self) -> List[Tuple[str, re.Pattern, str]]:
        """Build regex patterns for skill extraction.
        
        Returns:
            List of tuples (pattern_name, compiled_regex, skill_type)
        """
        patterns = []
        
        # Experience patterns in Spanish
        experience_patterns = [
            (
                "experiencia_en",
                re.compile(
                    r"experiencia\s+(?:en|con)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "conocimientos_de",
                re.compile(
                    r"conocimientos?\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "manejo_de",
                re.compile(
                    r"manejo\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "tool"
            ),
            (
                "dominio_de",
                re.compile(
                    r"dominio\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "desarrollo_en",
                re.compile(
                    r"desarrollo\s+(?:en|con)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
        ]
        
        # Required skills patterns
        required_patterns = [
            (
                "requerimos",
                re.compile(
                    r"(?:requerimos|buscamos|necesitamos)\s+(?:personas?\s+con\s+)?(?:conocimientos?\s+en\s+)?([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+para\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "indispensable",
                re.compile(
                    r"(?:indispensable|fundamental|esencial)\s+(?:contar\s+con\s+)?(?:conocimientos?\s+en\s+)?([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
        ]
        
        # Technology stack patterns
        tech_stack_patterns = [
            (
                "tecnologias",
                re.compile(
                    r"(?:tecnologías?|herramientas?|lenguajes?)\s*:\s*([A-Za-z0-9\+\#\.\s,/\-\(\)]+?)(?:\n|$)",
                    re.IGNORECASE
                ),
                "mixed"
            ),
            (
                "stack_tecnologico",
                re.compile(
                    r"stack\s+(?:tecnológico|tech)\s*:\s*([A-Za-z0-9\+\#\.\s,/\-\(\)]+?)(?:\n|$)",
                    re.IGNORECASE
                ),
                "mixed"
            ),
        ]
        
        # List patterns (bullet points, numbered lists)
        list_patterns = [
            (
                "bullet_skills",
                re.compile(
                    r"(?:^|\n)\s*[\-\*\•]\s*([A-Za-z0-9\+\#\.\s]+?)(?:\n|$)",
                    re.MULTILINE
                ),
                "skill"
            ),
            (
                "numbered_skills",
                re.compile(
                    r"(?:^|\n)\s*\d+[\.\)]\s*([A-Za-z0-9\+\#\.\s]+?)(?:\n|$)",
                    re.MULTILINE
                ),
                "skill"
            ),
        ]
        
        # Certification patterns
        cert_patterns = [
            (
                "certificacion",
                re.compile(
                    r"(?:certificación|certificado)\s+(?:en\s+)?([A-Za-z0-9\+\#\.\s\-]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "certification"
            ),
        ]
        
        # Years of experience patterns
        experience_years_patterns = [
            (
                "años_experiencia",
                re.compile(
                    r"(\d+)\s*\+?\s*años?\s+(?:de\s+)?experiencia\s+(?:en\s+)?([A-Za-z0-9\+\#\.\s]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "skill_with_years"
            ),
        ]
        
        # Combine all patterns
        patterns.extend(experience_patterns)
        patterns.extend(required_patterns)
        patterns.extend(tech_stack_patterns)
        patterns.extend(list_patterns)
        patterns.extend(cert_patterns)
        patterns.extend(experience_years_patterns)
        
        return patterns
    
    def extract(self, text: str, source_section: str = "unknown") -> List[Dict[str, any]]:
        """Extract skills using regex patterns.
        
        Args:
            text: Text to extract skills from
            source_section: Section of job posting
            
        Returns:
            List of extracted skills with metadata
        """
        if not text:
            return []
        
        extracted_skills = []
        seen_skills = set()
        
        for pattern_name, regex, skill_type in self.patterns:
            matches = regex.finditer(text)
            
            for match in matches:
                if skill_type == "skill_with_years":
                    # Special handling for years of experience
                    years = match.group(1)
                    skill_text = match.group(2).strip().lower()
                    
                    if skill_text and skill_text not in seen_skills:
                        seen_skills.add(skill_text)
                        extracted_skills.append({
                            "skill_text": skill_text,
                            "skill_type": "explicit",
                            "extraction_method": "regex",
                            "pattern_name": pattern_name,
                            "confidence_score": 0.8,
                            "source_section": source_section,
                            "span_start": match.start(2),
                            "span_end": match.end(2),
                            "years_required": int(years),
                            "context": text[max(0, match.start()-30):match.end()+30]
                        })
                else:
                    # Normal skill extraction
                    skill_text = match.group(1).strip().lower()
                    
                    # Clean up extracted text
                    skill_text = self._clean_skill_text(skill_text)
                    
                    if skill_text and len(skill_text) > 1 and skill_text not in seen_skills:
                        # Additional validation
                        if self._is_valid_skill(skill_text):
                            seen_skills.add(skill_text)
                            
                            extracted_skills.append({
                                "skill_text": skill_text,
                                "skill_type": "explicit",
                                "extraction_method": "regex",
                                "pattern_name": pattern_name,
                                "confidence_score": 0.7,  # Lower than NER
                                "source_section": source_section,
                                "span_start": match.start(1),
                                "span_end": match.end(1),
                                "context": text[max(0, match.start()-30):match.end()+30]
                            })
        
        # Handle comma-separated lists within extracted skills
        expanded_skills = []
        for skill in extracted_skills:
            if ',' in skill['skill_text'] or ' y ' in skill['skill_text']:
                # Split and create individual skills
                parts = re.split(r'[,\s]+y\s+|,\s*', skill['skill_text'])
                for part in parts:
                    part = part.strip()
                    if part and self._is_valid_skill(part):
                        new_skill = skill.copy()
                        new_skill['skill_text'] = part
                        expanded_skills.append(new_skill)
            else:
                expanded_skills.append(skill)
        
        logger.debug(f"Regex extracted {len(expanded_skills)} skills from {source_section}")
        
        return expanded_skills
    
    def _clean_skill_text(self, text: str) -> str:
        """Clean extracted skill text.
        
        Args:
            text: Raw extracted text
            
        Returns:
            Cleaned skill text
        """
        # Remove common stop words at the beginning/end
        stop_words = [
            'el', 'la', 'los', 'las', 'un', 'una', 'de', 'del', 'al',
            'y', 'o', 'con', 'para', 'por', 'en', 'a'
        ]
        
        words = text.split()
        
        # Remove stop words from beginning
        while words and words[0].lower() in stop_words:
            words.pop(0)
        
        # Remove stop words from end
        while words and words[-1].lower() in stop_words:
            words.pop()
        
        cleaned = ' '.join(words)
        
        # Remove extra spaces and punctuation
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = re.sub(r'[^\w\s\+\#\.\-/]', '', cleaned)
        
        return cleaned.strip()
    
    def _is_valid_skill(self, skill_text: str) -> bool:
        """Validate if extracted text is likely a valid skill.
        
        Args:
            skill_text: Text to validate
            
        Returns:
            True if valid skill, False otherwise
        """
        # Check minimum length
        if len(skill_text) < 2:
            return False
        
        # Check if it's just numbers
        if skill_text.isdigit():
            return False
        
        # Check against blacklist of common false positives
        blacklist = [
            'años', 'año', 'experiencia', 'conocimiento', 'manejo',
            'desarrollo', 'persona', 'profesional', 'trabajo',
            'empresa', 'cliente', 'proyecto', 'equipo', 'area',
            'sistemas', 'tecnologia', 'informatica'  # Too generic
        ]
        
        if skill_text.lower() in blacklist:
            return False
        
        # Must contain at least one letter
        if not any(c.isalpha() for c in skill_text):
            return False
        
        return True
    
    def extract_from_job(self, job_data: Dict[str, str]) -> List[Dict[str, any]]:
        """Extract skills from all sections of a job posting.
        
        Args:
            job_data: Dictionary with job fields
            
        Returns:
            List of all extracted skills
        """
        all_skills = []
        
        # Process each section
        for section in ['title', 'description', 'requirements']:
            if job_data.get(section):
                section_skills = self.extract(job_data[section], section)
                all_skills.extend(section_skills)
        
        return all_skills
```

### src/extractor/esco_matcher.py
```python
import json
import logging
from typing import List, Dict, Optional, Set
from fuzzywuzzy import fuzz, process
import requests
from pathlib import Path
import yaml

logger = logging.getLogger(__name__)

class ESCOMatcher:
    """Match extracted skills to ESCO taxonomy."""
    
    def __init__(self, config_path: str = "config/esco_config.yaml"):
        self.config = self._load_config(config_path)
        self.skills_cache = {}
        self.local_mappings = self.config.get('tech_mappings', {})
        
        # Load local ESCO data if available
        self.local_esco_data = self._load_local_esco_data()
    
    def _load_config(self, config_path: str) -> dict:
        """Load ESCO configuration."""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config['esco']
        except Exception as e:
            logger.error(f"Failed to load ESCO config: {e}")
            return {}
    
    def _load_local_esco_data(self) -> Dict[str, Dict]:
        """Load local ESCO data files if available."""
        esco_data = {}
        
        # Try to load skills data
        skills_path = Path("data/esco/skills_es.csv")
        if skills_path.exists():
            try:
                import pandas as pd
                df = pd.read_csv(skills_path)
                
                for _, row in df.iterrows():
                    skill_uri = row.get('conceptUri', '')
                    esco_data[skill_uri] = {
                        'preferredLabel': row.get('preferredLabel', ''),
                        'altLabels': row.get('altLabels', '').split('|') if row.get('altLabels') else [],
                        'description': row.get('description', ''),
                        'skillType': row.get('skillType', '')
                    }
                
                logger.info(f"Loaded {len(esco_data)} ESCO skills from local file")
            except Exception as e:
                logger.error(f"Failed to load local ESCO data: {e}")
        
        return esco_data
    
    def match_skill(self, skill_text: str, threshold: float = 0.8) -> Optional[Dict[str, any]]:
        """Match a skill to ESCO taxonomy.
        
        Args:
            skill_text: Skill text to match
            threshold: Minimum similarity threshold (0-1)
            
        Returns:
            ESCO match data or None
        """
        skill_lower = skill_text.lower().strip()
        
        # Check direct mappings first
        if skill_lower in self.local_mappings:
            esco_uri = self.local_mappings[skill_lower]
            
            # Get details from local data or cache
            if esco_uri in self.local_esco_data:
                return {
                    'esco_uri': esco_uri,
                    'esco_preferred_label': self.local_esco_data[esco_uri]['preferredLabel'],
                    'match_type': 'direct',
                    'match_score': 1.0
                }
        
        # Try fuzzy matching against local data
        if self.local_esco_data:
            best_match = self._fuzzy_match_local(skill_text, threshold)
            if best_match:
                return best_match
        
        # Try API lookup (if configured and no local match)
        if self.config.get('base_url'):
            api_match = self._api_lookup(skill_text)
            if api_match:
                return api_match
        
        return None
    
    def _fuzzy_match_local(self, skill_text: str, threshold: float) -> Optional[Dict[str, any]]:
        """Fuzzy match against local ESCO data.
        
        Args:
            skill_text: Skill to match
            threshold: Minimum score threshold
            
        Returns:
            Best match or None
        """
        # Collect all labels for matching
        all_labels = []
        for uri, data in self.local_esco_data.items():
            # Add preferred label
            all_labels.append((data['preferredLabel'].lower(), uri, 'preferred'))
            
            # Add alternative labels
            for alt_label in data.get('altLabels', []):
                if alt_label:
                    all_labels.append((alt_label.lower(), uri, 'alternative'))
        
        # Find best match
        if all_labels:
            # Use token sort ratio for better matching of multi-word skills
            matches = process.extract(
                skill_text.lower(),
                [label[0] for label in all_labels],
                scorer=fuzz.token_sort_ratio,
                limit=3
            )
            
            for match_text, score in matches:
                if score >= threshold * 100:  # fuzzywuzzy uses 0-100 scale
                    # Find the corresponding URI
                    for label_text, uri, label_type in all_labels:
                        if label_text == match_text:
                            return {
                                'esco_uri': uri,
                                'esco_preferred_label': self.local_esco_data[uri]['preferredLabel'],
                                'match_type': f'fuzzy_{label_type}',
                                'match_score': score / 100.0,
                                'matched_text': match_text
                            }
        
        return None
    
    def _api_lookup(self, skill_text: str) -> Optional[Dict[str, any]]:
        """Look up skill using ESCO API.
        
        Args:
            skill_text: Skill to look up
            
        Returns:
            API match data or None
        """
        try:
            # Check cache first
            if skill_text in self.skills_cache:
                return self.skills_cache[skill_text]
            
            # Prepare API request
            api_url = f"{self.config['base_url']}{self.config['endpoints']['search']}"
            
            params = {
                'text': skill_text,
                'language': self.config['language']['primary'],
                'type': 'skill',
                'limit': 5
            }
            
            headers = {
                'Accept': 'application/json',
                'Accept-Language': self.config['language']['primary']
            }
            
            # Make request
            response = requests.get(api_url, params=params, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                if data.get('results'):
                    # Get first result
                    result = data['results'][0]
                    
                    match_data = {
                        'esco_uri': result.get('uri', ''),
                        'esco_preferred_label': result.get('preferredLabel', {}).get(
                            self.config['language']['primary'],
                            result.get('preferredLabel', {}).get('en', '')
                        ),
                        'match_type': 'api_search',
                        'match_score': result.get('score', 0.0)
                    }
                    
                    # Cache the result
                    self.skills_cache[skill_text] = match_data
                    
                    return match_data
            
        except Exception as e:
            logger.error(f"ESCO API lookup failed for '{skill_text}': {e}")
        
        return None
    
    def match_skills_batch(self, skills: List[str]) -> Dict[str, Optional[Dict]]:
        """Match multiple skills to ESCO taxonomy.
        
        Args:
            skills: List of skill texts
            
        Returns:
            Dictionary mapping skill text to ESCO match data
        """
        results = {}
        
        for skill in skills:
            match = self.match_skill(skill)
            results[skill] = match
        
        # Log statistics
        matched = sum(1 for v in results.values() if v is not None)
        logger.info(
            f"ESCO matching: {matched}/{len(skills)} skills matched "
            f"({matched/len(skills)*100:.1f}%)"
        )
        
        return results

### src/analyzer/dimension_reducer.py
```python
import logging
from typing import Tuple, Dict, Any, Optional
import numpy as np
from umap import UMAP
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import time

logger = logging.getLogger(__name__)

class DimensionReducer:
    """Reduce dimensionality of embeddings for visualization and clustering."""
    
    def __init__(self):
        self.reducers = {}
    
    def reduce_dimensions(self,
                         embeddings: np.ndarray,
                         method: str = 'umap',
                         n_components: int = 2,
                         **kwargs) -> Tuple[np.ndarray, Dict[str, Any]]:
        """Reduce dimensionality of embeddings.
        
        Args:
            embeddings: High-dimensional embeddings
            method: Reduction method ('umap', 'pca', 'tsne')
            n_components: Number of output dimensions
            **kwargs: Additional parameters for the method
            
        Returns:
            Tuple of (reduced embeddings, metadata)
        """
        logger.info(
            f"Reducing {embeddings.shape} to {n_components}D using {method}"
        )
        
        start_time = time.time()
        
        if method == 'umap':
            reduced, reducer = self._reduce_umap(embeddings, n_components, **kwargs)
        elif method == 'pca':
            reduced, reducer = self._reduce_pca(embeddings, n_components, **kwargs)
        elif method == 'tsne':
            reduced, reducer = self._reduce_tsne(embeddings, n_components, **kwargs)
        else:
            raise ValueError(f"Unknown reduction method: {method}")
        
        processing_time = time.time() - start_time
        
        # Store reducer for later use
        self.reducers[method] = reducer
        
        metadata = {
            'method': method,
            'n_components': n_components,
            'original_shape': embeddings.shape,
            'reduced_shape': reduced.shape,
            'processing_time': processing_time,
            'parameters': kwargs
        }
        
        logger.info(f"Dimension reduction complete in {processing_time:.2f}s")
        
        return reduced, metadata
    
    def _reduce_umap(self,
                    embeddings: np.ndarray,
                    n_components: int,
                    n_neighbors: int = 15,
                    min_dist: float = 0.1,
                    metric: str = 'cosine') -> Tuple[np.ndarray, UMAP]:
        """Reduce dimensions using UMAP.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            n_neighbors: UMAP n_neighbors parameter
            min_dist: UMAP min_dist parameter
            metric: Distance metric
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        reducer = UMAP(
            n_components=n_components,
            n_neighbors=n_neighbors,
            min_dist=min_dist,
            metric=metric,
            random_state=42,
            verbose=True
        )
        
        reduced = reducer.fit_transform(embeddings)
        
        return reduced, reducer
    
    def _reduce_pca(self,
                   embeddings: np.ndarray,
                   n_components: int) -> Tuple[np.ndarray, PCA]:
        """Reduce dimensions using PCA.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        reducer = PCA(n_components=n_components, random_state=42)
        reduced = reducer.fit_transform(embeddings)
        
        # Log explained variance
        if hasattr(reducer, 'explained_variance_ratio_'):
            total_variance = np.sum(reducer.explained_variance_ratio_)
            logger.info(f"PCA explained variance: {total_variance:.2%}")
        
        return reduced, reducer
    
    def _reduce_tsne(self,
                    embeddings: np.ndarray,
                    n_components: int,
                    perplexity: float = 30.0,
                    learning_rate: float = 200.0) -> Tuple[np.ndarray, TSNE]:
        """Reduce dimensions using t-SNE.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            perplexity: t-SNE perplexity parameter
            learning_rate: t-SNE learning rate
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        # For t-SNE, first reduce with PCA if dimensions > 50
        if embeddings.shape[1] > 50:
            logger.info("Pre-reducing with PCA for t-SNE")
            pca = PCA(n_components=50, random_state=42)
            embeddings = pca.fit_transform(embeddings)
        
        reducer = TSNE(
            n_components=n_components,
            perplexity=perplexity,
            learning_rate=learning_rate,
            random_state=42,
            verbose=1
        )
        
        reduced = reducer.fit_transform(embeddings)
        
        return reduced, reducer
    
    def transform_new_points(self,
                           embeddings: np.ndarray,
                           method: str = 'umap') -> np.ndarray:
        """Transform new points using existing reducer.
        
        Args:
            embeddings: New embeddings to transform
            method: Which reducer to use
            
        Returns:
            Transformed embeddings
        """
        if method not in self.reducers:
            raise ValueError(f"No {method} reducer available. Run reduce_dimensions first.")
        
        reducer = self.reducers[method]
        
        if method == 'tsne':
            logger.warning("t-SNE doesn't support transform. Returning original embeddings.")
            return embeddings
        
        return reducer.transform(embeddings)
    
    def create_embedding_map(self,
                           embeddings: np.ndarray,
                           labels: Optional[np.ndarray] = None,
                           skill_texts: Optional[list] = None) -> Dict[str, Any]:
        """Create a complete embedding map with 2D coordinates.
        
        Args:
            embeddings: Original embeddings
            labels: Cluster labels (optional)
            skill_texts: Skill names (optional)
            
        Returns:
            Dictionary with 2D coordinates and metadata
        """
        # Reduce to 2D
        coords_2d, metadata = self.reduce_dimensions(embeddings, method='umap', n_components=2)
        
        # Create map
        embedding_map = {
            'coordinates': coords_2d,
            'metadata': metadata
        }
        
        if labels is not None:
            embedding_map['labels'] = labels
        
        if skill_texts is not None:
            embedding_map['skills'] = skill_texts
        
        # Add statistics
        embedding_map['stats'] = {
            'x_range': (float(np.min(coords_2d[:, 0])), float(np.max(coords_2d[:, 0]))),
            'y_range': (float(np.min(coords_2d[:, 1])), float(np.max(coords_2d[:, 1]))),
            'center': (float(np.mean(coords_2d[:, 0])), float(np.mean(coords_2d[:, 1])))
        }
        
        return embedding_map

### src/analyzer/report_generator.py
```python
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import os
from pathlib import Path
from reportlab.lib import colors
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, PageBreak, Image
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from database.operations import DatabaseOperations

logger = logging.getLogger(__name__)

class ReportGenerator:
    """Generate PDF reports with analysis results."""
    
    def __init__(self, output_dir: str = None):
        self.output_dir = output_dir or "./outputs"
        os.makedirs(self.output_dir, exist_ok=True)
        self.db_ops = DatabaseOperations()
        self.styles = getSampleStyleSheet()
        self._add_custom_styles()
    
    def _add_custom_styles(self):
        """Add custom styles for the report."""
        self.styles.add(ParagraphStyle(
            name='CustomTitle',
            parent=self.styles['Title'],
            fontSize=24,
            textColor=colors.HexColor('#1a73e8'),
            spaceAfter=30
        ))
        
        self.styles.add(ParagraphStyle(
            name='SectionHeader',
            parent=self.styles['Heading1'],
            fontSize=16,
            textColor=colors.HexColor('#1a73e8'),
            spaceAfter=12
        ))
        
        self.styles.add(ParagraphStyle(
            name='SubsectionHeader',
            parent=self.styles['Heading2'],
            fontSize=14,
            textColor=colors.HexColor('#5f6368'),
            spaceAfter=10
        ))
    
    def generate_full_report(self, 
                           country: Optional[str] = None,
                           include_visualizations: bool = True) -> str:
        """Generate comprehensive analysis report.
        
        Args:
            country: Country code to filter by (optional)
            include_visualizations: Whether to include charts
            
        Returns:
            Path to generated report
        """
        # Create timestamp for filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        country_suffix = f"_{country}" if country else "_all"
        filename = f"labor_market_analysis{country_suffix}_{timestamp}.pdf"
        filepath = os.path.join(self.output_dir, filename)
        
        # Create document
        doc = SimpleDocTemplate(
            filepath,
            pagesize=A4,
            rightMargin=72,
            leftMargin=72,
            topMargin=72,
            bottomMargin=18
        )
        
        # Build content
        story = []
        
        # Title page
        story.extend(self._create_title_page(country))
        story.append(PageBreak())
        
        # Executive summary
        story.extend(self._create_executive_summary(country))
        story.append(PageBreak())
        
        # Skills analysis
        story.extend(self._create_skills_analysis(country, include_visualizations))
        story.append(PageBreak())
        
        # Clustering results
        story.extend(self._create_clustering_analysis(include_visualizations))
        story.append(PageBreak())
        
        # Temporal trends (if available)
        story.extend(self._create_temporal_analysis(country, include_visualizations))
        story.append(PageBreak())
        
        # Methodology
        story.extend(self._create_methodology_section())
        
        # Build PDF
        doc.build(story)
        
        logger.info(f"Report generated: {filepath}")
        return filepath
    
    def _create_title_page(self, country: Optional[str]) -> List:
        """Create title page elements."""
        elements = []
        
        # Title
        title_text = "Observatorio de Demanda Laboral Tecnológica"
        if country:
            country_names = {'CO': 'Colombia', 'MX': 'México', 'AR': 'Argentina'}
            title_text += f"\n{country_names.get(country, country)}"
        else:
            title_text += "\nAmérica Latina"
        
        elements.append(Paragraph(title_text, self.styles['CustomTitle']))
        elements.append(Spacer(1, 0.5*inch))
        
        # Subtitle
        subtitle = "Análisis Automatizado de Habilidades Técnicas"
        elements.append(Paragraph(subtitle, self.styles['Heading2']))
        elements.append(Spacer(1, 0.3*inch))
        
        # Date
        date_text = f"Fecha de generación: {datetime.now().strftime('%d de %B de %Y')}"
        elements.append(Paragraph(date_text, self.styles['Normal']))
        elements.append(Spacer(1, 2*inch))
        
        # Authors/Institution
        elements.append(Paragraph("Universidad XYZ", self.styles['Normal']))
        elements.append(Paragraph("Facultad de Ingeniería", self.styles['Normal']))
        
        return elements
    
    def _create_executive_summary(self, country: Optional[str]) -> List:
        """Create executive summary section."""
        elements = []
        
        elements.append(Paragraph("Resumen Ejecutivo", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get summary statistics
        stats = self.db_ops.get_skill_statistics(country)
        
        # Summary text
        summary_points = [
            f"Se analizaron un total de {stats.get('total_unique_skills', 0)} habilidades técnicas únicas.",
            f"Las 5 habilidades más demandadas son: {', '.join([s['skill'] for s in stats.get('top_skills', [])[:5]])}.",
            "El análisis revela una fuerte demanda de habilidades en desarrollo web, cloud computing y ciencia de datos.",
            "Se identificaron patrones emergentes en tecnologías de inteligencia artificial y DevOps."
        ]
        
        for point in summary_points:
            elements.append(Paragraph(f"• {point}", self.styles['Normal']))
            elements.append(Spacer(1, 0.1*inch))
        
        return elements
    
    def _create_skills_analysis(self, country: Optional[str], include_viz: bool) -> List:
        """Create skills analysis section."""
        elements = []
        
        elements.append(Paragraph("Análisis de Habilidades", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get skill statistics
        stats = self.db_ops.get_skill_statistics(country)
        
        # Top skills table
        elements.append(Paragraph("Top 20 Habilidades Más Demandadas", self.styles['SubsectionHeader']))
        
        if stats.get('top_skills'):
            # Create table data
            table_data = [['Posición', 'Habilidad', 'Frecuencia']]
            for i, skill_data in enumerate(stats['top_skills'][:20], 1):
                table_data.append([
                    str(i),
                    skill_data['skill'],
                    str(skill_data['count'])
                ])
            
            # Create table
            table = Table(table_data, colWidths=[1*inch, 3*inch, 1.5*inch])
            table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            
            elements.append(table)
            elements.append(Spacer(1, 0.3*inch))
        
        # Add visualization if requested
        if include_viz:
            viz_path = self._create_skill_frequency_chart(stats.get('top_skills', [])[:15])
            if viz_path:
                elements.append(Image(viz_path, width=6*inch, height=4*inch))
                elements.append(Spacer(1, 0.2*inch))
        
        return elements
    
    def _create_clustering_analysis(self, include_viz: bool) -> List:
        """Create clustering analysis section."""
        elements = []
        
        elements.append(Paragraph("Análisis de Clustering", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get latest clustering results
        # Note: This would need to be implemented in DatabaseOperations
        # For now, we'll use placeholder text
        
        elements.append(Paragraph(
            "El análisis de clustering identificó grupos coherentes de habilidades "
            "que típicamente aparecen juntas en las ofertas laborales:",
            self.styles['Normal']
        ))
        elements.append(Spacer(1, 0.1*inch))
        
        # Placeholder cluster descriptions
        clusters = [
            "Frontend Development: React, Vue.js, CSS, JavaScript, HTML5",
            "Backend Development: Node.js, Python, Django, Flask, API REST",
            "Data Science: Python, R, Machine Learning, SQL, Pandas",
            "DevOps: Docker, Kubernetes, AWS, CI/CD, Jenkins",
            "Mobile Development: React Native, Flutter, iOS, Android"
        ]
        
        for cluster in clusters:
            elements.append(Paragraph(f"• {cluster}", self.styles['Normal']))
            elements.append(Spacer(1, 0.05*inch))
        
        return elements
    
    def _create_temporal_analysis(self, country: Optional[str], include_viz: bool) -> List:
        """Create temporal trends analysis section."""
        elements = []
        
        elements.append(Paragraph("Tendencias Temporales", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        elements.append(Paragraph(
            "El análisis temporal permite identificar la evolución de la demanda "
            "de habilidades técnicas a lo largo del tiempo.",
            self.styles['Normal']
        ))
        
        # Placeholder for temporal analysis
        trends = [
            "Crecimiento sostenido en demanda de habilidades cloud (AWS, Azure)",
            "Aumento significativo en tecnologías de IA/ML en los últimos 6 meses",
            "Estabilidad en frameworks tradicionales (Spring, .NET)",
            "Emergencia de nuevas herramientas DevOps"
        ]
        
        elements.append(Spacer(1, 0.1*inch))
        for trend in trends:
            elements.append(Paragraph(f"• {trend}", self.styles['Normal']))
            elements.append(Spacer(1, 0.05*inch))
        
        return elements
    
    def _create_methodology_section(self) -> List:
        """Create methodology section."""
        elements = []
        
        elements.append(Paragraph("Metodología", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        methodology_text = """
        Este análisis se realizó mediante un pipeline automatizado que incluye:
        
        1. Web Scraping: Recolección automática de ofertas laborales de portales como 
           Computrabajo, Bumeran y elempleo.com.
        
        2. Extracción de Habilidades: Combinación de técnicas de NER (Named Entity Recognition) 
           y expresiones regulares para identificar menciones de habilidades técnicas.
        
        3. Enriquecimiento con LLM: Uso de modelos de lenguaje para identificar habilidades 
           implícitas y normalizar variaciones.
        
        4. Análisis Semántico: Generación de embeddings multilingües y clustering para 
           identificar grupos de habilidades relacionadas.
        
        5. Visualización: Generación de reportes estáticos con métricas agregadas y 
           visualizaciones interpretables.
        """
        
        elements.append(Paragraph(methodology_text, self.styles['Normal']))
        
        return elements
    
    def _create_skill_frequency_chart(self, top_skills: List[Dict[str, Any]]) -> Optional[str]:
        """Create skill frequency bar chart.
        
        Args:
            top_skills: List of top skills with counts
            
        Returns:
            Path to saved chart image
        """
        if not top_skills:
            return None
        
        try:
            # Prepare data
            skills = [s['skill'] for s in top_skills]
            counts = [s['count'] for s in top_skills]
            
            # Create figure
            plt.figure(figsize=(10, 6))
            
            # Create horizontal bar chart
            bars = plt.barh(skills, counts, color='#1a73e8')
            
            # Customize
            plt.xlabel('Número de Vacantes', fontsize=12)
            plt.title('Habilidades Más Demandadas', fontsize=14, fontweight='bold')
            plt.gca().invert_yaxis()  # Highest on top
            
            # Add value labels
            for i, bar in enumerate(bars):
                width = bar.get_width()
                plt.text(width + 1, bar.get_y() + bar.get_height()/2, 
                        f'{counts[i]}', 
                        ha='left', va='center')
            
            plt.tight_layout()
            
            # Save
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(self.output_dir, f"skill_frequency_{timestamp}.png")
            plt.savefig(filepath, dpi=300, bbox_inches='tight')
            plt.close()
            
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating skill frequency chart: {e}")
            return None
    
    def enrich_extracted_skills(self, extracted_skills: List[Dict]) -> List[Dict]:
        """Enrich extracted skills with ESCO matches.
        
        Args:
            extracted_skills: List of extracted skill dictionaries
            
        Returns:
            Enriched skill list
        """
        # Get unique skill texts
        unique_skills = list(set(skill['skill_text'] for skill in extracted_skills))
        
        # Match all unique skills
        esco_matches = self.match_skills_batch(unique_skills)
        
        # Enrich original skills
        enriched = []
        for skill in extracted_skills:
            enriched_skill = skill.copy()
            
            match = esco_matches.get(skill['skill_text'])
            if match:
                enriched_skill.update({
                    'esco_uri': match['esco_uri'],
                    'esco_preferred_label': match['esco_preferred_label'],
                    'esco_match_type': match['match_type'],
                    'esco_match_score': match['match_score']
                })
            
            enriched.append(enriched_skill)
        
        return enriched
```

### src/extractor/pipeline.py
```python
import logging
from typing import List, Dict, Optional
from database.operations import DatabaseOperations
from .ner_extractor import NERExtractor
from .regex_patterns import RegexExtractor
from .esco_matcher import ESCOMatcher
from config.settings import get_settings
import time

logger = logging.getLogger(__name__)

class ExtractionPipeline:
    """Main pipeline for skill extraction from job postings."""
    
    def __init__(self):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        
        # Initialize extractors
        logger.info("Initializing extraction components...")
        self.ner_extractor = NERExtractor()
        self.regex_extractor = RegexExtractor()
        self.esco_matcher = ESCOMatcher()
        
        logger.info("Extraction pipeline initialized")
    
    def process_batch(self, batch_size: int = 100) -> Dict[str, any]:
        """Process a batch of unprocessed jobs.
        
        Args:
            batch_size: Number of jobs to process
            
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'jobs_processed': 0,
            'skills_extracted': 0,
            'esco_matches': 0,
            'errors': 0
        }
        
        try:
            # Get unprocessed jobs
            jobs = self.db_ops.get_unprocessed_jobs(limit=batch_size)
            logger.info(f"Processing {len(jobs)} jobs")
            
            for job in jobs:
                try:
                    # Process individual job
                    skills = self.process_job(job)
                    
                    if skills:
                        # Save extracted skills
                        self.db_ops.insert_extracted_skills(
                            str(job.job_id),
                            skills
                        )
                        
                        # Mark job as processed
                        self.db_ops.mark_job_processed(str(job.job_id))
                        
                        # Update stats
                        stats['jobs_processed'] += 1
                        stats['skills_extracted'] += len(skills)
                        stats['esco_matches'] += sum(
                            1 for s in skills if s.get('esco_uri')
                        )
                    
                except Exception as e:
                    logger.error(f"Error processing job {job.job_id}: {e}")
                    stats['errors'] += 1
            
            # Calculate processing time
            stats['processing_time'] = time.time() - start_time
            stats['jobs_per_second'] = stats['jobs_processed'] / stats['processing_time'] if stats['processing_time'] > 0 else 0
            
            logger.info(
                f"Batch complete: {stats['jobs_processed']} jobs, "
                f"{stats['skills_extracted']} skills extracted, "
                f"{stats['esco_matches']} ESCO matches, "
                f"{stats['errors']} errors, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"Batch processing failed: {e}")
            raise
    
    def process_job(self, job) -> List[Dict[str, any]]:
        """Process a single job to extract skills.
        
        Args:
            job: Job object from database
            
        Returns:
            List of extracted and enriched skills
        """
        # Prepare job data
        job_data = {
            'title': job.title,
            'description': job.description,
            'requirements': job.requirements
        }
        
        # Extract skills using NER
        ner_skills = self.ner_extractor.extract_from_job(job_data)
        
        # Extract skills using regex
        regex_skills = self.regex_extractor.extract_from_job(job_data)
        
        # Combine and deduplicate
        all_skills = self._combine_skills(ner_skills, regex_skills)
        
        # Enrich with ESCO matches
        enriched_skills = self.esco_matcher.enrich_extracted_skills(all_skills)
        
        logger.debug(
            f"Job {job.job_id}: {len(ner_skills)} NER skills, "
            f"{len(regex_skills)} regex skills, "
            f"{len(enriched_skills)} total after deduplication"
        )
        
        return enriched_skills
    
    def _combine_skills(self, ner_skills: List[Dict], regex_skills: List[Dict]) -> List[Dict]:
        """Combine and deduplicate skills from different extractors.
        
        Args:
            ner_skills: Skills from NER
            regex_skills: Skills from regex
            
        Returns:
            Combined and deduplicated skill list
        """
        # Use skill text and source section as unique key
        seen_skills = {}
        
        # Process NER skills first (higher confidence)
        for skill in ner_skills:
            key = (skill['skill_text'], skill['source_section'])
            
            if key not in seen_skills:
                seen_skills[key] = skill
            else:
                # Update confidence if higher
                if skill['confidence_score'] > seen_skills[key]['confidence_score']:
                    seen_skills[key] = skill
        
        # Process regex skills
        for skill in regex_skills:
            key = (skill['skill_text'], skill['source_section'])
            
            if key not in seen_skills:
                seen_skills[key] = skill
            else:
                # Merge extraction methods
                existing = seen_skills[key]
                if existing['extraction_method'] != skill['extraction_method']:
                    existing['extraction_method'] = 'ner+regex'
                    existing['confidence_score'] = min(0.95, existing['confidence_score'] + 0.1)
        
        return list(seen_skills.values())
    
    def run_continuous(self, batch_size: int = 100, wait_time: int = 60):
        """Run extraction continuously.
        
        Args:
            batch_size: Jobs per batch
            wait_time: Seconds to wait between batches
        """
        logger.info(f"Starting continuous extraction (batch_size={batch_size}, wait={wait_time}s)")
        
        while True:
            try:
                stats = self.process_batch(batch_size)
                
                if stats['jobs_processed'] == 0:
                    logger.info(f"No jobs to process. Waiting {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    # Short pause between batches
                    time.sleep(5)
                    
            except KeyboardInterrupt:
                logger.info("Extraction stopped by user")
                break
            except Exception as e:
                logger.error(f"Extraction error: {e}")
                logger.info(f"Waiting {wait_time}s before retry...")
                time.sleep(wait_time)
```

---

## 6. LLM Processor Module Files

### src/llm_processor/__init__.py
```python
from .llm_handler import LLMHandler
from .prompts import PromptGenerator
from .esco_normalizer import ESCONormalizer
from .validator import SkillValidator

__all__ = ['LLMHandler', 'PromptGenerator', 'ESCONormalizer', 'SkillValidator']
```

### src/llm_processor/llm_handler.py
```python
import logging
from typing import List, Dict, Optional, Any
from llama_cpp import Llama
import openai
from config.settings import get_settings
import json
import time

logger = logging.getLogger(__name__)

class LLMHandler:
    """Handle LLM interactions for skill processing."""
    
    def __init__(self, model_type: str = "local"):
        self.settings = get_settings()
        self.model_type = model_type
        
        if model_type == "local":
            self._init_local_model()
        elif model_type == "openai":
            self._init_openai()
        else:
            raise ValueError(f"Unknown model type: {model_type}")
    
    def _init_local_model(self):
        """Initialize local LLaMA/Mistral model."""
        try:
            logger.info(f"Loading local model from {self.settings.llm_model_path}")
            
            self.model = Llama(
                model_path=self.settings.llm_model_path,
                n_ctx=self.settings.llm_context_length,
                n_gpu_layers=self.settings.llm_n_gpu_layers,
                verbose=False
            )
            
            logger.info("Local model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load local model: {e}")
            raise
    
    def _init_openai(self):
        """Initialize OpenAI API client."""
        if not self.settings.openai_api_key:
            raise ValueError("OpenAI API key not configured")
        
        openai.api_key = self.settings.openai_api_key
        self.model_name = self.settings.openai_model
        logger.info(f"OpenAI API initialized with model {self.model_name}")
    
    def process_skills(self, 
                      job_data: Dict[str, Any],
                      extracted_skills: List[Dict[str, Any]],
                      prompt_template: str) -> Dict[str, Any]:
        """Process skills using LLM.
        
        Args:
            job_data: Job information
            extracted_skills: Initially extracted skills
            prompt_template: Formatted prompt template
            
        Returns:
            LLM response with processed skills
        """
        start_time = time.time()
        
        try:
            if self.model_type == "local":
                response = self._process_local(prompt_template)
            else:
                response = self._process_openai(prompt_template)
            
            # Parse response
            result = self._parse_response(response)
            
            # Add metadata
            result['processing_time'] = time.time() - start_time
            result['model_type'] = self.model_type
            result['model_name'] = getattr(self, 'model_name', 'local_mistral')
            
            return result
            
        except Exception as e:
            logger.error(f"LLM processing failed: {e}")
            raise
    
    def _process_local(self, prompt: str) -> str:
        """Process using local model.
        
        Args:
            prompt: Complete prompt
            
        Returns:
            Model response
        """
        response = self.model(
            prompt,
            max_tokens=self.settings.llm_max_tokens,
            temperature=self.settings.llm_temperature,
            stop=["</response>", "\n\n\n"],
            echo=False
        )
        
        return response['choices'][0]['text']
    
    def _process_openai(self, prompt: str) -> str:
        """Process using OpenAI API.
        
        Args:
            prompt: Complete prompt
            
        Returns:
            Model response
        """
        response = openai.ChatCompletion.create(
            model=self.model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert in analyzing job postings and extracting technical skills. Respond in Spanish."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            max_tokens=self.settings.llm_max_tokens,
            temperature=self.settings.llm_temperature
        )
        
        return response.choices[0].message.content
    
    def _parse_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response.
        
        Args:
            response: Raw LLM response
            
        Returns:
            Parsed skills data
        """
        # Try to extract JSON if present
        if "```json" in response:
            # Extract JSON block
            start = response.find("```json") + 7
            end = response.find("```", start)
            json_str = response[start:end].strip()
            
            try:
                return json.loads(json_str)
            except json.JSONDecodeError:
                logger.warning("Failed to parse JSON from response")
        
        # Fallback: Parse structured text response
        result = {
            "explicit_skills": [],
            "implicit_skills": [],
            "normalized_skills": [],
            "deduplicated_skills": []
        }
        
        lines = response.strip().split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            
            # Detect sections
            if "habilidades explícitas" in line.lower():
                current_section = "explicit_skills"
            elif "habilidades implícitas" in line.lower():
                current_section = "implicit_skills"
            elif "habilidades normalizadas" in line.lower():
                current_section = "normalized_skills"
            elif "deduplicadas" in line.lower():
                current_section = "deduplicated_skills"
            elif line and current_section and (line.startswith('-') or line.startswith('*')):
                # Extract skill from bullet point
                skill_text = line.lstrip('-*').strip()
                
                # Parse skill with reasoning if present
                if ':' in skill_text:
                    skill, reasoning = skill_text.split(':', 1)
                    result[current_section].append({
                        "skill": skill.strip(),
                        "reasoning": reasoning.strip()
                    })
                else:
                    result[current_section].append({
                        "skill": skill_text
                    })
        
        return result
```

### src/llm_processor/prompts.py
```python
from typing import List, Dict, Any
import json

class PromptGenerator:
    """Generate prompts for LLM skill processing."""
    
    def __init__(self):
        self.templates = self._load_templates()
    
    def _load_templates(self) -> Dict[str, str]:
        """Load prompt templates."""
        return {
            "skill_processing": """Eres un experto en análisis de ofertas laborales tecnológicas en América Latina.

Datos de la vacante:
- Título: {job_title}
- Descripción: {job_description}
- Requisitos: {job_requirements}

Habilidades extraídas inicialmente:
{extracted_skills}

Por favor, realiza las siguientes tareas:

1. **Validación de habilidades explícitas**: Revisa las habilidades extraídas y confirma cuáles son realmente habilidades técnicas relevantes.

2. **Detección de habilidades implícitas**: Basándote en el contexto del puesto, identifica habilidades técnicas que serían necesarias pero no están mencionadas explícitamente. Por ejemplo:
   - Si menciona "desarrollo web full stack" → probablemente necesite Git, bases de datos, APIs REST
   - Si menciona "análisis de datos" → probablemente necesite SQL, Python/R, visualización
   - Si menciona "DevOps" → probablemente necesite CI/CD, contenedores, cloud

3. **Normalización con ESCO**: Para cada habilidad, proporciona la forma normalizada según estándares internacionales:
   - Usa nombres estándar (ej: "JS" → "JavaScript", "React.js" → "React")
   - Mantén el español cuando sea apropiado
   - Agrupa variaciones (ej: "MySQL/MariaDB" → "MySQL")

4. **Deduplicación**: Elimina habilidades duplicadas o redundantes:
   - Combina variaciones del mismo concepto
   - Elimina términos demasiado genéricos
   - Mantén el término más específico cuando haya jerarquía

Responde en el siguiente formato JSON:
```json
{{
  "explicit_skills": [
    {{"skill": "nombre", "confidence": 0.9, "original": "texto_original"}}
  ],
  "implicit_skills": [
    {{"skill": "nombre", "confidence": 0.7, "reasoning": "justificación"}}
  ],
  "normalized_skills": [
    {{"original": "skill_original", "normalized": "skill_normalizado", "esco_match": "posible_uri"}}
  ],
  "deduplicated_skills": [
    {{"skill": "nombre_final", "type": "explicit|implicit", "category": "programming|database|framework|tool|soft_skill"}}
  ]
}}
```""",

            "simple_inference": """Analiza esta oferta de trabajo y extrae SOLO las habilidades técnicas implícitas que no están mencionadas pero serían necesarias.

Título: {job_title}
Descripción resumida: {job_summary}
Habilidades ya identificadas: {known_skills}

Lista únicamente las habilidades técnicas implícitas con su justificación:
""",

            "normalization": """Normaliza las siguientes habilidades técnicas según estándares internacionales y la taxonomía ESCO:

Habilidades a normalizar:
{skills_list}

Para cada habilidad, proporciona:
- Forma normalizada
- Categoría (lenguaje/framework/base de datos/herramienta/metodología)
- Término ESCO equivalente si existe

Responde en formato de lista:
""",

            "deduplication": """Elimina duplicados y agrupa las siguientes habilidades:

Habilidades:
{skills_list}

Reglas:
- Combina variaciones del mismo concepto (ej: JS, JavaScript, javascript → JavaScript)
- Mantén el término más específico cuando haya jerarquía
- Elimina términos genéricos si hay específicos

Lista final sin duplicados:
"""
        }
    
    def generate_skill_processing_prompt(self, 
                                       job_data: Dict[str, Any],
                                       extracted_skills: List[Dict[str, Any]]) -> str:
        """Generate prompt for complete skill processing.
        
        Args:
            job_data: Job information
            extracted_skills: Initially extracted skills
            
        Returns:
            Formatted prompt
        """
        # Format extracted skills
        skills_text = self._format_extracted_skills(extracted_skills)
        
        prompt = self.templates["skill_processing"].format(
            job_title=job_data.get('title', 'No especificado'),
            job_description=job_data.get('description', 'No especificado'),
            job_requirements=job_data.get('requirements', 'No especificado'),
            extracted_skills=skills_text
        )
        
        return prompt
    
    def generate_inference_prompt(self,
                                job_title: str,
                                job_summary: str,
                                known_skills: List[str]) -> str:
        """Generate prompt for implicit skill inference.
        
        Args:
            job_title: Job title
            job_summary: Brief job description
            known_skills: Already identified skills
            
        Returns:
            Formatted prompt
        """
        skills_list = ", ".join(known_skills) if known_skills else "Ninguna"
        
        return self.templates["simple_inference"].format(
            job_title=job_title,
            job_summary=job_summary,
            known_skills=skills_list
        )
    
    def generate_normalization_prompt(self, skills: List[str]) -> str:
        """Generate prompt for skill normalization.
        
        Args:
            skills: List of skills to normalize
            
        Returns:
            Formatted prompt
        """
        skills_text = "\n".join(f"- {skill}" for skill in skills)
        
        return self.templates["normalization"].format(
            skills_list=skills_text
        )
    
    def generate_deduplication_prompt(self, skills: List[str]) -> str:
        """Generate prompt for skill deduplication.
        
        Args:
            skills: List of skills to deduplicate
            
        Returns:
            Formatted prompt
        """
        skills_text = "\n".join(f"- {skill}" for skill in skills)
        
        return self.templates["deduplication"].format(
            skills_list=skills_text
        )
    
    def _format_extracted_skills(self, skills: List[Dict[str, Any]]) -> str:
        """Format extracted skills for prompt.
        
        Args:
            skills: List of skill dictionaries
            
        Returns:
            Formatted text
        """
        formatted_skills = []
        
        # Group by source section
        by_section = {}
        for skill in skills:
            section = skill.get('source_section', 'unknown')
            if section not in by_section:
                by_section[section] = []
            by_section[section].append(skill)
        
        # Format each section
        for section, section_skills in by_section.items():
            formatted_skills.append(f"\nDe {section}:")
            for skill in section_skills:
                method = skill.get('extraction_method', 'unknown')
                confidence = skill.get('confidence_score', 0)
                text = skill.get('skill_text', '')
                
                formatted_skills.append(
                    f"  - {text} (método: {method}, confianza: {confidence:.2f})"
                )
        
        return "\n".join(formatted_skills)

### src/llm_processor/esco_normalizer.py
```python
import logging
from typing import List, Dict, Any, Optional
from fuzzywuzzy import fuzz
import yaml

logger = logging.getLogger(__name__)

class ESCONormalizer:
    """Normalize skills using ESCO taxonomy with LLM assistance."""
    
    def __init__(self, config_path: str = "config/esco_config.yaml"):
        self.config = self._load_config(config_path)
        self.normalization_rules = self._build_normalization_rules()
    
    def _load_config(self, config_path: str) -> dict:
        """Load ESCO configuration."""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config['esco']
        except Exception as e:
            logger.error(f"Failed to load ESCO config: {e}")
            return {}
    
    def _build_normalization_rules(self) -> Dict[str, str]:
        """Build normalization rules from config and common variations."""
        rules = {}
        
        # Load from config
        if 'tech_mappings' in self.config:
            rules.update(self.config['tech_mappings'])
        
        # Add common variations
        common_variations = {
            # JavaScript variations
            'js': 'JavaScript',
            'javascript': 'JavaScript',
            'java script': 'JavaScript',
            
            # Python variations
            'python3': 'Python',
            'python 3': 'Python',
            'python2': 'Python',
            
            # React variations
            'reactjs': 'React',
            'react.js': 'React',
            'react js': 'React',
            'react native': 'React Native',
            
            # Node variations
            'nodejs': 'Node.js',
            'node js': 'Node.js',
            'node': 'Node.js',
            
            # Database variations
            'postgres': 'PostgreSQL',
            'mysql': 'MySQL',
            'maria db': 'MariaDB',
            'mariadb': 'MariaDB',
            'mongo': 'MongoDB',
            'mongo db': 'MongoDB',
            
            # .NET variations
            'dotnet': '.NET',
            'dot net': '.NET',
            '.net core': '.NET Core',
            'asp.net': 'ASP.NET',
            
            # Other common variations
            'c++': 'C++',
            'c#': 'C#',
            'c sharp': 'C#',
            'objective c': 'Objective-C',
            'obj-c': 'Objective-C',
            
            # Spanish to English mappings
            'base de datos': 'Database',
            'desarrollo web': 'Web Development',
            'desarrollo móvil': 'Mobile Development',
            'aprendizaje automático': 'Machine Learning',
            'inteligencia artificial': 'Artificial Intelligence',
            'ciencia de datos': 'Data Science',
            'computación en la nube': 'Cloud Computing',
            'control de versiones': 'Version Control',
        }
        
        # Convert all keys to lowercase
        for key, value in common_variations.items():
            rules[key.lower()] = value
        
        return rules
    
    def normalize_skill(self, skill: str) -> Dict[str, Any]:
        """Normalize a single skill.
        
        Args:
            skill: Raw skill text
            
        Returns:
            Normalized skill data
        """
        skill_lower = skill.lower().strip()
        
        # Direct mapping
        if skill_lower in self.normalization_rules:
            return {
                'original': skill,
                'normalized': self.normalization_rules[skill_lower],
                'method': 'direct_mapping',
                'confidence': 1.0
            }
        
        # Try fuzzy matching
        best_match = None
        best_score = 0
        
        for pattern, normalized in self.normalization_rules.items():
            score = fuzz.ratio(skill_lower, pattern)
            if score > best_score and score >= 85:
                best_score = score
                best_match = normalized
        
        if best_match:
            return {
                'original': skill,
                'normalized': best_match,
                'method': 'fuzzy_mapping',
                'confidence': best_score / 100.0
            }
        
        # Category-based normalization
        normalized = self._category_normalization(skill)
        if normalized != skill:
            return {
                'original': skill,
                'normalized': normalized,
                'method': 'category_rules',
                'confidence': 0.8
            }
        
        # No normalization found
        return {
            'original': skill,
            'normalized': skill,
            'method': 'no_normalization',
            'confidence': 0.5
        }
    
    def _category_normalization(self, skill: str) -> str:
        """Apply category-based normalization rules.
        
        Args:
            skill: Skill to normalize
            
        Returns:
            Normalized skill
        """
        skill_lower = skill.lower()
        
        # Framework detection
        if 'framework' in skill_lower:
            skill = skill.replace('framework', '').replace('Framework', '').strip()
        
        # Version removal for certain technologies
        version_patterns = [
            (r'python\s*\d+\.?\d*', 'Python'),
            (r'java\s*\d+', 'Java'),
            (r'angular\s*\d+', 'Angular'),
            (r'vue\s*\d+', 'Vue.js'),
            (r'react\s*\d+', 'React'),
        ]
        
        import re
        for pattern, replacement in version_patterns:
            if re.search(pattern, skill_lower):
                return replacement
        
        # Capitalize properly
        # Special cases
        special_cases = {
            'mysql': 'MySQL',
            'postgresql': 'PostgreSQL',
            'mongodb': 'MongoDB',
            'javascript': 'JavaScript',
            'typescript': 'TypeScript',
            'graphql': 'GraphQL',
            'nodejs': 'Node.js',
            'reactjs': 'React',
            'vuejs': 'Vue.js',
        }
        
        if skill_lower in special_cases:
            return special_cases[skill_lower]
        
        # Default: capitalize first letter of each word
        return ' '.join(word.capitalize() for word in skill.split())
    
    def normalize_skills_batch(self, skills: List[str]) -> List[Dict[str, Any]]:
        """Normalize multiple skills.
        
        Args:
            skills: List of skills to normalize
            
        Returns:
            List of normalized skill data
        """
        normalized = []
        
        for skill in skills:
            result = self.normalize_skill(skill)
            normalized.append(result)
        
        return normalized
    
    def deduplicate_normalized_skills(self, normalized_skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Deduplicate normalized skills.
        
        Args:
            normalized_skills: List of normalized skill dictionaries
            
        Returns:
            Deduplicated list
        """
        # Group by normalized form
        skill_groups = {}
        
        for skill_data in normalized_skills:
            normalized = skill_data['normalized']
            
            if normalized not in skill_groups:
                skill_groups[normalized] = {
                    'normalized': normalized,
                    'originals': [],
                    'best_confidence': 0,
                    'methods': set()
                }
            
            skill_groups[normalized]['originals'].append(skill_data['original'])
            skill_groups[normalized]['methods'].add(skill_data['method'])
            skill_groups[normalized]['best_confidence'] = max(
                skill_groups[normalized]['best_confidence'],
                skill_data['confidence']
            )
        
        # Convert back to list
        deduplicated = []
        for normalized, group_data in skill_groups.items():
            deduplicated.append({
                'normalized': normalized,
                'original_variations': group_data['originals'],
                'confidence': group_data['best_confidence'],
                'methods': list(group_data['methods'])
            })
        
        return deduplicated

### src/llm_processor/validator.py
```python
import logging
from typing import List, Dict, Any, Set
import re

logger = logging.getLogger(__name__)

class SkillValidator:
    """Validate and filter skills."""
    
    def __init__(self):
        self.blacklist = self._build_blacklist()
        self.whitelist = self._build_whitelist()
        self.categories = self._build_categories()
    
    def _build_blacklist(self) -> Set[str]:
        """Build blacklist of non-skill terms."""
        return {
            # Generic terms
            'experiencia', 'años', 'año', 'conocimiento', 'conocimientos',
            'habilidad', 'habilidades', 'capacidad', 'competencia',
            'desarrollo', 'trabajo', 'empresa', 'cliente', 'proyecto',
            'equipo', 'persona', 'profesional', 'área', 'proceso',
            
            # Too generic tech terms
            'tecnología', 'tecnologías', 'sistema', 'sistemas',
            'informática', 'computación', 'software', 'hardware',
            'programación', 'desarrollo de software',
            
            # Common words
            'bueno', 'excelente', 'alto', 'nivel', 'manejo',
            'uso', 'gestión', 'administración', 'análisis',
            
            # Methodologies too generic
            'metodología', 'metodologías', 'mejores prácticas',
            
            # Soft skills (we focus on technical)
            'comunicación', 'liderazgo', 'trabajo en equipo',
            'responsabilidad', 'proactividad', 'creatividad'
        }
    
    def _build_whitelist(self) -> Set[str]:
        """Build whitelist of known valid skills."""
        return {
            # Programming languages
            'python', 'java', 'javascript', 'typescript', 'c++', 'c#',
            'php', 'ruby', 'go', 'golang', 'rust', 'kotlin', 'swift',
            'objective-c', 'r', 'scala', 'perl', 'matlab', 'julia',
            
            # Frameworks and libraries
            'react', 'angular', 'vue', 'django', 'flask', 'spring',
            'express', 'laravel', 'rails', 'fastapi', 'nextjs',
            '.net', 'asp.net', 'tensorflow', 'pytorch', 'keras',
            
            # Databases
            'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',
            'cassandra', 'dynamodb', 'oracle', 'sql server', 'sqlite',
            'neo4j', 'couchdb', 'firebase', 'supabase',
            
            # Cloud and DevOps
            'aws', 'azure', 'gcp', 'google cloud', 'docker', 'kubernetes',
            'jenkins', 'terraform', 'ansible', 'puppet', 'chef',
            'gitlab', 'github', 'bitbucket', 'circleci', 'travis',
            
            # Tools and platforms
            'git', 'jira', 'confluence', 'slack', 'linux', 'windows',
            'macos', 'ubuntu', 'centos', 'debian', 'nginx', 'apache',
            'grafana', 'prometheus', 'elasticsearch', 'kibana',
            
            # Data and ML
            'machine learning', 'deep learning', 'data science',
            'big data', 'spark', 'hadoop', 'kafka', 'airflow',
            'pandas', 'numpy', 'scikit-learn', 'matplotlib',
            
            # Mobile
            'android', 'ios', 'react native', 'flutter', 'xamarin',
            'swift', 'kotlin', 'objective-c', 'cordova', 'ionic',
            
            # Other
            'api', 'rest', 'graphql', 'websocket', 'microservices',
            'ci/cd', 'agile', 'scrum', 'kanban', 'tdd', 'bdd'
        }
    
    def _build_categories(self) -> Dict[str, Set[str]]:
        """Build skill categories."""
        return {
            'programming_language': {
                'python', 'java', 'javascript', 'typescript', 'c++', 'c#',
                'php', 'ruby', 'go', 'rust', 'kotlin', 'swift', 'r'
            },
            'framework': {
                'react', 'angular', 'vue', 'django', 'flask', 'spring',
                'express', 'laravel', 'rails', '.net', 'nextjs'
            },
            'database': {
                'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',
                'cassandra', 'dynamodb', 'oracle', 'sql server'
            },
            'cloud_platform': {
                'aws', 'azure', 'gcp', 'google cloud', 'heroku', 'digitalocean'
            },
            'devops_tool': {
                'docker', 'kubernetes', 'jenkins', 'terraform', 'ansible',
                'git', 'github', 'gitlab', 'ci/cd'
            },
            'data_ml': {
                'machine learning', 'deep learning', 'tensorflow', 'pytorch',
                'pandas', 'numpy', 'spark', 'hadoop'
            },
            'mobile': {
                'android', 'ios', 'react native', 'flutter', 'xamarin'
            },
            'methodology': {
                'agile', 'scrum', 'kanban', 'devops', 'tdd', 'bdd'
            }
        }
    
    def validate_skill(self, skill: str) -> Dict[str, Any]:
        """Validate a single skill.
        
        Args:
            skill: Skill to validate
            
        Returns:
            Validation result
        """
        skill_lower = skill.lower().strip()
        
        # Check blacklist
        if skill_lower in self.blacklist:
            return {
                'skill': skill,
                'valid': False,
                'reason': 'blacklisted',
                'confidence': 0.0
            }
        
        # Check whitelist
        if skill_lower in self.whitelist:
            category = self._categorize_skill(skill_lower)
            return {
                'skill': skill,
                'valid': True,
                'reason': 'whitelisted',
                'category': category,
                'confidence': 1.0
            }
        
        # Length check
        if len(skill_lower) < 2 or len(skill_lower) > 50:
            return {
                'skill': skill,
                'valid': False,
                'reason': 'invalid_length',
                'confidence': 0.0
            }
        
        # Pattern validation
        if not self._validate_pattern(skill_lower):
            return {
                'skill': skill,
                'valid': False,
                'reason': 'invalid_pattern',
                'confidence': 0.0
            }
        
        # Partial matches in whitelist
        for valid_skill in self.whitelist:
            if valid_skill in skill_lower or skill_lower in valid_skill:
                category = self._categorize_skill(valid_skill)
                return {
                    'skill': skill,
                    'valid': True,
                    'reason': 'partial_match',
                    'category': category,
                    'confidence': 0.7
                }
        
        # If not in whitelist but passes other checks
        return {
            'skill': skill,
            'valid': True,
            'reason': 'pattern_valid',
            'category': 'uncategorized',
            'confidence': 0.5
        }
    
    def _validate_pattern(self, skill: str) -> bool:
        """Validate skill pattern.
        
        Args:
            skill: Skill to validate
            
        Returns:
            True if pattern is valid
        """
        # Must contain at least one letter
        if not re.search(r'[a-zA-Z]', skill):
            return False
        
        # Check for suspicious patterns
        suspicious_patterns = [
            r'^\d+,  # Only numbers
            r'^[^a-zA-Z0-9\s\.\+\#\-/]+,  # Only special chars
            r'\b(años?|experiencia|conocimientos?)\b',  # Common non-skills
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, skill, re.IGNORECASE):
                return False
        
        return True
    
    def _categorize_skill(self, skill: str) -> str:
        """Categorize a skill.
        
        Args:
            skill: Skill to categorize
            
        Returns:
            Category name
        """
        skill_lower = skill.lower()
        
        for category, skills in self.categories.items():
            if skill_lower in skills:
                return category
        
        # Try partial matches
        for category, skills in self.categories.items():
            for cat_skill in skills:
                if cat_skill in skill_lower or skill_lower in cat_skill:
                    return category
        
        return 'uncategorized'
    
    def validate_skills_batch(self, skills: List[str]) -> List[Dict[str, Any]]:
        """Validate multiple skills.
        
        Args:
            skills: List of skills to validate
            
        Returns:
            List of validation results
        """
        results = []
        
        for skill in skills:
            result = self.validate_skill(skill)
            results.append(result)
        
        # Log statistics
        valid_count = sum(1 for r in results if r['valid'])
        logger.info(
            f"Skill validation: {valid_count}/{len(skills)} valid "
            f"({valid_count/len(skills)*100:.1f}%)"
        )
        
        return results
    
    def filter_valid_skills(self, skills: List[str], min_confidence: float = 0.5) -> List[str]:
        """Filter only valid skills.
        
        Args:
            skills: List of skills
            min_confidence: Minimum confidence threshold
            
        Returns:
            List of valid skills
        """
        results = self.validate_skills_batch(skills)
        
        valid_skills = [
            r['skill'] for r in results
            if r['valid'] and r['confidence'] >= min_confidence
        ]
        
        return valid_skills

### src/llm_processor/pipeline.py
```python
import logging
from typing import List, Dict, Any
from database.operations import DatabaseOperations
from .llm_handler import LLMHandler
from .prompts import PromptGenerator
from .esco_normalizer import ESCONormalizer
from .validator import SkillValidator
from config.settings import get_settings
import time

logger = logging.getLogger(__name__)

class LLMProcessingPipeline:
    """Main pipeline for LLM-based skill processing."""
    
    def __init__(self, model_type: str = "local"):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        
        logger.info("Initializing LLM processing components...")
        self.llm_handler = LLMHandler(model_type)
        self.prompt_generator = PromptGenerator()
        self.normalizer = ESCONormalizer()
        self.validator = SkillValidator()
        
        logger.info("LLM processing pipeline initialized")
    
    def process_batch(self, batch_size: int = 50) -> Dict[str, Any]:
        """Process a batch of jobs with extracted skills.
        
        Args:
            batch_size: Number of jobs to process
            
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'jobs_processed': 0,
            'skills_enhanced': 0,
            'implicit_skills_found': 0,
            'skills_normalized': 0,
            'errors': 0
        }
        
        try:
            # Get jobs with extracted skills needing LLM processing
            jobs_data = self.db_ops.get_extracted_skills_for_processing(limit=batch_size)
            logger.info(f"Processing {len(jobs_data)} jobs with LLM")
            
            for job_data in jobs_data:
                try:
                    # Process individual job
                    enhanced_skills = self.process_job(job_data)
                    
                    if enhanced_skills:
                        # Save enhanced skills
                        self.db_ops.insert_enhanced_skills(
                            job_data['job_id'],
                            enhanced_skills
                        )
                        
                        # Update statistics
                        stats['jobs_processed'] += 1
                        stats['skills_enhanced'] += len(enhanced_skills)
                        stats['implicit_skills_found'] += sum(
                            1 for s in enhanced_skills 
                            if s.get('skill_type') == 'implicit'
                        )
                        stats['skills_normalized'] += sum(
                            1 for s in enhanced_skills
                            if s.get('normalized_skill') != s.get('original_skill_text')
                        )
                    
                except Exception as e:
                    logger.error(f"Error processing job {job_data['job_id']}: {e}")
                    stats['errors'] += 1
            
            # Calculate processing time
            stats['processing_time'] = time.time() - start_time
            stats['jobs_per_minute'] = (stats['jobs_processed'] / stats['processing_time']) * 60 if stats['processing_time'] > 0 else 0
            
            logger.info(
                f"LLM batch complete: {stats['jobs_processed']} jobs, "
                f"{stats['skills_enhanced']} skills enhanced, "
                f"{stats['implicit_skills_found']} implicit skills found, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"LLM batch processing failed: {e}")
            raise
    
    def process_job(self, job_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Process a single job with LLM.
        
        Args:
            job_data: Job data with extracted skills
            
        Returns:
            List of enhanced skills
        """
        # Generate prompt
        prompt = self.prompt_generator.generate_skill_processing_prompt(
            {
                'title': job_data['job_title'],
                'description': job_data['job_description'],
                'requirements': job_data['job_requirements']
            },
            job_data['extracted_skills']
        )
        
        # Process with LLM
        llm_response = self.llm_handler.process_skills(
            job_data,
            job_data['extracted_skills'],
            prompt
        )
        
        # Process LLM response
        enhanced_skills = self._process_llm_response(
            llm_response,
            job_data['extracted_skills']
        )
        
        return enhanced_skills
    
    def _process_llm_response(self, 
                            llm_response: Dict[str, Any],
                            original_skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process LLM response into enhanced skills.
        
        Args:
            llm_response: Response from LLM
            original_skills: Original extracted skills
            
        Returns:
            List of enhanced skill records
        """
        enhanced_skills = []
        
        # Create a mapping of original skills
        original_map = {
            skill['skill_text'].lower(): skill 
            for skill in original_skills
        }
        
        # Process explicit skills (validated by LLM)
        for skill_data in llm_response.get('explicit_skills', []):
            skill_text = skill_data.get('skill', '').lower()
            
            # Validate skill
            validation = self.validator.validate_skill(skill_text)
            if not validation['valid']:
                continue
            
            # Normalize skill
            normalization = self.normalizer.normalize_skill(skill_text)
            
            enhanced_skills.append({
                'original_skill_text': skill_data.get('original', skill_text),
                'normalized_skill': normalization['normalized'],
                'skill_type': 'explicit',
                'esco_concept_uri': None,  # To be matched later
                'esco_preferred_label': None,
                'llm_confidence': skill_data.get('confidence', 0.8),
                'llm_reasoning': None,
                'is_duplicate': False,
                'llm_model': llm_response.get('model_name', 'unknown')
            })
        
        # Process implicit skills (inferred by LLM)
        for skill_data in llm_response.get('implicit_skills', []):
            skill_text = skill_data.get('skill', '').lower()
            
            # Validate skill
            validation = self.validator.validate_skill(skill_text)
            if not validation['valid']:
                continue
            
            # Normalize skill
            normalization = self.normalizer.normalize_skill(skill_text)
            
            enhanced_skills.append({
                'original_skill_text': None,
                'normalized_skill': normalization['normalized'],
                'skill_type': 'implicit',
                'esco_concept_uri': None,
                'esco_preferred_label': None,
                'llm_confidence': skill_data.get('confidence', 0.7),
                'llm_reasoning': skill_data.get('reasoning', ''),
                'is_duplicate': False,
                'llm_model': llm_response.get('model_name', 'unknown')
            })
        
        # Deduplicate skills
        enhanced_skills = self._deduplicate_skills(enhanced_skills)
        
        return enhanced_skills
    
    def _deduplicate_skills(self, skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Deduplicate enhanced skills.
        
        Args:
            skills: List of enhanced skills
            
        Returns:
            Deduplicated list with duplicates marked
        """
        seen_normalized = {}
        deduplicated = []
        
        # Sort by confidence (highest first)
        sorted_skills = sorted(
            skills, 
            key=lambda x: x.get('llm_confidence', 0),
            reverse=True
        )
        
        for skill in sorted_skills:
            normalized = skill['normalized_skill'].lower()
            
            if normalized not in seen_normalized:
                # First occurrence
                seen_normalized[normalized] = skill
                deduplicated.append(skill)
            else:
                # Duplicate found
                skill['is_duplicate'] = True
                skill['duplicate_of_id'] = id(seen_normalized[normalized])
                deduplicated.append(skill)
        
        return deduplicated
    
    def run_continuous(self, batch_size: int = 50, wait_time: int = 60):
        """Run LLM processing continuously.
        
        Args:
            batch_size: Jobs per batch
            wait_time: Seconds to wait between batches
        """
        logger.info(f"Starting continuous LLM processing (batch_size={batch_size}, wait={wait_time}s)")
        
        while True:
            try:
                stats = self.process_batch(batch_size)
                
                if stats['jobs_processed'] == 0:
                    logger.info(f"No jobs to process. Waiting {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    # Short pause between batches
                    time.sleep(10)
                    
            except KeyboardInterrupt:
                logger.info("LLM processing stopped by user")
                break
            except Exception as e:
                logger.error(f"LLM processing error: {e}")
                logger.info(f"Waiting {wait_time}s before retry...")
                time.sleep(wait_time)
```

---

## 7. Embedder Module Files

### src/embedder/__init__.py
```python
from .vectorizer import SkillVectorizer
from .model_loader import EmbeddingModelLoader
from .batch_processor import BatchProcessor

__all__ = ['SkillVectorizer', 'EmbeddingModelLoader', 'BatchProcessor']

### src/embedder/vectorizer.py
```python
import logging
from typing import List, Dict, Any, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import torch

logger = logging.getLogger(__name__)

class SkillVectorizer:
    """Generate embeddings for skills."""
    
    def __init__(self, model_name: str = None):
        self.model_name = model_name or "intfloat/multilingual-e5-base"
        self.model = None
        self.device = None
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialize the embedding model."""
        try:
            # Check for GPU availability
            if torch.cuda.is_available():
                self.device = 'cuda'
                logger.info(f"Using GPU for embeddings")
            else:
                self.device = 'cpu'
                logger.info(f"Using CPU for embeddings")
            
            # Load model
            logger.info(f"Loading embedding model: {self.model_name}")
            self.model = SentenceTransformer(self.model_name, device=self.device)
            
            # Get embedding dimension
            self.embedding_dim = self.model.get_sentence_embedding_dimension()
            logger.info(f"Model loaded. Embedding dimension: {self.embedding_dim}")
            
        except Exception as e:
            logger.error(f"Failed to initialize embedding model: {e}")
            raise
    
    def vectorize(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """Generate embeddings for a list of texts.
        
        Args:
            texts: List of texts to embed
            batch_size: Batch size for processing
            
        Returns:
            Array of embeddings
        """
        if not texts:
            return np.array([])
        
        try:
            # For E5 models, add instruction prefix
            if 'e5' in self.model_name.lower():
                texts = [f"query: {text}" for text in texts]
            
            # Generate embeddings
            embeddings = self.model.encode(
                texts,
                batch_size=batch_size,
                show_progress_bar=len(texts) > 100,
                convert_to_numpy=True,
                normalize_embeddings=True  # L2 normalization
            )
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Vectorization failed: {e}")
            raise
    
    def vectorize_single(self, text: str) -> np.ndarray:
        """Generate embedding for a single text.
        
        Args:
            text: Text to embed
            
        Returns:
            Embedding vector
        """
        return self.vectorize([text])[0]
    
    def compute_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """Compute cosine similarity between two embeddings.
        
        Args:
            embedding1: First embedding
            embedding2: Second embedding
            
        Returns:
            Cosine similarity score
        """
        # Assuming normalized embeddings, cosine similarity is just dot product
        return float(np.dot(embedding1, embedding2))
    
    def find_similar_skills(self, 
                          query_embedding: np.ndarray,
                          skill_embeddings: List[Dict[str, Any]],
                          top_k: int = 10,
                          threshold: float = 0.7) -> List[Dict[str, Any]]:
        """Find similar skills based on embeddings.
        
        Args:
            query_embedding: Query skill embedding
            skill_embeddings: List of skill embedding dictionaries
            top_k: Number of top results to return
            threshold: Minimum similarity threshold
            
        Returns:
            List of similar skills with scores
        """
        similarities = []
        
        for skill_data in skill_embeddings:
            embedding = skill_data['embedding']
            similarity = self.compute_similarity(query_embedding, embedding)
            
            if similarity >= threshold:
                similarities.append({
                    'skill': skill_data['skill_text'],
                    'similarity': similarity,
                    'embedding_id': skill_data.get('embedding_id')
                })
        
        # Sort by similarity
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        return similarities[:top_k]

### src/embedder/model_loader.py
```python
import logging
from typing import Dict, Any, Optional
import os
import json
from pathlib import Path
import torch
from transformers import AutoModel, AutoTokenizer
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)

class EmbeddingModelLoader:
    """Load and manage embedding models."""
    
    # Model configurations
    MODEL_CONFIGS = {
        'multilingual-e5-base': {
            'name': 'intfloat/multilingual-e5-base',
            'dimension': 768,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': 'query: '
        },
        'multilingual-e5-large': {
            'name': 'intfloat/multilingual-e5-large',
            'dimension': 1024,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': 'query: '
        },
        'beto': {
            'name': 'dccuchile/bert-base-spanish-wwm-cased',
            'dimension': 768,
            'max_length': 512,
            'type': 'transformers',
            'instruction_prefix': None
        },
        'labse': {
            'name': 'sentence-transformers/LaBSE',
            'dimension': 768,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': None
        },
        'multilingual-minilm': {
            'name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
            'dimension': 384,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': None
        }
    }
    
    def __init__(self, cache_dir: Optional[str] = None):
        self.cache_dir = cache_dir or "./data/cache/embeddings"
        os.makedirs(self.cache_dir, exist_ok=True)
        self.loaded_models = {}
    
    def load_model(self, model_key: str) -> Any:
        """Load an embedding model.
        
        Args:
            model_key: Key from MODEL_CONFIGS
            
        Returns:
            Loaded model
        """
        if model_key in self.loaded_models:
            logger.info(f"Model {model_key} already loaded")
            return self.loaded_models[model_key]
        
        if model_key not in self.MODEL_CONFIGS:
            raise ValueError(f"Unknown model key: {model_key}")
        
        config = self.MODEL_CONFIGS[model_key]
        
        try:
            if config['type'] == 'sentence-transformers':
                model = self._load_sentence_transformer(config)
            elif config['type'] == 'transformers':
                model = self._load_transformers_model(config)
            else:
                raise ValueError(f"Unknown model type: {config['type']}")
            
            self.loaded_models[model_key] = model
            logger.info(f"Successfully loaded model: {model_key}")
            
            return model
            
        except Exception as e:
            logger.error(f"Failed to load model {model_key}: {e}")
            raise
    
    def _load_sentence_transformer(self, config: Dict[str, Any]) -> SentenceTransformer:
        """Load a sentence-transformers model.
        
        Args:
            config: Model configuration
            
        Returns:
            Loaded model
        """
        model = SentenceTransformer(
            config['name'],
            cache_folder=self.cache_dir
        )
        
        # Verify dimension
        actual_dim = model.get_sentence_embedding_dimension()
        if actual_dim != config['dimension']:
            logger.warning(
                f"Model dimension mismatch: expected {config['dimension']}, "
                f"got {actual_dim}"
            )
        
        return model
    
    def _load_transformers_model(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Load a transformers model with tokenizer.
        
        Args:
            config: Model configuration
            
        Returns:
            Dictionary with model and tokenizer
        """
        model = AutoModel.from_pretrained(
            config['name'],
            cache_dir=self.cache_dir
        )
        
        tokenizer = AutoTokenizer.from_pretrained(
            config['name'],
            cache_dir=self.cache_dir
        )
        
        return {
            'model': model,
            'tokenizer': tokenizer,
            'config': config
        }
    
    def get_model_info(self, model_key: str) -> Dict[str, Any]:
        """Get information about a model.
        
        Args:
            model_key: Model key
            
        Returns:
            Model information
        """
        if model_key not in self.MODEL_CONFIGS:
            raise ValueError(f"Unknown model key: {model_key}")
        
        return self.MODEL_CONFIGS[model_key].copy()
    
    def list_available_models(self) -> Dict[str, Dict[str, Any]]:
        """List all available models.
        
        Returns:
            Dictionary of model configurations
        """
        return self.MODEL_CONFIGS.copy()
    
    def download_all_models(self):
        """Download all configured models."""
        logger.info("Downloading all configured embedding models...")
        
        for model_key in self.MODEL_CONFIGS:
            try:
                logger.info(f"Downloading {model_key}...")
                self.load_model(model_key)
            except Exception as e:
                logger.error(f"Failed to download {model_key}: {e}")
    
    def clear_cache(self):
        """Clear model cache."""
        import shutil
        
        if os.path.exists(self.cache_dir):
            shutil.rmtree(self.cache_dir)
            os.makedirs(self.cache_dir)
            logger.info("Model cache cleared")
        
        self.loaded_models.clear()

### src/embedder/batch_processor.py
```python
import logging
from typing import List, Dict, Any
import numpy as np
from database.operations import DatabaseOperations
from .vectorizer import SkillVectorizer
from config.settings import get_settings
import time
from tqdm import tqdm

logger = logging.getLogger(__name__)

class BatchProcessor:
    """Process skill embeddings in batches."""
    
    def __init__(self, model_name: str = None):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        self.vectorizer = SkillVectorizer(model_name)
        self.batch_size = self.settings.embedding_batch_size
    
    def process_all_skills(self) -> Dict[str, Any]:
        """Process all skills that need embeddings.
        
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'skills_processed': 0,
            'embeddings_created': 0,
            'errors': 0,
            'processing_time': 0
        }
        
        try:
            # Get skills without embeddings
            skills = self.db_ops.get_unique_skills_for_embedding()
            logger.info(f"Found {len(skills)} skills without embeddings")
            
            if not skills:
                return stats
            
            # Process in batches
            for i in tqdm(range(0, len(skills), self.batch_size), desc="Embedding skills"):
                batch = skills[i:i + self.batch_size]
                
                try:
                    # Generate embeddings
                    embeddings = self.vectorizer.vectorize(batch)
                    
                    # Prepare data for database
                    embedding_data = []
                    for skill_text, embedding in zip(batch, embeddings):
                        embedding_data.append({
                            'skill_text': skill_text,
                            'embedding': embedding.tolist(),  # Convert to list for pgvector
                            'model_name': self.vectorizer.model_name,
                            'model_version': '1.0'
                        })
                    
                    # Save to database
                    self.db_ops.insert_skill_embeddings(embedding_data)
                    
                    stats['skills_processed'] += len(batch)
                    stats['embeddings_created'] += len(embedding_data)
                    
                except Exception as e:
                    logger.error(f"Error processing batch {i//self.batch_size}: {e}")
                    stats['errors'] += 1
            
            stats['processing_time'] = time.time() - start_time
            
            logger.info(
                f"Embedding complete: {stats['skills_processed']} skills processed, "
                f"{stats['embeddings_created']} embeddings created, "
                f"{stats['errors']} errors, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"Embedding batch processing failed: {e}")
            raise
    
    def update_embeddings(self, force: bool = False) -> Dict[str, Any]:
        """Update embeddings for new or modified skills.
        
        Args:
            force: Force re-embedding of all skills
            
        Returns:
            Update statistics
        """
        if force:
            logger.warning("Force update requested - this will re-embed all skills")
            # Clear existing embeddings
            # Note: Implement this method in DatabaseOperations if needed
        
        return self.process_all_skills()
    
    def compute_similarity_matrix(self, skill_list: List[str] = None) -> np.ndarray:
        """Compute similarity matrix for skills.
        
        Args:
            skill_list: List of skills to compare (None for all)
            
        Returns:
            Similarity matrix
        """
        # Get embeddings from database
        all_embeddings = self.db_ops.get_all_embeddings()
        
        if skill_list:
            # Filter to requested skills
            embeddings_data = [
                e for e in all_embeddings 
                if e['skill_text'] in skill_list
            ]
        else:
            embeddings_data = all_embeddings
        
        if not embeddings_data:
            return np.array([])
        
        # Extract embedding vectors
        embeddings = np.array([e['embedding'] for e in embeddings_data])
        
        # Compute similarity matrix
        # Since embeddings are normalized, cosine similarity is just dot product
        similarity_matrix = np.dot(embeddings, embeddings.T)
        
        return similarity_matrix
    
    def find_duplicate_skills(self, threshold: float = 0.95) -> List[Dict[str, Any]]:
        """Find potential duplicate skills based on embedding similarity.
        
        Args:
            threshold: Similarity threshold for duplicates
            
        Returns:
            List of potential duplicates
        """
        all_embeddings = self.db_ops.get_all_embeddings()
        
        if len(all_embeddings) < 2:
            return []
        
        duplicates = []
        
        # Compare all pairs
        for i in range(len(all_embeddings)):
            for j in range(i + 1, len(all_embeddings)):
                skill1 = all_embeddings[i]
                skill2 = all_embeddings[j]
                
                similarity = self.vectorizer.compute_similarity(
                    np.array(skill1['embedding']),
                    np.array(skill2['embedding'])
                )
                
                if similarity >= threshold:
                    duplicates.append({
                        'skill1': skill1['skill_text'],
                        'skill2': skill2['skill_text'],
                        'similarity': similarity
                    })
        
        # Sort by similarity
        duplicates.sort(key=lambda x: x['similarity'], reverse=True)
        
        logger.info(f"Found {len(duplicates)} potential duplicate skill pairs")
        
        return duplicates
    
    def get_skill_recommendations(self, 
                                job_skills: List[str],
                                top_k: int = 10) -> List[Dict[str, Any]]:
        """Get skill recommendations based on current job skills.
        
        Args:
            job_skills: Current skills in job
            top_k: Number of recommendations
            
        Returns:
            List of recommended skills with scores
        """
        # Get embeddings for input skills
        input_embeddings = self.vectorizer.vectorize(job_skills)
        
        # Average the embeddings to get job profile
        job_profile = np.mean(input_embeddings, axis=0)
        
        # Get all skill embeddings
        all_embeddings = self.db_ops.get_all_embeddings()
        
        # Find similar skills
        recommendations = []
        
        for skill_data in all_embeddings:
            # Skip if already in job skills
            if skill_data['skill_text'] in job_skills:
                continue
            
            similarity = self.vectorizer.compute_similarity(
                job_profile,
                np.array(skill_data['embedding'])
            )
            
            recommendations.append({
                'skill': skill_data['skill_text'],
                'score': similarity
            })
        
        # Sort and return top K
        recommendations.sort(key=lambda x: x['score'], reverse=True)
        
        return recommendations[:top_k]

---

## 8. Analyzer Module Files

### src/analyzer/__init__.py
```python
from .clustering import SkillClusterer
from .dimension_reducer import DimensionReducer
from .report_generator import ReportGenerator
from .visualizations import VisualizationGenerator

__all__ = ['SkillClusterer', 'DimensionReducer', 'ReportGenerator', 'VisualizationGenerator']

### src/analyzer/clustering.py
```python
import logging
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from sklearn.cluster import HDBSCAN, KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score
import pandas as pd
from database.operations import DatabaseOperations
from config.settings import get_settings

logger = logging.getLogger(__name__)

class SkillClusterer:
    """Perform clustering on skill embeddings."""
    
    def __init__(self):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        self.clustering_method = 'hdbscan'
    
    def cluster_skills(self, 
                      embeddings: np.ndarray,
                      skill_texts: List[str],
                      method: str = 'hdbscan',
                      **kwargs) -> Dict[str, Any]:
        """Cluster skills based on embeddings.
        
        Args:
            embeddings: Skill embedding matrix
            skill_texts: List of skill texts
            method: Clustering method ('hdbscan' or 'kmeans')
            **kwargs: Additional parameters for clustering
            
        Returns:
            Clustering results
        """
        if len(embeddings) < 2:
            logger.warning("Not enough data points for clustering")
            return {}
        
        logger.info(f"Clustering {len(embeddings)} skills using {method}")
        
        if method == 'hdbscan':
            results = self._cluster_hdbscan(embeddings, skill_texts, **kwargs)
        elif method == 'kmeans':
            results = self._cluster_kmeans(embeddings, skill_texts, **kwargs)
        else:
            raise ValueError(f"Unknown clustering method: {method}")
        
        # Calculate metrics
        results['metrics'] = self._calculate_metrics(embeddings, results['labels'])
        
        # Characterize clusters
        results['cluster_info'] = self._characterize_clusters(
            results['labels'],
            skill_texts
        )
        
        return results
    
    def _cluster_hdbscan(self, 
                        embeddings: np.ndarray,
                        skill_texts: List[str],
                        min_cluster_size: int = None,
                        min_samples: int = None) -> Dict[str, Any]:
        """Perform HDBSCAN clustering.
        
        Args:
            embeddings: Embedding matrix
            skill_texts: Skill texts
            min_cluster_size: Minimum cluster size
            min_samples: Minimum samples for core points
            
        Returns:
            Clustering results
        """
        # Use settings if not provided
        min_cluster_size = min_cluster_size or self.settings.cluster_min_size
        min_samples = min_samples or self.settings.cluster_min_samples
        
        # Perform clustering
        clusterer = HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            metric='euclidean',
            cluster_selection_method='eom',
            prediction_data=True
        )
        
        labels = clusterer.fit_predict(embeddings)
        
        # Get cluster persistence (stability)
        cluster_persistence = clusterer.cluster_persistence_
        
        results = {
            'method': 'hdbscan',
            'labels': labels,
            'n_clusters': len(set(labels)) - (1 if -1 in labels else 0),
            'n_noise': list(labels).count(-1),
            'parameters': {
                'min_cluster_size': min_cluster_size,
                'min_samples': min_samples
            },
            'cluster_persistence': cluster_persistence,
            'clusterer': clusterer
        }
        
        logger.info(
            f"HDBSCAN found {results['n_clusters']} clusters "
            f"with {results['n_noise']} noise points"
        )
        
        return results
    
    def _cluster_kmeans(self,
                       embeddings: np.ndarray,
                       skill_texts: List[str],
                       n_clusters: int = 20) -> Dict[str, Any]:
        """Perform K-means clustering.
        
        Args:
            embeddings: Embedding matrix
            skill_texts: Skill texts
            n_clusters: Number of clusters
            
        Returns:
            Clustering results
        """
        # Perform clustering
        clusterer = KMeans(
            n_clusters=n_clusters,
            n_init=10,
            max_iter=300,
            random_state=42
        )
        
        labels = clusterer.fit_predict(embeddings)
        
        results = {
            'method': 'kmeans',
            'labels': labels,
            'n_clusters': n_clusters,
            'n_noise': 0,
            'parameters': {
                'n_clusters': n_clusters
            },
            'cluster_centers': clusterer.cluster_centers_,
            'inertia': clusterer.inertia_,
            'clusterer': clusterer
        }
        
        logger.info(f"K-means created {n_clusters} clusters")
        
        return results
    
    def _calculate_metrics(self, embeddings: np.ndarray, labels: np.ndarray) -> Dict[str, float]:
        """Calculate clustering quality metrics.
        
        Args:
            embeddings: Embedding matrix
            labels: Cluster labels
            
        Returns:
            Dictionary of metrics
        """
        metrics = {}
        
        # Remove noise points for metrics
        mask = labels >= 0
        if np.sum(mask) < 2:
            logger.warning("Not enough clustered points for metrics")
            return metrics
        
        try:
            # Silhouette score
            metrics['silhouette_score'] = silhouette_score(
                embeddings[mask],
                labels[mask]
            )
            
            # Davies-Bouldin index (lower is better)
            metrics['davies_bouldin_index'] = davies_bouldin_score(
                embeddings[mask],
                labels[mask]
            )
            
            # Cluster statistics
            unique_labels = np.unique(labels[labels >= 0])
            cluster_sizes = [np.sum(labels == label) for label in unique_labels]
            
            metrics['avg_cluster_size'] = np.mean(cluster_sizes)
            metrics['std_cluster_size'] = np.std(cluster_sizes)
            metrics['min_cluster_size'] = np.min(cluster_sizes)
            metrics['max_cluster_size'] = np.max(cluster_sizes)
            
        except Exception as e:
            logger.error(f"Error calculating metrics: {e}")
        
        return metrics
    
    def _characterize_clusters(self, 
                             labels: np.ndarray,
                             skill_texts: List[str]) -> List[Dict[str, Any]]:
        """Characterize each cluster.
        
        Args:
            labels: Cluster labels
            skill_texts: List of skill texts
            
        Returns:
            List of cluster characteristics
        """
        cluster_info = []
        
        # Create DataFrame for easier analysis
        df = pd.DataFrame({
            'skill': skill_texts,
            'cluster': labels
        })
        
        # Analyze each cluster
        unique_labels = sorted(set(labels))
        
        for label in unique_labels:
            if label == -1:  # Skip noise cluster
                continue
            
            cluster_skills = df[df['cluster'] == label]['skill'].tolist()
            
            # Get most common skills
            skill_counts = pd.Series(cluster_skills).value_counts()
            
            cluster_data = {
                'cluster_id': int(label),
                'size': len(cluster_skills),
                'top_skills': skill_counts.head(10).to_dict(),
                'all_skills': cluster_skills,
                'label': self._generate_cluster_label(skill_counts.head(5).index.tolist())
            }
            
            cluster_info.append(cluster_data)
        
        # Sort by size
        cluster_info.sort(key=lambda x: x['size'], reverse=True)
        
        return cluster_info
    
    def _generate_cluster_label(self, top_skills: List[str]) -> str:
        """Generate a descriptive label for a cluster.
        
        Args:
            top_skills: Top skills in cluster
            
        Returns:
            Cluster label
        """
        # Simple heuristic-based labeling
        skill_lower = [s.lower() for s in top_skills]
        
        if any('frontend' in s or 'react' in s or 'angular' in s or 'vue' in s for s in skill_lower):
            return "Frontend Development"
        elif any('backend' in s or 'node' in s or 'django' in s or 'spring' in s for s in skill_lower):
            return "Backend Development"
        elif any('data' in s or 'analytics' in s or 'sql' in s for s in skill_lower):
            return "Data & Analytics"
        elif any('machine learning' in s or 'ml' in s or 'ai' in s for s in skill_lower):
            return "Machine Learning & AI"
        elif any('devops' in s or 'docker' in s or 'kubernetes' in s for s in skill_lower):
            return "DevOps & Infrastructure"
        elif any('mobile' in s or 'android' in s or 'ios' in s for s in skill_lower):
            return "Mobile Development"
        elif any('cloud' in s or 'aws' in s or 'azure' in s for s in skill_lower):
            return "Cloud Computing"
        else:
            # Use most common skill as label
            return f"{top_skills[0]} & Related"
    
    def run_clustering_pipeline(self) -> Dict[str, Any]:
        """Run complete clustering pipeline on all skills.
        
        Returns:
            Complete clustering results
        """
        # Get all embeddings
        embeddings_data = self.db_ops.get_all_embeddings()
        
        if not embeddings_data:
            logger.warning("No embeddings found for clustering")
            return {}
        
        # Extract data
        skill_texts = [e['skill_text'] for e in embeddings_data]
        embeddings = np.array([e['embedding'] for e in embeddings_data])
        
        logger.info(f"Running clustering on {len(embeddings)} skills")
        
        # Run clustering
        results = self.cluster_skills(embeddings, skill_texts)
        
        # Save results to database
        self.db_ops.save_analysis_results(
            analysis_type='clustering',
            results={
                'n_clusters': results['n_clusters'],
                'n_noise': results['n_noise'],
                'metrics': results['metrics'],
                'cluster_info': results['cluster_info']
            },
            parameters=results['parameters']
        )
        
        return results
        name = re.sub(pattern, '', name, flags=re.IGNORECASE)
    
    return name.strip()

def anonymize_text(text: str) -> str:
    """Anonymize personal information in text.
    
    Args:
        text: Text to anonymize
        
    Returns:
        Anonymized text
    """
    if not text:
        return ""
    
    # Email addresses
    text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', 
                  '[EMAIL]', text)
    
    # Phone numbers (various formats)
    phone_patterns = [
        r'\+?\d{1,3}[-.\s]?\(?\d{1,4}\)?[-.\s]?\d{1,4}[-.\s]?\d{1,9}',
        r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
        r'\b\d{10}\b'
    ]
    
    for pattern in phone_patterns:
        text = re.sub(pattern, '[PHONE]', text)
    
    # URLs
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
                  '[URL]', text)
    
    # Names (basic - this is imperfect)
    # This would need a more sophisticated approach in production
    
    return text

### src/utils/metrics.py
```python
import numpy as np
from typing import Dict, List, Any, Tuple
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix
import pandas as pd

def calculate_extraction_metrics(true_skills: List[str], 
                               extracted_skills: List[str]) -> Dict[str, float]:
    """Calculate metrics for skill extraction.
    
    Args:
        true_skills: Ground truth skills
        extracted_skills: Extracted skills
        
    Returns:
        Dictionary of metrics
    """
    true_set = set(true_skills)
    extracted_set = set(extracted_skills)
    
    # Basic metrics
    true_positives = len(true_set & extracted_set)
    false_positives = len(extracted_set - true_set)
    false_negatives = len(true_set - extracted_set)
    
    # Calculate precision, recall, F1
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    return {
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'true_positives': true_positives,
        'false_positives': false_positives,
        'false_negatives': false_negatives
    }

def calculate_clustering_metrics(embeddings: np.ndarray, 
                               labels: np.ndarray) -> Dict[str, float]:
    """Calculate clustering quality metrics.
    
    Args:
        embeddings: Embedding matrix
        labels: Cluster labels
        
    Returns:
        Dictionary of metrics
    """
    from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
    
    metrics = {}
    
    # Remove noise points
    mask = labels >= 0
    if np.sum(mask) < 2:
        return metrics
    
    try:
        metrics['silhouette_score'] = silhouette_score(embeddings[mask], labels[mask])
        metrics['davies_bouldin_score'] = davies_bouldin_score(embeddings[mask], labels[mask])
        metrics['calinski_harabasz_score'] = calinski_harabasz_score(embeddings[mask], labels[mask])
        
        # Cluster statistics
        unique_labels = np.unique(labels[mask])
        cluster_sizes = [np.sum(labels == label) for label in unique_labels]
        
        metrics['n_clusters'] = len(unique_labels)
        metrics['avg_cluster_size'] = np.mean(cluster_sizes)
        metrics['std_cluster_size'] = np.std(cluster_sizes)
        metrics['min_cluster_size'] = np.min(cluster_sizes)
        metrics['max_cluster_size'] = np.max(cluster_sizes)
        
    except Exception as e:
        print(f"Error calculating clustering metrics: {e}")
    
    return metrics

def calculate_diversity_metrics(skills: List[str]) -> Dict[str, float]:
    """Calculate diversity metrics for skills.
    
    Args:
        skills: List of skills
        
    Returns:
        Dictionary of diversity metrics
    """
    if not skills:
        return {}
    
    # Unique skills
    unique_skills = set(skills)
    
    # Calculate frequencies
    skill_counts = pd.Series(skills).value_counts()
    
    # Shannon entropy
    probabilities = skill_counts / len(skills)
    entropy = -np.sum(probabilities * np.log2(probabilities))
    
    # Gini coefficient
    sorted_counts = np.sort(skill_counts.values)
    n = len(sorted_counts)
    index = np.arange(1, n + 1)
    gini = (2 * np.sum(index * sorted_counts)) / (n * np.sum(sorted_counts)) - (n + 1) / n
    
    return {
        'total_skills': len(skills),
        'unique_skills': len(unique_skills),
        'diversity_ratio': len(unique_skills) / len(skills),
        'shannon_entropy': entropy,
        'gini_coefficient': gini,
        'top_skill_concentration': skill_counts.iloc[0] / len(skills) if len(skill_counts) > 0 else 0
    }

def generate_statistics(data: Dict[str, Any]) -> Dict[str, Any]:
    """Generate comprehensive statistics from analysis data.
    
    Args:
        data: Analysis data
        
    Returns:
        Dictionary of statistics
    """
    stats = {}
    
    # Job statistics
    if 'jobs' in data:
        stats['job_stats'] = {
            'total_jobs': len(data['jobs']),
            'jobs_by_country': pd.Series([j['country'] for j in data['jobs']]).value_counts().to_dict(),
            'jobs_by_portal': pd.Series([j['portal'] for j in data['jobs']]).value_counts().to_dict()
        }
    
    # Skill statistics
    if 'skills' in data:
        all_skills = [s for job in data['skills'] for s in job['skills']]
        stats['skill_stats'] = calculate_diversity_metrics(all_skills)
    
    # Temporal statistics
    if 'temporal_data' in data:
        df = pd.DataFrame(data['temporal_data'])
        if not df.empty and 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
            stats['temporal_stats'] = {
                'date_range': (df['date'].min().isoformat(), df['date'].max().isoformat()),
                'total_days': (df['date'].max() - df['date'].min()).days,
                'avg_jobs_per_day': len(df) / ((df['date'].max() - df['date'].min()).days + 1)
            }
    
    return stats

def calculate_performance_metrics(processing_times: Dict[str, float]) -> Dict[str, Any]:
    """Calculate system performance metrics.
    
    Args:
        processing_times: Dictionary of module processing times
        
    Returns:
        Performance metrics
    """
    total_time = sum(processing_times.values())
    
    metrics = {
        'total_processing_time': total_time,
        'module_times': processing_times,
        'module_percentages': {
            module: (time / total_time * 100) if total_time > 0 else 0
            for module, time in processing_times.items()
        }
    }
    
    # Identify bottlenecks
    sorted_modules = sorted(processing_times.items(), key=lambda x: x[1], reverse=True)
    metrics['bottlenecks'] = [module for module, _ in sorted_modules[:3]]
    
    return metrics

### src/utils/logger.py
```python
import logging
import sys
from typing import Optional

def get_logger(name: str, 
               level: str = 'INFO',
               log_file: Optional[str] = None) -> logging.Logger:
    """Get configured logger instance.
    
    Args:
        name: Logger name
        level: Log level
        log_file: Optional log file path
        
    Returns:
        Configured logger
    """
    logger = logging.getLogger(name)
    
    # Don't reconfigure if already set up
    if logger.handlers:
        return logger
    
    logger.setLevel(getattr(logging, level.upper()))
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    
    # Format
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # File handler if specified
    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

def log_function_call(func):
    """Decorator to log function calls.
    
    Args:
        func: Function to decorate
        
    Returns:
        Decorated function
    """
    logger = get_logger(func.__module__)
    
    def wrapper(*args, **kwargs):
        logger.debug(f"Calling {func.__name__} with args={args}, kwargs={kwargs}")
        try:
            result = func(*args, **kwargs)
            logger.debug(f"{func.__name__} completed successfully")
            return result
        except Exception as e:
            logger.error(f"{func.__name__} failed with error: {e}")
            raise
    
    return wrapper

def setup_module_logger(module_name: str) -> logging.Logger:
    """Set up logger for a specific module.
    
    Args:
        module_name: Name of the module
        
    Returns:
        Configured logger
    """
    from config.settings import get_settings
    settings = get_settings()
    
    return get_logger(
        module_name,
        level=settings.log_level,
        log_file=settings.log_file
    )

---

## 10. Scripts

### scripts/setup_database.py
```python
#!/usr/bin/env python3
"""
Set up the PostgreSQL database for the Labor Market Observatory.
"""

import os
import sys
import psycopg2
from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT
import logging

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.database import get_database_config
from config.settings import get_settings

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_database():
    """Create the database if it doesn't exist."""
    config = get_database_config()
    
    # Connect to PostgreSQL server
    conn = psycopg2.connect(
        host=config['host'],
        port=config['port'],
        user=config['user'],
        password=config['password'],
        database='postgres'  # Connect to default database
    )
    conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)
    
    cursor = conn.cursor()
    
    # Check if database exists
    cursor.execute(
        "SELECT 1 FROM pg_database WHERE datname = %s",
        (config['database'],)
    )
    
    if cursor.fetchone() is None:
        # Create database
        cursor.execute(f"CREATE DATABASE {config['database']}")
        logger.info(f"Database '{config['database']}' created successfully")
    else:
        logger.info(f"Database '{config['database']}' already exists")
    
    cursor.close()
    conn.close()

def setup_extensions():
    """Set up required PostgreSQL extensions."""
    settings = get_settings()
    
    conn = psycopg2.connect(settings.database_url)
    cursor = conn.cursor()
    
    try:
        # Create extensions
        cursor.execute("CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";")
        cursor.execute("CREATE EXTENSION IF NOT EXISTS \"pgvector\";")
        
        conn.commit()
        logger.info("Extensions created successfully")
        
    except Exception as e:
        logger.error(f"Error creating extensions: {e}")
        conn.rollback()
        raise
    finally:
        cursor.close()
        conn.close()

def run_migrations():
    """Run database migrations."""
    settings = get_settings()
    
    # Path to migration files
    migration_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        'src', 'database', 'migrations'
    )
    
    conn = psycopg2.connect(settings.database_url)
    cursor = conn.cursor()
    
    try:
        # Get all SQL files
        sql_files = sorted([f for f in os.listdir(migration_dir) if f.endswith('.sql')])
        
        for sql_file in sql_files:
            logger.info(f"Running migration: {sql_file}")
            
            with open(os.path.join(migration_dir, sql_file), 'r') as f:
                sql_content = f.read()
            
            # Execute migration
            cursor.execute(sql_content)
        
        conn.commit()
        logger.info("All migrations completed successfully")
        
    except Exception as e:
        logger.error(f"Error running migrations: {e}")
        conn.rollback()
        raise
    finally:
        cursor.close()
        conn.close()

def main():
    """Main setup function."""
    logger.info("Starting database setup...")
    
    try:
        # Create database
        create_database()
        
        # Set up extensions
        setup_extensions()
        
        # Run migrations
        run_migrations()
        
        logger.info("Database setup completed successfully!")
        
    except Exception as e:
        logger.error(f"Database setup failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()(#1-root-configuration-files)
2. [Database Setup Files](#2-database-setup-files)
3. [Configuration Module](#3-configuration-module)
4. [Scraper Module Files](#4-scraper-module-files)
5. [Extractor Module Files](#5-extractor-module-files)
6. [LLM Processor Module Files](#6-llm-processor-module-files)
7. [Embedder Module Files](#7-embedder-module-files)
8. [Analyzer Module Files](#8-analyzer-module-files)
9. [Orchestrator and Utilities](#9-orchestrator-and-utilities)
10. [Scripts](#10-scripts)

---

## 1. Root Configuration Files

### requirements.txt
```
# Core
python-dotenv==1.0.0
pydantic==2.5.3
typer==0.9.0
tqdm==4.66.1

# Web Scraping
scrapy==2.11.0
scrapy-selenium==0.0.7
beautifulsoup4==4.12.2
lxml==4.9.3
fake-useragent==1.4.0
requests==2.31.0

# Database
psycopg2-binary==2.9.9
sqlalchemy==2.0.23
pgvector==0.2.3
alembic==1.13.1

# NLP
spacy==3.7.2
langdetect==1.0.9
regex==2023.12.25

# Machine Learning
transformers==4.36.2
sentence-transformers==2.2.2
torch==2.1.2
llama-cpp-python==0.2.32
openai==1.6.1

# Data Processing
pandas==2.1.4
numpy==1.24.3
scikit-learn==1.3.2
umap-learn==0.5.5
hdbscan==0.8.33

# Visualization
matplotlib==3.8.2
seaborn==0.13.0
reportlab==4.0.8
pillow==10.1.0

# Testing
pytest==7.4.4
pytest-asyncio==0.23.3
pytest-cov==4.1.0

# Development
black==23.12.1
flake8==7.0.0
mypy==1.8.0
pre-commit==3.6.0
```

### .env.example
```bash
# Database Configuration
DATABASE_URL=postgresql://labor_user:your_password@localhost:5432/labor_observatory
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=0

# Scraping Configuration
SCRAPER_USER_AGENT="Mozilla/5.0 (Windows NT 10.0; Win64; x64) Academic Research Bot"
SCRAPER_CONCURRENT_REQUESTS=16
SCRAPER_DOWNLOAD_DELAY=1.0
SCRAPER_RETRY_TIMES=3

# ESCO API Configuration
ESCO_API_URL=https://ec.europa.eu/esco/api
ESCO_VERSION=1.1.0
ESCO_LANGUAGE=es

# LLM Configuration
LLM_MODEL_PATH=./data/models/mistral-7b-instruct.Q4_K_M.gguf
LLM_CONTEXT_LENGTH=4096
LLM_MAX_TOKENS=512
LLM_TEMPERATURE=0.7
LLM_N_GPU_LAYERS=35

# OpenAI Fallback (Optional)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-3.5-turbo

# Embedding Configuration
EMBEDDING_MODEL=intfloat/multilingual-e5-base
EMBEDDING_BATCH_SIZE=32
EMBEDDING_CACHE_DIR=./data/cache/embeddings

# Analysis Configuration
CLUSTER_MIN_SIZE=5
CLUSTER_MIN_SAMPLES=3
UMAP_N_NEIGHBORS=15
UMAP_MIN_DIST=0.1

# Output Configuration
OUTPUT_DIR=./outputs
REPORT_FORMAT=pdf
LOG_LEVEL=INFO
LOG_FILE=./logs/labor_observatory.log
```

### .gitignore
```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/
.venv

# IDE
.vscode/
.idea/
*.swp
*.swo

# Data and Models
data/models/*.gguf
data/models/*/
data/cache/
outputs/
logs/

# Database
*.db
*.sqlite3

# Environment
.env
.env.local

# OS
.DS_Store
Thumbs.db

# Testing
.coverage
htmlcov/
.pytest_cache/

# Scrapy
.scrapy/

# Notebooks
.ipynb_checkpoints/
```

### setup.py
```python
from setuptools import setup, find_packages

setup(
    name="labor-observatory",
    version="1.0.0",
    author="Your Team",
    description="Automated Labor Market Observatory for Latin America",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    python_requires=">=3.10",
    install_requires=[
        line.strip()
        for line in open("requirements.txt")
        if line.strip() and not line.startswith("#")
    ],
    entry_points={
        "console_scripts": [
            "labor-observatory=orchestrator:app",
        ],
    },
)
```

---

## 2. Database Setup Files

### src/database/migrations/001_initial_schema.sql
```sql
-- Create database
CREATE DATABASE IF NOT EXISTS labor_observatory
  WITH ENCODING 'UTF8'
  LC_COLLATE = 'en_US.UTF-8'
  LC_CTYPE = 'en_US.UTF-8';

\c labor_observatory;

-- Enable extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgvector";

-- Create tables
CREATE TABLE IF NOT EXISTS raw_jobs (
    job_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    portal VARCHAR(50) NOT NULL,
    country CHAR(2) NOT NULL,
    url TEXT NOT NULL,
    title TEXT NOT NULL,
    company TEXT,
    location TEXT,
    description TEXT NOT NULL,
    requirements TEXT,
    salary_raw TEXT,
    contract_type VARCHAR(50),
    remote_type VARCHAR(50),
    posted_date DATE,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    content_hash VARCHAR(64) UNIQUE,
    raw_html TEXT,
    is_processed BOOLEAN DEFAULT FALSE,
    
    CONSTRAINT chk_country CHECK (country IN ('CO', 'MX', 'AR')),
    CONSTRAINT chk_portal CHECK (portal IN ('computrabajo', 'bumeran', 'elempleo'))
);

CREATE INDEX idx_portal_country ON raw_jobs(portal, country);
CREATE INDEX idx_scraped_at ON raw_jobs(scraped_at);
CREATE INDEX idx_processed ON raw_jobs(is_processed);

CREATE TABLE IF NOT EXISTS extracted_skills (
    extraction_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    job_id UUID REFERENCES raw_jobs(job_id) ON DELETE CASCADE,
    skill_text TEXT NOT NULL,
    skill_type VARCHAR(50),
    extraction_method VARCHAR(50),
    confidence_score FLOAT,
    source_section VARCHAR(50),
    span_start INTEGER,
    span_end INTEGER,
    esco_uri TEXT,
    extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_job_skills ON extracted_skills(job_id);
CREATE INDEX idx_skill_text ON extracted_skills(skill_text);

CREATE TABLE IF NOT EXISTS enhanced_skills (
    enhancement_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    job_id UUID REFERENCES raw_jobs(job_id) ON DELETE CASCADE,
    original_skill_text TEXT,
    normalized_skill TEXT NOT NULL,
    skill_type VARCHAR(50),
    esco_concept_uri TEXT,
    esco_preferred_label TEXT,
    llm_confidence FLOAT,
    llm_reasoning TEXT,
    is_duplicate BOOLEAN DEFAULT FALSE,
    duplicate_of_id UUID,
    enhanced_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    llm_model VARCHAR(100)
);

CREATE INDEX idx_job_enhanced ON enhanced_skills(job_id);
CREATE INDEX idx_normalized ON enhanced_skills(normalized_skill);

CREATE TABLE IF NOT EXISTS skill_embeddings (
    embedding_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    skill_text TEXT UNIQUE NOT NULL,
    embedding vector(768) NOT NULL,
    model_name VARCHAR(100) NOT NULL,
    model_version VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_skill_lookup ON skill_embeddings(skill_text);
CREATE INDEX idx_embedding_similarity ON skill_embeddings 
  USING ivfflat (embedding vector_cosine_ops)
  WITH (lists = 100);

CREATE TABLE IF NOT EXISTS analysis_results (
    analysis_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    analysis_type VARCHAR(50),
    country CHAR(2),
    date_range_start DATE,
    date_range_end DATE,
    parameters JSONB,
    results JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_analysis_type ON analysis_results(analysis_type);
CREATE INDEX idx_analysis_date ON analysis_results(created_at);

-- Create views
CREATE VIEW skill_frequency AS
SELECT 
    es.normalized_skill,
    COUNT(DISTINCT es.job_id) as job_count,
    COUNT(*) as total_mentions,
    ARRAY_AGG(DISTINCT rj.country) as countries
FROM enhanced_skills es
JOIN raw_jobs rj ON es.job_id = rj.job_id
WHERE es.is_duplicate = FALSE
GROUP BY es.normalized_skill
ORDER BY job_count DESC;

CREATE VIEW country_skill_distribution AS
SELECT 
    rj.country,
    es.normalized_skill,
    COUNT(DISTINCT es.job_id) as job_count
FROM enhanced_skills es
JOIN raw_jobs rj ON es.job_id = rj.job_id
WHERE es.is_duplicate = FALSE
GROUP BY rj.country, es.normalized_skill
ORDER BY rj.country, job_count DESC;
```

### src/database/models.py
```python
from sqlalchemy import Column, String, Text, Boolean, Float, Integer, DateTime, Date, ForeignKey, JSON
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from pgvector.sqlalchemy import Vector
import uuid

Base = declarative_base()

class RawJob(Base):
    __tablename__ = 'raw_jobs'
    
    job_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    portal = Column(String(50), nullable=False)
    country = Column(String(2), nullable=False)
    url = Column(Text, nullable=False)
    title = Column(Text, nullable=False)
    company = Column(Text)
    location = Column(Text)
    description = Column(Text, nullable=False)
    requirements = Column(Text)
    salary_raw = Column(Text)
    contract_type = Column(String(50))
    remote_type = Column(String(50))
    posted_date = Column(Date)
    scraped_at = Column(DateTime, server_default=func.now())
    content_hash = Column(String(64), unique=True)
    raw_html = Column(Text)
    is_processed = Column(Boolean, default=False)
    
    # Relationships
    extracted_skills = relationship("ExtractedSkill", back_populates="job")
    enhanced_skills = relationship("EnhancedSkill", back_populates="job")

class ExtractedSkill(Base):
    __tablename__ = 'extracted_skills'
    
    extraction_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    job_id = Column(UUID(as_uuid=True), ForeignKey('raw_jobs.job_id'))
    skill_text = Column(Text, nullable=False)
    skill_type = Column(String(50))
    extraction_method = Column(String(50))
    confidence_score = Column(Float)
    source_section = Column(String(50))
    span_start = Column(Integer)
    span_end = Column(Integer)
    esco_uri = Column(Text)
    extracted_at = Column(DateTime, server_default=func.now())
    
    # Relationships
    job = relationship("RawJob", back_populates="extracted_skills")

class EnhancedSkill(Base):
    __tablename__ = 'enhanced_skills'
    
    enhancement_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    job_id = Column(UUID(as_uuid=True), ForeignKey('raw_jobs.job_id'))
    original_skill_text = Column(Text)
    normalized_skill = Column(Text, nullable=False)
    skill_type = Column(String(50))
    esco_concept_uri = Column(Text)
    esco_preferred_label = Column(Text)
    llm_confidence = Column(Float)
    llm_reasoning = Column(Text)
    is_duplicate = Column(Boolean, default=False)
    duplicate_of_id = Column(UUID(as_uuid=True))
    enhanced_at = Column(DateTime, server_default=func.now())
    llm_model = Column(String(100))
    
    # Relationships
    job = relationship("RawJob", back_populates="enhanced_skills")

class SkillEmbedding(Base):
    __tablename__ = 'skill_embeddings'
    
    embedding_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    skill_text = Column(Text, unique=True, nullable=False)
    embedding = Column(Vector(768), nullable=False)
    model_name = Column(String(100), nullable=False)
    model_version = Column(String(50))
    created_at = Column(DateTime, server_default=func.now())

class AnalysisResult(Base):
    __tablename__ = 'analysis_results'
    
    analysis_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    analysis_type = Column(String(50))
    country = Column(String(2))
    date_range_start = Column(Date)
    date_range_end = Column(Date)
    parameters = Column(JSONB)
    results = Column(JSONB)
    created_at = Column(DateTime, server_default=func.now())
```

### src/database/operations.py
```python
from typing import List, Optional, Dict, Any
from datetime import datetime
from sqlalchemy import create_engine, and_, or_, func
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.exc import IntegrityError
import os
from .models import Base, RawJob, ExtractedSkill, EnhancedSkill, SkillEmbedding, AnalysisResult
import hashlib
import logging

logger = logging.getLogger(__name__)

class DatabaseOperations:
    def __init__(self, database_url: Optional[str] = None):
        self.database_url = database_url or os.getenv('DATABASE_URL')
        self.engine = create_engine(
            self.database_url,
            pool_size=20,
            max_overflow=0,
            pool_pre_ping=True
        )
        Base.metadata.create_all(self.engine)
        self.SessionLocal = sessionmaker(bind=self.engine)
    
    def get_session(self) -> Session:
        """Get a new database session."""
        return self.SessionLocal()
    
    def insert_job(self, job_data: Dict[str, Any]) -> Optional[str]:
        """Insert a new job posting."""
        session = self.get_session()
        try:
            # Generate content hash
            content = f"{job_data['title']}{job_data['description']}{job_data.get('requirements', '')}"
            content_hash = hashlib.sha256(content.encode()).hexdigest()
            
            job = RawJob(
                **job_data,
                content_hash=content_hash
            )
            session.add(job)
            session.commit()
            
            job_id = str(job.job_id)
            logger.info(f"Inserted job {job_id}")
            return job_id
            
        except IntegrityError:
            session.rollback()
            logger.warning(f"Duplicate job detected: {job_data['url']}")
            return None

### src/analyzer/visualizations.py
```python
import logging
from typing import Dict, Any, List, Optional, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from datetime import datetime
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class VisualizationGenerator:
    """Generate static visualizations for analysis results."""
    
    def __init__(self, output_dir: str = None):
        self.output_dir = output_dir or "./outputs"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Set style
        plt.style.use('seaborn-v0_8-darkgrid')
        sns.set_palette("husl")
        
        # Set font for better Spanish support
        plt.rcParams['font.family'] = 'DejaVu Sans'
    
    def create_all_visualizations(self, 
                                analysis_data: Dict[str, Any],
                                country: Optional[str] = None) -> List[str]:
        """Create all standard visualizations.
        
        Args:
            analysis_data: Dictionary with analysis results
            country: Country code for filtering
            
        Returns:
            List of generated file paths
        """
        generated_files = []
        
        # Skill frequency chart
        if 'skill_statistics' in analysis_data:
            path = self.create_skill_frequency_chart(
                analysis_data['skill_statistics'],
                country
            )
            if path:
                generated_files.append(path)
        
        # Cluster visualization
        if 'clustering_results' in analysis_data:
            path = self.create_cluster_visualization(
                analysis_data['clustering_results']
            )
            if path:
                generated_files.append(path)
        
        # Geographic distribution
        if 'geographic_data' in analysis_data:
            path = self.create_geographic_distribution(
                analysis_data['geographic_data']
            )
            if path:
                generated_files.append(path)
        
        # Skill co-occurrence heatmap
        if 'skill_cooccurrence' in analysis_data:
            path = self.create_skill_cooccurrence_heatmap(
                analysis_data['skill_cooccurrence']
            )
            if path:
                generated_files.append(path)
        
        return generated_files
    
    def create_skill_frequency_chart(self,
                                   skill_stats: Dict[str, Any],
                                   country: Optional[str] = None,
                                   top_n: int = 20) -> Optional[str]:
        """Create horizontal bar chart of top skills.
        
        Args:
            skill_stats: Skill statistics data
            country: Country filter
            top_n: Number of top skills to show
            
        Returns:
            Path to saved chart
        """
        try:
            # Prepare data
            top_skills = skill_stats.get('top_skills', [])[:top_n]
            if not top_skills:
                logger.warning("No skill data for frequency chart")
                return None
            
            df = pd.DataFrame(top_skills)
            
            # Create figure
            fig, ax = plt.subplots(figsize=(12, 8))
            
            # Create horizontal bar chart
            bars = ax.barh(df['skill'], df['count'], 
                          color=plt.cm.viridis(np.linspace(0.3, 0.9, len(df))))
            
            # Customize
            ax.set_xlabel('Número de Vacantes', fontsize=14)
            ax.set_ylabel('Habilidad Técnica', fontsize=14)
            
            title = f'Top {top_n} Habilidades Más Demandadas'
            if country:
                country_names = {'CO': 'Colombia', 'MX': 'México', 'AR': 'Argentina'}
                title += f' - {country_names.get(country, country)}'
            ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
            
            # Add value labels
            for bar in bars:
                width = bar.get_width()
                ax.text(width + 1, bar.get_y() + bar.get_height()/2,
                       f'{int(width)}',
                       ha='left', va='center', fontsize=10)
            
            # Adjust layout
            plt.tight_layout()
            ax.invert_yaxis()  # Highest on top
            
            # Grid
            ax.grid(True, axis='x', alpha=0.3)
            ax.set_axisbelow(True)
            
            # Save
            filename = self._generate_filename('skill_frequency', country)
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created skill frequency chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating skill frequency chart: {e}")
            return None
    
    def create_cluster_visualization(self,
                                   clustering_results: Dict[str, Any]) -> Optional[str]:
        """Create 2D scatter plot of skill clusters.
        
        Args:
            clustering_results: Clustering analysis results
            
        Returns:
            Path to saved visualization
        """
        try:
            # Extract data
            coordinates = clustering_results.get('coordinates_2d')
            labels = clustering_results.get('labels')
            skills = clustering_results.get('skills', [])
            
            if coordinates is None or labels is None:
                logger.warning("Missing data for cluster visualization")
                return None
            
            # Create figure
            fig, ax = plt.subplots(figsize=(14, 10))
            
            # Get unique labels
            unique_labels = set(labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            
            # Color map
            colors = plt.cm.rainbow(np.linspace(0, 1, n_clusters))
            
            # Plot each cluster
            for k, col in zip(sorted(unique_labels), colors):
                if k == -1:
                    # Noise points in black
                    col = 'black'
                    label = 'Ruido'
                else:
                    label = f'Cluster {k}'
                
                class_member_mask = (labels == k)
                xy = coordinates[class_member_mask]
                
                ax.scatter(xy[:, 0], xy[:, 1], 
                         c=[col], 
                         label=label,
                         alpha=0.6,
                         s=30)
            
            # Add labels for some points (avoid overlap)
            if skills:
                # Sample points to label
                n_labels = min(30, len(skills))
                indices = np.random.choice(len(skills), n_labels, replace=False)
                
                for idx in indices:
                    ax.annotate(skills[idx], 
                              (coordinates[idx, 0], coordinates[idx, 1]),
                              fontsize=8,
                              alpha=0.7)
            
            # Customize
            ax.set_title('Visualización de Clusters de Habilidades', 
                        fontsize=16, fontweight='bold')
            ax.set_xlabel('UMAP Dimension 1', fontsize=12)
            ax.set_ylabel('UMAP Dimension 2', fontsize=12)
            
            # Legend
            ax.legend(loc='upper right', bbox_to_anchor=(1.15, 1))
            
            # Remove ticks
            ax.set_xticks([])
            ax.set_yticks([])
            
            # Save
            filename = self._generate_filename('skill_clusters')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created cluster visualization: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating cluster visualization: {e}")
            return None
    
    def create_geographic_distribution(self,
                                     geo_data: Dict[str, Any]) -> Optional[str]:
        """Create geographic distribution chart.
        
        Args:
            geo_data: Geographic distribution data
            
        Returns:
            Path to saved chart
        """
        try:
            # Create figure
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # Job distribution by country
            countries = ['Colombia', 'México', 'Argentina']
            job_counts = [
                geo_data.get('CO', {}).get('total_jobs', 0),
                geo_data.get('MX', {}).get('total_jobs', 0),
                geo_data.get('AR', {}).get('total_jobs', 0)
            ]
            
            # Pie chart
            colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
            wedges, texts, autotexts = ax1.pie(job_counts, 
                                              labels=countries,
                                              colors=colors,
                                              autopct='%1.1f%%',
                                              startangle=90)
            
            ax1.set_title('Distribución de Vacantes por País', 
                         fontsize=14, fontweight='bold')
            
            # Skills per country
            skill_counts = [
                geo_data.get('CO', {}).get('unique_skills', 0),
                geo_data.get('MX', {}).get('unique_skills', 0),
                geo_data.get('AR', {}).get('unique_skills', 0)
            ]
            
            bars = ax2.bar(countries, skill_counts, color=colors)
            ax2.set_title('Habilidades Únicas por País', 
                         fontsize=14, fontweight='bold')
            ax2.set_ylabel('Número de Habilidades', fontsize=12)
            
            # Add value labels
            for bar in bars:
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height,
                        f'{int(height)}',
                        ha='center', va='bottom')
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('geographic_distribution')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created geographic distribution chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating geographic distribution: {e}")
            return None
    
    def create_skill_cooccurrence_heatmap(self,
                                        cooccurrence_data: Dict[str, Any],
                                        top_n: int = 15) -> Optional[str]:
        """Create heatmap of skill co-occurrences.
        
        Args:
            cooccurrence_data: Skill co-occurrence matrix
            top_n: Number of top skills to include
            
        Returns:
            Path to saved heatmap
        """
        try:
            # Convert to DataFrame
            df = pd.DataFrame(cooccurrence_data.get('matrix', []))
            skills = cooccurrence_data.get('skills', [])
            
            if df.empty or not skills:
                logger.warning("No co-occurrence data available")
                return None
            
            # Select top skills
            if len(skills) > top_n:
                # Sum co-occurrences for each skill
                skill_importance = df.sum(axis=0) + df.sum(axis=1)
                top_indices = skill_importance.nlargest(top_n).index
                df = df.loc[top_indices, top_indices]
                skills = [skills[i] for i in top_indices]
            
            # Create figure
            fig, ax = plt.subplots(figsize=(12, 10))
            
            # Create heatmap
            sns.heatmap(df, 
                       xticklabels=skills,
                       yticklabels=skills,
                       cmap='YlOrRd',
                       cbar_kws={'label': 'Co-ocurrencias'},
                       square=True,
                       linewidths=0.5,
                       ax=ax)
            
            # Customize
            ax.set_title('Matriz de Co-ocurrencia de Habilidades', 
                        fontsize=16, fontweight='bold', pad=20)
            
            # Rotate labels
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('skill_cooccurrence')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created co-occurrence heatmap: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating co-occurrence heatmap: {e}")
            return None
    
    def create_temporal_trends(self,
                             temporal_data: Dict[str, Any],
                             skills: List[str] = None) -> Optional[str]:
        """Create temporal trend visualization.
        
        Args:
            temporal_data: Temporal trend data
            skills: List of skills to plot (default: top 5)
            
        Returns:
            Path to saved chart
        """
        try:
            # Prepare data
            df = pd.DataFrame(temporal_data.get('trends', []))
            
            if df.empty:
                logger.warning("No temporal data available")
                return None
            
            # Convert date column
            df['date'] = pd.to_datetime(df['date'])
            
            # Select skills to plot
            if not skills:
                # Get top 5 skills by total mentions
                skill_totals = df.groupby('skill')['count'].sum()
                skills = skill_totals.nlargest(5).index.tolist()
            
            # Create figure
            fig, ax = plt.subplots(figsize=(14, 8))
            
            # Plot each skill
            for skill in skills:
                skill_data = df[df['skill'] == skill]
                ax.plot(skill_data['date'], skill_data['count'], 
                       marker='o', label=skill, linewidth=2)
            
            # Customize
            ax.set_title('Tendencias Temporales de Habilidades', 
                        fontsize=16, fontweight='bold')
            ax.set_xlabel('Fecha', fontsize=12)
            ax.set_ylabel('Número de Menciones', fontsize=12)
            
            # Format x-axis
            import matplotlib.dates as mdates
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
            ax.xaxis.set_major_locator(mdates.MonthLocator())
            plt.xticks(rotation=45)
            
            # Legend
            ax.legend(loc='upper left', bbox_to_anchor=(1, 1))
            
            # Grid
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # Save
            filename = self._generate_filename('temporal_trends')
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            logger.info(f"Created temporal trends chart: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating temporal trends: {e}")
            return None
    
    def _generate_filename(self, chart_type: str, country: Optional[str] = None) -> str:
        """Generate filename with timestamp.
        
        Args:
            chart_type: Type of chart
            country: Country code (optional)
            
        Returns:
            Generated filename
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        country_suffix = f"_{country}" if country else ""
        return f"{chart_type}{country_suffix}_{timestamp}.png"

---

## 9. Orchestrator and Utilities

### src/orchestrator.py
```python
#!/usr/bin/env python3
"""
Main orchestrator for the Labor Market Observatory pipeline.
"""

import logging
import sys
from typing import Optional
import typer
from rich.console import Console
from rich.table import Table
from rich.progress import track
import time

from config.settings import get_settings
from config.logging_config import setup_logging
from database.operations import DatabaseOperations
from scraper.spiders.computrabajo_spider import ComputrabajoSpider
from scraper.spiders.bumeran_spider import BumeranSpider
from scraper.spiders.elempleo_spider import ElempleoSpider
from extractor.pipeline import ExtractionPipeline
from llm_processor.pipeline import LLMProcessingPipeline
from embedder.batch_processor import BatchProcessor
from analyzer.clustering import SkillClusterer
from analyzer.dimension_reducer import DimensionReducer
from analyzer.report_generator import ReportGenerator
from analyzer.visualizations import VisualizationGenerator

# Initialize
app = typer.Typer(help="Labor Market Observatory CLI")
console = Console()
settings = get_settings()
logger = setup_logging(settings.log_level, settings.log_file)

@app.command()
def scrape(
    country: str = typer.Argument(..., help="Country code (CO, MX, AR)"),
    portal: str = typer.Argument(..., help="Portal name (computrabajo, bumeran, elempleo)"),
    pages: int = typer.Option(10, help="Number of pages to scrape")
):
    """Run web scraping for a specific portal and country."""
    console.print(f"[bold green]Starting scraper for {portal} in {country}[/bold green]")
    
    # Validate inputs
    if country not in settings.supported_countries:
        console.print(f"[red]Invalid country: {country}[/red]")
        raise typer.Exit(1)
    
    if portal not in settings.supported_portals:
        console.print(f"[red]Invalid portal: {portal}[/red]")
        raise typer.Exit(1)
    
    # Run appropriate spider
    try:
        from scrapy.crawler import CrawlerProcess
        from scrapy.utils.project import get_project_settings
        
        # Get scrapy settings
        scrapy_settings = get_project_settings()
        scrapy_settings.update({
            'LOG_LEVEL': 'INFO',
            'CLOSESPIDER_PAGECOUNT': pages
        })
        
        process = CrawlerProcess(scrapy_settings)
        
        # Select spider
        if portal == 'computrabajo':
            spider_class = ComputrabajoSpider
        elif portal == 'bumeran':
            spider_class = BumeranSpider
        elif portal == 'elempleo':
            spider_class = ElempleoSpider
        
        # Run spider
        process.crawl(spider_class, country=country)
        process.start()
        
        console.print("[bold green]Scraping completed![/bold green]")
        
    except Exception as e:
        console.print(f"[red]Scraping failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def extract(
    batch_size: int = typer.Option(100, help="Batch size for processing")
):
    """Extract skills from scraped job postings."""
    console.print("[bold green]Starting skill extraction...[/bold green]")
    
    try:
        pipeline = ExtractionPipeline()
        
        with console.status("[bold green]Extracting skills...") as status:
            stats = pipeline.process_batch(batch_size)
        
        # Display results
        table = Table(title="Extraction Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Jobs Processed", str(stats['jobs_processed']))
        table.add_row("Skills Extracted", str(stats['skills_extracted']))
        table.add_row("ESCO Matches", str(stats['esco_matches']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        table.add_row("Jobs/Second", f"{stats['jobs_per_second']:.2f}")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Extraction failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def enhance(
    batch_size: int = typer.Option(50, help="Batch size for LLM processing"),
    model: str = typer.Option("local", help="Model type (local or openai)")
):
    """Enhance extracted skills using LLM."""
    console.print(f"[bold green]Starting LLM enhancement with {model} model...[/bold green]")
    
    try:
        pipeline = LLMProcessingPipeline(model_type=model)
        
        with console.status("[bold green]Processing with LLM...") as status:
            stats = pipeline.process_batch(batch_size)
        
        # Display results
        table = Table(title="LLM Enhancement Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Jobs Processed", str(stats['jobs_processed']))
        table.add_row("Skills Enhanced", str(stats['skills_enhanced']))
        table.add_row("Implicit Skills Found", str(stats['implicit_skills_found']))
        table.add_row("Skills Normalized", str(stats['skills_normalized']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Enhancement failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def embed(
    model: Optional[str] = typer.Option(None, help="Embedding model name")
):
    """Generate embeddings for all skills."""
    console.print("[bold green]Starting embedding generation...[/bold green]")
    
    try:
        processor = BatchProcessor(model_name=model)
        
        with console.status("[bold green]Generating embeddings...") as status:
            stats = processor.process_all_skills()
        
        # Display results
        table = Table(title="Embedding Results")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Skills Processed", str(stats['skills_processed']))
        table.add_row("Embeddings Created", str(stats['embeddings_created']))
        table.add_row("Errors", str(stats['errors']))
        table.add_row("Processing Time", f"{stats['processing_time']:.2f}s")
        
        console.print(table)
        
    except Exception as e:
        console.print(f"[red]Embedding failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def analyze(
    method: str = typer.Option("hdbscan", help="Clustering method")
):
    """Run clustering analysis on skill embeddings."""
    console.print(f"[bold green]Starting clustering analysis with {method}...[/bold green]")
    
    try:
        clusterer = SkillClusterer()
        
        with console.status("[bold green]Running clustering...") as status:
            results = clusterer.run_clustering_pipeline()
        
        # Display results
        if results:
            table = Table(title="Clustering Results")
            table.add_column("Metric", style="cyan")
            table.add_column("Value", style="magenta")
            
            table.add_row("Number of Clusters", str(results['n_clusters']))
            table.add_row("Noise Points", str(results['n_noise']))
            table.add_row("Silhouette Score", f"{results['metrics'].get('silhouette_score', 0):.3f}")
            
            console.print(table)
            
            # Show top clusters
            console.print("\n[bold]Top 5 Clusters:[/bold]")
            for cluster in results['cluster_info'][:5]:
                console.print(f"  • {cluster['label']}: {cluster['size']} skills")
        
    except Exception as e:
        console.print(f"[red]Analysis failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def report(
    country: Optional[str] = typer.Option(None, help="Country code to filter"),
    format: str = typer.Option("pdf", help="Report format (pdf)")
):
    """Generate analysis report."""
    console.print("[bold green]Generating report...[/bold green]")
    
    try:
        generator = ReportGenerator()
        
        with console.status("[bold green]Creating report...") as status:
            filepath = generator.generate_full_report(
                country=country,
                include_visualizations=True
            )
        
        console.print(f"[bold green]Report generated: {filepath}[/bold green]")
        
    except Exception as e:
        console.print(f"[red]Report generation failed: {e}[/red]")
        raise typer.Exit(1)

@app.command()
def pipeline(
    country: str = typer.Argument(..., help="Country code"),
    portal: str = typer.Argument(..., help="Portal name"),
    full: bool = typer.Option(False, help="Run full pipeline including scraping")
):
    """Run complete pipeline."""
    console.print("[bold green]Running complete pipeline...[/bold green]")
    
    steps = []
    
    if full:
        steps.append(("Scraping", lambda: scrape(country, portal, pages=5)))
    
    steps.extend([
        ("Extraction", lambda: extract(batch_size=100)),
        ("LLM Enhancement", lambda: enhance(batch_size=50)),
        ("Embedding", lambda: embed()),
        ("Clustering", lambda: analyze()),
        ("Report Generation", lambda: report(country=country))
    ])
    
    for step_name, step_func in track(steps, description="Processing..."):
        try:
            console.print(f"\n[bold cyan]Running: {step_name}[/bold cyan]")
            step_func()
            time.sleep(1)  # Brief pause between steps
        except Exception as e:
            console.print(f"[red]Step '{step_name}' failed: {e}[/red]")
            raise typer.Exit(1)
    
    console.print("\n[bold green]Pipeline completed successfully![/bold green]")

@app.command()
def status():
    """Show system status and statistics."""
    console.print("[bold green]Labor Market Observatory Status[/bold green]\n")
    
    try:
        db_ops = DatabaseOperations()
        
        # Get statistics
        stats = db_ops.get_skill_statistics()
        
        # Display overall stats
        table = Table(title="System Statistics")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Total Unique Skills", str(stats.get('total_unique_skills', 0)))
        table.add_row("Database Size", "N/A")  # Would need to implement
        
        console.print(table)
        
        # Top skills
        if stats.get('top_skills'):
            console.print("\n[bold]Top 10 Skills:[/bold]")
            for i, skill in enumerate(stats['top_skills'][:10], 1):
                console.print(f"  {i}. {skill['skill']} ({skill['count']} jobs)")
        
    except Exception as e:
        console.print(f"[red]Failed to get status: {e}[/red]")
        raise typer.Exit(1)

if __name__ == "__main__":
    app()

### src/utils/__init__.py
```python
from .validators import validate_country, validate_portal, validate_skill
from .cleaners import clean_text, normalize_text, remove_html
from .metrics import calculate_metrics, generate_statistics
from .logger import get_logger

__all__ = [
    'validate_country', 'validate_portal', 'validate_skill',
    'clean_text', 'normalize_text', 'remove_html',
    'calculate_metrics', 'generate_statistics',
    'get_logger'
]
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting job: {e}")
            raise
        finally:
            session.close()
    
    def get_unprocessed_jobs(self, limit: int = 100) -> List[RawJob]:
        """Get unprocessed job postings."""
        session = self.get_session()
        try:
            jobs = session.query(RawJob).filter(
                RawJob.is_processed == False
            ).limit(limit).all()
            return jobs
        finally:
            session.close()
    
    def mark_job_processed(self, job_id: str):
        """Mark a job as processed."""
        session = self.get_session()
        try:
            session.query(RawJob).filter(
                RawJob.job_id == job_id
            ).update({"is_processed": True})
            session.commit()
        finally:
            session.close()
    
    def insert_extracted_skills(self, job_id: str, skills: List[Dict[str, Any]]):
        """Insert extracted skills for a job."""
        session = self.get_session()
        try:
            for skill_data in skills:
                skill = ExtractedSkill(
                    job_id=job_id,
                    **skill_data
                )
                session.add(skill)
            session.commit()
            logger.info(f"Inserted {len(skills)} extracted skills for job {job_id}")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting extracted skills: {e}")
            raise
        finally:
            session.close()
    
    def get_extracted_skills_for_processing(self, limit: int = 100) -> List[Dict[str, Any]]:
        """Get jobs with extracted skills that need LLM processing."""
        session = self.get_session()
        try:
            # Get jobs that have extracted skills but no enhanced skills
            subquery = session.query(EnhancedSkill.job_id).distinct()
            
            jobs = session.query(RawJob).join(ExtractedSkill).filter(
                ~RawJob.job_id.in_(subquery)
            ).limit(limit).all()
            
            result = []
            for job in jobs:
                skills = session.query(ExtractedSkill).filter(
                    ExtractedSkill.job_id == job.job_id
                ).all()
                
                result.append({
                    'job_id': str(job.job_id),
                    'job_title': job.title,
                    'job_description': job.description,
                    'job_requirements': job.requirements,
                    'extracted_skills': [
                        {
                            'skill_text': skill.skill_text,
                            'extraction_method': skill.extraction_method,
                            'source_section': skill.source_section,
                            'confidence_score': skill.confidence_score
                        }
                        for skill in skills
                    ]
                })
            
            return result
        finally:
            session.close()
    
    def insert_enhanced_skills(self, job_id: str, skills: List[Dict[str, Any]]):
        """Insert enhanced skills for a job."""
        session = self.get_session()
        try:
            for skill_data in skills:
                skill = EnhancedSkill(
                    job_id=job_id,
                    **skill_data
                )
                session.add(skill)
            session.commit()
            logger.info(f"Inserted {len(skills)} enhanced skills for job {job_id}")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting enhanced skills: {e}")
            raise
        finally:
            session.close()
    
    def get_unique_skills_for_embedding(self) -> List[str]:
        """Get unique normalized skills that don't have embeddings yet."""
        session = self.get_session()
        try:
            # Get skills that don't have embeddings
            embedded_skills = session.query(SkillEmbedding.skill_text).distinct()
            
            unique_skills = session.query(
                EnhancedSkill.normalized_skill
            ).filter(
                EnhancedSkill.is_duplicate == False,
                ~EnhancedSkill.normalized_skill.in_(embedded_skills)
            ).distinct().all()
            
            return [skill[0] for skill in unique_skills]
        finally:
            session.close()
    
    def insert_skill_embeddings(self, embeddings: List[Dict[str, Any]]):
        """Insert skill embeddings."""
        session = self.get_session()
        try:
            for emb_data in embeddings:
                embedding = SkillEmbedding(**emb_data)
                session.add(embedding)
            session.commit()
            logger.info(f"Inserted {len(embeddings)} skill embeddings")
        except Exception as e:
            session.rollback()
            logger.error(f"Error inserting embeddings: {e}")
            raise
        finally:
            session.close()
    
    def get_all_embeddings(self) -> List[Dict[str, Any]]:
        """Get all skill embeddings for clustering."""
        session = self.get_session()
        try:
            embeddings = session.query(SkillEmbedding).all()
            return [
                {
                    'skill_text': emb.skill_text,
                    'embedding': emb.embedding,
                    'embedding_id': str(emb.embedding_id)
                }
                for emb in embeddings
            ]
        finally:
            session.close()
    
    def save_analysis_results(self, analysis_type: str, results: Dict[str, Any], 
                            parameters: Dict[str, Any], country: Optional[str] = None):
        """Save analysis results."""
        session = self.get_session()
        try:
            analysis = AnalysisResult(
                analysis_type=analysis_type,
                country=country,
                parameters=parameters,
                results=results
            )
            session.add(analysis)
            session.commit()
            logger.info(f"Saved {analysis_type} analysis results")
        except Exception as e:
            session.rollback()
            logger.error(f"Error saving analysis results: {e}")
            raise
        finally:
            session.close()
    
    def get_skill_statistics(self, country: Optional[str] = None) -> Dict[str, Any]:
        """Get skill statistics by country."""
        session = self.get_session()
        try:
            query = session.query(
                EnhancedSkill.normalized_skill,
                func.count(func.distinct(EnhancedSkill.job_id)).label('job_count')
            ).join(RawJob).filter(
                EnhancedSkill.is_duplicate == False
            )
            
            if country:
                query = query.filter(RawJob.country == country)
            
            results = query.group_by(
                EnhancedSkill.normalized_skill
            ).order_by(
                func.count(func.distinct(EnhancedSkill.job_id)).desc()
            ).limit(50).all()
            
            return {
                'top_skills': [
                    {'skill': skill, 'count': count}
                    for skill, count in results
                ],
                'total_unique_skills': session.query(
                    func.count(func.distinct(EnhancedSkill.normalized_skill))
                ).filter(EnhancedSkill.is_duplicate == False).scalar()
            }
        finally:
            session.close()
```

---

## 3. Configuration Module

### src/config/__init__.py
```python
from .settings import Settings, get_settings
from .database import get_database_url
from .logging_config import setup_logging

__all__ = ['Settings', 'get_settings', 'get_database_url', 'setup_logging']
```

### src/config/settings.py
```python
from pydantic_settings import BaseSettings
from pydantic import Field, validator
from typing import Optional, List
import os
from functools import lru_cache

class Settings(BaseSettings):
    # Database
    database_url: str = Field(..., env='DATABASE_URL')
    database_pool_size: int = Field(20, env='DATABASE_POOL_SIZE')
    
    # Scraping
    scraper_user_agent: str = Field(..., env='SCRAPER_USER_AGENT')
    scraper_concurrent_requests: int = Field(16, env='SCRAPER_CONCURRENT_REQUESTS')
    scraper_download_delay: float = Field(1.0, env='SCRAPER_DOWNLOAD_DELAY')
    scraper_retry_times: int = Field(3, env='SCRAPER_RETRY_TIMES')
    
    # ESCO
    esco_api_url: str = Field('https://ec.europa.eu/esco/api', env='ESCO_API_URL')
    esco_version: str = Field('1.1.0', env='ESCO_VERSION')
    esco_language: str = Field('es', env='ESCO_LANGUAGE')
    
    # LLM
    llm_model_path: str = Field(..., env='LLM_MODEL_PATH')
    llm_context_length: int = Field(4096, env='LLM_CONTEXT_LENGTH')
    llm_max_tokens: int = Field(512, env='LLM_MAX_TOKENS')
    llm_temperature: float = Field(0.7, env='LLM_TEMPERATURE')
    llm_n_gpu_layers: int = Field(35, env='LLM_N_GPU_LAYERS')
    
    # OpenAI (Optional)
    openai_api_key: Optional[str] = Field(None, env='OPENAI_API_KEY')
    openai_model: str = Field('gpt-3.5-turbo', env='OPENAI_MODEL')
    
    # Embeddings
    embedding_model: str = Field('intfloat/multilingual-e5-base', env='EMBEDDING_MODEL')
    embedding_batch_size: int = Field(32, env='EMBEDDING_BATCH_SIZE')
    embedding_cache_dir: str = Field('./data/cache/embeddings', env='EMBEDDING_CACHE_DIR')
    
    # Analysis
    cluster_min_size: int = Field(5, env='CLUSTER_MIN_SIZE')
    cluster_min_samples: int = Field(3, env='CLUSTER_MIN_SAMPLES')
    umap_n_neighbors: int = Field(15, env='UMAP_N_NEIGHBORS')
    umap_min_dist: float = Field(0.1, env='UMAP_MIN_DIST')
    
    # Output
    output_dir: str = Field('./outputs', env='OUTPUT_DIR')
    report_format: str = Field('pdf', env='REPORT_FORMAT')
    log_level: str = Field('INFO', env='LOG_LEVEL')
    log_file: str = Field('./logs/labor_observatory.log', env='LOG_FILE')
    
    # Supported countries and portals
    supported_countries: List[str] = ['CO', 'MX', 'AR']
    supported_portals: List[str] = ['computrabajo', 'bumeran', 'elempleo']
    
    class Config:
        env_file = '.env'
        env_file_encoding = 'utf-8'
    
    @validator('output_dir', 'log_file', 'embedding_cache_dir')
    def create_directories(cls, v):
        os.makedirs(os.path.dirname(v) if os.path.dirname(v) else v, exist_ok=True)
        return v

@lru_cache()
def get_settings() -> Settings:
    """Get cached settings instance."""
    return Settings()
```

### src/config/database.py
```python
import os
from urllib.parse import urlparse

def get_database_url() -> str:
    """Get database URL from environment or construct from components."""
    if os.getenv('DATABASE_URL'):
        return os.getenv('DATABASE_URL')
    
    # Construct from individual components
    user = os.getenv('DB_USER', 'labor_user')
    password = os.getenv('DB_PASSWORD', 'password')
    host = os.getenv('DB_HOST', 'localhost')
    port = os.getenv('DB_PORT', '5432')
    name = os.getenv('DB_NAME', 'labor_observatory')
    
    return f"postgresql://{user}:{password}@{host}:{port}/{name}"

def get_database_config() -> dict:
    """Parse database URL into components."""
    url = get_database_url()
    parsed = urlparse(url)
    
    return {
        'host': parsed.hostname,
        'port': parsed.port or 5432,
        'user': parsed.username,
        'password': parsed.password,
        'database': parsed.path.lstrip('/')
    }
```

### src/config/logging_config.py
```python
import logging
import logging.handlers
import os
from datetime import datetime

def setup_logging(log_level: str = 'INFO', log_file: str = None):
    """Configure logging for the entire application."""
    
    # Create logs directory if needed
    if log_file:
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    # Create formatters
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
    )
    console_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )
    
    # Get root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, log_level.upper()))
    
    # Remove existing handlers
    root_logger.handlers.clear()
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)
    
    # File handler with rotation
    if log_file:
        file_handler = logging.handlers.RotatingFileHandler(
            log_file,
            maxBytes=10 * 1024 * 1024,  # 10MB
            backupCount=5
        )
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(file_formatter)
        root_logger.addHandler(file_handler)
    
    # Specific loggers configuration
    logging.getLogger('scrapy').setLevel(logging.WARNING)
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('transformers').setLevel(logging.WARNING)
    
    return root_logger
```

### config/esco_config.yaml
```yaml
# ESCO Configuration for Spanish Language
esco:
  version: "1.1.0"
  base_url: "https://ec.europa.eu/esco/api"
  
  # Endpoints
  endpoints:
    skills: "/resource/skill"
    occupations: "/resource/occupation"
    search: "/search"
  
  # Spanish language configuration
  language:
    primary: "es"
    fallback: "en"
  
  # Skill types to extract
  skill_types:
    - "skill/competence"
    - "knowledge"
    - "skill"
  
  # Search parameters
  search:
    limit: 100
    fields:
      - "preferredLabel"
      - "altLabels"
      - "description"
      - "broaderConcept"
    
  # Mapping rules for common tech terms
  tech_mappings:
    # Programming languages
    "python": "http://data.europa.eu/esco/skill/3897c3c6-556b-4b8a-bfeb-234b0f716950"
    "java": "http://data.europa.eu/esco/skill/b4b36f5a-d5e6-4d78-b4ed-5b0b0e4f7a8a"
    "javascript": "http://data.europa.eu/esco/skill/7c7c5c49-0122-4b2e-8f6e-4e6b3f3e5d5f"
    "react": "http://data.europa.eu/esco/skill/react-framework"
    "node.js": "http://data.europa.eu/esco/skill/nodejs-runtime"
    
    # Databases
    "sql": "http://data.europa.eu/esco/skill/sql-language"
    "mysql": "http://data.europa.eu/esco/skill/mysql-database"
    "postgresql": "http://data.europa.eu/esco/skill/postgresql-database"
    "mongodb": "http://data.europa.eu/esco/skill/mongodb-database"
    
    # Cloud platforms
    "aws": "http://data.europa.eu/esco/skill/amazon-web-services"
    "azure": "http://data.europa.eu/esco/skill/microsoft-azure"
    "gcp": "http://data.europa.eu/esco/skill/google-cloud-platform"
    
    # DevOps
    "docker": "http://data.europa.eu/esco/skill/docker-containerization"
    "kubernetes": "http://data.europa.eu/esco/skill/kubernetes-orchestration"
    "git": "http://data.europa.eu/esco/skill/git-version-control"
    
    # Soft skills (Spanish)
    "trabajo en equipo": "http://data.europa.eu/esco/skill/teamwork"
    "comunicación": "http://data.europa.eu/esco/skill/communication"
    "liderazgo": "http://data.europa.eu/esco/skill/leadership"
    "resolución de problemas": "http://data.europa.eu/esco/skill/problem-solving"
```

---

## 4. Scraper Module Files

### src/scraper/__init__.py
```python
from .spiders.computrabajo_spider import ComputrabajoSpider
from .spiders.bumeran_spider import BumeranSpider
from .spiders.elempleo_spider import ElempleoSpider

__all__ = ['ComputrabajoSpider', 'BumeranSpider', 'ElempleoSpider']
```

### src/scraper/scrapy.cfg
```ini
[settings]
default = scraper.settings

[deploy]
project = labor_observatory_scraper
```

### src/scraper/items.py
```python
import scrapy
from scrapy.item import Field
from datetime import datetime
import re

class JobItem(scrapy.Item):
    # Required fields
    portal = Field()
    country = Field()
    url = Field()
    title = Field()
    description = Field()
    
    # Optional fields
    company = Field()
    location = Field()
    requirements = Field()
    salary_raw = Field()
    contract_type = Field()
    remote_type = Field()
    posted_date = Field()
    raw_html = Field()
    
    # Metadata
    scraped_at = Field()
    
    def clean_text(self, text):
        """Clean and normalize text fields."""
        if not text:
            return None
        
        # Remove extra whitespace
        text = ' '.join(text.split())
        
        # Remove HTML entities
        text = re.sub(r'&[a-zA-Z]+;', ' ', text)
        
        return text.strip()
    
    def __setitem__(self, key, value):
        # Clean text fields
        if key in ['title', 'description', 'requirements', 'company', 'location'] and value:
            value = self.clean_text(value)
        
        # Set scraped_at automatically
        if key == 'scraped_at':
            value = datetime.now()
        
        super().__setitem__(key, value)
```

### src/scraper/pipelines.py
```python
import logging
from datetime import datetime
from typing import Optional
import re
from scrapy.exceptions import DropItem
from database.operations import DatabaseOperations

logger = logging.getLogger(__name__)

class ValidationPipeline:
    """Validate scraped items."""
    
    def process_item(self, item, spider):
        # Check required fields
        required_fields = ['portal', 'country', 'url', 'title', 'description']
        
        for field in required_fields:
            if not item.get(field):
                raise DropItem(f"Missing required field: {field}")
        
        # Validate country code
        if item['country'] not in ['CO', 'MX', 'AR']:
            raise DropItem(f"Invalid country code: {item['country']}")
        
        # Validate portal
        if item['portal'] not in ['computrabajo', 'bumeran', 'elempleo']:
            raise DropItem(f"Invalid portal: {item['portal']}")
        
        # Ensure minimum description length
        if len(item['description']) < 50:
            raise DropItem("Description too short")
        
        return item

class NormalizationPipeline:
    """Normalize item fields."""
    
    def process_item(self, item, spider):
        # Normalize contract type
        if item.get('contract_type'):
            item['contract_type'] = self.normalize_contract_type(item['contract_type'])
        
        # Normalize remote type
        if item.get('remote_type'):
            item['remote_type'] = self.normalize_remote_type(item['remote_type'])
        
        # Parse posted date
        if item.get('posted_date'):
            item['posted_date'] = self.parse_date(item['posted_date'])
        
        # Set scraped_at
        item['scraped_at'] = datetime.now()
        
        return item
    
    def normalize_contract_type(self, contract: str) -> str:
        """Normalize contract type to standard values."""
        contract_lower = contract.lower()
        
        if any(term in contract_lower for term in ['tiempo completo', 'full time', 'completo']):
            return 'full_time'
        elif any(term in contract_lower for term in ['medio tiempo', 'part time', 'parcial']):
            return 'part_time'
        elif any(term in contract_lower for term in ['freelance', 'independiente', 'autonomo']):
            return 'freelance'
        elif any(term in contract_lower for term in ['contrato', 'temporal', 'proyecto']):
            return 'contract'
        elif any(term in contract_lower for term in ['pasantia', 'practica', 'internship']):
            return 'internship'
        else:
            return 'other'
    
    def normalize_remote_type(self, remote: str) -> str:
        """Normalize remote work type."""
        remote_lower = remote.lower()
        
        if any(term in remote_lower for term in ['remoto', 'remote', 'teletrabajo']):
            return 'remote'
        elif any(term in remote_lower for term in ['hibrido', 'hybrid', 'mixto']):
            return 'hybrid'
        elif any(term in remote_lower for term in ['presencial', 'oficina', 'on-site']):
            return 'on_site'
        else:
            return 'not_specified'
    
    def parse_date(self, date_str: str) -> Optional[datetime]:
        """Parse various date formats."""
        # Common Spanish date patterns
        patterns = [
            r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})',  # DD/MM/YYYY or DD-MM-YYYY
            r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})',  # YYYY/MM/DD or YYYY-MM-DD
            r'hace (\d+) días?',  # "hace X días"
            r'(\d+) días? atrás',  # "X días atrás"
            r'hoy',  # "hoy"
            r'ayer',  # "ayer"
        ]
        
        # Try to match patterns
        for pattern in patterns:
            match = re.search(pattern, date_str, re.IGNORECASE)
            if match:
                if 'hace' in pattern or 'atrás' in pattern:
                    days_ago = int(match.group(1))
                    return datetime.now() - timedelta(days=days_ago)
                elif pattern == r'hoy':
                    return datetime.now().date()
                elif pattern == r'ayer':
                    return datetime.now() - timedelta(days=1)
                else:
                    # Handle date formats
                    try:
                        if len(match.groups()) == 3:
                            if match.group(1).isdigit() and len(match.group(1)) == 4:
                                # YYYY/MM/DD format
                                return datetime(
                                    int(match.group(1)),
                                    int(match.group(2)),
                                    int(match.group(3))
                                ).date()
                            else:
                                # DD/MM/YYYY format
                                return datetime(
                                    int(match.group(3)),
                                    int(match.group(2)),
                                    int(match.group(1))
                                ).date()
                    except ValueError:
                        pass
        
        return None

class DatabasePipeline:
    """Save items to PostgreSQL database."""
    
    def __init__(self):
        self.db_ops = None
    
    def open_spider(self, spider):
        self.db_ops = DatabaseOperations()
        logger.info(f"Database pipeline opened for spider: {spider.name}")
    
    def process_item(self, item, spider):
        try:
            # Convert item to dict
            job_data = dict(item)
            
            # Remove metadata fields
            job_data.pop('scraped_at', None)
            
            # Insert into database
            job_id = self.db_ops.insert_job(job_data)
            
            if job_id:
                logger.info(f"Saved job {job_id}: {item['title']}")
            else:
                logger.warning(f"Duplicate job skipped: {item['url']}")
            
            return item
            
        except Exception as e:
            logger.error(f"Error saving job to database: {e}")
            raise
```

### src/scraper/settings.py
```python
import os
from config.settings import get_settings

settings = get_settings()

BOT_NAME = 'labor_observatory_scraper'

SPIDER_MODULES = ['scraper.spiders']
NEWSPIDER_MODULE = 'scraper.spiders'

# Obey robots.txt rules
ROBOTSTXT_OBEY = True

# Configure maximum concurrent requests
CONCURRENT_REQUESTS = settings.scraper_concurrent_requests
CONCURRENT_REQUESTS_PER_DOMAIN = 8

# Configure delay
DOWNLOAD_DELAY = settings.scraper_download_delay
RANDOMIZE_DOWNLOAD_DELAY = True

# Disable cookies
COOKIES_ENABLED = False

# User agent
USER_AGENT = settings.scraper_user_agent

# Override default headers
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'Cache-Control': 'no-cache',
    'Pragma': 'no-cache',
}

# Retry configuration
RETRY_TIMES = settings.scraper_retry_times
RETRY_HTTP_CODES = [500, 502, 503, 504, 408, 429]

# Configure pipelines
ITEM_PIPELINES = {
    'scraper.pipelines.ValidationPipeline': 100,
    'scraper.pipelines.NormalizationPipeline': 200,
    'scraper.pipelines.DatabasePipeline': 300,
}

# AutoThrottle extension
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 1
AUTOTHROTTLE_MAX_DELAY = 10
AUTOTHROTTLE_TARGET_CONCURRENCY = 8.0
AUTOTHROTTLE_DEBUG = False

# Memory usage
MEMUSAGE_ENABLED = True
MEMUSAGE_LIMIT_MB = 2048
MEMUSAGE_WARNING_MB = 1536

# Logging
LOG_LEVEL = 'INFO'
LOG_FORMAT = '%(levelname)s: %(message)s'

# Cache
HTTPCACHE_ENABLED = False

# Download timeout
DOWNLOAD_TIMEOUT = 30

# Telnet Console (disabled for production)
TELNETCONSOLE_ENABLED = False

# Middleware settings
DOWNLOADER_MIDDLEWARES = {
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,
    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110,
}
```

### src/scraper/middlewares.py
```python
import random
import logging
from scrapy.downloadermiddlewares.retry import RetryMiddleware
from scrapy.utils.response import response_status_message
from fake_useragent import UserAgent

logger = logging.getLogger(__name__)

class RotateUserAgentMiddleware:
    """Rotate user agents for each request."""
    
    def __init__(self):
        self.ua = UserAgent()
        self.user_agent_list = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
        ]
    
    def process_request(self, request, spider):
        try:
            # Try to use fake-useragent
            ua = self.ua.random
        except:
            # Fallback to predefined list
            ua = random.choice(self.user_agent_list)
        
        request.headers['User-Agent'] = ua + ' Academic Research Bot'

class CustomRetryMiddleware(RetryMiddleware):
    """Custom retry middleware with exponential backoff."""
    
    def process_response(self, request, response, spider):
        if response.status in self.retry_http_codes:
            reason = response_status_message(response.status)
            
            # Log retry attempt
            retry_times = request.meta.get('retry_times', 0) + 1
            logger.warning(
                f"Retrying {request.url} (attempt {retry_times}): {reason}"
            )
            
            # Exponential backoff
            request.meta['download_delay'] = 2 ** retry_times
            
            return self._retry(request, reason, spider) or response
        
        return response
```

### src/scraper/spiders/__init__.py
```python
# Spider modules initialization
```

### src/scraper/spiders/base_spider.py
```python
import scrapy
from abc import ABC, abstractmethod
import logging
from datetime import datetime
from urllib.parse import urljoin

logger = logging.getLogger(__name__)

class BaseJobSpider(scrapy.Spider, ABC):
    """Base spider class for job portals."""
    
    def __init__(self, country=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.country = country
        self.total_scraped = 0
        self.start_time = datetime.now()
    
    @abstractmethod
    def parse_job(self, response):
        """Parse individual job posting. Must be implemented by subclasses."""
        pass
    
    def extract_text(self, selector, xpath_or_css, method='xpath'):
        """Safely extract text from selector."""
        try:
            if method == 'xpath':
                texts = selector.xpath(xpath_or_css).getall()
            else:
                texts = selector.css(xpath_or_css).getall()
            
            # Join and clean text
            text = ' '.join(texts)
            return ' '.join(text.split()) if text else None
        except Exception as e:
            logger.error(f"Error extracting text: {e}")
            return None
    
    def build_absolute_url(self, response, relative_url):
        """Build absolute URL from relative URL."""
        return urljoin(response.url, relative_url)
    
    def log_progress(self):
        """Log scraping progress."""
        elapsed = (datetime.now() - self.start_time).total_seconds()
        rate = self.total_scraped / elapsed if elapsed > 0 else 0
        
        logger.info(
            f"Spider {self.name} - Country: {self.country} - "
            f"Scraped: {self.total_scraped} - Rate: {rate:.2f} jobs/sec"
        )
```

### src/scraper/spiders/computrabajo_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import re
from urllib.parse import urlencode
import logging

logger = logging.getLogger(__name__)

class ComputrabajoSpider(BaseJobSpider):
    name = 'computrabajo'
    allowed_domains = ['computrabajo.com', 'computrabajo.com.co', 
                      'computrabajo.com.mx', 'computrabajo.com.ar']
    
    # URL patterns by country
    country_urls = {
        'CO': 'https://www.computrabajo.com.co',
        'MX': 'https://www.computrabajo.com.mx',
        'AR': 'https://www.computrabajo.com.ar'
    }
    
    # Tech-related search terms
    tech_keywords = [
        'desarrollador', 'developer', 'programador', 'software',
        'data', 'analyst', 'engineer', 'fullstack', 'frontend',
        'backend', 'devops', 'cloud', 'mobile', 'web'
    ]
    
    def start_requests(self):
        if not self.country or self.country not in self.country_urls:
            raise ValueError(f"Invalid country: {self.country}")
        
        base_url = self.country_urls[self.country]
        
        # Generate search URLs for tech keywords
        for keyword in self.tech_keywords:
            search_url = f"{base_url}/trabajo-de-{keyword}"
            yield Request(
                url=search_url,
                callback=self.parse_search_results,
                meta={'keyword': keyword, 'page': 1}
            )
    
    def parse_search_results(self, response):
        """Parse search results page."""
        # Extract job listings
        job_cards = response.css('article.js-o-card')
        
        for card in job_cards:
            # Extract job URL
            job_url = card.css('a.js-o-card__link::attr(href)').get()
            if job_url:
                absolute_url = self.build_absolute_url(response, job_url)
                yield Request(
                    url=absolute_url,
                    callback=self.parse_job,
                    meta={'search_keyword': response.meta.get('keyword')}
                )
        
        # Check for next page
        current_page = response.meta.get('page', 1)
        next_page_link = response.css('a[aria-label="Siguiente"]::attr(href)').get()
        
        if next_page_link and current_page < 10:  # Limit to 10 pages per keyword
            next_url = self.build_absolute_url(response, next_page_link)
            yield Request(
                url=next_url,
                callback=self.parse_search_results,
                meta={
                    'keyword': response.meta.get('keyword'),
                    'page': current_page + 1
                }
            )
    
    def parse_job(self, response):
        """Parse individual job posting."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'computrabajo'
        item['country'] = self.country
        item['url'] = response.url
        
        # Title
        item['title'] = self.extract_text(
            response, 
            '//h1[@class="fwB fs24"]//text()',
            'xpath'
        )
        
        # Company
        item['company'] = self.extract_text(
            response,
            '//a[@class="dIB fs16 js-o-link"]//text()',
            'xpath'
        )
        
        # Location
        location_parts = response.xpath(
            '//div[@class="fs16 fc_base mt5"]//span//text()'
        ).getall()
        item['location'] = ', '.join(location_parts) if location_parts else None
        
        # Description and requirements
        description_sections = response.xpath(
            '//div[@class="mbB"]//p//text() | //div[@class="mbB"]//li//text()'
        ).getall()
        
        full_text = ' '.join(description_sections)
        
        # Try to separate requirements
        req_pattern = r'(?:requisitos|requerimientos|requirements|experiencia|conocimientos):(.*?)(?:funciones|responsabilidades|beneficios|$)'
        req_match = re.search(req_pattern, full_text, re.IGNORECASE | re.DOTALL)
        
        if req_match:
            item['requirements'] = req_match.group(1).strip()
            item['description'] = full_text.replace(req_match.group(0), '').strip()
        else:
            item['description'] = full_text
            item['requirements'] = None
        
        # Salary
        salary_text = self.extract_text(
            response,
            '//span[@class="fs16 fc_aux"]//text()[contains(., "$")]',
            'xpath'
        )
        item['salary_raw'] = salary_text
        
        # Contract type
        contract_info = response.xpath(
            '//span[@class="fs13 fc_aux"]//text()'
        ).getall()
        
        for info in contract_info:
            info_lower = info.lower()
            if any(term in info_lower for term in ['tiempo completo', 'full time', 'part time']):
                item['contract_type'] = info
            elif any(term in info_lower for term in ['remoto', 'presencial', 'hibrido']):
                item['remote_type'] = info
        
        # Posted date
        date_text = self.extract_text(
            response,
            '//span[@class="fs13 fc_aux"][contains(text(), "Publicado")]//text()',
            'xpath'
        )
        if date_text:
            item['posted_date'] = date_text.replace('Publicado', '').strip()
        
        # Raw HTML (for debugging/reprocessing)
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

### src/scraper/spiders/bumeran_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import json
import re
import logging

logger = logging.getLogger(__name__)

class BumeranSpider(BaseJobSpider):
    name = 'bumeran'
    allowed_domains = ['bumeran.com', 'bumeran.com.mx', 'bumeran.com.ar']
    
    # URL patterns by country
    country_urls = {
        'MX': 'https://www.bumeran.com.mx',
        'AR': 'https://www.bumeran.com.ar'
    }
    
    # Tech categories
    tech_categories = [
        'informatica-telecomunicaciones',
        'tecnologia-sistemas',
        'desarrollo-programacion'
    ]
    
    def start_requests(self):
        if self.country not in self.country_urls:
            raise ValueError(f"Bumeran not available for country: {self.country}")
        
        base_url = self.country_urls[self.country]
        
        # Generate category URLs
        for category in self.tech_categories:
            category_url = f"{base_url}/empleos-{category}.html"
            yield Request(
                url=category_url,
                callback=self.parse_category,
                meta={'category': category, 'page': 1}
            )
    
    def parse_category(self, response):
        """Parse category listing page."""
        # Check if page uses React/JSON data
        scripts = response.xpath('//script[contains(text(), "__INITIAL_STATE__")]/text()').getall()
        
        if scripts:
            # Extract JSON data from script
            for script in scripts:
                match = re.search(r'__INITIAL_STATE__\s*=\s*({.*?});', script, re.DOTALL)
                if match:
                    try:
                        data = json.loads(match.group(1))
                        jobs = self.extract_jobs_from_json(data)
                        
                        for job in jobs:
                            yield Request(
                                url=job['url'],
                                callback=self.parse_job,
                                meta={'job_data': job}
                            )
                    except json.JSONDecodeError:
                        logger.error("Failed to parse JSON data")
        else:
            # Fallback to HTML parsing
            job_links = response.css('div.Card__CardContentWrapper a::attr(href)').getall()
            
            for link in job_links:
                absolute_url = self.build_absolute_url(response, link)
                yield Request(url=absolute_url, callback=self.parse_job)
        
        # Handle pagination
        current_page = response.meta.get('page', 1)
        if current_page < 10:  # Limit pages
            next_page = current_page + 1
            next_url = response.url.replace(
                f'.html', 
                f'-pagina-{next_page}.html'
            )
            yield Request(
                url=next_url,
                callback=self.parse_category,
                meta={
                    'category': response.meta.get('category'),
                    'page': next_page
                }
            )
    
    def extract_jobs_from_json(self, data):
        """Extract job data from JSON structure."""
        jobs = []
        
        # Navigate through possible JSON structures
        try:
            if 'results' in data:
                job_list = data['results'].get('jobs', [])
            elif 'jobs' in data:
                job_list = data['jobs']
            else:
                return jobs
            
            for job in job_list:
                job_info = {
                    'url': job.get('url', ''),
                    'title': job.get('title', ''),
                    'company': job.get('company', {}).get('name', ''),
                    'location': job.get('location', '')
                }
                if job_info['url']:
                    jobs.append(job_info)
        except Exception as e:
            logger.error(f"Error extracting jobs from JSON: {e}")
        
        return jobs
    
    def parse_job(self, response):
        """Parse individual job posting."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'bumeran'
        item['country'] = self.country
        item['url'] = response.url
        
        # Try to get pre-parsed data
        job_data = response.meta.get('job_data', {})
        
        # Title
        item['title'] = job_data.get('title') or self.extract_text(
            response,
            'h1[class*="Title"]::text',
            'css'
        )
        
        # Company
        item['company'] = job_data.get('company') or self.extract_text(
            response,
            'h2[class*="Company"]::text',
            'css'
        )
        
        # Location
        item['location'] = job_data.get('location') or self.extract_text(
            response,
            'span[class*="Location"]::text',
            'css'
        )
        
        # Description
        description_selectors = [
            'div[class*="Description"]',
            'div.detalle-aviso',
            'div#description'
        ]
        
        for selector in description_selectors:
            desc_elements = response.css(f'{selector} ::text').getall()
            if desc_elements:
                item['description'] = ' '.join(desc_elements)
                break
        
        # Requirements - often within description
        if item.get('description'):
            req_pattern = r'(?:requisitos|requerimientos|experiencia|competencias):(.*?)(?:beneficios|funciones|responsabilidades|$)'
            req_match = re.search(req_pattern, item['description'], re.IGNORECASE | re.DOTALL)
            
            if req_match:
                item['requirements'] = req_match.group(1).strip()
        
        # Salary
        salary_selectors = [
            'span[class*="Salary"]::text',
            'div[class*="salary"]::text',
            'span:contains("$")::text'
        ]
        
        for selector in salary_selectors:
            salary = response.css(selector).get()
            if salary and '$' in salary:
                item['salary_raw'] = salary
                break
        
        # Contract type and remote
        tags = response.css('span[class*="Tag"]::text').getall()
        for tag in tags:
            tag_lower = tag.lower()
            if any(term in tag_lower for term in ['tiempo completo', 'part time', 'freelance']):
                item['contract_type'] = tag
            elif any(term in tag_lower for term in ['remoto', 'presencial', 'hibrido']):
                item['remote_type'] = tag
        
        # Posted date
        date_text = response.css('span[class*="Date"]::text').get()
        if date_text:
            item['posted_date'] = date_text
        
        # Raw HTML
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

### src/scraper/spiders/elempleo_spider.py
```python
import scrapy
from scrapy import Request
from scraper.items import JobItem
from .base_spider import BaseJobSpider
import re
import logging
from urllib.parse import urljoin, urlparse, parse_qs

logger = logging.getLogger(__name__)

class ElempleoSpider(BaseJobSpider):
    name = 'elempleo'
    allowed_domains = ['elempleo.com']
    
    # Only available for Colombia
    country_urls = {
        'CO': 'https://www.elempleo.com'
    }
    
    # Tech-related categories in elempleo
    tech_categories = {
        'tecnologia': '1100',
        'sistemas': '1100',
        'informatica': '1100'
    }
    
    def start_requests(self):
        if self.country != 'CO':
            raise ValueError("elempleo.com is only available for Colombia (CO)")
        
        base_url = self.country_urls['CO']
        
        # Search URLs for technology jobs
        search_base = f"{base_url}/colombia/empleos"
        
        # Generate search requests
        for category_name, category_id in self.tech_categories.items():
            search_params = {
                'categoria': category_id,
                'pagina': 1
            }
            
            search_url = f"{search_base}?categoria={category_id}"
            
            yield Request(
                url=search_url,
                callback=self.parse_search_results,
                meta={
                    'category': category_name,
                    'page': 1
                }
            )
    
    def parse_search_results(self, response):
        """Parse search results from elempleo."""
        # Extract job cards
        job_cards = response.css('div.result-item')
        
        if not job_cards:
            # Try alternative selectors
            job_cards = response.css('article.js-offer')
        
        for card in job_cards:
            # Extract job URL
            job_link = card.css('a.js-offer-title::attr(href)').get()
            if not job_link:
                job_link = card.css('h2 a::attr(href)').get()
            
            if job_link:
                # Handle both relative and absolute URLs
                if not job_link.startswith('http'):
                    job_link = urljoin(response.url, job_link)
                
                # Extract basic info from card
                card_data = {
                    'title': card.css('h2 a::text').get(),
                    'company': card.css('span.info-company-name::text').get(),
                    'location': card.css('span.info-city::text').get()
                }
                
                yield Request(
                    url=job_link,
                    callback=self.parse_job,
                    meta={'card_data': card_data}
                )
        
        # Handle pagination
        current_page = response.meta.get('page', 1)
        next_page_link = response.css('a.js-btn-next::attr(href)').get()
        
        if not next_page_link:
            # Alternative pagination
            pagination_links = response.css('ul.pagination a::attr(href)').getall()
            for link in pagination_links:
                if f'pagina={current_page + 1}' in link:
                    next_page_link = link
                    break
        
        if next_page_link and current_page < 10:  # Limit to 10 pages
            next_url = urljoin(response.url, next_page_link)
            yield Request(
                url=next_url,
                callback=self.parse_search_results,
                meta={
                    'category': response.meta.get('category'),
                    'page': current_page + 1
                }
            )
    
    def parse_job(self, response):
        """Parse individual job posting from elempleo."""
        item = JobItem()
        
        # Basic fields
        item['portal'] = 'elempleo'
        item['country'] = 'CO'
        item['url'] = response.url
        
        # Get card data if available
        card_data = response.meta.get('card_data', {})
        
        # Title
        item['title'] = self.extract_text(
            response,
            'h1.offer-title::text',
            'css'
        ) or card_data.get('title')
        
        # Company
        item['company'] = self.extract_text(
            response,
            'div.company-name a::text',
            'css'
        ) or self.extract_text(
            response,
            'span.offer-company::text',
            'css'
        ) or card_data.get('company')
        
        # Location
        location_parts = response.css('div.offer-location span::text').getall()
        if location_parts:
            item['location'] = ', '.join(location_parts)
        else:
            item['location'] = card_data.get('location')
        
        # Main content sections
        content_sections = response.css('div.offer-description')
        
        # Description
        description_html = content_sections.css('div#description').get()
        if description_html:
            # Clean HTML and extract text
            desc_text = re.sub(r'<[^>]+>', ' ', description_html)
            item['description'] = ' '.join(desc_text.split())
        
        # Requirements
        requirements_section = content_sections.css('div#requirements')
        if requirements_section:
            req_items = requirements_section.css('li::text').getall()
            if req_items:
                item['requirements'] = ' '.join(req_items)
            else:
                req_text = requirements_section.css('::text').getall()
                item['requirements'] = ' '.join(req_text)
        
        # If requirements not in separate section, try to extract from description
        if not item.get('requirements') and item.get('description'):
            req_pattern = r'(?:requisitos|perfil|experiencia|conocimientos):(.*?)(?:funciones|responsabilidades|ofrecemos|$)'
            req_match = re.search(req_pattern, item['description'], re.IGNORECASE | re.DOTALL)
            if req_match:
                item['requirements'] = req_match.group(1).strip()
        
        # Salary
        salary_element = response.css('span.offer-salary::text').get()
        if salary_element:
            item['salary_raw'] = salary_element
        else:
            # Look for salary in description
            salary_pattern = r'\$[\d.,]+ (?:millones|COP|pesos)'
            salary_match = re.search(salary_pattern, item.get('description', ''))
            if salary_match:
                item['salary_raw'] = salary_match.group(0)
        
        # Contract type
        contract_info = response.css('div.offer-info span::text').getall()
        for info in contract_info:
            info_lower = info.lower()
            if any(term in info_lower for term in ['contrato', 'tiempo completo', 'medio tiempo']):
                item['contract_type'] = info
            elif any(term in info_lower for term in ['remoto', 'presencial', 'teletrabajo']):
                item['remote_type'] = info
        
        # Posted date
        date_element = response.css('span.offer-date::text').get()
        if date_element:
            item['posted_date'] = date_element
        else:
            # Try to extract from meta tags
            date_meta = response.css('meta[property="article:published_time"]::attr(content)').get()
            if date_meta:
                item['posted_date'] = date_meta.split('T')[0]
        
        # Additional fields from structured data
        try:
            # Check for JSON-LD structured data
            json_ld = response.css('script[type="application/ld+json"]::text').get()
            if json_ld:
                import json
                data = json.loads(json_ld)
                
                # Extract additional info if available
                if isinstance(data, dict):
                    if 'title' in data and not item.get('title'):
                        item['title'] = data['title']
                    if 'hiringOrganization' in data and not item.get('company'):
                        item['company'] = data['hiringOrganization'].get('name')
                    if 'jobLocation' in data and not item.get('location'):
                        location = data['jobLocation']
                        if isinstance(location, dict):
                            item['location'] = location.get('address', {}).get('addressLocality')
        except Exception as e:
            logger.debug(f"Could not parse structured data: {e}")
        
        # Raw HTML
        item['raw_html'] = response.text
        
        # Update progress
        self.total_scraped += 1
        if self.total_scraped % 100 == 0:
            self.log_progress()
        
        yield item
```

---

## 5. Extractor Module Files

### src/extractor/__init__.py
```python
from .pipeline import ExtractionPipeline
from .ner_extractor import NERExtractor
from .regex_patterns import RegexExtractor
from .esco_matcher import ESCOMatcher

__all__ = ['ExtractionPipeline', 'NERExtractor', 'RegexExtractor', 'ESCOMatcher']
```

### src/extractor/ner_extractor.py
```python
import spacy
from spacy.tokens import Doc, Span
from typing import List, Dict, Tuple, Optional
import logging
import os
from config.settings import get_settings

logger = logging.getLogger(__name__)

class NERExtractor:
    """Extract skills using Named Entity Recognition."""
    
    def __init__(self, model_path: Optional[str] = None):
        self.settings = get_settings()
        
        # Load spaCy model
        if model_path and os.path.exists(model_path):
            self.nlp = spacy.load(model_path)
            logger.info(f"Loaded custom NER model from {model_path}")
        else:
            # Load default Spanish model
            try:
                self.nlp = spacy.load("es_core_news_lg")
                logger.info("Loaded default Spanish model")
            except:
                logger.warning("Spanish model not found, downloading...")
                os.system("python -m spacy download es_core_news_lg")
                self.nlp = spacy.load("es_core_news_lg")
        
        # Add custom pipeline components
        self._add_tech_entity_ruler()
    
    def _add_tech_entity_ruler(self):
        """Add rule-based entity recognition for tech terms."""
        ruler = self.nlp.add_pipe("entity_ruler", before="ner")
        
        # Define patterns for common tech skills
        patterns = [
            # Programming languages
            {"label": "SKILL", "pattern": [{"LOWER": "python"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "java"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "javascript"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "typescript"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "c++"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "c#"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "php"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "ruby"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "go"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "golang"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "rust"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "kotlin"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "swift"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "r"}]},
            
            # Frameworks
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "react"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "react"}, {"LOWER": "native"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "angular"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "vue"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "vue"}, {"LOWER": "js"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "django"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "flask"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "spring"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "spring"}, {"LOWER": "boot"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "node"}, {"LOWER": "js"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "nodejs"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "express"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": ".net"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "laravel"}]},
            {"label": "FRAMEWORK", "pattern": [{"LOWER": "rails"}]},
            
            # Databases
            {"label": "DATABASE", "pattern": [{"LOWER": "mysql"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "postgresql"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "postgres"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "mongodb"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "redis"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "elasticsearch"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "oracle"}]},
            {"label": "DATABASE", "pattern": [{"LOWER": "sql"}, {"LOWER": "server"}]},
            
            # Cloud & DevOps
            {"label": "PLATFORM", "pattern": [{"LOWER": "aws"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "amazon"}, {"LOWER": "web"}, {"LOWER": "services"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "azure"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "gcp"}]},
            {"label": "PLATFORM", "pattern": [{"LOWER": "google"}, {"LOWER": "cloud"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "docker"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "kubernetes"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "jenkins"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "git"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "github"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "gitlab"}]},
            
            # Data & ML
            {"label": "SKILL", "pattern": [{"LOWER": "machine"}, {"LOWER": "learning"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "deep"}, {"LOWER": "learning"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "data"}, {"LOWER": "science"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "big"}, {"LOWER": "data"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "tensorflow"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "pytorch"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "scikit"}, {"LOWER": "learn"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "pandas"}]},
            {"label": "TOOL", "pattern": [{"LOWER": "numpy"}]},
            
            # Methodologies
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "agile"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "scrum"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "kanban"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "devops"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "ci"}, {"LOWER": "cd"}]},
            {"label": "METHODOLOGY", "pattern": [{"LOWER": "tdd"}]},
        ]
        
        # Add Spanish variations
        spanish_patterns = [
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "web"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "movil"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "desarrollo"}, {"LOWER": "móvil"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "base"}, {"LOWER": "de"}, {"LOWER": "datos"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "inteligencia"}, {"LOWER": "artificial"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "aprendizaje"}, {"LOWER": "automatico"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "aprendizaje"}, {"LOWER": "automático"}]},
            {"label": "SKILL", "pattern": [{"LOWER": "ciencia"}, {"LOWER": "de"}, {"LOWER": "datos"}]},
        ]
        
        ruler.add_patterns(patterns + spanish_patterns)
    
    def extract(self, text: str, source_section: str = "unknown") -> List[Dict[str, any]]:
        """Extract skills from text using NER.
        
        Args:
            text: Text to extract skills from
            source_section: Section of job posting (title, description, requirements)
            
        Returns:
            List of extracted skills with metadata
        """
        if not text:
            return []
        
        # Process text with spaCy
        doc = self.nlp(text)
        
        extracted_skills = []
        seen_skills = set()
        
        # Extract entities
        for ent in doc.ents:
            if ent.label_ in ["SKILL", "FRAMEWORK", "DATABASE", "PLATFORM", "TOOL", "METHODOLOGY"]:
                skill_text = ent.text.lower().strip()
                
                # Skip if already seen
                if skill_text in seen_skills:
                    continue
                
                seen_skills.add(skill_text)
                
                extracted_skills.append({
                    "skill_text": skill_text,
                    "skill_type": "explicit",
                    "extraction_method": "ner",
                    "entity_label": ent.label_,
                    "confidence_score": 0.9,  # High confidence for NER
                    "source_section": source_section,
                    "span_start": ent.start_char,
                    "span_end": ent.end_char,
                    "context": text[max(0, ent.start_char-50):ent.end_char+50]
                })
        
        logger.debug(f"NER extracted {len(extracted_skills)} skills from {source_section}")
        
        return extracted_skills
    
    def extract_from_job(self, job_data: Dict[str, str]) -> List[Dict[str, any]]:
        """Extract skills from all sections of a job posting.
        
        Args:
            job_data: Dictionary with job fields (title, description, requirements)
            
        Returns:
            List of all extracted skills
        """
        all_skills = []
        
        # Extract from title
        if job_data.get('title'):
            title_skills = self.extract(job_data['title'], 'title')
            all_skills.extend(title_skills)
        
        # Extract from description
        if job_data.get('description'):
            desc_skills = self.extract(job_data['description'], 'description')
            all_skills.extend(desc_skills)
        
        # Extract from requirements
        if job_data.get('requirements'):
            req_skills = self.extract(job_data['requirements'], 'requirements')
            all_skills.extend(req_skills)
        
        return all_skills
```

### src/extractor/regex_patterns.py
```python
import re
from typing import List, Dict, Tuple
import logging

logger = logging.getLogger(__name__)

class RegexExtractor:
    """Extract skills using regular expressions."""
    
    def __init__(self):
        # Define regex patterns for skill extraction
        self.patterns = self._build_patterns()
    
    def _build_patterns(self) -> List[Tuple[str, re.Pattern, str]]:
        """Build regex patterns for skill extraction.
        
        Returns:
            List of tuples (pattern_name, compiled_regex, skill_type)
        """
        patterns = []
        
        # Experience patterns in Spanish
        experience_patterns = [
            (
                "experiencia_en",
                re.compile(
                    r"experiencia\s+(?:en|con)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "conocimientos_de",
                re.compile(
                    r"conocimientos?\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "manejo_de",
                re.compile(
                    r"manejo\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "tool"
            ),
            (
                "dominio_de",
                re.compile(
                    r"dominio\s+(?:de|en)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "desarrollo_en",
                re.compile(
                    r"desarrollo\s+(?:en|con)\s+([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+y\s+|\s+o\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
        ]
        
        # Required skills patterns
        required_patterns = [
            (
                "requerimos",
                re.compile(
                    r"(?:requerimos|buscamos|necesitamos)\s+(?:personas?\s+con\s+)?(?:conocimientos?\s+en\s+)?([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|\s+para\s+|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
            (
                "indispensable",
                re.compile(
                    r"(?:indispensable|fundamental|esencial)\s+(?:contar\s+con\s+)?(?:conocimientos?\s+en\s+)?([A-Za-z0-9\+\#\.\s,/-]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "skill"
            ),
        ]
        
        # Technology stack patterns
        tech_stack_patterns = [
            (
                "tecnologias",
                re.compile(
                    r"(?:tecnologías?|herramientas?|lenguajes?)\s*:\s*([A-Za-z0-9\+\#\.\s,/\-\(\)]+?)(?:\n|$)",
                    re.IGNORECASE
                ),
                "mixed"
            ),
            (
                "stack_tecnologico",
                re.compile(
                    r"stack\s+(?:tecnológico|tech)\s*:\s*([A-Za-z0-9\+\#\.\s,/\-\(\)]+?)(?:\n|$)",
                    re.IGNORECASE
                ),
                "mixed"
            ),
        ]
        
        # List patterns (bullet points, numbered lists)
        list_patterns = [
            (
                "bullet_skills",
                re.compile(
                    r"(?:^|\n)\s*[\-\*\•]\s*([A-Za-z0-9\+\#\.\s]+?)(?:\n|$)",
                    re.MULTILINE
                ),
                "skill"
            ),
            (
                "numbered_skills",
                re.compile(
                    r"(?:^|\n)\s*\d+[\.\)]\s*([A-Za-z0-9\+\#\.\s]+?)(?:\n|$)",
                    re.MULTILINE
                ),
                "skill"
            ),
        ]
        
        # Certification patterns
        cert_patterns = [
            (
                "certificacion",
                re.compile(
                    r"(?:certificación|certificado)\s+(?:en\s+)?([A-Za-z0-9\+\#\.\s\-]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "certification"
            ),
        ]
        
        # Years of experience patterns
        experience_years_patterns = [
            (
                "años_experiencia",
                re.compile(
                    r"(\d+)\s*\+?\s*años?\s+(?:de\s+)?experiencia\s+(?:en\s+)?([A-Za-z0-9\+\#\.\s]+?)(?:\s*[,\.]|$)",
                    re.IGNORECASE
                ),
                "skill_with_years"
            ),
        ]
        
        # Combine all patterns
        patterns.extend(experience_patterns)
        patterns.extend(required_patterns)
        patterns.extend(tech_stack_patterns)
        patterns.extend(list_patterns)
        patterns.extend(cert_patterns)
        patterns.extend(experience_years_patterns)
        
        return patterns
    
    def extract(self, text: str, source_section: str = "unknown") -> List[Dict[str, any]]:
        """Extract skills using regex patterns.
        
        Args:
            text: Text to extract skills from
            source_section: Section of job posting
            
        Returns:
            List of extracted skills with metadata
        """
        if not text:
            return []
        
        extracted_skills = []
        seen_skills = set()
        
        for pattern_name, regex, skill_type in self.patterns:
            matches = regex.finditer(text)
            
            for match in matches:
                if skill_type == "skill_with_years":
                    # Special handling for years of experience
                    years = match.group(1)
                    skill_text = match.group(2).strip().lower()
                    
                    if skill_text and skill_text not in seen_skills:
                        seen_skills.add(skill_text)
                        extracted_skills.append({
                            "skill_text": skill_text,
                            "skill_type": "explicit",
                            "extraction_method": "regex",
                            "pattern_name": pattern_name,
                            "confidence_score": 0.8,
                            "source_section": source_section,
                            "span_start": match.start(2),
                            "span_end": match.end(2),
                            "years_required": int(years),
                            "context": text[max(0, match.start()-30):match.end()+30]
                        })
                else:
                    # Normal skill extraction
                    skill_text = match.group(1).strip().lower()
                    
                    # Clean up extracted text
                    skill_text = self._clean_skill_text(skill_text)
                    
                    if skill_text and len(skill_text) > 1 and skill_text not in seen_skills:
                        # Additional validation
                        if self._is_valid_skill(skill_text):
                            seen_skills.add(skill_text)
                            
                            extracted_skills.append({
                                "skill_text": skill_text,
                                "skill_type": "explicit",
                                "extraction_method": "regex",
                                "pattern_name": pattern_name,
                                "confidence_score": 0.7,  # Lower than NER
                                "source_section": source_section,
                                "span_start": match.start(1),
                                "span_end": match.end(1),
                                "context": text[max(0, match.start()-30):match.end()+30]
                            })
        
        # Handle comma-separated lists within extracted skills
        expanded_skills = []
        for skill in extracted_skills:
            if ',' in skill['skill_text'] or ' y ' in skill['skill_text']:
                # Split and create individual skills
                parts = re.split(r'[,\s]+y\s+|,\s*', skill['skill_text'])
                for part in parts:
                    part = part.strip()
                    if part and self._is_valid_skill(part):
                        new_skill = skill.copy()
                        new_skill['skill_text'] = part
                        expanded_skills.append(new_skill)
            else:
                expanded_skills.append(skill)
        
        logger.debug(f"Regex extracted {len(expanded_skills)} skills from {source_section}")
        
        return expanded_skills
    
    def _clean_skill_text(self, text: str) -> str:
        """Clean extracted skill text.
        
        Args:
            text: Raw extracted text
            
        Returns:
            Cleaned skill text
        """
        # Remove common stop words at the beginning/end
        stop_words = [
            'el', 'la', 'los', 'las', 'un', 'una', 'de', 'del', 'al',
            'y', 'o', 'con', 'para', 'por', 'en', 'a'
        ]
        
        words = text.split()
        
        # Remove stop words from beginning
        while words and words[0].lower() in stop_words:
            words.pop(0)
        
        # Remove stop words from end
        while words and words[-1].lower() in stop_words:
            words.pop()
        
        cleaned = ' '.join(words)
        
        # Remove extra spaces and punctuation
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = re.sub(r'[^\w\s\+\#\.\-/]', '', cleaned)
        
        return cleaned.strip()
    
    def _is_valid_skill(self, skill_text: str) -> bool:
        """Validate if extracted text is likely a valid skill.
        
        Args:
            skill_text: Text to validate
            
        Returns:
            True if valid skill, False otherwise
        """
        # Check minimum length
        if len(skill_text) < 2:
            return False
        
        # Check if it's just numbers
        if skill_text.isdigit():
            return False
        
        # Check against blacklist of common false positives
        blacklist = [
            'años', 'año', 'experiencia', 'conocimiento', 'manejo',
            'desarrollo', 'persona', 'profesional', 'trabajo',
            'empresa', 'cliente', 'proyecto', 'equipo', 'area',
            'sistemas', 'tecnologia', 'informatica'  # Too generic
        ]
        
        if skill_text.lower() in blacklist:
            return False
        
        # Must contain at least one letter
        if not any(c.isalpha() for c in skill_text):
            return False
        
        return True
    
    def extract_from_job(self, job_data: Dict[str, str]) -> List[Dict[str, any]]:
        """Extract skills from all sections of a job posting.
        
        Args:
            job_data: Dictionary with job fields
            
        Returns:
            List of all extracted skills
        """
        all_skills = []
        
        # Process each section
        for section in ['title', 'description', 'requirements']:
            if job_data.get(section):
                section_skills = self.extract(job_data[section], section)
                all_skills.extend(section_skills)
        
        return all_skills
```

### src/extractor/esco_matcher.py
```python
import json
import logging
from typing import List, Dict, Optional, Set
from fuzzywuzzy import fuzz, process
import requests
from pathlib import Path
import yaml

logger = logging.getLogger(__name__)

class ESCOMatcher:
    """Match extracted skills to ESCO taxonomy."""
    
    def __init__(self, config_path: str = "config/esco_config.yaml"):
        self.config = self._load_config(config_path)
        self.skills_cache = {}
        self.local_mappings = self.config.get('tech_mappings', {})
        
        # Load local ESCO data if available
        self.local_esco_data = self._load_local_esco_data()
    
    def _load_config(self, config_path: str) -> dict:
        """Load ESCO configuration."""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config['esco']
        except Exception as e:
            logger.error(f"Failed to load ESCO config: {e}")
            return {}
    
    def _load_local_esco_data(self) -> Dict[str, Dict]:
        """Load local ESCO data files if available."""
        esco_data = {}
        
        # Try to load skills data
        skills_path = Path("data/esco/skills_es.csv")
        if skills_path.exists():
            try:
                import pandas as pd
                df = pd.read_csv(skills_path)
                
                for _, row in df.iterrows():
                    skill_uri = row.get('conceptUri', '')
                    esco_data[skill_uri] = {
                        'preferredLabel': row.get('preferredLabel', ''),
                        'altLabels': row.get('altLabels', '').split('|') if row.get('altLabels') else [],
                        'description': row.get('description', ''),
                        'skillType': row.get('skillType', '')
                    }
                
                logger.info(f"Loaded {len(esco_data)} ESCO skills from local file")
            except Exception as e:
                logger.error(f"Failed to load local ESCO data: {e}")
        
        return esco_data
    
    def match_skill(self, skill_text: str, threshold: float = 0.8) -> Optional[Dict[str, any]]:
        """Match a skill to ESCO taxonomy.
        
        Args:
            skill_text: Skill text to match
            threshold: Minimum similarity threshold (0-1)
            
        Returns:
            ESCO match data or None
        """
        skill_lower = skill_text.lower().strip()
        
        # Check direct mappings first
        if skill_lower in self.local_mappings:
            esco_uri = self.local_mappings[skill_lower]
            
            # Get details from local data or cache
            if esco_uri in self.local_esco_data:
                return {
                    'esco_uri': esco_uri,
                    'esco_preferred_label': self.local_esco_data[esco_uri]['preferredLabel'],
                    'match_type': 'direct',
                    'match_score': 1.0
                }
        
        # Try fuzzy matching against local data
        if self.local_esco_data:
            best_match = self._fuzzy_match_local(skill_text, threshold)
            if best_match:
                return best_match
        
        # Try API lookup (if configured and no local match)
        if self.config.get('base_url'):
            api_match = self._api_lookup(skill_text)
            if api_match:
                return api_match
        
        return None
    
    def _fuzzy_match_local(self, skill_text: str, threshold: float) -> Optional[Dict[str, any]]:
        """Fuzzy match against local ESCO data.
        
        Args:
            skill_text: Skill to match
            threshold: Minimum score threshold
            
        Returns:
            Best match or None
        """
        # Collect all labels for matching
        all_labels = []
        for uri, data in self.local_esco_data.items():
            # Add preferred label
            all_labels.append((data['preferredLabel'].lower(), uri, 'preferred'))
            
            # Add alternative labels
            for alt_label in data.get('altLabels', []):
                if alt_label:
                    all_labels.append((alt_label.lower(), uri, 'alternative'))
        
        # Find best match
        if all_labels:
            # Use token sort ratio for better matching of multi-word skills
            matches = process.extract(
                skill_text.lower(),
                [label[0] for label in all_labels],
                scorer=fuzz.token_sort_ratio,
                limit=3
            )
            
            for match_text, score in matches:
                if score >= threshold * 100:  # fuzzywuzzy uses 0-100 scale
                    # Find the corresponding URI
                    for label_text, uri, label_type in all_labels:
                        if label_text == match_text:
                            return {
                                'esco_uri': uri,
                                'esco_preferred_label': self.local_esco_data[uri]['preferredLabel'],
                                'match_type': f'fuzzy_{label_type}',
                                'match_score': score / 100.0,
                                'matched_text': match_text
                            }
        
        return None
    
    def _api_lookup(self, skill_text: str) -> Optional[Dict[str, any]]:
        """Look up skill using ESCO API.
        
        Args:
            skill_text: Skill to look up
            
        Returns:
            API match data or None
        """
        try:
            # Check cache first
            if skill_text in self.skills_cache:
                return self.skills_cache[skill_text]
            
            # Prepare API request
            api_url = f"{self.config['base_url']}{self.config['endpoints']['search']}"
            
            params = {
                'text': skill_text,
                'language': self.config['language']['primary'],
                'type': 'skill',
                'limit': 5
            }
            
            headers = {
                'Accept': 'application/json',
                'Accept-Language': self.config['language']['primary']
            }
            
            # Make request
            response = requests.get(api_url, params=params, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                if data.get('results'):
                    # Get first result
                    result = data['results'][0]
                    
                    match_data = {
                        'esco_uri': result.get('uri', ''),
                        'esco_preferred_label': result.get('preferredLabel', {}).get(
                            self.config['language']['primary'],
                            result.get('preferredLabel', {}).get('en', '')
                        ),
                        'match_type': 'api_search',
                        'match_score': result.get('score', 0.0)
                    }
                    
                    # Cache the result
                    self.skills_cache[skill_text] = match_data
                    
                    return match_data
            
        except Exception as e:
            logger.error(f"ESCO API lookup failed for '{skill_text}': {e}")
        
        return None
    
    def match_skills_batch(self, skills: List[str]) -> Dict[str, Optional[Dict]]:
        """Match multiple skills to ESCO taxonomy.
        
        Args:
            skills: List of skill texts
            
        Returns:
            Dictionary mapping skill text to ESCO match data
        """
        results = {}
        
        for skill in skills:
            match = self.match_skill(skill)
            results[skill] = match
        
        # Log statistics
        matched = sum(1 for v in results.values() if v is not None)
        logger.info(
            f"ESCO matching: {matched}/{len(skills)} skills matched "
            f"({matched/len(skills)*100:.1f}%)"
        )
        
        return results

### src/analyzer/dimension_reducer.py
```python
import logging
from typing import Tuple, Dict, Any, Optional
import numpy as np
from umap import UMAP
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import time

logger = logging.getLogger(__name__)

class DimensionReducer:
    """Reduce dimensionality of embeddings for visualization and clustering."""
    
    def __init__(self):
        self.reducers = {}
    
    def reduce_dimensions(self,
                         embeddings: np.ndarray,
                         method: str = 'umap',
                         n_components: int = 2,
                         **kwargs) -> Tuple[np.ndarray, Dict[str, Any]]:
        """Reduce dimensionality of embeddings.
        
        Args:
            embeddings: High-dimensional embeddings
            method: Reduction method ('umap', 'pca', 'tsne')
            n_components: Number of output dimensions
            **kwargs: Additional parameters for the method
            
        Returns:
            Tuple of (reduced embeddings, metadata)
        """
        logger.info(
            f"Reducing {embeddings.shape} to {n_components}D using {method}"
        )
        
        start_time = time.time()
        
        if method == 'umap':
            reduced, reducer = self._reduce_umap(embeddings, n_components, **kwargs)
        elif method == 'pca':
            reduced, reducer = self._reduce_pca(embeddings, n_components, **kwargs)
        elif method == 'tsne':
            reduced, reducer = self._reduce_tsne(embeddings, n_components, **kwargs)
        else:
            raise ValueError(f"Unknown reduction method: {method}")
        
        processing_time = time.time() - start_time
        
        # Store reducer for later use
        self.reducers[method] = reducer
        
        metadata = {
            'method': method,
            'n_components': n_components,
            'original_shape': embeddings.shape,
            'reduced_shape': reduced.shape,
            'processing_time': processing_time,
            'parameters': kwargs
        }
        
        logger.info(f"Dimension reduction complete in {processing_time:.2f}s")
        
        return reduced, metadata
    
    def _reduce_umap(self,
                    embeddings: np.ndarray,
                    n_components: int,
                    n_neighbors: int = 15,
                    min_dist: float = 0.1,
                    metric: str = 'cosine') -> Tuple[np.ndarray, UMAP]:
        """Reduce dimensions using UMAP.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            n_neighbors: UMAP n_neighbors parameter
            min_dist: UMAP min_dist parameter
            metric: Distance metric
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        reducer = UMAP(
            n_components=n_components,
            n_neighbors=n_neighbors,
            min_dist=min_dist,
            metric=metric,
            random_state=42,
            verbose=True
        )
        
        reduced = reducer.fit_transform(embeddings)
        
        return reduced, reducer
    
    def _reduce_pca(self,
                   embeddings: np.ndarray,
                   n_components: int) -> Tuple[np.ndarray, PCA]:
        """Reduce dimensions using PCA.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        reducer = PCA(n_components=n_components, random_state=42)
        reduced = reducer.fit_transform(embeddings)
        
        # Log explained variance
        if hasattr(reducer, 'explained_variance_ratio_'):
            total_variance = np.sum(reducer.explained_variance_ratio_)
            logger.info(f"PCA explained variance: {total_variance:.2%}")
        
        return reduced, reducer
    
    def _reduce_tsne(self,
                    embeddings: np.ndarray,
                    n_components: int,
                    perplexity: float = 30.0,
                    learning_rate: float = 200.0) -> Tuple[np.ndarray, TSNE]:
        """Reduce dimensions using t-SNE.
        
        Args:
            embeddings: Input embeddings
            n_components: Output dimensions
            perplexity: t-SNE perplexity parameter
            learning_rate: t-SNE learning rate
            
        Returns:
            Tuple of (reduced embeddings, reducer)
        """
        # For t-SNE, first reduce with PCA if dimensions > 50
        if embeddings.shape[1] > 50:
            logger.info("Pre-reducing with PCA for t-SNE")
            pca = PCA(n_components=50, random_state=42)
            embeddings = pca.fit_transform(embeddings)
        
        reducer = TSNE(
            n_components=n_components,
            perplexity=perplexity,
            learning_rate=learning_rate,
            random_state=42,
            verbose=1
        )
        
        reduced = reducer.fit_transform(embeddings)
        
        return reduced, reducer
    
    def transform_new_points(self,
                           embeddings: np.ndarray,
                           method: str = 'umap') -> np.ndarray:
        """Transform new points using existing reducer.
        
        Args:
            embeddings: New embeddings to transform
            method: Which reducer to use
            
        Returns:
            Transformed embeddings
        """
        if method not in self.reducers:
            raise ValueError(f"No {method} reducer available. Run reduce_dimensions first.")
        
        reducer = self.reducers[method]
        
        if method == 'tsne':
            logger.warning("t-SNE doesn't support transform. Returning original embeddings.")
            return embeddings
        
        return reducer.transform(embeddings)
    
    def create_embedding_map(self,
                           embeddings: np.ndarray,
                           labels: Optional[np.ndarray] = None,
                           skill_texts: Optional[list] = None) -> Dict[str, Any]:
        """Create a complete embedding map with 2D coordinates.
        
        Args:
            embeddings: Original embeddings
            labels: Cluster labels (optional)
            skill_texts: Skill names (optional)
            
        Returns:
            Dictionary with 2D coordinates and metadata
        """
        # Reduce to 2D
        coords_2d, metadata = self.reduce_dimensions(embeddings, method='umap', n_components=2)
        
        # Create map
        embedding_map = {
            'coordinates': coords_2d,
            'metadata': metadata
        }
        
        if labels is not None:
            embedding_map['labels'] = labels
        
        if skill_texts is not None:
            embedding_map['skills'] = skill_texts
        
        # Add statistics
        embedding_map['stats'] = {
            'x_range': (float(np.min(coords_2d[:, 0])), float(np.max(coords_2d[:, 0]))),
            'y_range': (float(np.min(coords_2d[:, 1])), float(np.max(coords_2d[:, 1]))),
            'center': (float(np.mean(coords_2d[:, 0])), float(np.mean(coords_2d[:, 1])))
        }
        
        return embedding_map

### src/analyzer/report_generator.py
```python
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import os
from pathlib import Path
from reportlab.lib import colors
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, PageBreak, Image
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from database.operations import DatabaseOperations

logger = logging.getLogger(__name__)

class ReportGenerator:
    """Generate PDF reports with analysis results."""
    
    def __init__(self, output_dir: str = None):
        self.output_dir = output_dir or "./outputs"
        os.makedirs(self.output_dir, exist_ok=True)
        self.db_ops = DatabaseOperations()
        self.styles = getSampleStyleSheet()
        self._add_custom_styles()
    
    def _add_custom_styles(self):
        """Add custom styles for the report."""
        self.styles.add(ParagraphStyle(
            name='CustomTitle',
            parent=self.styles['Title'],
            fontSize=24,
            textColor=colors.HexColor('#1a73e8'),
            spaceAfter=30
        ))
        
        self.styles.add(ParagraphStyle(
            name='SectionHeader',
            parent=self.styles['Heading1'],
            fontSize=16,
            textColor=colors.HexColor('#1a73e8'),
            spaceAfter=12
        ))
        
        self.styles.add(ParagraphStyle(
            name='SubsectionHeader',
            parent=self.styles['Heading2'],
            fontSize=14,
            textColor=colors.HexColor('#5f6368'),
            spaceAfter=10
        ))
    
    def generate_full_report(self, 
                           country: Optional[str] = None,
                           include_visualizations: bool = True) -> str:
        """Generate comprehensive analysis report.
        
        Args:
            country: Country code to filter by (optional)
            include_visualizations: Whether to include charts
            
        Returns:
            Path to generated report
        """
        # Create timestamp for filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        country_suffix = f"_{country}" if country else "_all"
        filename = f"labor_market_analysis{country_suffix}_{timestamp}.pdf"
        filepath = os.path.join(self.output_dir, filename)
        
        # Create document
        doc = SimpleDocTemplate(
            filepath,
            pagesize=A4,
            rightMargin=72,
            leftMargin=72,
            topMargin=72,
            bottomMargin=18
        )
        
        # Build content
        story = []
        
        # Title page
        story.extend(self._create_title_page(country))
        story.append(PageBreak())
        
        # Executive summary
        story.extend(self._create_executive_summary(country))
        story.append(PageBreak())
        
        # Skills analysis
        story.extend(self._create_skills_analysis(country, include_visualizations))
        story.append(PageBreak())
        
        # Clustering results
        story.extend(self._create_clustering_analysis(include_visualizations))
        story.append(PageBreak())
        
        # Temporal trends (if available)
        story.extend(self._create_temporal_analysis(country, include_visualizations))
        story.append(PageBreak())
        
        # Methodology
        story.extend(self._create_methodology_section())
        
        # Build PDF
        doc.build(story)
        
        logger.info(f"Report generated: {filepath}")
        return filepath
    
    def _create_title_page(self, country: Optional[str]) -> List:
        """Create title page elements."""
        elements = []
        
        # Title
        title_text = "Observatorio de Demanda Laboral Tecnológica"
        if country:
            country_names = {'CO': 'Colombia', 'MX': 'México', 'AR': 'Argentina'}
            title_text += f"\n{country_names.get(country, country)}"
        else:
            title_text += "\nAmérica Latina"
        
        elements.append(Paragraph(title_text, self.styles['CustomTitle']))
        elements.append(Spacer(1, 0.5*inch))
        
        # Subtitle
        subtitle = "Análisis Automatizado de Habilidades Técnicas"
        elements.append(Paragraph(subtitle, self.styles['Heading2']))
        elements.append(Spacer(1, 0.3*inch))
        
        # Date
        date_text = f"Fecha de generación: {datetime.now().strftime('%d de %B de %Y')}"
        elements.append(Paragraph(date_text, self.styles['Normal']))
        elements.append(Spacer(1, 2*inch))
        
        # Authors/Institution
        elements.append(Paragraph("Universidad XYZ", self.styles['Normal']))
        elements.append(Paragraph("Facultad de Ingeniería", self.styles['Normal']))
        
        return elements
    
    def _create_executive_summary(self, country: Optional[str]) -> List:
        """Create executive summary section."""
        elements = []
        
        elements.append(Paragraph("Resumen Ejecutivo", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get summary statistics
        stats = self.db_ops.get_skill_statistics(country)
        
        # Summary text
        summary_points = [
            f"Se analizaron un total de {stats.get('total_unique_skills', 0)} habilidades técnicas únicas.",
            f"Las 5 habilidades más demandadas son: {', '.join([s['skill'] for s in stats.get('top_skills', [])[:5]])}.",
            "El análisis revela una fuerte demanda de habilidades en desarrollo web, cloud computing y ciencia de datos.",
            "Se identificaron patrones emergentes en tecnologías de inteligencia artificial y DevOps."
        ]
        
        for point in summary_points:
            elements.append(Paragraph(f"• {point}", self.styles['Normal']))
            elements.append(Spacer(1, 0.1*inch))
        
        return elements
    
    def _create_skills_analysis(self, country: Optional[str], include_viz: bool) -> List:
        """Create skills analysis section."""
        elements = []
        
        elements.append(Paragraph("Análisis de Habilidades", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get skill statistics
        stats = self.db_ops.get_skill_statistics(country)
        
        # Top skills table
        elements.append(Paragraph("Top 20 Habilidades Más Demandadas", self.styles['SubsectionHeader']))
        
        if stats.get('top_skills'):
            # Create table data
            table_data = [['Posición', 'Habilidad', 'Frecuencia']]
            for i, skill_data in enumerate(stats['top_skills'][:20], 1):
                table_data.append([
                    str(i),
                    skill_data['skill'],
                    str(skill_data['count'])
                ])
            
            # Create table
            table = Table(table_data, colWidths=[1*inch, 3*inch, 1.5*inch])
            table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            
            elements.append(table)
            elements.append(Spacer(1, 0.3*inch))
        
        # Add visualization if requested
        if include_viz:
            viz_path = self._create_skill_frequency_chart(stats.get('top_skills', [])[:15])
            if viz_path:
                elements.append(Image(viz_path, width=6*inch, height=4*inch))
                elements.append(Spacer(1, 0.2*inch))
        
        return elements
    
    def _create_clustering_analysis(self, include_viz: bool) -> List:
        """Create clustering analysis section."""
        elements = []
        
        elements.append(Paragraph("Análisis de Clustering", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        # Get latest clustering results
        # Note: This would need to be implemented in DatabaseOperations
        # For now, we'll use placeholder text
        
        elements.append(Paragraph(
            "El análisis de clustering identificó grupos coherentes de habilidades "
            "que típicamente aparecen juntas en las ofertas laborales:",
            self.styles['Normal']
        ))
        elements.append(Spacer(1, 0.1*inch))
        
        # Placeholder cluster descriptions
        clusters = [
            "Frontend Development: React, Vue.js, CSS, JavaScript, HTML5",
            "Backend Development: Node.js, Python, Django, Flask, API REST",
            "Data Science: Python, R, Machine Learning, SQL, Pandas",
            "DevOps: Docker, Kubernetes, AWS, CI/CD, Jenkins",
            "Mobile Development: React Native, Flutter, iOS, Android"
        ]
        
        for cluster in clusters:
            elements.append(Paragraph(f"• {cluster}", self.styles['Normal']))
            elements.append(Spacer(1, 0.05*inch))
        
        return elements
    
    def _create_temporal_analysis(self, country: Optional[str], include_viz: bool) -> List:
        """Create temporal trends analysis section."""
        elements = []
        
        elements.append(Paragraph("Tendencias Temporales", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        elements.append(Paragraph(
            "El análisis temporal permite identificar la evolución de la demanda "
            "de habilidades técnicas a lo largo del tiempo.",
            self.styles['Normal']
        ))
        
        # Placeholder for temporal analysis
        trends = [
            "Crecimiento sostenido en demanda de habilidades cloud (AWS, Azure)",
            "Aumento significativo en tecnologías de IA/ML en los últimos 6 meses",
            "Estabilidad en frameworks tradicionales (Spring, .NET)",
            "Emergencia de nuevas herramientas DevOps"
        ]
        
        elements.append(Spacer(1, 0.1*inch))
        for trend in trends:
            elements.append(Paragraph(f"• {trend}", self.styles['Normal']))
            elements.append(Spacer(1, 0.05*inch))
        
        return elements
    
    def _create_methodology_section(self) -> List:
        """Create methodology section."""
        elements = []
        
        elements.append(Paragraph("Metodología", self.styles['SectionHeader']))
        elements.append(Spacer(1, 0.2*inch))
        
        methodology_text = """
        Este análisis se realizó mediante un pipeline automatizado que incluye:
        
        1. Web Scraping: Recolección automática de ofertas laborales de portales como 
           Computrabajo, Bumeran y elempleo.com.
        
        2. Extracción de Habilidades: Combinación de técnicas de NER (Named Entity Recognition) 
           y expresiones regulares para identificar menciones de habilidades técnicas.
        
        3. Enriquecimiento con LLM: Uso de modelos de lenguaje para identificar habilidades 
           implícitas y normalizar variaciones.
        
        4. Análisis Semántico: Generación de embeddings multilingües y clustering para 
           identificar grupos de habilidades relacionadas.
        
        5. Visualización: Generación de reportes estáticos con métricas agregadas y 
           visualizaciones interpretables.
        """
        
        elements.append(Paragraph(methodology_text, self.styles['Normal']))
        
        return elements
    
    def _create_skill_frequency_chart(self, top_skills: List[Dict[str, Any]]) -> Optional[str]:
        """Create skill frequency bar chart.
        
        Args:
            top_skills: List of top skills with counts
            
        Returns:
            Path to saved chart image
        """
        if not top_skills:
            return None
        
        try:
            # Prepare data
            skills = [s['skill'] for s in top_skills]
            counts = [s['count'] for s in top_skills]
            
            # Create figure
            plt.figure(figsize=(10, 6))
            
            # Create horizontal bar chart
            bars = plt.barh(skills, counts, color='#1a73e8')
            
            # Customize
            plt.xlabel('Número de Vacantes', fontsize=12)
            plt.title('Habilidades Más Demandadas', fontsize=14, fontweight='bold')
            plt.gca().invert_yaxis()  # Highest on top
            
            # Add value labels
            for i, bar in enumerate(bars):
                width = bar.get_width()
                plt.text(width + 1, bar.get_y() + bar.get_height()/2, 
                        f'{counts[i]}', 
                        ha='left', va='center')
            
            plt.tight_layout()
            
            # Save
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(self.output_dir, f"skill_frequency_{timestamp}.png")
            plt.savefig(filepath, dpi=300, bbox_inches='tight')
            plt.close()
            
            return filepath
            
        except Exception as e:
            logger.error(f"Error creating skill frequency chart: {e}")
            return None
    
    def enrich_extracted_skills(self, extracted_skills: List[Dict]) -> List[Dict]:
        """Enrich extracted skills with ESCO matches.
        
        Args:
            extracted_skills: List of extracted skill dictionaries
            
        Returns:
            Enriched skill list
        """
        # Get unique skill texts
        unique_skills = list(set(skill['skill_text'] for skill in extracted_skills))
        
        # Match all unique skills
        esco_matches = self.match_skills_batch(unique_skills)
        
        # Enrich original skills
        enriched = []
        for skill in extracted_skills:
            enriched_skill = skill.copy()
            
            match = esco_matches.get(skill['skill_text'])
            if match:
                enriched_skill.update({
                    'esco_uri': match['esco_uri'],
                    'esco_preferred_label': match['esco_preferred_label'],
                    'esco_match_type': match['match_type'],
                    'esco_match_score': match['match_score']
                })
            
            enriched.append(enriched_skill)
        
        return enriched
```

### src/extractor/pipeline.py
```python
import logging
from typing import List, Dict, Optional
from database.operations import DatabaseOperations
from .ner_extractor import NERExtractor
from .regex_patterns import RegexExtractor
from .esco_matcher import ESCOMatcher
from config.settings import get_settings
import time

logger = logging.getLogger(__name__)

class ExtractionPipeline:
    """Main pipeline for skill extraction from job postings."""
    
    def __init__(self):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        
        # Initialize extractors
        logger.info("Initializing extraction components...")
        self.ner_extractor = NERExtractor()
        self.regex_extractor = RegexExtractor()
        self.esco_matcher = ESCOMatcher()
        
        logger.info("Extraction pipeline initialized")
    
    def process_batch(self, batch_size: int = 100) -> Dict[str, any]:
        """Process a batch of unprocessed jobs.
        
        Args:
            batch_size: Number of jobs to process
            
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'jobs_processed': 0,
            'skills_extracted': 0,
            'esco_matches': 0,
            'errors': 0
        }
        
        try:
            # Get unprocessed jobs
            jobs = self.db_ops.get_unprocessed_jobs(limit=batch_size)
            logger.info(f"Processing {len(jobs)} jobs")
            
            for job in jobs:
                try:
                    # Process individual job
                    skills = self.process_job(job)
                    
                    if skills:
                        # Save extracted skills
                        self.db_ops.insert_extracted_skills(
                            str(job.job_id),
                            skills
                        )
                        
                        # Mark job as processed
                        self.db_ops.mark_job_processed(str(job.job_id))
                        
                        # Update stats
                        stats['jobs_processed'] += 1
                        stats['skills_extracted'] += len(skills)
                        stats['esco_matches'] += sum(
                            1 for s in skills if s.get('esco_uri')
                        )
                    
                except Exception as e:
                    logger.error(f"Error processing job {job.job_id}: {e}")
                    stats['errors'] += 1
            
            # Calculate processing time
            stats['processing_time'] = time.time() - start_time
            stats['jobs_per_second'] = stats['jobs_processed'] / stats['processing_time'] if stats['processing_time'] > 0 else 0
            
            logger.info(
                f"Batch complete: {stats['jobs_processed']} jobs, "
                f"{stats['skills_extracted']} skills extracted, "
                f"{stats['esco_matches']} ESCO matches, "
                f"{stats['errors']} errors, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"Batch processing failed: {e}")
            raise
    
    def process_job(self, job) -> List[Dict[str, any]]:
        """Process a single job to extract skills.
        
        Args:
            job: Job object from database
            
        Returns:
            List of extracted and enriched skills
        """
        # Prepare job data
        job_data = {
            'title': job.title,
            'description': job.description,
            'requirements': job.requirements
        }
        
        # Extract skills using NER
        ner_skills = self.ner_extractor.extract_from_job(job_data)
        
        # Extract skills using regex
        regex_skills = self.regex_extractor.extract_from_job(job_data)
        
        # Combine and deduplicate
        all_skills = self._combine_skills(ner_skills, regex_skills)
        
        # Enrich with ESCO matches
        enriched_skills = self.esco_matcher.enrich_extracted_skills(all_skills)
        
        logger.debug(
            f"Job {job.job_id}: {len(ner_skills)} NER skills, "
            f"{len(regex_skills)} regex skills, "
            f"{len(enriched_skills)} total after deduplication"
        )
        
        return enriched_skills
    
    def _combine_skills(self, ner_skills: List[Dict], regex_skills: List[Dict]) -> List[Dict]:
        """Combine and deduplicate skills from different extractors.
        
        Args:
            ner_skills: Skills from NER
            regex_skills: Skills from regex
            
        Returns:
            Combined and deduplicated skill list
        """
        # Use skill text and source section as unique key
        seen_skills = {}
        
        # Process NER skills first (higher confidence)
        for skill in ner_skills:
            key = (skill['skill_text'], skill['source_section'])
            
            if key not in seen_skills:
                seen_skills[key] = skill
            else:
                # Update confidence if higher
                if skill['confidence_score'] > seen_skills[key]['confidence_score']:
                    seen_skills[key] = skill
        
        # Process regex skills
        for skill in regex_skills:
            key = (skill['skill_text'], skill['source_section'])
            
            if key not in seen_skills:
                seen_skills[key] = skill
            else:
                # Merge extraction methods
                existing = seen_skills[key]
                if existing['extraction_method'] != skill['extraction_method']:
                    existing['extraction_method'] = 'ner+regex'
                    existing['confidence_score'] = min(0.95, existing['confidence_score'] + 0.1)
        
        return list(seen_skills.values())
    
    def run_continuous(self, batch_size: int = 100, wait_time: int = 60):
        """Run extraction continuously.
        
        Args:
            batch_size: Jobs per batch
            wait_time: Seconds to wait between batches
        """
        logger.info(f"Starting continuous extraction (batch_size={batch_size}, wait={wait_time}s)")
        
        while True:
            try:
                stats = self.process_batch(batch_size)
                
                if stats['jobs_processed'] == 0:
                    logger.info(f"No jobs to process. Waiting {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    # Short pause between batches
                    time.sleep(5)
                    
            except KeyboardInterrupt:
                logger.info("Extraction stopped by user")
                break
            except Exception as e:
                logger.error(f"Extraction error: {e}")
                logger.info(f"Waiting {wait_time}s before retry...")
                time.sleep(wait_time)
```

---

## 6. LLM Processor Module Files

### src/llm_processor/__init__.py
```python
from .llm_handler import LLMHandler
from .prompts import PromptGenerator
from .esco_normalizer import ESCONormalizer
from .validator import SkillValidator

__all__ = ['LLMHandler', 'PromptGenerator', 'ESCONormalizer', 'SkillValidator']
```

### src/llm_processor/llm_handler.py
```python
import logging
from typing import List, Dict, Optional, Any
from llama_cpp import Llama
import openai
from config.settings import get_settings
import json
import time

logger = logging.getLogger(__name__)

class LLMHandler:
    """Handle LLM interactions for skill processing."""
    
    def __init__(self, model_type: str = "local"):
        self.settings = get_settings()
        self.model_type = model_type
        
        if model_type == "local":
            self._init_local_model()
        elif model_type == "openai":
            self._init_openai()
        else:
            raise ValueError(f"Unknown model type: {model_type}")
    
    def _init_local_model(self):
        """Initialize local LLaMA/Mistral model."""
        try:
            logger.info(f"Loading local model from {self.settings.llm_model_path}")
            
            self.model = Llama(
                model_path=self.settings.llm_model_path,
                n_ctx=self.settings.llm_context_length,
                n_gpu_layers=self.settings.llm_n_gpu_layers,
                verbose=False
            )
            
            logger.info("Local model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load local model: {e}")
            raise
    
    def _init_openai(self):
        """Initialize OpenAI API client."""
        if not self.settings.openai_api_key:
            raise ValueError("OpenAI API key not configured")
        
        openai.api_key = self.settings.openai_api_key
        self.model_name = self.settings.openai_model
        logger.info(f"OpenAI API initialized with model {self.model_name}")
    
    def process_skills(self, 
                      job_data: Dict[str, Any],
                      extracted_skills: List[Dict[str, Any]],
                      prompt_template: str) -> Dict[str, Any]:
        """Process skills using LLM.
        
        Args:
            job_data: Job information
            extracted_skills: Initially extracted skills
            prompt_template: Formatted prompt template
            
        Returns:
            LLM response with processed skills
        """
        start_time = time.time()
        
        try:
            if self.model_type == "local":
                response = self._process_local(prompt_template)
            else:
                response = self._process_openai(prompt_template)
            
            # Parse response
            result = self._parse_response(response)
            
            # Add metadata
            result['processing_time'] = time.time() - start_time
            result['model_type'] = self.model_type
            result['model_name'] = getattr(self, 'model_name', 'local_mistral')
            
            return result
            
        except Exception as e:
            logger.error(f"LLM processing failed: {e}")
            raise
    
    def _process_local(self, prompt: str) -> str:
        """Process using local model.
        
        Args:
            prompt: Complete prompt
            
        Returns:
            Model response
        """
        response = self.model(
            prompt,
            max_tokens=self.settings.llm_max_tokens,
            temperature=self.settings.llm_temperature,
            stop=["</response>", "\n\n\n"],
            echo=False
        )
        
        return response['choices'][0]['text']
    
    def _process_openai(self, prompt: str) -> str:
        """Process using OpenAI API.
        
        Args:
            prompt: Complete prompt
            
        Returns:
            Model response
        """
        response = openai.ChatCompletion.create(
            model=self.model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert in analyzing job postings and extracting technical skills. Respond in Spanish."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            max_tokens=self.settings.llm_max_tokens,
            temperature=self.settings.llm_temperature
        )
        
        return response.choices[0].message.content
    
    def _parse_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response.
        
        Args:
            response: Raw LLM response
            
        Returns:
            Parsed skills data
        """
        # Try to extract JSON if present
        if "```json" in response:
            # Extract JSON block
            start = response.find("```json") + 7
            end = response.find("```", start)
            json_str = response[start:end].strip()
            
            try:
                return json.loads(json_str)
            except json.JSONDecodeError:
                logger.warning("Failed to parse JSON from response")
        
        # Fallback: Parse structured text response
        result = {
            "explicit_skills": [],
            "implicit_skills": [],
            "normalized_skills": [],
            "deduplicated_skills": []
        }
        
        lines = response.strip().split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            
            # Detect sections
            if "habilidades explícitas" in line.lower():
                current_section = "explicit_skills"
            elif "habilidades implícitas" in line.lower():
                current_section = "implicit_skills"
            elif "habilidades normalizadas" in line.lower():
                current_section = "normalized_skills"
            elif "deduplicadas" in line.lower():
                current_section = "deduplicated_skills"
            elif line and current_section and (line.startswith('-') or line.startswith('*')):
                # Extract skill from bullet point
                skill_text = line.lstrip('-*').strip()
                
                # Parse skill with reasoning if present
                if ':' in skill_text:
                    skill, reasoning = skill_text.split(':', 1)
                    result[current_section].append({
                        "skill": skill.strip(),
                        "reasoning": reasoning.strip()
                    })
                else:
                    result[current_section].append({
                        "skill": skill_text
                    })
        
        return result
```

### src/llm_processor/prompts.py
```python
from typing import List, Dict, Any
import json

class PromptGenerator:
    """Generate prompts for LLM skill processing."""
    
    def __init__(self):
        self.templates = self._load_templates()
    
    def _load_templates(self) -> Dict[str, str]:
        """Load prompt templates."""
        return {
            "skill_processing": """Eres un experto en análisis de ofertas laborales tecnológicas en América Latina.

Datos de la vacante:
- Título: {job_title}
- Descripción: {job_description}
- Requisitos: {job_requirements}

Habilidades extraídas inicialmente:
{extracted_skills}

Por favor, realiza las siguientes tareas:

1. **Validación de habilidades explícitas**: Revisa las habilidades extraídas y confirma cuáles son realmente habilidades técnicas relevantes.

2. **Detección de habilidades implícitas**: Basándote en el contexto del puesto, identifica habilidades técnicas que serían necesarias pero no están mencionadas explícitamente. Por ejemplo:
   - Si menciona "desarrollo web full stack" → probablemente necesite Git, bases de datos, APIs REST
   - Si menciona "análisis de datos" → probablemente necesite SQL, Python/R, visualización
   - Si menciona "DevOps" → probablemente necesite CI/CD, contenedores, cloud

3. **Normalización con ESCO**: Para cada habilidad, proporciona la forma normalizada según estándares internacionales:
   - Usa nombres estándar (ej: "JS" → "JavaScript", "React.js" → "React")
   - Mantén el español cuando sea apropiado
   - Agrupa variaciones (ej: "MySQL/MariaDB" → "MySQL")

4. **Deduplicación**: Elimina habilidades duplicadas o redundantes:
   - Combina variaciones del mismo concepto
   - Elimina términos demasiado genéricos
   - Mantén el término más específico cuando haya jerarquía

Responde en el siguiente formato JSON:
```json
{{
  "explicit_skills": [
    {{"skill": "nombre", "confidence": 0.9, "original": "texto_original"}}
  ],
  "implicit_skills": [
    {{"skill": "nombre", "confidence": 0.7, "reasoning": "justificación"}}
  ],
  "normalized_skills": [
    {{"original": "skill_original", "normalized": "skill_normalizado", "esco_match": "posible_uri"}}
  ],
  "deduplicated_skills": [
    {{"skill": "nombre_final", "type": "explicit|implicit", "category": "programming|database|framework|tool|soft_skill"}}
  ]
}}
```""",

            "simple_inference": """Analiza esta oferta de trabajo y extrae SOLO las habilidades técnicas implícitas que no están mencionadas pero serían necesarias.

Título: {job_title}
Descripción resumida: {job_summary}
Habilidades ya identificadas: {known_skills}

Lista únicamente las habilidades técnicas implícitas con su justificación:
""",

            "normalization": """Normaliza las siguientes habilidades técnicas según estándares internacionales y la taxonomía ESCO:

Habilidades a normalizar:
{skills_list}

Para cada habilidad, proporciona:
- Forma normalizada
- Categoría (lenguaje/framework/base de datos/herramienta/metodología)
- Término ESCO equivalente si existe

Responde en formato de lista:
""",

            "deduplication": """Elimina duplicados y agrupa las siguientes habilidades:

Habilidades:
{skills_list}

Reglas:
- Combina variaciones del mismo concepto (ej: JS, JavaScript, javascript → JavaScript)
- Mantén el término más específico cuando haya jerarquía
- Elimina términos genéricos si hay específicos

Lista final sin duplicados:
"""
        }
    
    def generate_skill_processing_prompt(self, 
                                       job_data: Dict[str, Any],
                                       extracted_skills: List[Dict[str, Any]]) -> str:
        """Generate prompt for complete skill processing.
        
        Args:
            job_data: Job information
            extracted_skills: Initially extracted skills
            
        Returns:
            Formatted prompt
        """
        # Format extracted skills
        skills_text = self._format_extracted_skills(extracted_skills)
        
        prompt = self.templates["skill_processing"].format(
            job_title=job_data.get('title', 'No especificado'),
            job_description=job_data.get('description', 'No especificado'),
            job_requirements=job_data.get('requirements', 'No especificado'),
            extracted_skills=skills_text
        )
        
        return prompt
    
    def generate_inference_prompt(self,
                                job_title: str,
                                job_summary: str,
                                known_skills: List[str]) -> str:
        """Generate prompt for implicit skill inference.
        
        Args:
            job_title: Job title
            job_summary: Brief job description
            known_skills: Already identified skills
            
        Returns:
            Formatted prompt
        """
        skills_list = ", ".join(known_skills) if known_skills else "Ninguna"
        
        return self.templates["simple_inference"].format(
            job_title=job_title,
            job_summary=job_summary,
            known_skills=skills_list
        )
    
    def generate_normalization_prompt(self, skills: List[str]) -> str:
        """Generate prompt for skill normalization.
        
        Args:
            skills: List of skills to normalize
            
        Returns:
            Formatted prompt
        """
        skills_text = "\n".join(f"- {skill}" for skill in skills)
        
        return self.templates["normalization"].format(
            skills_list=skills_text
        )
    
    def generate_deduplication_prompt(self, skills: List[str]) -> str:
        """Generate prompt for skill deduplication.
        
        Args:
            skills: List of skills to deduplicate
            
        Returns:
            Formatted prompt
        """
        skills_text = "\n".join(f"- {skill}" for skill in skills)
        
        return self.templates["deduplication"].format(
            skills_list=skills_text
        )
    
    def _format_extracted_skills(self, skills: List[Dict[str, Any]]) -> str:
        """Format extracted skills for prompt.
        
        Args:
            skills: List of skill dictionaries
            
        Returns:
            Formatted text
        """
        formatted_skills = []
        
        # Group by source section
        by_section = {}
        for skill in skills:
            section = skill.get('source_section', 'unknown')
            if section not in by_section:
                by_section[section] = []
            by_section[section].append(skill)
        
        # Format each section
        for section, section_skills in by_section.items():
            formatted_skills.append(f"\nDe {section}:")
            for skill in section_skills:
                method = skill.get('extraction_method', 'unknown')
                confidence = skill.get('confidence_score', 0)
                text = skill.get('skill_text', '')
                
                formatted_skills.append(
                    f"  - {text} (método: {method}, confianza: {confidence:.2f})"
                )
        
        return "\n".join(formatted_skills)

### src/llm_processor/esco_normalizer.py
```python
import logging
from typing import List, Dict, Any, Optional
from fuzzywuzzy import fuzz
import yaml

logger = logging.getLogger(__name__)

class ESCONormalizer:
    """Normalize skills using ESCO taxonomy with LLM assistance."""
    
    def __init__(self, config_path: str = "config/esco_config.yaml"):
        self.config = self._load_config(config_path)
        self.normalization_rules = self._build_normalization_rules()
    
    def _load_config(self, config_path: str) -> dict:
        """Load ESCO configuration."""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config['esco']
        except Exception as e:
            logger.error(f"Failed to load ESCO config: {e}")
            return {}
    
    def _build_normalization_rules(self) -> Dict[str, str]:
        """Build normalization rules from config and common variations."""
        rules = {}
        
        # Load from config
        if 'tech_mappings' in self.config:
            rules.update(self.config['tech_mappings'])
        
        # Add common variations
        common_variations = {
            # JavaScript variations
            'js': 'JavaScript',
            'javascript': 'JavaScript',
            'java script': 'JavaScript',
            
            # Python variations
            'python3': 'Python',
            'python 3': 'Python',
            'python2': 'Python',
            
            # React variations
            'reactjs': 'React',
            'react.js': 'React',
            'react js': 'React',
            'react native': 'React Native',
            
            # Node variations
            'nodejs': 'Node.js',
            'node js': 'Node.js',
            'node': 'Node.js',
            
            # Database variations
            'postgres': 'PostgreSQL',
            'mysql': 'MySQL',
            'maria db': 'MariaDB',
            'mariadb': 'MariaDB',
            'mongo': 'MongoDB',
            'mongo db': 'MongoDB',
            
            # .NET variations
            'dotnet': '.NET',
            'dot net': '.NET',
            '.net core': '.NET Core',
            'asp.net': 'ASP.NET',
            
            # Other common variations
            'c++': 'C++',
            'c#': 'C#',
            'c sharp': 'C#',
            'objective c': 'Objective-C',
            'obj-c': 'Objective-C',
            
            # Spanish to English mappings
            'base de datos': 'Database',
            'desarrollo web': 'Web Development',
            'desarrollo móvil': 'Mobile Development',
            'aprendizaje automático': 'Machine Learning',
            'inteligencia artificial': 'Artificial Intelligence',
            'ciencia de datos': 'Data Science',
            'computación en la nube': 'Cloud Computing',
            'control de versiones': 'Version Control',
        }
        
        # Convert all keys to lowercase
        for key, value in common_variations.items():
            rules[key.lower()] = value
        
        return rules
    
    def normalize_skill(self, skill: str) -> Dict[str, Any]:
        """Normalize a single skill.
        
        Args:
            skill: Raw skill text
            
        Returns:
            Normalized skill data
        """
        skill_lower = skill.lower().strip()
        
        # Direct mapping
        if skill_lower in self.normalization_rules:
            return {
                'original': skill,
                'normalized': self.normalization_rules[skill_lower],
                'method': 'direct_mapping',
                'confidence': 1.0
            }
        
        # Try fuzzy matching
        best_match = None
        best_score = 0
        
        for pattern, normalized in self.normalization_rules.items():
            score = fuzz.ratio(skill_lower, pattern)
            if score > best_score and score >= 85:
                best_score = score
                best_match = normalized
        
        if best_match:
            return {
                'original': skill,
                'normalized': best_match,
                'method': 'fuzzy_mapping',
                'confidence': best_score / 100.0
            }
        
        # Category-based normalization
        normalized = self._category_normalization(skill)
        if normalized != skill:
            return {
                'original': skill,
                'normalized': normalized,
                'method': 'category_rules',
                'confidence': 0.8
            }
        
        # No normalization found
        return {
            'original': skill,
            'normalized': skill,
            'method': 'no_normalization',
            'confidence': 0.5
        }
    
    def _category_normalization(self, skill: str) -> str:
        """Apply category-based normalization rules.
        
        Args:
            skill: Skill to normalize
            
        Returns:
            Normalized skill
        """
        skill_lower = skill.lower()
        
        # Framework detection
        if 'framework' in skill_lower:
            skill = skill.replace('framework', '').replace('Framework', '').strip()
        
        # Version removal for certain technologies
        version_patterns = [
            (r'python\s*\d+\.?\d*', 'Python'),
            (r'java\s*\d+', 'Java'),
            (r'angular\s*\d+', 'Angular'),
            (r'vue\s*\d+', 'Vue.js'),
            (r'react\s*\d+', 'React'),
        ]
        
        import re
        for pattern, replacement in version_patterns:
            if re.search(pattern, skill_lower):
                return replacement
        
        # Capitalize properly
        # Special cases
        special_cases = {
            'mysql': 'MySQL',
            'postgresql': 'PostgreSQL',
            'mongodb': 'MongoDB',
            'javascript': 'JavaScript',
            'typescript': 'TypeScript',
            'graphql': 'GraphQL',
            'nodejs': 'Node.js',
            'reactjs': 'React',
            'vuejs': 'Vue.js',
        }
        
        if skill_lower in special_cases:
            return special_cases[skill_lower]
        
        # Default: capitalize first letter of each word
        return ' '.join(word.capitalize() for word in skill.split())
    
    def normalize_skills_batch(self, skills: List[str]) -> List[Dict[str, Any]]:
        """Normalize multiple skills.
        
        Args:
            skills: List of skills to normalize
            
        Returns:
            List of normalized skill data
        """
        normalized = []
        
        for skill in skills:
            result = self.normalize_skill(skill)
            normalized.append(result)
        
        return normalized
    
    def deduplicate_normalized_skills(self, normalized_skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Deduplicate normalized skills.
        
        Args:
            normalized_skills: List of normalized skill dictionaries
            
        Returns:
            Deduplicated list
        """
        # Group by normalized form
        skill_groups = {}
        
        for skill_data in normalized_skills:
            normalized = skill_data['normalized']
            
            if normalized not in skill_groups:
                skill_groups[normalized] = {
                    'normalized': normalized,
                    'originals': [],
                    'best_confidence': 0,
                    'methods': set()
                }
            
            skill_groups[normalized]['originals'].append(skill_data['original'])
            skill_groups[normalized]['methods'].add(skill_data['method'])
            skill_groups[normalized]['best_confidence'] = max(
                skill_groups[normalized]['best_confidence'],
                skill_data['confidence']
            )
        
        # Convert back to list
        deduplicated = []
        for normalized, group_data in skill_groups.items():
            deduplicated.append({
                'normalized': normalized,
                'original_variations': group_data['originals'],
                'confidence': group_data['best_confidence'],
                'methods': list(group_data['methods'])
            })
        
        return deduplicated

### src/llm_processor/validator.py
```python
import logging
from typing import List, Dict, Any, Set
import re

logger = logging.getLogger(__name__)

class SkillValidator:
    """Validate and filter skills."""
    
    def __init__(self):
        self.blacklist = self._build_blacklist()
        self.whitelist = self._build_whitelist()
        self.categories = self._build_categories()
    
    def _build_blacklist(self) -> Set[str]:
        """Build blacklist of non-skill terms."""
        return {
            # Generic terms
            'experiencia', 'años', 'año', 'conocimiento', 'conocimientos',
            'habilidad', 'habilidades', 'capacidad', 'competencia',
            'desarrollo', 'trabajo', 'empresa', 'cliente', 'proyecto',
            'equipo', 'persona', 'profesional', 'área', 'proceso',
            
            # Too generic tech terms
            'tecnología', 'tecnologías', 'sistema', 'sistemas',
            'informática', 'computación', 'software', 'hardware',
            'programación', 'desarrollo de software',
            
            # Common words
            'bueno', 'excelente', 'alto', 'nivel', 'manejo',
            'uso', 'gestión', 'administración', 'análisis',
            
            # Methodologies too generic
            'metodología', 'metodologías', 'mejores prácticas',
            
            # Soft skills (we focus on technical)
            'comunicación', 'liderazgo', 'trabajo en equipo',
            'responsabilidad', 'proactividad', 'creatividad'
        }
    
    def _build_whitelist(self) -> Set[str]:
        """Build whitelist of known valid skills."""
        return {
            # Programming languages
            'python', 'java', 'javascript', 'typescript', 'c++', 'c#',
            'php', 'ruby', 'go', 'golang', 'rust', 'kotlin', 'swift',
            'objective-c', 'r', 'scala', 'perl', 'matlab', 'julia',
            
            # Frameworks and libraries
            'react', 'angular', 'vue', 'django', 'flask', 'spring',
            'express', 'laravel', 'rails', 'fastapi', 'nextjs',
            '.net', 'asp.net', 'tensorflow', 'pytorch', 'keras',
            
            # Databases
            'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',
            'cassandra', 'dynamodb', 'oracle', 'sql server', 'sqlite',
            'neo4j', 'couchdb', 'firebase', 'supabase',
            
            # Cloud and DevOps
            'aws', 'azure', 'gcp', 'google cloud', 'docker', 'kubernetes',
            'jenkins', 'terraform', 'ansible', 'puppet', 'chef',
            'gitlab', 'github', 'bitbucket', 'circleci', 'travis',
            
            # Tools and platforms
            'git', 'jira', 'confluence', 'slack', 'linux', 'windows',
            'macos', 'ubuntu', 'centos', 'debian', 'nginx', 'apache',
            'grafana', 'prometheus', 'elasticsearch', 'kibana',
            
            # Data and ML
            'machine learning', 'deep learning', 'data science',
            'big data', 'spark', 'hadoop', 'kafka', 'airflow',
            'pandas', 'numpy', 'scikit-learn', 'matplotlib',
            
            # Mobile
            'android', 'ios', 'react native', 'flutter', 'xamarin',
            'swift', 'kotlin', 'objective-c', 'cordova', 'ionic',
            
            # Other
            'api', 'rest', 'graphql', 'websocket', 'microservices',
            'ci/cd', 'agile', 'scrum', 'kanban', 'tdd', 'bdd'
        }
    
    def _build_categories(self) -> Dict[str, Set[str]]:
        """Build skill categories."""
        return {
            'programming_language': {
                'python', 'java', 'javascript', 'typescript', 'c++', 'c#',
                'php', 'ruby', 'go', 'rust', 'kotlin', 'swift', 'r'
            },
            'framework': {
                'react', 'angular', 'vue', 'django', 'flask', 'spring',
                'express', 'laravel', 'rails', '.net', 'nextjs'
            },
            'database': {
                'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',
                'cassandra', 'dynamodb', 'oracle', 'sql server'
            },
            'cloud_platform': {
                'aws', 'azure', 'gcp', 'google cloud', 'heroku', 'digitalocean'
            },
            'devops_tool': {
                'docker', 'kubernetes', 'jenkins', 'terraform', 'ansible',
                'git', 'github', 'gitlab', 'ci/cd'
            },
            'data_ml': {
                'machine learning', 'deep learning', 'tensorflow', 'pytorch',
                'pandas', 'numpy', 'spark', 'hadoop'
            },
            'mobile': {
                'android', 'ios', 'react native', 'flutter', 'xamarin'
            },
            'methodology': {
                'agile', 'scrum', 'kanban', 'devops', 'tdd', 'bdd'
            }
        }
    
    def validate_skill(self, skill: str) -> Dict[str, Any]:
        """Validate a single skill.
        
        Args:
            skill: Skill to validate
            
        Returns:
            Validation result
        """
        skill_lower = skill.lower().strip()
        
        # Check blacklist
        if skill_lower in self.blacklist:
            return {
                'skill': skill,
                'valid': False,
                'reason': 'blacklisted',
                'confidence': 0.0
            }
        
        # Check whitelist
        if skill_lower in self.whitelist:
            category = self._categorize_skill(skill_lower)
            return {
                'skill': skill,
                'valid': True,
                'reason': 'whitelisted',
                'category': category,
                'confidence': 1.0
            }
        
        # Length check
        if len(skill_lower) < 2 or len(skill_lower) > 50:
            return {
                'skill': skill,
                'valid': False,
                'reason': 'invalid_length',
                'confidence': 0.0
            }
        
        # Pattern validation
        if not self._validate_pattern(skill_lower):
            return {
                'skill': skill,
                'valid': False,
                'reason': 'invalid_pattern',
                'confidence': 0.0
            }
        
        # Partial matches in whitelist
        for valid_skill in self.whitelist:
            if valid_skill in skill_lower or skill_lower in valid_skill:
                category = self._categorize_skill(valid_skill)
                return {
                    'skill': skill,
                    'valid': True,
                    'reason': 'partial_match',
                    'category': category,
                    'confidence': 0.7
                }
        
        # If not in whitelist but passes other checks
        return {
            'skill': skill,
            'valid': True,
            'reason': 'pattern_valid',
            'category': 'uncategorized',
            'confidence': 0.5
        }
    
    def _validate_pattern(self, skill: str) -> bool:
        """Validate skill pattern.
        
        Args:
            skill: Skill to validate
            
        Returns:
            True if pattern is valid
        """
        # Must contain at least one letter
        if not re.search(r'[a-zA-Z]', skill):
            return False
        
        # Check for suspicious patterns
        suspicious_patterns = [
            r'^\d+,  # Only numbers
            r'^[^a-zA-Z0-9\s\.\+\#\-/]+,  # Only special chars
            r'\b(años?|experiencia|conocimientos?)\b',  # Common non-skills
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, skill, re.IGNORECASE):
                return False
        
        return True
    
    def _categorize_skill(self, skill: str) -> str:
        """Categorize a skill.
        
        Args:
            skill: Skill to categorize
            
        Returns:
            Category name
        """
        skill_lower = skill.lower()
        
        for category, skills in self.categories.items():
            if skill_lower in skills:
                return category
        
        # Try partial matches
        for category, skills in self.categories.items():
            for cat_skill in skills:
                if cat_skill in skill_lower or skill_lower in cat_skill:
                    return category
        
        return 'uncategorized'
    
    def validate_skills_batch(self, skills: List[str]) -> List[Dict[str, Any]]:
        """Validate multiple skills.
        
        Args:
            skills: List of skills to validate
            
        Returns:
            List of validation results
        """
        results = []
        
        for skill in skills:
            result = self.validate_skill(skill)
            results.append(result)
        
        # Log statistics
        valid_count = sum(1 for r in results if r['valid'])
        logger.info(
            f"Skill validation: {valid_count}/{len(skills)} valid "
            f"({valid_count/len(skills)*100:.1f}%)"
        )
        
        return results
    
    def filter_valid_skills(self, skills: List[str], min_confidence: float = 0.5) -> List[str]:
        """Filter only valid skills.
        
        Args:
            skills: List of skills
            min_confidence: Minimum confidence threshold
            
        Returns:
            List of valid skills
        """
        results = self.validate_skills_batch(skills)
        
        valid_skills = [
            r['skill'] for r in results
            if r['valid'] and r['confidence'] >= min_confidence
        ]
        
        return valid_skills

### src/llm_processor/pipeline.py
```python
import logging
from typing import List, Dict, Any
from database.operations import DatabaseOperations
from .llm_handler import LLMHandler
from .prompts import PromptGenerator
from .esco_normalizer import ESCONormalizer
from .validator import SkillValidator
from config.settings import get_settings
import time

logger = logging.getLogger(__name__)

class LLMProcessingPipeline:
    """Main pipeline for LLM-based skill processing."""
    
    def __init__(self, model_type: str = "local"):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        
        logger.info("Initializing LLM processing components...")
        self.llm_handler = LLMHandler(model_type)
        self.prompt_generator = PromptGenerator()
        self.normalizer = ESCONormalizer()
        self.validator = SkillValidator()
        
        logger.info("LLM processing pipeline initialized")
    
    def process_batch(self, batch_size: int = 50) -> Dict[str, Any]:
        """Process a batch of jobs with extracted skills.
        
        Args:
            batch_size: Number of jobs to process
            
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'jobs_processed': 0,
            'skills_enhanced': 0,
            'implicit_skills_found': 0,
            'skills_normalized': 0,
            'errors': 0
        }
        
        try:
            # Get jobs with extracted skills needing LLM processing
            jobs_data = self.db_ops.get_extracted_skills_for_processing(limit=batch_size)
            logger.info(f"Processing {len(jobs_data)} jobs with LLM")
            
            for job_data in jobs_data:
                try:
                    # Process individual job
                    enhanced_skills = self.process_job(job_data)
                    
                    if enhanced_skills:
                        # Save enhanced skills
                        self.db_ops.insert_enhanced_skills(
                            job_data['job_id'],
                            enhanced_skills
                        )
                        
                        # Update statistics
                        stats['jobs_processed'] += 1
                        stats['skills_enhanced'] += len(enhanced_skills)
                        stats['implicit_skills_found'] += sum(
                            1 for s in enhanced_skills 
                            if s.get('skill_type') == 'implicit'
                        )
                        stats['skills_normalized'] += sum(
                            1 for s in enhanced_skills
                            if s.get('normalized_skill') != s.get('original_skill_text')
                        )
                    
                except Exception as e:
                    logger.error(f"Error processing job {job_data['job_id']}: {e}")
                    stats['errors'] += 1
            
            # Calculate processing time
            stats['processing_time'] = time.time() - start_time
            stats['jobs_per_minute'] = (stats['jobs_processed'] / stats['processing_time']) * 60 if stats['processing_time'] > 0 else 0
            
            logger.info(
                f"LLM batch complete: {stats['jobs_processed']} jobs, "
                f"{stats['skills_enhanced']} skills enhanced, "
                f"{stats['implicit_skills_found']} implicit skills found, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"LLM batch processing failed: {e}")
            raise
    
    def process_job(self, job_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Process a single job with LLM.
        
        Args:
            job_data: Job data with extracted skills
            
        Returns:
            List of enhanced skills
        """
        # Generate prompt
        prompt = self.prompt_generator.generate_skill_processing_prompt(
            {
                'title': job_data['job_title'],
                'description': job_data['job_description'],
                'requirements': job_data['job_requirements']
            },
            job_data['extracted_skills']
        )
        
        # Process with LLM
        llm_response = self.llm_handler.process_skills(
            job_data,
            job_data['extracted_skills'],
            prompt
        )
        
        # Process LLM response
        enhanced_skills = self._process_llm_response(
            llm_response,
            job_data['extracted_skills']
        )
        
        return enhanced_skills
    
    def _process_llm_response(self, 
                            llm_response: Dict[str, Any],
                            original_skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process LLM response into enhanced skills.
        
        Args:
            llm_response: Response from LLM
            original_skills: Original extracted skills
            
        Returns:
            List of enhanced skill records
        """
        enhanced_skills = []
        
        # Create a mapping of original skills
        original_map = {
            skill['skill_text'].lower(): skill 
            for skill in original_skills
        }
        
        # Process explicit skills (validated by LLM)
        for skill_data in llm_response.get('explicit_skills', []):
            skill_text = skill_data.get('skill', '').lower()
            
            # Validate skill
            validation = self.validator.validate_skill(skill_text)
            if not validation['valid']:
                continue
            
            # Normalize skill
            normalization = self.normalizer.normalize_skill(skill_text)
            
            enhanced_skills.append({
                'original_skill_text': skill_data.get('original', skill_text),
                'normalized_skill': normalization['normalized'],
                'skill_type': 'explicit',
                'esco_concept_uri': None,  # To be matched later
                'esco_preferred_label': None,
                'llm_confidence': skill_data.get('confidence', 0.8),
                'llm_reasoning': None,
                'is_duplicate': False,
                'llm_model': llm_response.get('model_name', 'unknown')
            })
        
        # Process implicit skills (inferred by LLM)
        for skill_data in llm_response.get('implicit_skills', []):
            skill_text = skill_data.get('skill', '').lower()
            
            # Validate skill
            validation = self.validator.validate_skill(skill_text)
            if not validation['valid']:
                continue
            
            # Normalize skill
            normalization = self.normalizer.normalize_skill(skill_text)
            
            enhanced_skills.append({
                'original_skill_text': None,
                'normalized_skill': normalization['normalized'],
                'skill_type': 'implicit',
                'esco_concept_uri': None,
                'esco_preferred_label': None,
                'llm_confidence': skill_data.get('confidence', 0.7),
                'llm_reasoning': skill_data.get('reasoning', ''),
                'is_duplicate': False,
                'llm_model': llm_response.get('model_name', 'unknown')
            })
        
        # Deduplicate skills
        enhanced_skills = self._deduplicate_skills(enhanced_skills)
        
        return enhanced_skills
    
    def _deduplicate_skills(self, skills: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Deduplicate enhanced skills.
        
        Args:
            skills: List of enhanced skills
            
        Returns:
            Deduplicated list with duplicates marked
        """
        seen_normalized = {}
        deduplicated = []
        
        # Sort by confidence (highest first)
        sorted_skills = sorted(
            skills, 
            key=lambda x: x.get('llm_confidence', 0),
            reverse=True
        )
        
        for skill in sorted_skills:
            normalized = skill['normalized_skill'].lower()
            
            if normalized not in seen_normalized:
                # First occurrence
                seen_normalized[normalized] = skill
                deduplicated.append(skill)
            else:
                # Duplicate found
                skill['is_duplicate'] = True
                skill['duplicate_of_id'] = id(seen_normalized[normalized])
                deduplicated.append(skill)
        
        return deduplicated
    
    def run_continuous(self, batch_size: int = 50, wait_time: int = 60):
        """Run LLM processing continuously.
        
        Args:
            batch_size: Jobs per batch
            wait_time: Seconds to wait between batches
        """
        logger.info(f"Starting continuous LLM processing (batch_size={batch_size}, wait={wait_time}s)")
        
        while True:
            try:
                stats = self.process_batch(batch_size)
                
                if stats['jobs_processed'] == 0:
                    logger.info(f"No jobs to process. Waiting {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    # Short pause between batches
                    time.sleep(10)
                    
            except KeyboardInterrupt:
                logger.info("LLM processing stopped by user")
                break
            except Exception as e:
                logger.error(f"LLM processing error: {e}")
                logger.info(f"Waiting {wait_time}s before retry...")
                time.sleep(wait_time)
```

---

## 7. Embedder Module Files

### src/embedder/__init__.py
```python
from .vectorizer import SkillVectorizer
from .model_loader import EmbeddingModelLoader
from .batch_processor import BatchProcessor

__all__ = ['SkillVectorizer', 'EmbeddingModelLoader', 'BatchProcessor']

### src/embedder/vectorizer.py
```python
import logging
from typing import List, Dict, Any, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import torch

logger = logging.getLogger(__name__)

class SkillVectorizer:
    """Generate embeddings for skills."""
    
    def __init__(self, model_name: str = None):
        self.model_name = model_name or "intfloat/multilingual-e5-base"
        self.model = None
        self.device = None
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialize the embedding model."""
        try:
            # Check for GPU availability
            if torch.cuda.is_available():
                self.device = 'cuda'
                logger.info(f"Using GPU for embeddings")
            else:
                self.device = 'cpu'
                logger.info(f"Using CPU for embeddings")
            
            # Load model
            logger.info(f"Loading embedding model: {self.model_name}")
            self.model = SentenceTransformer(self.model_name, device=self.device)
            
            # Get embedding dimension
            self.embedding_dim = self.model.get_sentence_embedding_dimension()
            logger.info(f"Model loaded. Embedding dimension: {self.embedding_dim}")
            
        except Exception as e:
            logger.error(f"Failed to initialize embedding model: {e}")
            raise
    
    def vectorize(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """Generate embeddings for a list of texts.
        
        Args:
            texts: List of texts to embed
            batch_size: Batch size for processing
            
        Returns:
            Array of embeddings
        """
        if not texts:
            return np.array([])
        
        try:
            # For E5 models, add instruction prefix
            if 'e5' in self.model_name.lower():
                texts = [f"query: {text}" for text in texts]
            
            # Generate embeddings
            embeddings = self.model.encode(
                texts,
                batch_size=batch_size,
                show_progress_bar=len(texts) > 100,
                convert_to_numpy=True,
                normalize_embeddings=True  # L2 normalization
            )
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Vectorization failed: {e}")
            raise
    
    def vectorize_single(self, text: str) -> np.ndarray:
        """Generate embedding for a single text.
        
        Args:
            text: Text to embed
            
        Returns:
            Embedding vector
        """
        return self.vectorize([text])[0]
    
    def compute_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """Compute cosine similarity between two embeddings.
        
        Args:
            embedding1: First embedding
            embedding2: Second embedding
            
        Returns:
            Cosine similarity score
        """
        # Assuming normalized embeddings, cosine similarity is just dot product
        return float(np.dot(embedding1, embedding2))
    
    def find_similar_skills(self, 
                          query_embedding: np.ndarray,
                          skill_embeddings: List[Dict[str, Any]],
                          top_k: int = 10,
                          threshold: float = 0.7) -> List[Dict[str, Any]]:
        """Find similar skills based on embeddings.
        
        Args:
            query_embedding: Query skill embedding
            skill_embeddings: List of skill embedding dictionaries
            top_k: Number of top results to return
            threshold: Minimum similarity threshold
            
        Returns:
            List of similar skills with scores
        """
        similarities = []
        
        for skill_data in skill_embeddings:
            embedding = skill_data['embedding']
            similarity = self.compute_similarity(query_embedding, embedding)
            
            if similarity >= threshold:
                similarities.append({
                    'skill': skill_data['skill_text'],
                    'similarity': similarity,
                    'embedding_id': skill_data.get('embedding_id')
                })
        
        # Sort by similarity
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        return similarities[:top_k]

### src/embedder/model_loader.py
```python
import logging
from typing import Dict, Any, Optional
import os
import json
from pathlib import Path
import torch
from transformers import AutoModel, AutoTokenizer
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)

class EmbeddingModelLoader:
    """Load and manage embedding models."""
    
    # Model configurations
    MODEL_CONFIGS = {
        'multilingual-e5-base': {
            'name': 'intfloat/multilingual-e5-base',
            'dimension': 768,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': 'query: '
        },
        'multilingual-e5-large': {
            'name': 'intfloat/multilingual-e5-large',
            'dimension': 1024,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': 'query: '
        },
        'beto': {
            'name': 'dccuchile/bert-base-spanish-wwm-cased',
            'dimension': 768,
            'max_length': 512,
            'type': 'transformers',
            'instruction_prefix': None
        },
        'labse': {
            'name': 'sentence-transformers/LaBSE',
            'dimension': 768,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': None
        },
        'multilingual-minilm': {
            'name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
            'dimension': 384,
            'max_length': 512,
            'type': 'sentence-transformers',
            'instruction_prefix': None
        }
    }
    
    def __init__(self, cache_dir: Optional[str] = None):
        self.cache_dir = cache_dir or "./data/cache/embeddings"
        os.makedirs(self.cache_dir, exist_ok=True)
        self.loaded_models = {}
    
    def load_model(self, model_key: str) -> Any:
        """Load an embedding model.
        
        Args:
            model_key: Key from MODEL_CONFIGS
            
        Returns:
            Loaded model
        """
        if model_key in self.loaded_models:
            logger.info(f"Model {model_key} already loaded")
            return self.loaded_models[model_key]
        
        if model_key not in self.MODEL_CONFIGS:
            raise ValueError(f"Unknown model key: {model_key}")
        
        config = self.MODEL_CONFIGS[model_key]
        
        try:
            if config['type'] == 'sentence-transformers':
                model = self._load_sentence_transformer(config)
            elif config['type'] == 'transformers':
                model = self._load_transformers_model(config)
            else:
                raise ValueError(f"Unknown model type: {config['type']}")
            
            self.loaded_models[model_key] = model
            logger.info(f"Successfully loaded model: {model_key}")
            
            return model
            
        except Exception as e:
            logger.error(f"Failed to load model {model_key}: {e}")
            raise
    
    def _load_sentence_transformer(self, config: Dict[str, Any]) -> SentenceTransformer:
        """Load a sentence-transformers model.
        
        Args:
            config: Model configuration
            
        Returns:
            Loaded model
        """
        model = SentenceTransformer(
            config['name'],
            cache_folder=self.cache_dir
        )
        
        # Verify dimension
        actual_dim = model.get_sentence_embedding_dimension()
        if actual_dim != config['dimension']:
            logger.warning(
                f"Model dimension mismatch: expected {config['dimension']}, "
                f"got {actual_dim}"
            )
        
        return model
    
    def _load_transformers_model(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Load a transformers model with tokenizer.
        
        Args:
            config: Model configuration
            
        Returns:
            Dictionary with model and tokenizer
        """
        model = AutoModel.from_pretrained(
            config['name'],
            cache_dir=self.cache_dir
        )
        
        tokenizer = AutoTokenizer.from_pretrained(
            config['name'],
            cache_dir=self.cache_dir
        )
        
        return {
            'model': model,
            'tokenizer': tokenizer,
            'config': config
        }
    
    def get_model_info(self, model_key: str) -> Dict[str, Any]:
        """Get information about a model.
        
        Args:
            model_key: Model key
            
        Returns:
            Model information
        """
        if model_key not in self.MODEL_CONFIGS:
            raise ValueError(f"Unknown model key: {model_key}")
        
        return self.MODEL_CONFIGS[model_key].copy()
    
    def list_available_models(self) -> Dict[str, Dict[str, Any]]:
        """List all available models.
        
        Returns:
            Dictionary of model configurations
        """
        return self.MODEL_CONFIGS.copy()
    
    def download_all_models(self):
        """Download all configured models."""
        logger.info("Downloading all configured embedding models...")
        
        for model_key in self.MODEL_CONFIGS:
            try:
                logger.info(f"Downloading {model_key}...")
                self.load_model(model_key)
            except Exception as e:
                logger.error(f"Failed to download {model_key}: {e}")
    
    def clear_cache(self):
        """Clear model cache."""
        import shutil
        
        if os.path.exists(self.cache_dir):
            shutil.rmtree(self.cache_dir)
            os.makedirs(self.cache_dir)
            logger.info("Model cache cleared")
        
        self.loaded_models.clear()

### src/embedder/batch_processor.py
```python
import logging
from typing import List, Dict, Any
import numpy as np
from database.operations import DatabaseOperations
from .vectorizer import SkillVectorizer
from config.settings import get_settings
import time
from tqdm import tqdm

logger = logging.getLogger(__name__)

class BatchProcessor:
    """Process skill embeddings in batches."""
    
    def __init__(self, model_name: str = None):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        self.vectorizer = SkillVectorizer(model_name)
        self.batch_size = self.settings.embedding_batch_size
    
    def process_all_skills(self) -> Dict[str, Any]:
        """Process all skills that need embeddings.
        
        Returns:
            Processing statistics
        """
        start_time = time.time()
        stats = {
            'skills_processed': 0,
            'embeddings_created': 0,
            'errors': 0,
            'processing_time': 0
        }
        
        try:
            # Get skills without embeddings
            skills = self.db_ops.get_unique_skills_for_embedding()
            logger.info(f"Found {len(skills)} skills without embeddings")
            
            if not skills:
                return stats
            
            # Process in batches
            for i in tqdm(range(0, len(skills), self.batch_size), desc="Embedding skills"):
                batch = skills[i:i + self.batch_size]
                
                try:
                    # Generate embeddings
                    embeddings = self.vectorizer.vectorize(batch)
                    
                    # Prepare data for database
                    embedding_data = []
                    for skill_text, embedding in zip(batch, embeddings):
                        embedding_data.append({
                            'skill_text': skill_text,
                            'embedding': embedding.tolist(),  # Convert to list for pgvector
                            'model_name': self.vectorizer.model_name,
                            'model_version': '1.0'
                        })
                    
                    # Save to database
                    self.db_ops.insert_skill_embeddings(embedding_data)
                    
                    stats['skills_processed'] += len(batch)
                    stats['embeddings_created'] += len(embedding_data)
                    
                except Exception as e:
                    logger.error(f"Error processing batch {i//self.batch_size}: {e}")
                    stats['errors'] += 1
            
            stats['processing_time'] = time.time() - start_time
            
            logger.info(
                f"Embedding complete: {stats['skills_processed']} skills processed, "
                f"{stats['embeddings_created']} embeddings created, "
                f"{stats['errors']} errors, "
                f"{stats['processing_time']:.2f}s"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"Embedding batch processing failed: {e}")
            raise
    
    def update_embeddings(self, force: bool = False) -> Dict[str, Any]:
        """Update embeddings for new or modified skills.
        
        Args:
            force: Force re-embedding of all skills
            
        Returns:
            Update statistics
        """
        if force:
            logger.warning("Force update requested - this will re-embed all skills")
            # Clear existing embeddings
            # Note: Implement this method in DatabaseOperations if needed
        
        return self.process_all_skills()
    
    def compute_similarity_matrix(self, skill_list: List[str] = None) -> np.ndarray:
        """Compute similarity matrix for skills.
        
        Args:
            skill_list: List of skills to compare (None for all)
            
        Returns:
            Similarity matrix
        """
        # Get embeddings from database
        all_embeddings = self.db_ops.get_all_embeddings()
        
        if skill_list:
            # Filter to requested skills
            embeddings_data = [
                e for e in all_embeddings 
                if e['skill_text'] in skill_list
            ]
        else:
            embeddings_data = all_embeddings
        
        if not embeddings_data:
            return np.array([])
        
        # Extract embedding vectors
        embeddings = np.array([e['embedding'] for e in embeddings_data])
        
        # Compute similarity matrix
        # Since embeddings are normalized, cosine similarity is just dot product
        similarity_matrix = np.dot(embeddings, embeddings.T)
        
        return similarity_matrix
    
    def find_duplicate_skills(self, threshold: float = 0.95) -> List[Dict[str, Any]]:
        """Find potential duplicate skills based on embedding similarity.
        
        Args:
            threshold: Similarity threshold for duplicates
            
        Returns:
            List of potential duplicates
        """
        all_embeddings = self.db_ops.get_all_embeddings()
        
        if len(all_embeddings) < 2:
            return []
        
        duplicates = []
        
        # Compare all pairs
        for i in range(len(all_embeddings)):
            for j in range(i + 1, len(all_embeddings)):
                skill1 = all_embeddings[i]
                skill2 = all_embeddings[j]
                
                similarity = self.vectorizer.compute_similarity(
                    np.array(skill1['embedding']),
                    np.array(skill2['embedding'])
                )
                
                if similarity >= threshold:
                    duplicates.append({
                        'skill1': skill1['skill_text'],
                        'skill2': skill2['skill_text'],
                        'similarity': similarity
                    })
        
        # Sort by similarity
        duplicates.sort(key=lambda x: x['similarity'], reverse=True)
        
        logger.info(f"Found {len(duplicates)} potential duplicate skill pairs")
        
        return duplicates
    
    def get_skill_recommendations(self, 
                                job_skills: List[str],
                                top_k: int = 10) -> List[Dict[str, Any]]:
        """Get skill recommendations based on current job skills.
        
        Args:
            job_skills: Current skills in job
            top_k: Number of recommendations
            
        Returns:
            List of recommended skills with scores
        """
        # Get embeddings for input skills
        input_embeddings = self.vectorizer.vectorize(job_skills)
        
        # Average the embeddings to get job profile
        job_profile = np.mean(input_embeddings, axis=0)
        
        # Get all skill embeddings
        all_embeddings = self.db_ops.get_all_embeddings()
        
        # Find similar skills
        recommendations = []
        
        for skill_data in all_embeddings:
            # Skip if already in job skills
            if skill_data['skill_text'] in job_skills:
                continue
            
            similarity = self.vectorizer.compute_similarity(
                job_profile,
                np.array(skill_data['embedding'])
            )
            
            recommendations.append({
                'skill': skill_data['skill_text'],
                'score': similarity
            })
        
        # Sort and return top K
        recommendations.sort(key=lambda x: x['score'], reverse=True)
        
        return recommendations[:top_k]

---

## 8. Analyzer Module Files

### src/analyzer/__init__.py
```python
from .clustering import SkillClusterer
from .dimension_reducer import DimensionReducer
from .report_generator import ReportGenerator
from .visualizations import VisualizationGenerator

__all__ = ['SkillClusterer', 'DimensionReducer', 'ReportGenerator', 'VisualizationGenerator']

### src/analyzer/clustering.py
```python
import logging
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from sklearn.cluster import HDBSCAN, KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score
import pandas as pd
from database.operations import DatabaseOperations
from config.settings import get_settings

logger = logging.getLogger(__name__)

class SkillClusterer:
    """Perform clustering on skill embeddings."""
    
    def __init__(self):
        self.settings = get_settings()
        self.db_ops = DatabaseOperations()
        self.clustering_method = 'hdbscan'
    
    def cluster_skills(self, 
                      embeddings: np.ndarray,
                      skill_texts: List[str],
                      method: str = 'hdbscan',
                      **kwargs) -> Dict[str, Any]:
        """Cluster skills based on embeddings.
        
        Args:
            embeddings: Skill embedding matrix
            skill_texts: List of skill texts
            method: Clustering method ('hdbscan' or 'kmeans')
            **kwargs: Additional parameters for clustering
            
        Returns:
            Clustering results
        """
        if len(embeddings) < 2:
            logger.warning("Not enough data points for clustering")
            return {}
        
        logger.info(f"Clustering {len(embeddings)} skills using {method}")
        
        if method == 'hdbscan':
            results = self._cluster_hdbscan(embeddings, skill_texts, **kwargs)
        elif method == 'kmeans':
            results = self._cluster_kmeans(embeddings, skill_texts, **kwargs)
        else:
            raise ValueError(f"Unknown clustering method: {method}")
        
        # Calculate metrics
        results['metrics'] = self._calculate_metrics(embeddings, results['labels'])
        
        # Characterize clusters
        results['cluster_info'] = self._characterize_clusters(
            results['labels'],
            skill_texts
        )
        
        return results
    
    def _cluster_hdbscan(self, 
                        embeddings: np.ndarray,
                        skill_texts: List[str],
                        min_cluster_size: int = None,
                        min_samples: int = None) -> Dict[str, Any]:
        """Perform HDBSCAN clustering.
        
        Args:
            embeddings: Embedding matrix
            skill_texts: Skill texts
            min_cluster_size: Minimum cluster size
            min_samples: Minimum samples for core points
            
        Returns:
            Clustering results
        """
        # Use settings if not provided
        min_cluster_size = min_cluster_size or self.settings.cluster_min_size
        min_samples = min_samples or self.settings.cluster_min_samples
        
        # Perform clustering
        clusterer = HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            metric='euclidean',
            cluster_selection_method='eom',
            prediction_data=True
        )
        
        labels = clusterer.fit_predict(embeddings)
        
        # Get cluster persistence (stability)
        cluster_persistence = clusterer.cluster_persistence_
        
        results = {
            'method': 'hdbscan',
            'labels': labels,
            'n_clusters': len(set(labels)) - (1 if -1 in labels else 0),
            'n_noise': list(labels).count(-1),
            'parameters': {
                'min_cluster_size': min_cluster_size,
                'min_samples': min_samples
            },
            'cluster_persistence': cluster_persistence,
            'clusterer': clusterer
        }
        
        logger.info(
            f"HDBSCAN found {results['n_clusters']} clusters "
            f"with {results['n_noise']} noise points"
        )
        
        return results
    
    def _cluster_kmeans(self,
                       embeddings: np.ndarray,
                       skill_texts: List[str],
                       n_clusters: int = 20) -> Dict[str, Any]:
        """Perform K-means clustering.
        
        Args:
            embeddings: Embedding matrix
            skill_texts: Skill texts
            n_clusters: Number of clusters
            
        Returns:
            Clustering results
        """
        # Perform clustering
        clusterer = KMeans(
            n_clusters=n_clusters,
            n_init=10,
            max_iter=300,
            random_state=42
        )
        
        labels = clusterer.fit_predict(embeddings)
        
        results = {
            'method': 'kmeans',
            'labels': labels,
            'n_clusters': n_clusters,
            'n_noise': 0,
            'parameters': {
                'n_clusters': n_clusters
            },
            'cluster_centers': clusterer.cluster_centers_,
            'inertia': clusterer.inertia_,
            'clusterer': clusterer
        }
        
        logger.info(f"K-means created {n_clusters} clusters")
        
        return results
    
    def _calculate_metrics(self, embeddings: np.ndarray, labels: np.ndarray) -> Dict[str, float]:
        """Calculate clustering quality metrics.
        
        Args:
            embeddings: Embedding matrix
            labels: Cluster labels
            
        Returns:
            Dictionary of metrics
        """
        metrics = {}
        
        # Remove noise points for metrics
        mask = labels >= 0
        if np.sum(mask) < 2:
            logger.warning("Not enough clustered points for metrics")
            return metrics
        
        try:
            # Silhouette score
            metrics['silhouette_score'] = silhouette_score(
                embeddings[mask],
                labels[mask]
            )
            
            # Davies-Bouldin index (lower is better)
            metrics['davies_bouldin_index'] = davies_bouldin_score(
                embeddings[mask],
                labels[mask]
            )
            
            # Cluster statistics
            unique_labels = np.unique(labels[labels >= 0])
            cluster_sizes = [np.sum(labels == label) for label in unique_labels]
            
            metrics['avg_cluster_size'] = np.mean(cluster_sizes)
            metrics['std_cluster_size'] = np.std(cluster_sizes)
            metrics['min_cluster_size'] = np.min(cluster_sizes)
            metrics['max_cluster_size'] = np.max(cluster_sizes)
            
        except Exception as e:
            logger.error(f"Error calculating metrics: {e}")
        
        return metrics
    
    def _characterize_clusters(self, 
                             labels: np.ndarray,
                             skill_texts: List[str]) -> List[Dict[str, Any]]:
        """Characterize each cluster.
        
        Args:
            labels: Cluster labels
            skill_texts: List of skill texts
            
        Returns:
            List of cluster characteristics
        """
        cluster_info = []
        
        # Create DataFrame for easier analysis
        df = pd.DataFrame({
            'skill': skill_texts,
            'cluster': labels
        })
        
        # Analyze each cluster
        unique_labels = sorted(set(labels))
        
        for label in unique_labels:
            if label == -1:  # Skip noise cluster
                continue
            
            cluster_skills = df[df['cluster'] == label]['skill'].tolist()
            
            # Get most common skills
            skill_counts = pd.Series(cluster_skills).value_counts()
            
            cluster_data = {
                'cluster_id': int(label),
                'size': len(cluster_skills),
                'top_skills': skill_counts.head(10).to_dict(),
                'all_skills': cluster_skills,
                'label': self._generate_cluster_label(skill_counts.head(5).index.tolist())
            }
            
            cluster_info.append(cluster_data)
        
        # Sort by size
        cluster_info.sort(key=lambda x: x['size'], reverse=True)
        
        return cluster_info
    
    def _generate_cluster_label(self, top_skills: List[str]) -> str:
        """Generate a descriptive label for a cluster.
        
        Args:
            top_skills: Top skills in cluster
            
        Returns:
            Cluster label
        """
        # Simple heuristic-based labeling
        skill_lower = [s.lower() for s in top_skills]
        
        if any('frontend' in s or 'react' in s or 'angular' in s or 'vue' in s for s in skill_lower):
            return "Frontend Development"
        elif any('backend' in s or 'node' in s or 'django' in s or 'spring' in s for s in skill_lower):
            return "Backend Development"
        elif any('data' in s or 'analytics' in s or 'sql' in s for s in skill_lower):
            return "Data & Analytics"
        elif any('machine learning' in s or 'ml' in s or 'ai' in s for s in skill_lower):
            return "Machine Learning & AI"
        elif any('devops' in s or 'docker' in s or 'kubernetes' in s for s in skill_lower):
            return "DevOps & Infrastructure"
        elif any('mobile' in s or 'android' in s or 'ios' in s for s in skill_lower):
            return "Mobile Development"
        elif any('cloud' in s or 'aws' in s or 'azure' in s for s in skill_lower):
            return "Cloud Computing"
        else:
            # Use most common skill as label
            return f"{top_skills[0]} & Related"
    
    def run_clustering_pipeline(self) -> Dict[str, Any]:
        """Run complete clustering pipeline on all skills.
        
        Returns:
            Complete clustering results
        """
        # Get all embeddings
        embeddings_data = self.db_ops.get_all_embeddings()
        
        if not embeddings_data:
            logger.warning("No embeddings found for clustering")
            return {}
        
        # Extract data
        skill_texts = [e['skill_text'] for e in embeddings_data]
        embeddings = np.array([e['embedding'] for e in embeddings_data])
        
        logger.info(f"Running clustering on {len(embeddings)} skills")
        
        # Run clustering
        results = self.cluster_skills(embeddings, skill_texts)
        
        # Save results to database
        self.db_ops.save_analysis_results(
            analysis_type='clustering',
            results={
                'n_clusters': results['n_clusters'],
                'n_noise': results['n_noise'],
                'metrics': results['metrics'],
                'cluster_info': results['cluster_info']
            },
            parameters=results['parameters']
        )
        
        return results