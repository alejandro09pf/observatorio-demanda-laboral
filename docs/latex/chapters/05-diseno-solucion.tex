\chapter{DISEÑO DE LA SOLUCIÓN}

A partir de los requerimientos y restricciones identificados en el capítulo anterior, este capítulo presenta el diseño de la solución propuesta. Se describen los fundamentos teóricos de las técnicas seleccionadas, las pruebas experimentales que validaron su viabilidad, y la arquitectura resultante del sistema.

\section{Antecedentes Teóricos}

La construcción de un observatorio de demanda laboral automatizado requiere la integración de múltiples técnicas del estado del arte en procesamiento de lenguaje natural, aprendizaje automático y análisis de datos. Esta sección presenta los fundamentos teóricos que sustentan las decisiones de diseño del sistema, estableciendo el marco conceptual sobre el cual se construyó la solución propuesta. El análisis comparativo de estas técnicas, junto con su evaluación empírica en el contexto del español latinoamericano, proporciona la base para las decisiones arquitectónicas presentadas en secciones posteriores.

\subsection{Reconocimiento de Entidades Nombradas (NER)}

El Reconocimiento de Entidades Nombradas es una tarea fundamental de NLP que consiste en localizar y clasificar entidades en texto dentro de categorías predefinidas \cite{nadeau2007}. Los sistemas NER modernos se basan en modelos de aprendizaje supervisado, particularmente arquitecturas basadas en transformers \cite{devlin2019}. Para el dominio de habilidades técnicas, NER presenta ventajas teóricas significativas: alta precisión para entidades conocidas en datasets balanceados ($>$90\%), contextualización bidireccional para desambiguación, y eficiencia computacional con latencias de milisegundos por documento \cite{zhang2018}. Sin embargo, enfrenta limitaciones críticas: dependencia del vocabulario de entrenamiento, baja cobertura en dominios especializados, y la incapacidad de inferir habilidades implícitas no mencionadas explícitamente en el texto \cite{canete2020}.

En este proyecto se adoptó un enfoque híbrido que combina el modelo base \texttt{es\_core\_news\_lg} de spaCy con un EntityRuler personalizado poblado con 666 patrones de habilidades técnicas de la taxonomía ESCO, permitiendo reconocimiento directo de terminología técnica no presente en el modelo pre-entrenado \cite{fareri2021}. Los resultados experimentales revelaron que NER en este contexto específico presentó desempeño mixto: si bien aporta cobertura adicional de skills contextuales (mejora de +6.91pp F1 Pre-ESCO), introduce ruido que degrada precisión Post-ESCO (-6.64pp versus configuración Regex-Only). A pesar de este trade-off, NER se mantuvo en Pipeline A dado que incrementa recall detectando menciones contextuales que patrones deterministas omiten, como se documenta en la sección de pruebas de modelos.

\subsection{Extracción basada en Expresiones Regulares}

Las expresiones regulares (regex) son patrones de búsqueda basados en lenguajes formales que permiten identificar secuencias de caracteres con estructuras específicas \cite{friedl2006}. En el contexto de extracción de habilidades técnicas, regex fue particularmente efectiva para tecnologías con nomenclaturas estandarizadas: versiones numeradas (``Python 3.x'', ``Java 8+''), frameworks con sufijos convencionales (``.js'', ``SQL''), y acrónimos técnicos (``REST API'', ``CI/CD''). La implementación final consistió en 548 patrones organizados en 18 categorías técnicas (lenguajes de programación, frameworks, bases de datos, plataformas cloud, DevOps, ciencia de datos, entre otras). Sus ventajas incluyeron precisión determinística del 100\% en patrones bien definidos, transparencia y auditabilidad, velocidad extrema (microsegundos por documento), y ausencia de requisitos de entrenamiento. Las limitaciones principales fueron fragilidad ante variaciones ortográficas, mantenimiento manual intensivo de patrones, y ausencia de comprensión contextual \cite{chiticariu2013}.

\subsection{Extracción basada en Modelos de Lenguaje Grandes (LLMs)}

Los Large Language Models representaron un cambio de paradigma en NLP, basándose en arquitecturas transformer pre-entrenadas sobre corpus masivos mediante objetivos de modelado de lenguaje \cite{brown2020, touvron2023}. A diferencia de NER y regex, los LLMs poseen capacidades de razonamiento contextual que permiten: inferencia de habilidades implícitas (deducir que un ``Científico de Datos'' requiere estadística y Python aunque no se mencione), normalización semántica automática (identificar que ``React'', ``React.js'' y ``ReactJS'' son equivalentes), desambiguación contextual, adaptación al lenguaje informal y ``Spanglish'', y razonamiento explicable mediante prompt engineering \cite{zhang2022, wei2023, vilares2016}.

Sin embargo, presentaron limitaciones significativas: latencia 15-25x mayor que Pipeline A (tiempo típico de procesamiento de 15-25s por documento vs. 1s), no-determinismo con temperatura $>$ 0, alucinaciones que generaron habilidades incorrectas, alto costo computacional (GPU integrado o APIs de pago), y sesgo lingüístico hacia el inglés con rendimiento degradado en español técnico \cite{elazar2023, ji2023}.

\subsection{Justificación del Enfoque Dual}

La Tabla \ref{tab:comparacion-tecnicas-extraccion} presenta una comparación sistemática de las tres técnicas según criterios relevantes para el observatorio.

\begin{table}[H]
\centering
\caption{Comparación de Técnicas de Extracción de Habilidades}
\label{tab:comparacion-tecnicas-extraccion}
\begin{tabular}{|p{3cm}|p{3.5cm}|p{3.5cm}|p{3.5cm}|}
\hline
\textbf{Criterio} & \textbf{Pipeline A (NER+Regex)} & \textbf{Regex Solo} & \textbf{LLMs (Gemma 3 4B)} \\
\hline
Precision Post-ESCO & 65.50\% & 86.36\% & \textbf{89.25\%} \\
\hline
Recall Post-ESCO & \textbf{81.25\%} & 73.08\% & 79.81\% \\
\hline
F1-Score Pre-ESCO & 24.98\% & 18.07\% & \textbf{46.23\%} \\
\hline
F1-Score Post-ESCO & 72.53\% & 79.17\% & \textbf{84.26\%} \\
\hline
Inferencia implícita & No soportado & No soportado & Sí (capacidad arquitectónica) \\
\hline
Latencia & 0.97 s/job & $\sim$0.32 s/job & 18s (P50), 15-25s (P25-P75) \\
\hline
Costo computacional & Bajo (CPU) & Muy bajo (CPU) & Alto (GPU integrado) \\
\hline
Mantenimiento & Alto (filtros) & Alto (patrones) & Bajo (prompt eng.) \\
\hline
Español LATAM & Medio & Alto & Alto \\
\hline
\end{tabular}
\end{table}

\textit{Nota: Los valores presentados en la Tabla \ref{tab:comparacion-tecnicas-extraccion} fueron obtenidos mediante evaluación experimental sobre el conjunto de datos gold standard de 300 ofertas laborales (100 por país: CO, MX, AR). El F1-Score Post-ESCO refleja el rendimiento después de normalización mediante el mismo código de mapeo ESCO para comparación justa. Ambos pipelines operan de forma independiente sobre el mismo texto de entrada.}

Dado que ninguna técnica individual satisface todos los requisitos, se implementaron dos pipelines en paralelo para comparación empírica: Pipeline A (NER + Regex) diseñado para procesamiento escalable del corpus completo mediante reglas determinísticas; y Pipeline B (LLMs) implementado experimentalmente sobre gold standard para evaluar enriquecimiento semántico y detección de habilidades implícitas. Ambos pipelines procesan las mismas ofertas de forma independiente, permitiendo comparación directa de sus capacidades. La evaluación experimental determinó que Pipeline B supera cuantitativamente a Pipeline A pero con mayor costo computacional, validando la necesidad de arquitectura dual que permita seleccionar el pipeline óptimo según el caso de uso: Pipeline A para análisis masivo y Pipeline B para validación cualitativa o detección de skills emergentes (evaluación comparativa detallada en el capítulo de Resultados) \cite{li2023}.

\subsection{Modelos de Lenguaje Grandes: Selección y Evaluación}

Habiendo establecido que el Pipeline B requirió capacidades de LLMs para inferencia de habilidades implícitas y normalización semántica, el siguiente desafío consistió en seleccionar un modelo específico que balanceara rendimiento lingüístico con restricciones computacionales del proyecto.

\textbf{Justificación de la Selección:}

La selección de modelos LLM se fundamentó en cuatro criterios técnicos principales. Primero, se priorizaron modelos open-source dada la naturaleza académica del proyecto y la necesidad de reproducibilidad científica. Segundo, el estado del arte en extracción de habilidades con LLMs estableció a Llama 3 como referente base, con estudios demostrando efectividad de LLMs fine-tuned alcanzando F1-Score de 64.8\% sobre SkillSpan dataset \cite{herandi2024} y validaciones de Llama-3 8B para contextos multilingües europeos \cite{kavas2024}. Adicionalmente, se documentó que el prompting con LLMs requiere prompt engineering cuidadoso para tareas de extracción estructurada \cite{nguyen2024}. Tercero, el rango de parámetros 3-4B se determinó mediante restricción de hardware disponible: el procesamiento local con GPU integrado (MacBook Air M4 con 16 GB de memoria unificada y aceleración Metal) permite ejecutar modelos de hasta 4.3B parámetros con cuantización Q4\_K\_M (4 bits por peso), requiriendo aproximadamente 2.4-2.8 GB de memoria unificada según la fórmula:

\begin{equation}
\text{Memoria} \approx \frac{\text{Parámetros} \times 4 \text{ bits}}{8 \text{ bits/byte}} + \text{overhead KV-cache}
\end{equation}

donde el overhead del KV-cache representa típicamente 15-20\% adicional. Modelos de 7B+ parámetros excederían los 5 GB de memoria unificada con cuantización Q4, comprometiendo estabilidad del sistema durante inferencia. Cuarto, se consultaron rankings especializados (Hugging Face Open LLM Leaderboard filtrado por español, LMArena) para identificar candidatos con desempeño reportado en tareas de NLP multilingüe.

\textbf{Candidatos Evaluados:}

Se identificaron cuatro modelos LLM de código abierto como candidatos principales, evaluados según criterios de costo computacional, rendimiento en español latinoamericano, soporte multilingüe y capacidad de despliegue local sin dependencias de APIs comerciales. Dado que el Pipeline B fue utilizado únicamente para comparación experimental contra el gold standard, los modelos se ejecutaron localmente con GPU integrado (aceleración Metal en Apple Silicon) para validar su viabilidad técnica en contextos académicos con recursos limitados.

Gemma 3 4B es un modelo ligero desarrollado por Google basado en la arquitectura Gemini, con 4 mil millones de parámetros optimizado para despliegue eficiente en hardware limitado. El modelo permite ejecución con GPU integrado mediante cuantización Q4 (tamaño compacto), cuenta con licencia permisiva Gemma para uso académico y comercial, ofrece tokenización eficiente heredada de la familia Gemini, y provee soporte multilingüe con cobertura de español latinoamericano. Sin embargo, presenta menor capacidad de razonamiento complejo comparado con modelos más grandes, vocabulario técnico potencialmente limitado debido al tamaño reducido, y documentación aún en desarrollo al ser un modelo relativamente nuevo.

Llama 3.2 3B es la versión compacta de la familia LLaMA 3 de Meta AI, entrenado sobre un corpus masivo multilingüe con enfoque en eficiencia y 3 mil millones de parámetros que representan el balance extremo entre rendimiento y recursos computacionales. El modelo requiere memoria mínima (3-4 GB con cuantización Q4), implementa arquitectura optimizada con Grouped-Query Attention para inferencia rápida, ofrece tokenización eficiente de la familia LLaMA 3, cuenta con licencia LLaMA 3 Community License permisiva para proyectos académicos, y proporciona latencia reducida ideal para procesamiento batch. No obstante, presenta capacidad de razonamiento más limitada que modelos grandes, posible degradación en tareas complejas de inferencia, y menor cobertura de vocabulario técnico especializado.

Qwen 2.5 3B es parte de la familia Qwen (Qianwen) de Alibaba Cloud optimizada para multilingüismo con énfasis en idiomas asiáticos y europeos, con 3 mil millones de parámetros que destacan por su arquitectura de atención eficiente. El modelo ofrece precisión superior en tareas de extracción estructurada, tokenización eficiente multilingüe, requisitos moderados de memoria (3-4 GB con Q4), y licencia Apache 2.0 permisiva. Por otro lado, muestra conservadurismo excesivo con recall bajo en contextos ambiguos, menor cobertura de vocabulario técnico latinoamericano, y tendencia a omitir skills implícitas.

Phi-3.5 Mini de Microsoft Research es un modelo ultra-compacto de 3.8 mil millones de parámetros entrenado con datos sintéticos de alta calidad. El modelo implementa arquitectura extremadamente eficiente con baja latencia, demuestra capacidad de razonamiento avanzado para su tamaño, provee soporte multilingüe, y cuenta con licencia MIT. Sin embargo, presenta inconsistencias en generación de JSON estructurado causando pérdidas durante parsing, menor robustez en instrucciones complejas comparado con modelos más grandes, y vocabulario técnico limitado en español.

\textbf{Evaluación Comparativa:}

Se evaluaron los cuatro modelos candidatos (Gemma 3 4B, Llama 3.2 3B, Qwen 2.5 3B, Phi-3.5 Mini) mediante un experimento comparativo inicial sobre 10 ofertas laborales. La evaluación midió Precision, Recall, F1-score para habilidades explícitas e implícitas, latencia promedio, throughput y presencia de alucinaciones. Basándose en estos resultados preliminares, Gemma 3 4B fue seleccionado como ganador y posteriormente procesó el conjunto completo de 300 ofertas laborales del gold standard (cuyo protocolo de construcción se describe detalladamente en el capítulo de Desarrollo, sección Protocolo de Anotación Manual) para validación exhaustiva.

Los modelos fueron evaluados con prompt engineering específico para español latinoamericano, incluyendo instrucciones para manejar ``Spanglish'', ejemplos contextuales con ofertas reales, normalización con taxonomía ESCO, y solicitud de formato JSON estructurado. Los parámetros de inferencia utilizados fueron: temperatura 0.3 (balance entre determinismo y creatividad), max\_tokens 3072, y system prompt estandarizado.

La Tabla \ref{tab:comparacion-llms} resume la comparación cuantitativa entre los cuatro modelos evaluados.

\begin{table}[H]
\centering
\caption{Comparación de Large Language Models para Extracción de Habilidades}
\label{tab:comparacion-llms}
\begin{tabular}{|p{3.5cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
\hline
\textbf{Criterio} & \textbf{Gemma 3 4B} & \textbf{Llama 3.2 3B} & \textbf{Qwen 2.5 3B} & \textbf{Phi-3.5 Mini} \\
\hline
Parámetros & 4B & 3B & 3B & 3.8B \\
\hline
F1 Pre-ESCO & \textbf{46.23\%} & 39.7\% & 38.9\% & 35.2\% \\
\hline
Jobs evaluados & 299 & 10 & 10 & 10 \\
\hline
Skills/job prom. & 27.8 & 24.7 & 11.2 & 15.8 \\
\hline
Alucinaciones & \textbf{0} & 7 (DS) & 0 & 0 \\
\hline
JSON válido & 99\% & 90\% & 95\% & 60\% \\
\hline
Latencia (GPU) & 18s (P50) & 15.24s & N/D & N/D \\
\hline
Memoria (Q4) & 4-6 GB & 3-4 GB & 3-4 GB & 3-4 GB \\
\hline
Licencia & Gemma & LLaMA 3 CL & Apache 2.0 & MIT \\
\hline
\end{tabular}
\end{table}

\textit{Nota: Los cuatro modelos fueron evaluados comparativamente sobre 10 ofertas laborales para selección inicial. Gemma 3 4B demostró mejor rendimiento en la evaluación preliminar y fue seleccionado para procesar el conjunto completo de 300 ofertas del gold standard, logrando 299/300 ofertas procesadas exitosamente (99.3\% cobertura; 2 jobs con mode collapse). Llama, Qwen y Phi procesaron solo las 10 ofertas iniciales, siendo descartados por menor rendimiento y problemas técnicos. F1 Pre-ESCO mide rendimiento antes de normalización taxonómica. Alucinaciones (DS) indica skills de Data Science extraídas erróneamente. JSON válido mide porcentaje de respuestas con formato estructurado correcto.}

Los resultados experimentales determinaron la selección definitiva de Gemma 3 4B para el Pipeline B, fundamentado en cinco hallazgos principales: precisión superior en la evaluación de 10 ofertas preliminares, ausencia de alucinaciones sistemáticas detectadas en otros modelos, cobertura efectiva de habilidades implícitas validada en las 300 ofertas completas, captura de tecnologías emergentes no presentes en ESCO, y robustez operacional con tasa de éxito superior al 99\%. La validación manual confirmó que Gemma genera outputs limpios sin fabricar skills, mientras que alternativas como Llama 3.2 presentaron alucinaciones sistemáticas en ofertas técnicas (métricas cuantitativas detalladas en el capítulo de Resultados).

El trade-off de latencia fue considerado aceptable para procesamiento batch, donde mayor tiempo de inferencia se compensa con eliminación de alucinaciones y mayor calidad semántica. La arquitectura final despliega Gemma 3 4B con cuantización Q4 (4-6 GB memoria unificada), temperatura 0.3, y max\_tokens 3072, ejecutándose localmente sin dependencias de APIs comerciales y garantizando privacidad de datos laborales sensibles.

\subsection{Embeddings Semánticos y Búsqueda de Similitud}

Habiendo establecido las técnicas de extracción de habilidades (NER, Regex, LLMs) y el modelo para procesamiento lingüístico (Gemma), el siguiente desafío arquitectónico consistió en normalizar las habilidades extraídas contra una taxonomía de referencia y agruparlas para descubrir patrones emergentes. Esta normalización es crítica para eliminar variantes sintácticas (``React'', ``React.js'', ``ReactJS''), mapear términos coloquiales a conceptos ESCO formales, y habilitar análisis comparativo entre países y portales.

En el observatorio, los embeddings semánticos se utilizan para el agrupamiento de habilidades y descubrimiento de perfiles emergentes mediante clustering. El modelo \texttt{intfloat/multilingual-\allowbreak e5-base} fue seleccionado por su soporte multilingüe nativo (768D, 100 idiomas, entrenado con contrastive learning) \cite{wang2024}, normalización L2 integrada, tamaño compacto (278M parámetros ejecutables en CPU $<$100ms/batch), y licencia MIT.

\textbf{Intento de Uso para Matching ESCO:}

Inicialmente se intentó utilizar embeddings semánticos para normalización de habilidades extraídas contra taxonomía ESCO mediante búsqueda de similitud coseno (Layer 3 del matcher). Sin embargo, la evaluación experimental con 15 habilidades técnicas agregadas manualmente a ESCO reveló limitaciones críticas del modelo E5 para vocabulario técnico especializado. Con threshold de similitud coseno = 0.75, se obtuvo tasa de matches correctos de 6.7\% (1/15) y falsos positivos de 93.3\% (14/15). Ejemplos de mapeos incorrectos: ``React'' se relacionó con ``neoplasia'' (0.828), ``Docker'' con ``Facebook'' (0.825), y ``Machine Learning'' con ``gas natural'' (0.825). El análisis de causa raíz identificó tres factores: entrenamiento en lenguaje natural vs. vocabulario técnico, confusión entre brand names y palabras comunes, y contaminación del espacio vectorial por 13,939 habilidades ESCO de dominios no técnicos.

La evaluación de thresholds demostró que no existe un valor que balancee precision y recall: thresholds bajos ($<$0.80) generan falsos positivos críticos, mientras que thresholds altos ($\geq$0.92) excluyen matches exactos. Con base en esta evidencia, se tomó la decisión de deshabilitar la capa de búsqueda semántica (Layer 3), operando el sistema de matching ESCO con estrategia de dos capas: Layer 1 (Exact Match) mediante búsqueda SQL con confidence = 1.00, y Layer 2 (Fuzzy Match) con threshold = 0.92 para capturar variantes ortográficas. Los embeddings E5 se mantienen únicamente para clustering de habilidades, donde han demostrado efectividad en capturar relaciones semánticas para agrupamiento no supervisado.

\subsection{Técnicas de Clustering No Supervisado}

Una vez que las habilidades fueron extraídas, normalizadas y representadas como embeddings vectoriales de 768 dimensiones, el objetivo final del observatorio consistió en descubrir patrones y perfiles laborales emergentes sin categorías predefinidas. El sistema integró dos algoritmos complementarios para proyección dimensional y agrupamiento basado en densidad, cuya efectividad para segmentación de ofertas laborales ha sido validada en estudios previos mediante combinación de técnicas de NLP y clustering \cite{lukauskas2023}.

\textbf{Configuración de UMAP:}

Para el observatorio se utilizó UMAP con los siguientes parámetros: \texttt{n\_neighbors=15} (balance entre estructura local y global), \texttt{min\_dist=0.1} (compactación de puntos cercanos), y \texttt{metric='cosine'} (métrica apropiada para embeddings normalizados). UMAP redujo vectores de 768 dimensiones a 2-3 dimensiones visualizables manteniendo propiedades semánticas.

\textbf{Configuración de HDBSCAN:}

Se seleccionó HDBSCAN sobre K-Means dado que el número de perfiles emergentes era desconocido a priori. Los parámetros de producción fueron: \texttt{min\_cluster\_size=12} (tamaño mínimo de clúster válido para interpretabilidad, determinado tras 70+ experimentos), \texttt{min\_samples=3} (puntos mínimos para núcleo de densidad), \texttt{cluster\_selection\_method='eom'} (Excess of Mass para clusters más estables), y \texttt{metric='euclidean'} (post-reducción dimensional con UMAP).

\subsection{Taxonomías de Habilidades Laborales}

Si bien las técnicas de extracción, embeddings y clustering constituyeron el núcleo analítico del observatorio, todas estas operaciones dependieron de un componente fundamental: una taxonomía de referencia que permitiera normalizar las habilidades extraídas del texto crudo y mapearlas a conceptos estandarizados. Sin esta normalización, habilidades equivalentes como ``React'', ``React.js'' y ``ReactJS'' hubieran sido tratadas como entidades distintas, fragmentando el análisis y degradando la calidad del clustering. La selección de la taxonomía apropiada requirió balancear cobertura, actualización, soporte multilingüe y accesibilidad.

Se evaluaron tres alternativas principales para la normalización de habilidades. La primera, O*NET (Occupational Information Network), es la taxonomía del Departamento de Trabajo de Estados Unidos que contiene más de 1,000 ocupaciones y 20,000 habilidades organizadas jerárquicamente con actualización periódica y datos salariales asociados. Esta taxonomía ha sido ampliamente utilizada en investigación de mercado laboral y extracción de habilidades \cite{zhang2022}. Sin embargo, presenta limitaciones significativas para el contexto latinoamericano: enfoque centrado en el mercado estadounidense, escasa cobertura de tecnologías emergentes populares en la región, y ausencia de soporte multilingüe nativo que requeriría traducción automática con riesgo de pérdida semántica.

La segunda alternativa, ESCO (European Skills, Competences, Qualifications and Occupations), es la taxonomía oficial de la Unión Europea que provee más de 13,000 habilidades, 3,000 ocupaciones, y soporte para 27 idiomas incluyendo español e inglés. ESCO ha sido adoptada extensivamente en sistemas de extracción de habilidades mediante LLMs y grafos de conocimiento \cite{kavargyris2025, kavas2025}, demostrando efectividad para matching de vacantes en contextos multilingües. La taxonomía ofrece etiquetas nativas en español que eliminan necesidad de traducción, cobertura amplia de habilidades tecnológicas organizadas mediante estructura ontológica con URIs únicos, y respaldo institucional de la Comisión Europea que garantiza mantenimiento a largo plazo. La versión v1.1.0 utilizada en este proyecto, publicada en 2021-2022, contiene 13,939 conceptos de habilidades con relaciones semánticas que facilitan navegación y expansión taxonómica. No obstante, ESCO presenta actualización menos frecuente que el ritmo del mercado tecnológico, resultando en menor cobertura de frameworks JavaScript modernos y librerías emergentes que aparecen constantemente en ofertas latinoamericanas.

La tercera categoría evaluada corresponde a taxonomías propietarias como LinkedIn Skills y Burning Glass Technologies, que se actualizan con mayor frecuencia capturando tecnologías emergentes casi en tiempo real. Sin embargo, estas alternativas presentan limitaciones críticas para investigación académica: acceso restringido mediante APIs de pago con costos prohibitivos para proyectos sin financiamiento, licencias restrictivas que impiden publicación de resultados derivados, y falta de transparencia metodológica sobre criterios de inclusión y relaciones entre conceptos.

Tras evaluar estas alternativas, se seleccionó ESCO v1.1.0 como base taxonómica fundamentado en cuatro razones principales que priorizan reproducibilidad y accesibilidad. Primero, el soporte multilingüe nativo en español e inglés eliminó la necesidad de traducción automática que hubiera introducido errores y ambigüedad semántica en vocabulario técnico especializado. Segundo, la licencia Creative Commons BY 4.0 permitió uso académico y potencial comercial futuro sin restricciones legales ni costos de licenciamiento. Tercero, la estructura ontológica con URIs persistentes (formato \texttt{http://data.europa.eu/esco/skill/...}) facilitó la integración con sistemas de recomendación, LLMs mediante prompt engineering, y exportación de resultados en formatos estandarizados como RDF y JSON-LD. Cuarto, la cobertura de 13,939 habilidades resultó suficiente para establecer baseline de matching, permitiendo identificar gaps sistemáticos que guiaron la estrategia de extensión taxonómica.

Para mitigar las limitaciones de cobertura de ESCO, se implementó una estrategia de extensión dual basada en análisis de gaps del corpus latinoamericano. La taxonomía ESCO base fue extendida con 152 habilidades técnicas modernas extraídas de la sección Hot Technologies de O*NET, priorizando tecnologías emergentes con alta frecuencia en el corpus pero ausentes en ESCO v1.1.0 como frameworks JavaScript modernos (Next.js, Nuxt.js), plataformas cloud nativas (Kubernetes, Terraform), y herramientas DevOps (GitHub Actions, ArgoCD). Adicionalmente, se agregaron 124 habilidades identificadas manualmente mediante análisis exploratorio del gold standard de 300 ofertas anotadas. Estas habilidades corresponden a términos técnicos latinoamericanos no presentes en O*NET ni ESCO (por ejemplo, variantes regionales de nombres de tecnologías, acrónimos locales, y tecnologías adoptadas específicamente en mercados hispanohablantes), además de skills emergentes publicadas entre 2022-2025 posteriores a la fecha de corte de ESCO v1.1.0. El proceso de selección de estas 124 habilidades se basó en tres criterios: presencia en al menos 5 ofertas del corpus (threshold de relevancia), ausencia de match exacto o fuzzy contra ESCO extendido con O*NET (confirmación de gap taxonómico), y validación manual de legitimidad técnica para evitar incluir errores ortográficos o menciones espurias.

La taxonomía extendida final, totalizando 14,215 skills (13,939 ESCO + 152 O*NET + 124 manuales), se almacenó en PostgreSQL (tabla \texttt{esco\_skills}) con esquema normalizado que preserva URIs originales, etiquetas preferidas en español e inglés, etiquetas alternativas, y metadatos de proveniencia. Se crearon índices B-tree en \texttt{preferred\_label\_es}, \texttt{preferred\_label\_en} y GIN index en \texttt{alt\_labels} para permitir búsquedas eficientes durante matching (latencia $<$5ms para exact match, $<$50ms para fuzzy matching sobre 14K términos).

Habiendo establecido las bases teóricas de las técnicas de extracción (NER, Regex, LLMs), modelos de lenguaje (Gemma, Llama), embeddings semánticos (E5 Multilingual), algoritmos de clustering (UMAP, HDBSCAN) y taxonomías de referencia (ESCO), la siguiente sección presentó la validación empírica de estas tecnologías sobre datos reales del mercado laboral latinoamericano. Las pruebas ejecutadas determinaron qué combinación de técnicas optimizó el balance entre precisión y cobertura para el contexto específico del español técnico, proporcionando evidencia cuantitativa que fundamentó las decisiones arquitectónicas del sistema.

\section{Pruebas de Modelos}

Los fundamentos teóricos presentados en la sección anterior establecieron las bases conceptuales para la selección de técnicas de extracción (NER, Regex, LLMs), tecnologías de embeddings (E5 Multilingual), y algoritmos de clustering (UMAP, HDBSCAN). Sin embargo, la viabilidad de estas técnicas en el contexto específico del español latinoamericano técnico requiere validación empírica con datos reales del dominio. Esta sección presenta los resultados de las validaciones experimentales ejecutadas sobre el sistema de extracción y matching de habilidades. A diferencia de benchmarks teóricos sobre datasets en inglés, estas pruebas se realizaron con ofertas laborales reales de portales latinoamericanos, proporcionando evidencia empírica específica del dominio que fundamentó decisiones arquitectónicas clave presentadas en la sección posterior.

\subsection{Conjunto de Datos}

El dataset del observatorio se compone de ofertas laborales tecnológicas recolectadas mediante web scraping de 7 portales principales en los países de Colombia, México y Argentina. El corpus final contiene 30,660 ofertas laborales únicas distribuidas como: México 17,835 (58.16\%), Colombia 9,479 (30.91\%), y Argentina 3,346 (10.93\%). Las ofertas abarcan el período de octubre 2018 a octubre 2025 (7 años de datos históricos), con 71.23\% de cobertura temporal (21,839 jobs con posted\_date válido). La mayoría de las ofertas (69\%, equivalente a 21,155 ofertas) corresponden al trimestre más reciente (Q4 2025), reflejando la naturaleza dinámica del mercado laboral tecnológico y la concentración del esfuerzo de recolección en datos actuales.

Cada oferta contiene los siguientes campos estructurados: \texttt{job\_id} (UUID único), \texttt{portal} (origen de la oferta), \texttt{country} (CO/MX/AR), \texttt{title} (título del cargo), \texttt{company} (empresa), \texttt{description} (descripción detallada del cargo), \texttt{requirements} (requisitos y habilidades), \texttt{salary\_raw} (rango salarial cuando disponible), \texttt{contract\_type} (tipo de contrato), \texttt{remote\_type} (modalidad presencial/remota/híbrida), \texttt{posted\_date} (fecha de publicación), y \texttt{content\_hash} (hash SHA-256 para deduplicación).

\subsection{Construcción del Conjunto de Datos}

\textbf{Proceso de Scraping:}

La recolección de datos se ejecutó mediante Scrapy 2.11, un framework asíncrono de scraping en Python que permitió procesamiento concurrente de múltiples requests. La estrategia de extracción se adaptó al tipo de portal: para sitios con HTML estático o APIs accesibles, se utilizó \texttt{requests} directo obteniendo datos en formato JSON desde endpoints que alimentan las vistas frontend (más eficiente); para portales con contenido dinámico cargado mediante JavaScript (Bumeran, ZonaJobs), se integraron Selenium 4.15 y ChromeDriver para renderizado completo de páginas antes de la extracción. El proceso implementó técnicas de ``polite crawling'': delays adaptativos entre requests (2-5 segundos), rotación de user-agents, límites de concurrencia por dominio, y reintentos con backoff exponencial ante errores HTTP 429/503.

La deduplicación de ofertas se realizó mediante hashing SHA-256 del contenido normalizado (title + company + description limpiados), almacenando el hash en campo \texttt{content\_hash} con restricción UNIQUE en PostgreSQL. Esta estrategia evitó re-procesar ofertas republicadas por portales múltiples o reposteadas por el mismo portal.

\textbf{Limpieza y Normalización:}

Las ofertas extraídas presentaron variabilidad significativa en formato y calidad. Se aplicó un pipeline de limpieza: eliminación de caracteres HTML residuales (\texttt{<br>}, \texttt{\&nbsp;}), normalización de espacios múltiples y saltos de línea, conversión de encoding a UTF-8, y extracción de texto plano de campos con formato rich text. Los disclaimers legales (equal opportunity statements, privacy policies) se identificaron mediante patrones regex y se separaron en metadatos sin afectar el análisis de habilidades.

\subsection{Validación de Técnicas de Extracción (Pipeline A)}

Se evaluó el rendimiento de los dos métodos del Pipeline A (Regex y NER) sobre el gold standard completo de 300 ofertas laborales manualmente anotadas. La Tabla \ref{tab:validacion-pipeline-a} presenta la comparación cuantitativa de ambas configuraciones en escenarios Pre-ESCO y Post-ESCO.

\begin{table}[H]
\centering
\caption{Validación de Técnicas de Extracción del Pipeline A (300 ofertas)}
\label{tab:validacion-pipeline-a}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Configuración} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Skills/Job} \\
\hline
\multicolumn{5}{|c|}{\textit{Evaluación Pre-ESCO (sin normalización taxonómica)}} \\
\hline
Regex Only & 33.92\% & 12.31\% & 18.07\% & 22.8 \\
NER+Regex & 22.54\% & 28.00\% & 24.98\% & 50.3 \\
\hline
\multicolumn{5}{|c|}{\textit{Evaluación Post-ESCO (con normalización ESCO)}} \\
\hline
Regex Only & \textbf{86.36\%} & 73.08\% & 79.17\% & --- \\
NER+Regex & 65.50\% & \textbf{81.25\%} & 72.53\% & --- \\
\hline
\end{tabular}
\end{table}

Los resultados experimentales demostraron que el módulo regex alcanzó alta precisión post-normalización (86.36\%) con velocidad submilisegundos, mientras que la adición de NER mejoró recall (+6.91pp Pre-ESCO) al costo de introducir ruido que degradó precision Post-ESCO. La decisión de mantener NER activo se fundamentó en priorizar cobertura de habilidades emergentes sobre precisión en normalización taxonómica, dado que ESCO v1.1.0 no cubre exhaustivamente vocabulario técnico latinoamericano actual. La arquitectura final combinó ambos métodos con deduplicación posterior, implementando matching contra taxonomía ESCO extendida (14,215 skills) mediante dos capas: Layer 1 (exact match) con confidence 1.00, y Layer 2 (fuzzy match con threshold $\geq$ 0.92) para variantes ortográficas. El análisis comparativo detallado, incluyendo análisis de trade-offs y justificación de la configuración seleccionada, se presenta en el Capítulo de Resultados.

\subsection{Evaluación del Sistema Completo con Gold Standard}

Se ejecutó un test end-to-end procesando 300 ofertas laborales del gold standard (100 por país: Colombia, México, Argentina) con el pipeline completo utilizando el matcher ESCO implementado de dos capas (exact + fuzzy). La Tabla \ref{tab:eval-sistema-completo} resume los resultados globales de la evaluación.

\begin{table}[H]
\centering
\caption{Resultados de Evaluación End-to-End del Sistema (300 ofertas)}
\label{tab:eval-sistema-completo}
\begin{tabular}{|l|r|}
\hline
\textbf{Métrica} & \textbf{Valor} \\
\hline
Jobs procesados exitosamente & 300/300 (100\%) \\
Total skills extraídas & 8,268 \\
Skills promedio por job & 27.6 \\
Skills matched con ESCO & 1,038 (12.6\%) \\
Emergent skills sin match & 7,230 (87.4\%) \\
Latencia promedio & 1.82 s/job \\
\hline
\multicolumn{2}{|c|}{\textit{Distribución por Capa de Matching}} \\
\hline
Layer 1 (exact match, conf=1.00) & 149 skills (43.1\%) \\
Layer 2 (fuzzy match, conf=0.92-1.00) & 197 skills (56.9\%) \\
\hline
\end{tabular}
\end{table}

Posteriormente se desarrolló un matcher experimental mejorado con mapeos manuales curados que incrementó el match rate a aproximadamente 25\%, permitiendo identificar y cuantificar con mayor precisión las habilidades emergentes ausentes en ESCO v1.1.0. Sin embargo, esta versión experimental no fue integrada al pipeline productivo para evitar introducir sesgos en la comparación entre Pipeline A y Pipeline B, manteniendo condiciones de evaluación equitativas donde ambos pipelines utilizan el mismo matcher sin bias.

El análisis por país reveló variación significativa en el match rate, presentado en la Tabla \ref{tab:match-rate-pais}.

\begin{table}[H]
\centering
\caption{Match Rate ESCO por País (Gold Standard)}
\label{tab:match-rate-pais}
\begin{tabular}{|l|c|l|}
\hline
\textbf{País} & \textbf{Match Rate} & \textbf{Característica Tecnológica} \\
\hline
Colombia & 15.3\% & Stacks enterprise tradicionales (Java, .NET, Oracle, SAP) \\
Argentina & 12.5\% & Balance intermedio \\
México & 11.3\% & Mayor adopción de frameworks modernos \\
\hline
\end{tabular}
\end{table}

La Tabla \ref{tab:top-skills-matched-emergent} presenta las habilidades más frecuentes en ambas categorías.

\begin{table}[H]
\centering
\caption{Top 10 Skills Matched y Emergent (Gold Standard)}
\label{tab:top-skills-matched-emergent}
\begin{tabular}{|l|r|l|r|}
\hline
\multicolumn{2}{|c|}{\textbf{Skills Matched con ESCO}} & \multicolumn{2}{c|}{\textbf{Emergent Skills (sin match)}} \\
\hline
\textbf{Skill} & \textbf{Menciones} & \textbf{Skill} & \textbf{Menciones} \\
\hline
Python & 14 & Data Science & 67 \\
Agile & 13 & HTML5 & 35 \\
SQL & 10 & CSS3 & 21 \\
JavaScript & 10 & Matplotlib & 15 \\
Git & 8 & Dashboards & 13 \\
FastAPI & 8 & Data Modeling & 9 \\
AWS Lambda & 8 & Data Analysis & 9 \\
Kubernetes & 6 & Flux & 8 \\
Go & 6 & Relay & 8 \\
GitLab CI/CD & 6 & SOAP & 8 \\
\hline
\end{tabular}
\end{table}

Las emergent skills confirman predominancia de conceptos amplios de análisis de datos (Data Science, Dashboards, Data Analysis) y versiones específicas de tecnologías (HTML5, CSS3) que ESCO v1.1.0 no distingue granularmente. Adicionalmente, se identificaron skills post-2022 de IA generativa (ChatGPT, LLM, Generative AI, Fine-tuning de LLMs) con frecuencias bajas pero alta relevancia estratégica.

El match rate de 12.6\% fue obtenido con el matcher baseline de dos capas (exact + fuzzy 0.92) operando sobre la taxonomía ESCO extendida de 14,215 skills (13,939 ESCO v1.1.0 + 152 O*NET + 124 agregadas manualmente). Este porcentaje es bajo pero esperado considerando dos factores principales. Primero, el corpus contiene alta frecuencia de conceptos amplios (Data Science, Data Analysis) y versiones específicas (HTML5, CSS3) que ESCO trata genéricamente sin distinguir granularidad. Segundo, el mercado laboral latinoamericano presenta alta demanda de tecnologías post-2022 (ChatGPT, Generative AI, LLM) ausentes en taxonomías europeas. Iteraciones posteriores con matcher enhanced de 4 capas incrementaron el coverage a $\sim$25\%, validando que las habilidades no matched (87.4\% baseline, 74.7\% enhanced) corresponden genuinamente a emergent skills y representan señal valiosa de innovación del mercado para análisis exploratorio posterior.

Estos resultados tienen implicaciones arquitectónicas importantes para el sistema. El alto porcentaje de emergent skills valida la decisión de implementar el Pipeline B con LLMs, ya que estas habilidades modernas probablemente corresponden a términos técnicos actuales que ESCO v1.1.0 no cubre pero que un LLM pre-entrenado puede reconocer contextualmente. Asimismo, la variación del match rate por país (CO 15.3\%, MX 11.3\%, AR 12.5\%) sugiere diferencias regionales en adopción tecnológica que deben considerarse en el análisis de clustering, potencialmente requiriendo ajuste de parámetros HDBSCAN por región. Finalmente, la alta frecuencia de conceptos amplios de datos (Data Science con 67 menciones) y skills de IA generativa post-2022 confirman la relevancia temporal del corpus y su alineación con tendencias del mercado tecnológico latinoamericano.

Para maximizar el valor analítico de las emergent skills, se propone un proceso de curación semi-automática que combinaría clustering semántico de las habilidades no matched mediante embeddings E5 y HDBSCAN, seguido de revisión manual de los clústeres más frecuentes. Los términos válidos y recurrentes (threshold de cinco o más menciones) se agregarían manualmente a la taxonomía ESCO local, mientras que los términos rechazados se documentarían con su justificación correspondiente. Este proceso iterativo permitiría que el match rate evolucione orgánicamente con el corpus, balanceando cobertura y control de calidad. En la implementación actual, las 124 habilidades agregadas manualmente fueron identificadas mediante análisis exploratorio inicial del corpus sin automatización del proceso de clustering.

\section{Arquitectura}

Las pruebas experimentales presentadas en la sección anterior proporcionaron evidencia empírica crítica que determinó las decisiones arquitectónicas del sistema. Los resultados demostraron que el enfoque dual NER+Regex alcanza precision de 78-95\% con latencias submilisegundos, que el matching con ESCO requiere expansión manual debido al bajo match rate inicial (12.6\%), y que los embeddings E5 presentan limitaciones para búsqueda semántica en vocabulario técnico especializado. Adicionalmente, las características del dominio  procesamiento batch de ofertas laborales sin requisitos de tiempo real, corpus de más de 30,000 ofertas, y recursos limitados propios de un proyecto académico restringieron las opciones arquitectónicas viables.

El sistema se diseñó como un observatorio automatizado end-to-end que integra siete etapas especializadas de procesamiento, desde la adquisición de ofertas laborales hasta la generación de visualizaciones analíticas. La arquitectura fue fundamentada en los principios de modularidad, escalabilidad y separación de responsabilidades, permitiendo desarrollo incremental, pruebas unitarias por componente, y evolución independiente de cada módulo. Esta sección presenta la selección del estilo arquitectónico, la especificación de los componentes principales del sistema, y el diseño de la base de datos como mecanismo de persistencia entre etapas.

\subsection{Modelo de Vistas Arquitectónicas}

La complejidad del sistema, que integra procesamiento síncrono de baja latencia para consultas de usuarios con procesamiento asíncrono distribuido de tareas computacionalmente intensivas, requiere múltiples perspectivas para su documentación completa. Se adoptó el Modelo 4+1 de Vistas Arquitectónicas \cite{kruchten1995}, el cual permite describir la arquitectura desde perspectivas complementarias enfocadas en las preocupaciones de diferentes stakeholders del proyecto.

Para este proyecto se documentan tres vistas principales: Vista Lógica (funcionalidad y componentes del sistema), Vista Física (topología de despliegue e infraestructura), y Vista de Procesos (comportamiento en tiempo de ejecución y concurrencia). Las siguientes subsecciones presentan cada vista arquitectónica, proporcionando una especificación completa de la arquitectura implementada.

\subsection{Vista Lógica: Componentes y Patrones Arquitectónicos}

La vista lógica describe la descomposición funcional del sistema en servicios especializados y los patrones arquitectónicos que gobiernan sus interacciones. El sistema implementa una arquitectura híbrida que combina tres patrones complementarios para satisfacer requisitos duales de latencia: operaciones síncronas de baja latencia (menos de 1 segundo) para consultas, y procesamiento asíncrono distribuido de tareas computacionalmente intensivas que pueden requerir minutos u horas.

\subsubsection{Patrón Híbrido Implementado}

El sistema integra tres patrones arquitectónicos fundamentales que trabajan de manera coordinada. El primer patrón corresponde al API Gateway, implementado mediante Nginx, que actúa como punto único de entrada para todas las peticiones HTTP/HTTPS externas. Este componente proporciona routing inteligente que enruta solicitudes hacia el servicio Frontend o API según la ruta solicitada, terminación SSL/TLS para gestión centralizada de certificados, rate limiting para protección contra abusos y auditoría de todas las peticiones del sistema.

El segundo patrón implementado son los Microservicios en Capas para comunicación Request/Response. Para operaciones que requieren respuesta inmediata, se estructura una arquitectura de tres capas: la capa de Presentación mediante Frontend con Next.js para renderizado y gestión de interfaz, la capa de Lógica de Negocio mediante API con FastAPI para endpoints REST y validación, y la capa de Persistencia mediante PostgreSQL para almacenamiento ACID. Este patrón se emplea para consultas de ofertas laborales, estadísticas agregadas, y operaciones que requieren latencias inferiores a 1 segundo.

El tercer patrón es la Event-Driven Architecture mediante comunicación Pub/Sub. Para operaciones computacionalmente intensivas, se implementa arquitectura orientada a eventos mediante el patrón Publisher/Subscriber. La API y Celery Beat actúan como publishers publicando tareas a una cola de mensajes gestionada por Redis, mientras que Celery Workers actúan como subscribers consumiendo estas tareas, ejecutando el procesamiento requerido, y persistiendo resultados en PostgreSQL. Este patrón se emplea para scraping automático de portales, extracción de habilidades con LLM en lotes, y clustering de habilidades.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{diagrams/VistaLogica.png}
\caption{Vista Lógica del Sistema. El diagrama presenta la arquitectura híbrida con sus tres patrones integrados: API Gateway (Nginx), Microservicios en Capas (Frontend, API, PostgreSQL) para operaciones síncronas, y Event-Driven Architecture (Redis, Celery) para procesamiento asíncrono distribuido.}
\label{fig:vista-logica}
\end{figure}

\subsubsection{Los Siete Servicios del Sistema}

La arquitectura se descompone en siete servicios especializados, cada uno con responsabilidades claramente delimitadas. El servicio NGINX actúa como API Gateway proporcionando el punto único de entrada HTTP/HTTPS, routing de peticiones hacia los servicios apropiados, terminación SSL/TLS, y compresión de respuestas. El servicio Frontend gestiona la interfaz de usuario mediante renderizado de páginas web con Server-Side Rendering utilizando Next.js, gestión de estado de aplicación, visualización de dashboards y estadísticas, y polling para monitoreo de tareas asíncronas.

El servicio API implementa la lógica de negocio del sistema mediante exposición de endpoints REST para operaciones CRUD, validación de datos de entrada, coordinación de servicios mediante publicación de tareas asíncronas, y consulta de estado de tareas en Redis. Este servicio implementa funcionalidad dual: Request/Response síncrono para consultas rápidas y Pub/Sub asíncrono para procesamiento batch. El servicio PostgreSQL proporciona persistencia ACID de datos estructurados, almacenamiento de embeddings de 768 dimensiones mediante extensión pgvector, garantía de trazabilidad mediante relaciones foreign key, y optimización de consultas mediante índices.

El servicio Redis cumple doble función como cola de mensajes para el patrón Pub/Sub actuando como broker de Celery, y como almacenamiento de resultados de tareas funcionando como backend de resultados de Celery. Adicionalmente provee cache de consultas frecuentes con TTL configurable y gestión de estado de tareas asíncronas. El servicio Celery Beat se encarga de la programación de tareas periódicas tipo cron tales como scraping cada 6 horas y limpieza diaria de datos antiguos, publicación de tareas programadas a la cola de Redis, y gestión de schedules de ejecución automática.

Finalmente, el servicio Celery Workers actúa como procesador distribuido mediante consumo de tareas de la cola de Redis, ejecución del pipeline CRISP-DM de procesamiento de datos, procesamiento de tareas en paralelo mediante múltiples workers escalables, persistencia de resultados en PostgreSQL, y reporte de progreso de tareas. Este servicio soporta escalamiento horizontal dinámico desde 1 hasta N workers sin requerir cambios de código.

\subsubsection{Justificación de la Arquitectura Híbrida}

La selección de una arquitectura híbrida se fundamentó en cinco razones principales: la dualidad de requisitos de latencia (consultas de usuarios requieren respuesta inmediata mientras que el procesamiento de datos requiere minutos u horas), escalabilidad horizontal selectiva (los workers pueden escalarse dinámicamente sin modificar código), simplicidad operativa con potencia de procesamiento (mantiene trazabilidad de sistemas modulares mientras obtiene paralelismo de sistemas distribuidos), optimización de recursos (Request/Response evita overhead para operaciones simples mientras que Event-Driven maximiza CPU para procesamiento intensivo), y madurez del ecosistema tecnológico (Celery y Redis es una combinación probada industrialmente con amplia documentación).

La Tabla \ref{tab:arch-comparison-hibrida} presenta la evaluación de tres estilos arquitectónicos considerados durante el diseño.

\begin{table}[H]
\centering
\caption{Comparación de Estilos Arquitectónicos Evaluados}
\label{tab:arch-comparison-hibrida}
\begin{tabular}{|p{3.2cm}|p{2.8cm}|p{3cm}|p{3.5cm}|}
\hline
\textbf{Criterio} & \textbf{Pipeline Lineal} & \textbf{Microservicios Puros} & \textbf{Arquitectura Híbrida} \\
\hline
Complejidad de implementación & Baja & Alta & Media \\
\hline
Escalabilidad horizontal & Limitada & Excelente & Excelente (workers) \\
\hline
Latencia de consultas & Alta (bloqueante) & Baja & Baja (menor a 1s) \\
\hline
Throughput de procesamiento & Bajo (secuencial) & Medio & Alto (paralelo) \\
\hline
Trazabilidad & Excelente & Media & Alta \\
\hline
Tolerancia a fallos & Baja & Alta & Alta \\
\hline
Time to market & Rápido & Lento & Medio \\
\hline
\end{tabular}
\end{table}

El pipeline lineal fue descartado por impedir paralelismo y limitar throughput. Los microservicios puros fueron descartados por introducir complejidad innecesaria para un proyecto académico con equipo de 2 desarrolladores. La arquitectura híbrida proporciona el balance óptimo entre capacidades técnicas y viabilidad operativa.

\subsection{Vista Física: Infraestructura y Despliegue}

La vista física describe el mapeo de componentes lógicos sobre infraestructura física, especificando hardware, contenedores Docker, configuración de red, y estrategia de despliegue. Esta sección detalla la topología de deployment que materializa la arquitectura lógica presentada anteriormente.

\subsubsection{Especificaciones del Entorno de Desarrollo}

El sistema se desarrolló y desplegó localmente en un MacBook Air M4 2024 que ejecuta todos los servicios mediante contenedores Docker. Las especificaciones del hardware utilizado comprenden procesador Apple M4 (10 cores: 4 performance + 6 efficiency), 16 GB de memoria unificada DDR5, almacenamiento interno de 512 GB SSD, y GPU integrado de 10 cores con soporte para aceleración Metal Framework.

El software base comprende macOS Sequoia 15.0, Docker Desktop 4.25+ para Apple Silicon con soporte ARM64, y Python 3.11 con MLX framework para ejecución de LLMs con aceleración Metal. La arquitectura ARM del chip M4 requiere imágenes Docker multi-arquitectura (linux/arm64) o construcción local mediante buildx. El GPU integrado permite ejecución de modelos LLM cuantizados (Q4) de hasta 4B parámetros con consumo aproximado de 4-6 GB de memoria unificada, siendo suficiente para procesamiento experimental del Pipeline B sobre el gold standard de 300 ofertas.

\subsubsection{Contenedores Docker y Orquestación}

El sistema se compone de contenedores Docker orquestados mediante Docker Compose. La Tabla \ref{tab:contenedores-docker} presenta los contenedores principales con sus especificaciones de recursos.

\begin{table}[H]
\centering
\caption{Contenedores Docker del Sistema}
\label{tab:contenedores-docker}
\begin{tabular}{|p{3cm}|p{3cm}|p{2.5cm}|p{1.5cm}|p{1.5cm}|}
\hline
\textbf{Servicio} & \textbf{Imagen} & \textbf{Puerto} & \textbf{CPU} & \textbf{RAM} \\
\hline
nginx & nginx:alpine & 80, 443 & 1 & 1 GB \\
\hline
frontend & frontend:latest & 3000 & 1 & 1 GB \\
\hline
api & api:latest & 8000 & 1 & 1 GB \\
\hline
postgres & postgres:15 & 5433→5432 & 2 & 4 GB \\
\hline
redis & redis:7-alpine & 6379 & 1 & 2 GB \\
\hline
celery\_beat & celery:latest & N/A & 0.5 & 512 MB \\
\hline
celery\_worker & celery:latest & N/A & 2 & 4 GB \\
\hline
\end{tabular}
\end{table}

El servicio \texttt{celery\_worker} puede replicarse dinámicamente (1, 2, 4, 8, o N instancias) sin cambios de código mediante el comando \texttt{docker-compose up -d --scale celery\_worker=N}, permitiendo escalamiento horizontal según la carga de procesamiento.

\begin{figure}[H]
\centering
\makebox[\textwidth]{\includegraphics[width=1.25\textwidth]{diagrams/VistaFisica.png}}
\caption{Vista Física del Sistema. El diagrama muestra el servidor de producción con sus especificaciones de hardware, los contenedores Docker orquestados por Docker Compose, mapeo de puertos, red bridge interna, y volúmenes persistentes para PostgreSQL y Redis.}
\label{fig:vista-fisica}
\end{figure}

\subsubsection{Configuración de Red y Volúmenes}

Los contenedores se conectan mediante la red bridge por defecto de Docker Compose, que proporciona resolución DNS automática por nombre de servicio. Los datos que deben persistir entre reinicios de contenedores se almacenan en volúmenes Docker nombrados: \texttt{postgres\_data} para la base de datos PostgreSQL y \texttt{redis\_data} para persistencia de Redis. El sistema implementa respaldos automáticos diarios de la base de datos PostgreSQL mediante \texttt{pg\_dump}, almacenando dumps comprimidos en el directorio \texttt{data/backups/} del host con rotación de 7 días.

\subsection{Vista de Procesos: Ejecución y Concurrencia}

La vista de procesos describe el comportamiento dinámico del sistema en tiempo de ejecución, abordando aspectos de concurrencia, distribución de procesamiento, throughput, y escalabilidad. Esta sección presenta el pipeline CRISP-DM que ejecutan los workers y la estrategia de escalamiento horizontal.

\subsubsection{Pipeline CRISP-DM y Flujo de Procesamiento}

El procesamiento de datos sigue la metodología CRISP-DM (Cross-Industry Standard Process for Data Mining) adaptada al dominio de análisis de mercado laboral. El pipeline se compone de 6 etapas secuenciales ejecutadas por los Celery Workers. La primera etapa de Scraping realiza recolección automatizada mediante Scrapy y Selenium desde 7 portales de empleo en 3 países, implementando deduplicación mediante hash SHA-256 del contenido normalizado.

La segunda etapa de Cleaning ejecuta normalización de texto, eliminación de HTML residual, conversión a UTF-8, y separación de disclaimers legales. La tercera etapa de Extraction realiza identificación de habilidades mediante Pipeline A (NER+Regex) o Pipeline B (Gemma 3 4B). Estos pipelines son variantes experimentales de esta etapa, no arquitecturas separadas. La cuarta etapa de Matching mapea las habilidades extraídas contra la taxonomía ESCO extendida (14,215 skills) mediante dos capas: Layer 1 con exact match y Layer 2 con fuzzy matching (threshold 0.92).

La quinta etapa de Embedding genera representaciones vectoriales de 768 dimensiones con modelo E5-multilingual para las habilidades extraídas. La sexta etapa de Clustering realiza reducción dimensional con UMAP (transformando de 768D a 2-3D), seguida de clustering con HDBSCAN para identificar perfiles laborales emergentes. Finalmente, la etapa de Visualization genera scatter plots UMAP 2D/3D, dendrogramas jerárquicos, y gráficos de distribución de habilidades por país y portal, exportando resultados como imágenes PNG y metadatos JSON para el frontend.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{diagrams/VistaProcesos.png}
\caption{Vista de Procesos del Sistema. El diagrama muestra el flujo completo de procesamiento desde el web scraping hasta la visualización, incluyendo los eventos de skill extraction, LLM processing, embedding generation, dimension reduction, clustering, y la orquestación del pipeline mediante Celery Workers con persistencia en PostgreSQL.}
\label{fig:vista-procesos}
\end{figure}

\subsubsection{Escalabilidad Horizontal de Workers}

La arquitectura permite escalamiento horizontal dinámico de workers mediante el comando \texttt{docker-\allowbreak compose up -d --scale celery\_worker=N}, sin modificar código ni reconfigurar servicios. Redis distribuye tareas equitativamente entre workers disponibles mediante load balancing automático. Cada worker consume tareas independientemente de la cola compartida, permitiendo procesamiento paralelo de múltiples ofertas laborales simultáneamente.

El sistema implementa mecanismos básicos de recuperación ante errores: las tareas fallidas se reintentan automáticamente hasta 3 veces con configuración \texttt{max\_retries=3} en las definiciones de tasks de Celery. El estado de ejecución de cada tarea se persiste en Redis, permitiendo monitoreo de progreso mediante el frontend. La base de datos PostgreSQL garantiza persistencia de resultados exitosos mediante transacciones ACID.

\subsubsection{Gestión de Tareas Asíncronas}

El ciclo de vida de una tarea asíncrona comprende los siguientes estados: QUEUED (tarea publicada a Redis), PENDING (tarea en cola esperando worker disponible), STARTED (worker tomó la tarea y comenzó procesamiento), PROGRESS (worker reporta progreso: 20\%, 40\%, 60\%, etc.), y finalmente SUCCESS (tarea completada exitosamente con resultado disponible) o FAILURE (tarea falló con error almacenado). El frontend monitorea el progreso mediante polling cada 3 segundos al endpoint \texttt{/api/tasks/\{task\_id\}}, actualizando la interfaz de usuario con el estado y porcentaje de completitud.

\subsection{Diseño de la Base de Datos}

La base de datos actuó como columna vertebral del sistema, implementando el patrón de persistencia de pipeline donde cada etapa escribió sus resultados en tablas especializadas. Se seleccionó PostgreSQL 15+ por su soporte JSON nativo (JSONB) para almacenar metadatos flexibles, extensión pgvector para vectores de alta dimensionalidad, robustez transaccional (ACID), capacidad de particionamiento para escalabilidad, y licencia libre (PostgreSQL License).

\begin{figure}[H]
\centering
\makebox[\textwidth]{\includegraphics[width=1.23\textwidth]{diagrams/DiagramaER.png}}
\caption{Diagrama Entidad-Relación de la Base de Datos del Observatorio}
\label{fig:diagrama-er}
\end{figure}

El esquema (Figura \ref{fig:diagrama-er}) se estructura en torno a trece tablas organizadas en tres capas funcionales. La capa de adquisición comprende raw\_jobs como tabla central que almacena ofertas laborales scrapeadas sin procesar, complementada por cleaned\_jobs que contiene el texto normalizado y libre de HTML listo para extracción. La capa de procesamiento incluye extracted\_skills con habilidades detectadas por Pipeline A mediante NER y expresiones regulares, enhanced\_skills con enriquecimiento semántico de Pipeline B mediante LLM capaz de inferir habilidades implícitas, y skill\_embeddings con representaciones vectoriales de 768 dimensiones generadas por E5-multilingual optimizadas mediante índice IVFFlat para búsquedas de similitud. La capa de análisis y evaluación contiene analysis\_results para persistir proyecciones UMAP, asignaciones de clusters HDBSCAN y metadatos JSONB de configuración, complementada por gold\_standard\_annotations con las anotaciones manuales de 300 ofertas utilizadas para evaluación comparativa de ambos pipelines.

La taxonomía ESCO se implementa mediante seis tablas relacionales. La tabla esco\_skills contiene la taxonomía extendida de 14,174 habilidades compuesta por ESCO v1.1.0 (13,939 skills oficiales), O*NET Hot Technologies (152 tecnologías emergentes del sector IT), y 83 habilidades agregadas manualmente tras análisis exploratorio del mercado latinoamericano. Esta tabla se complementa con esco\_skill\_labels para etiquetas multilingües y sinónimos en español e inglés, esco\_skill\_relations para relaciones jerárquicas y semánticas entre conceptos (broader, narrower, related), custom\_skill\_mappings para mapeos manuales de skills emergentes no contempladas en ESCO, esco\_skill\_groups para agrupación jerárquica de alto nivel con soporte de auto-referencia mediante parent\_group\_id, y esco\_skill\_families para clasificación por familias de competencias.

El diseño incorpora mecanismos críticos de calidad de datos y trazabilidad implementados mediante migraciones evolutivas. La deduplicación semántica (Migration 007) agrega campos is\_duplicate, duplicate\_of, duplicate\_similarity\_score y duplicate\_detection\_method a raw\_jobs, permitiendo identificar ofertas casi duplicadas mediante similitud textual y embeddings. El filtrado de junk data (Migration 006) incorpora is\_usable y unusable\_reason para marcar ofertas de prueba o con contenido insuficiente. Las métricas de evaluación de Pipeline B (Migration 009) añaden processing\_time\_seconds, tokens\_used y esco\_match\_method a enhanced\_skills, facilitando análisis de costo computacional y precisión de mapeo contra taxonomía ESCO mediante estrategia de tres capas (exact, fuzzy, semantic matching).

Todas las tablas derivadas mantienen referencia mediante llave foránea hacia raw\_jobs, garantizando trazabilidad completa desde cualquier resultado de análisis hasta la oferta laboral original que lo generó. La especificación detallada de los 87 campos totales distribuidos entre las 13 tablas, tipos de datos PostgreSQL, 34 índices optimizados (B-tree, GIN, IVFFlat), constraints de integridad referencial, configuración de PostgreSQL para procesamiento batch de más de 30,000 ofertas, y estrategias de persistencia transaccional se documentan exhaustivamente en el Apéndice C.

Habiendo documentado la arquitectura del sistema mediante tres vistas complementarias (Lógica, Física y Procesos) que especifican los patrones arquitectónicos híbridos, los siete servicios especializados, la infraestructura de despliegue con Docker, el pipeline CRISP-DM de 7 etapas, y el diseño de base de datos con 13 tablas organizadas en tres capas funcionales (adquisición, procesamiento, análisis y taxonomía ESCO), el stack tecnológico completo que materializa esta arquitectura se documenta detalladamente en el Apéndice B (Tablas \ref{tab:tech-stack-infra} y \ref{tab:tech-stack-analytics}). La selección de tecnologías se fundamentó en cinco criterios: licencias permisivas para uso académico, madurez y estabilidad demostrada, documentación académica con publicaciones revisadas por pares, reproducibilidad mediante control de versiones, y escalabilidad validada para procesamiento de más de 30,000 ofertas laborales.
