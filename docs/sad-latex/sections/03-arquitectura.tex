\section{Arquitectura}

\subsection{Descripción del sistema}

El Observatorio de Demanda Laboral es un sistema académico de investigación diseñado para automatizar el análisis de habilidades técnicas demandadas en el mercado laboral de América Latina. A través de técnicas avanzadas de procesamiento de lenguaje natural, embeddings semánticos y clustering no supervisado, el sistema permite identificar tendencias, perfiles emergentes y brechas de competencias en el sector tecnológico de Colombia, México y Argentina.

El sistema procesa ofertas laborales recolectadas automáticamente desde 8 portales de empleo principales (Computrabajo, Bumeran, ElEmpleo, HiringCafe, OCC Mundial, ZonaJobs, Indeed, Magneto). La arquitectura híbrida integra procesamiento síncrono de baja latencia para consultas de usuarios con procesamiento asíncrono distribuido de tareas computacionalmente intensivas, diseñada para maximizar precisión y reproducibilidad científica.

La plataforma implementa dos pipelines paralelos de extracción de habilidades mediante Pipeline A (NER con spaCy más Regex patterns más Matching ESCO de 3 capas) y Pipeline B (extracción basada en LLM con Gemma 3 4B para comparación científica). Los resultados se almacenan en PostgreSQL con trazabilidad completa, generan embeddings mediante E5 Multilingual de 768 dimensiones, aplican reducción dimensional con UMAP, ejecutan clustering con HDBSCAN, y exportan visualizaciones y reportes analíticos.

\subsection{Decisiones arquitectónicas}

El diseño arquitectónico del Observatorio de Demanda Laboral responde a un equilibrio entre objetivos científicos y restricciones operativas. Como proyecto de investigación académica desarrollado por un equipo de dos personas con recursos computacionales limitados, las decisiones arquitectónicas priorizan reproducibilidad científica, simplicidad operativa y trazabilidad completa sobre escalabilidad masiva o latencia mínima.

\subsubsection{Arquitectura Híbrida: Justificación y Patrones}

La decisión arquitectónica fundamental fue adoptar una arquitectura híbrida que combina tres patrones complementarios para satisfacer requisitos duales de latencia: operaciones síncronas de baja latencia (menos de 1 segundo) para consultas de usuarios, y procesamiento asíncrono distribuido de tareas computacionalmente intensivas que pueden requerir minutos u horas.

El primer patrón corresponde al API Gateway implementado mediante Nginx, que actúa como punto único de entrada para todas las peticiones HTTP/HTTPS externas proporcionando routing inteligente, terminación SSL/TLS, rate limiting para protección contra abusos, y logging centralizado para auditoría. El segundo patrón implementa Microservicios en Capas para comunicación Request/Response mediante arquitectura de tres capas: Presentación (Frontend con Next.js), Lógica de Negocio (API con FastAPI), y Persistencia (PostgreSQL). El tercer patrón utiliza Event-Driven Architecture mediante comunicación Pub/Sub donde la API y Celery Beat publican tareas a Redis, mientras que Celery Workers consumen estas tareas, ejecutan el procesamiento, y persisten resultados en PostgreSQL.

La selección de arquitectura híbrida se fundamentó en cinco razones principales: dualidad de requisitos de latencia (consultas inmediatas versus procesamiento de horas), escalabilidad horizontal selectiva (workers escalables dinámicamente sin modificar código), simplicidad operativa con potencia de procesamiento (mantiene trazabilidad mientras obtiene paralelismo), optimización de recursos (Request/Response evita overhead para operaciones simples mientras Event-Driven maximiza CPU para procesamiento intensivo), y madurez del ecosistema tecnológico (Celery y Redis como combinación probada industrialmente).

La Tabla \ref{tab:comparacion-arquitecturas} presenta la evaluación de estilos arquitectónicos considerados durante el diseño.

\begin{table}[H]
\centering
\caption{Comparación de Estilos Arquitectónicos Evaluados}
\label{tab:comparacion-arquitecturas}
\begin{tabular}{|p{3.2cm}|p{2.8cm}|p{3cm}|p{3.5cm}|}
\hline
\textbf{Criterio} & \textbf{Pipeline Lineal} & \textbf{Microservicios} & \textbf{Arquitectura Híbrida} \\
\hline
Complejidad & Baja & Alta & Media \\
\hline
Escalabilidad horizontal & Limitada & Excelente & Excelente (workers) \\
\hline
Latencia de consultas & Alta (bloqueante) & Baja & Baja (menor a 1s) \\
\hline
Throughput & Bajo (secuencial) & Medio & Alto (paralelo) \\
\hline
Trazabilidad & Excelente & Media & Alta \\
\hline
Tolerancia a fallos & Baja & Alta & Alta \\
\hline
Time to market & Rápido & Lento & Medio \\
\hline
\end{tabular}
\end{table}

\subsubsection{Pipeline Secuencial de 7 Etapas CRISP-DM}

Una vez establecido el estilo arquitectónico general, la siguiente decisión fue determinar la granularidad y especialización de cada etapa del pipeline. El diseño resultante divide el procesamiento en 7 etapas especializadas siguiendo la metodología CRISP-DM adaptada para minería de textos, donde cada componente mantiene una responsabilidad única y bien definida según el principio de separación de responsabilidades.

La Tabla \ref{tab:pipeline-etapas} describe las etapas del pipeline con sus tecnologías, responsabilidades y tiempos característicos de procesamiento.

\begin{table}[H]
\centering
\caption{Etapas del Pipeline CRISP-DM y Especificaciones Técnicas}
\label{tab:pipeline-etapas}
\begin{tabular}{|p{2.8cm}|p{3cm}|p{5cm}|p{2.2cm}|}
\hline
\textbf{Etapa} & \textbf{Tecnología} & \textbf{Responsabilidad} & \textbf{Tiempo} \\
\hline
1. Scraping & Scrapy + Selenium & Recolección automatizada desde 8 portales (Colombia, México, Argentina) & 2-4 horas \\
\hline
2. Cleaning & BeautifulSoup & Limpieza HTML, normalización Unicode, extracción texto plano & 5-10 minutos \\
\hline
3. Extraction & spaCy NER + Regex + LLM & Identificación habilidades explícitas (Pipeline A) e implícitas (Pipeline B) & 20-30 minutos \\
\hline
4. Enhancement & Gemma 3 4B & Enriquecimiento semántico, matching ESCO con 3 capas (exact/fuzzy/semantic) & 1-2 horas \\
\hline
5. Embedding & E5 Multilingual & Generación vectores densos 768D normalizados L2, construcción índice FAISS & 15-20 minutos \\
\hline
6. Clustering & UMAP + HDBSCAN & Reducción dimensional 768D a 2-3D, clustering no supervisado con detección ruido & 10-15 minutos \\
\hline
7. Visualization & Matplotlib + Seaborn & Generación gráficos interactivos, reportes analíticos, exportación multi-formato & 5-10 minutos \\
\hline
\end{tabular}
\end{table}

El diseño modular del pipeline garantiza que cada etapa opera de forma autónoma mediante el siguiente flujo: lee los datos procesados por la etapa anterior desde PostgreSQL, ejecuta su transformación especializada con validaciones internas, y persiste los resultados en tablas dedicadas para consumo de la siguiente etapa. Esta arquitectura facilita el debugging aislado al permitir inspeccionar resultados intermedios en cualquier punto del flujo, habilita la reejecución de etapas individuales sin reprocesar el dataset completo (reduciendo tiempo de desarrollo), y asegura trazabilidad end-to-end mediante foreign keys que conectan cada resultado analítico con la oferta laboral original que lo generó.

\subsubsection{Selección de Tecnologías Críticas}

La elección del stack tecnológico es una decisión arquitectónica fundamental que impacta directamente en la calidad de los resultados científicos, el rendimiento del sistema y la viabilidad operativa. Las tecnologías fueron seleccionadas tras evaluación comparativa rigurosa, priorizando madurez del ecosistema, calidad de documentación, comunidad activa de soporte, y alineación estratégica con los objetivos del proyecto.

PostgreSQL 15+ fue seleccionado como sistema de persistencia central por su combinación de robustez transaccional ACID, soporte nativo para JSONB permitiendo metadatos flexibles sin esquema fijo, extensión pgvector para almacenamiento de vectores densos de 768 dimensiones, y capacidades de particionamiento para escalabilidad futura. Para búsqueda vectorial de alta performance se implementó FAISS desarrollado por Facebook AI Research, que alcanza 30,147 queries por segundo mediante exact search con IndexFlatIP garantizando 100 por ciento de recall, superando por factor de 25x el rendimiento de pgvector en consultas de similitud.

El procesamiento de lenguaje natural utiliza spaCy con el modelo entrenado es core news lg de 97 millones de parámetros optimizado para español, complementado con EntityRuler poblado con las 14,174 skills de la taxonomía ESCO para reconocimiento de entidades de alta precisión con latencia inferior a 100 milisegundos por documento. La taxonomía de referencia corresponde a ESCO v1.1.0 publicada bajo licencia Creative Commons BY 4.0, que provee 13,939 habilidades con etiquetas en español e inglés organizadas mediante estructura ontológica con URIs persistentes, expandida estratégicamente con 152 habilidades de O*NET y 124 skills identificadas manualmente durante el gold standard totalizando 14,174 competencias normalizadas.

La Tabla \ref{tab:stack-tecnologico} resume las decisiones tecnológicas fundamentales con sus justificaciones técnicas.

\begin{table}[H]
\centering
\caption{Stack Tecnológico del Observatorio}
\label{tab:stack-tecnologico}
\begin{tabular}{|p{3.5cm}|p{3.5cm}|p{6cm}|}
\hline
\textbf{Componente} & \textbf{Tecnología} & \textbf{Justificación} \\
\hline
Base de datos & PostgreSQL 15+ & Soporte JSONB, pgvector, robustez ACID, particionamiento \\
\hline
Taxonomía & ESCO v1.1.0 & 13,000+ skills ES/EN, URIs persistentes, CC BY 4.0 \\
\hline
Scraping & Scrapy + Selenium & Asíncrono eficiente, manejo JavaScript dinámico \\
\hline
NLP español & spaCy es\_core\_news\_lg & 97M parámetros, EntityRuler, optimizado CPU \\
\hline
Embeddings & E5 multilingual-base & 768D, 100 idiomas, normalización L2 \\
\hline
Búsqueda vectorial & FAISS IndexFlatIP & 30K q/s, exact search, 25x vs pgvector \\
\hline
Reducción dimensional & UMAP & Preserva estructura local+global \\
\hline
Clustering & HDBSCAN & Sin especificar k, identifica ruido \\
\hline
Lenguaje & Python 3.11+ & Ecosistema científico, type hints \\
\hline
\end{tabular}
\end{table}

\subsubsection{Estrategia Dual de Pipelines}

La decisión arquitectónica más singular del proyecto es la implementación de dos pipelines paralelos e independientes para extracción de habilidades. Esta estrategia responde directamente al objetivo científico central del proyecto: evaluar rigurosamente si los modelos de lenguaje grandes pueden superar a técnicas tradicionales de NLP en la tarea específica de extracción de habilidades técnicas desde ofertas laborales en español.

El diseño experimental establece un grupo control (Pipeline A) y un grupo de tratamiento (Pipeline B), permitiendo comparación controlada con metodología científica. La Tabla \ref{tab:pipeline-comparison} presenta las características diferenciales de ambos pipelines.

\begin{table}[H]
\centering
\caption{Comparación de Pipeline A (Control) vs Pipeline B (Tratamiento)}
\label{tab:pipeline-comparison}
\begin{tabular}{|p{3cm}|p{5.5cm}|p{5.5cm}|}
\hline
\textbf{Criterio} & \textbf{Pipeline A (Control)} & \textbf{Pipeline B (Tratamiento)} \\
\hline
Técnica & NER con spaCy + Regex patterns & LLM local (Gemma 3 4B o Llama 3 3B) \\
\hline
Cobertura & 100\% ofertas laborales (30,660 jobs) & Subconjunto estratégico 300-1,000 ofertas \\
\hline
Precisión & 78-95\% según gold standard & 80-90\% esperada \\
\hline
Latencia & Menor a 2 segundos por oferta & 5-10 segundos por oferta \\
\hline
Habilidades & Explícitas con menciones literales & Explícitas + implícitas inferidas \\
\hline
Spanglish & Limitado a diccionario ESCO & Manejo contextual nativo \\
\hline
Rol & Baseline de alta precisión & Evaluación capacidad LLM \\
\hline
Restricción & N/A & Computacional (GPU/CPU local) \\
\hline
\end{tabular}
\end{table}

La clave metodológica es que ambos pipelines convergen en el mismo módulo de matching ESCO con estrategia de 3 capas (exact matching, fuzzy matching con umbral 85 por ciento, semantic matching con embeddings), garantizando que las diferencias observadas en los resultados provengan exclusivamente de la técnica de extracción utilizada en cada pipeline, no de variaciones en la normalización posterior. Esta arquitectura dual con punto de convergencia controlado permite responder la pregunta de investigación con validez científica.

El conjunto de decisiones documentadas en esta sección configura un sistema que equilibra pragmáticamente las restricciones de un proyecto académico desarrollado por dos personas con recursos computacionales limitados, con los estándares de rigor científico requeridos para investigación reproducible. La arquitectura híbrida con 7 etapas CRISP-DM especializadas, el stack tecnológico basado en herramientas maduras de código abierto con comunidad activa, y la estrategia dual de pipelines para comparación experimental controlada, conforman una arquitectura coherente y justificada que habilita la investigación propuesta dentro de los recursos disponibles manteniendo trazabilidad completa y reproducibilidad metodológica.

\subsection{Consideraciones de diseño y mitigación de limitaciones}

Para abordar las limitaciones inherentes a la arquitectura de pipeline lineal y optimizar el rendimiento dentro de las restricciones académicas, se implementaron seis estrategias de diseño complementarias que maximizan robustez, performance y calidad de datos. La Tabla \ref{tab:estrategias-optimizacion} resume las estrategias implementadas con sus beneficios cuantificados.

\begin{table}[H]
\centering
\caption{Estrategias de Optimización y Mitigación de Limitaciones}
\label{tab:estrategias-optimizacion}
\begin{tabular}{|p{3.5cm}|p{6cm}|p{4cm}|}
\hline
\textbf{Estrategia} & \textbf{Implementación} & \textbf{Beneficio Cuantificado} \\
\hline
Persistencia Intermedia y Checkpointing & Cada etapa persiste en PostgreSQL antes de continuar. Campo extraction\_status permite reanudar desde última etapa completada. & Sistema reinicia desde cualquier etapa sin reprocesar dataset completo \\
\hline
Deduplicación Multi-Nivel & Nivel 1: SHA-256 hash (title+company+desc). Nivel 2: UNIQUE(job\_id, skill, method). Nivel 3: UNIQUE(skill\_text) & Elimina duplicados con 0\% falsos positivos. Reduce de 30K a 23,188 ofertas únicas \\
\hline
Batch Processing Optimizado & Embeddings batch=32. Inserts BD batch=100. Cursor server-side PostgreSQL & Throughput 721 skills/s (vs 50 skills/s sin batching) \\
\hline
Índices BD Optimizados & Índices GIN para full-text en ESCO labels. Índices en foreign keys para joins & Matching 2,756 skills en menor a 5 segundos \\
\hline
Logging Estructurado y Monitoreo & Niveles DEBUG/INFO/WARNING/ERROR. Timestamps. Métricas por etapa. Progress bars tqdm & Identificación rápida de cuellos de botella y errores sistemáticos \\
\hline
Validación y Tests Automatizados & 37 tests en test\_embeddings.py. Validación normalización L2. Tests similitud semántica & Detección temprana de degradación de calidad \\
\hline
\end{tabular}
\end{table}

La estrategia de persistencia intermedia garantiza que fallos en etapas avanzadas del pipeline (como clustering después de 4 horas de procesamiento) no requieran reejecutar desde scraping, reduciendo significativamente el tiempo de debugging durante desarrollo. La deduplicación multi-nivel implementa tres capas de verificación progresivamente más estrictas: el hash SHA-256 en scraping previene almacenamiento de ofertas completamente idénticas, la restricción UNIQUE compuesta en extracción elimina redundancia de habilidades detectadas múltiples veces en la misma oferta por diferentes métodos, y la unicidad de skill text en embeddings asegura que cada competencia se vectorice exactamente una vez independientemente de su frecuencia en el corpus.

El batch processing optimizado reconoce que los modelos de embeddings (E5 Multilingual) y las operaciones de base de datos tienen overhead fijo por invocación, por lo que procesar 32 skills simultáneamente amortiza el costo de carga del modelo en GPU y reduce el número de roundtrips a PostgreSQL de 10,000 transacciones individuales a 100 transacciones batch, multiplicando throughput por factor de 14x. Los índices GIN (Generalized Inverted Index) de PostgreSQL permiten búsqueda full-text eficiente en las 14,174 etiquetas ESCO sin realizar table scans lineales, mientras que índices en foreign keys aceleran joins entre raw jobs y extracted skills de tiempo cuadrático a logarítmico.

El logging estructurado con niveles jerárquicos permite debugging granular durante desarrollo (DEBUG) mientras mantiene logs limpios en producción (INFO), con timestamps precisos que facilitan análisis de performance y detección de regresiones entre ejecuciones. La suite de 37 tests automatizados verifica invariantes críticos como normalización L2 de embeddings (norma euclidiana igual a 1.0 con tolerancia de 1e-6), ausencia de valores NaN o Infinito que corromperían clustering, y coherencia semántica donde habilidades relacionadas como Python y Django deben tener similitud coseno mayor a 0.7.

\subsection{Vistas arquitectónicas del sistema}

Para documentar completamente la arquitectura híbrida del observatorio, se presentan dos vistas complementarias siguiendo el Modelo 4+1 de Vistas Arquitectónicas. La Vista Lógica describe la organización funcional de componentes y sus relaciones, mientras que la Vista Física documenta la topología de despliegue en el servidor de producción con especificaciones de infraestructura.

\subsubsection{Vista Lógica}

La Vista Lógica (Figura \ref{fig:vista-logica-sad}) presenta la arquitectura híbrida con sus tres patrones integrados: API Gateway implementado con Nginx para routing y terminación SSL, Microservicios en Capas con comunicación Request/Response para operaciones síncronas (Frontend Next.js, API FastAPI, PostgreSQL), y Event-Driven Architecture con comunicación Pub/Sub mediante Redis para procesamiento asíncrono distribuido (Celery Beat publica tareas, Workers las ejecutan). El diagrama muestra cómo ambos flujos convergen en PostgreSQL como fuente única de verdad, garantizando consistencia de datos independientemente del mecanismo de acceso.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{diagrams/VistaLogica.png}
\caption{Vista Lógica del Sistema. Arquitectura híbrida integrando API Gateway (Nginx), Microservicios en Capas (comunicación síncrona Request/Response), y Event-Driven Architecture (comunicación asíncrona Pub/Sub mediante Redis y Celery).}
\label{fig:vista-logica-sad}
\end{figure}

\subsubsection{Vista Física}

La Vista Física (Figura \ref{fig:vista-fisica-sad}) documenta el despliegue en servidor de producción con especificaciones de hardware: CPU Intel Xeon E5-2686 v4 de 8 núcleos a 2.3 GHz, 32 GB RAM DDR4, almacenamiento SSD NVMe de 500 GB, y sistema operativo Ubuntu 22.04 LTS. El diagrama muestra los siete contenedores Docker orquestados mediante Docker Compose (frontend, api, postgres, redis, celery-worker, celery-beat, nginx), red bridge interna con aislamiento de seguridad, mapeo de puertos (80/443 para Nginx, 3000 para Frontend, 8000 para API), y volúmenes persistentes para PostgreSQL y Redis que garantizan durabilidad de datos ante reinicios de contenedores.

\begin{figure}[H]
\centering
\makebox[\textwidth]{\includegraphics[width=1.25\textwidth]{diagrams/VistaFisica.png}}
\caption{Vista Física del Sistema. Servidor de producción con especificaciones de hardware, contenedores Docker orquestados por Docker Compose, red bridge interna, mapeo de puertos, y volúmenes persistentes para PostgreSQL y Redis.}
\label{fig:vista-fisica-sad}
\end{figure}

La arquitectura implementada cumple con cinco características esenciales para el proyecto: modularidad donde cada componente puede ejecutarse y desarrollarse independientemente facilitando debugging y mantenimiento, trazabilidad completa mediante foreign keys que conectan cualquier resultado analítico con la oferta laboral original que lo generó, reproducibilidad científica garantizada por parámetros fijos documentados en código con semillas aleatorias controladas y versiones específicas de dependencias, escalabilidad horizontal mediante batch processing optimizado con índices de base de datos especializados y capacidad de particionamiento futuro, y resiliencia operativa con persistencia intermedia en cada etapa del pipeline más checkpoints que permiten recuperación ante fallos y manejo robusto de errores.

\subsection{Componentes del Sistema}

\subsubsection{Servicio de Web Scraping}

Administra la recolección automatizada de ofertas laborales desde 11 portales en 3 países. Implementado con Scrapy 2.11 (asíncrono) complementado con Selenium 4.15 para contenido JavaScript dinámico. Deduplicación mediante SHA-256 hash, almacenamiento en tabla raw\_jobs.

\subsubsection{Servicio de Extracción de Habilidades}

Identifica competencias técnicas mediante tres técnicas complementarias: NER con spaCy + EntityRuler ESCO, regex con 47 patterns, y normalización con matching de 2 capas (exact + fuzzy). Persistencia en extracted\_skills.

\subsubsection{Servicio de Procesamiento con LLM}

Enriquecimiento semántico usando Gemma 3 4B o Llama 3 3B (sujeto a evaluación comparativa). Maneja Spanglish técnico, normaliza con ESCO, genera justificaciones explicables. Persistencia en enhanced\_skills.

\subsubsection{Servicio de Generación de Embeddings}

Transforma habilidades en vectores densos 768D mediante E5 multilingual-base. Procesamiento por lotes (batch\_size=32), normalización L2, almacenamiento en skill\_embeddings con soporte pgvector. Construcción de índice FAISS para búsquedas rápidas.

\subsubsection{Servicio de Análisis y Visualización}

Descubrimiento de patrones mediante UMAP (768D $\rightarrow$ 2-3D), clustering HDBSCAN, generación de visualizaciones con matplotlib/seaborn, exportación multi-formato (PDF, PNG, CSV, JSON). Persistencia en analysis\_results.

\subsection{Diseño de la Base de Datos}

La base de datos actúa como columna vertebral del sistema, implementando el patrón de persistencia de pipeline donde cada etapa escribe resultados en tablas especializadas.

\subsubsection{Esquema de Tablas Principales}

La tabla raw\_jobs almacena ofertas tal como fueron scrapeadas con campos job\_id UUID, portal, country, url, title, description, content\_hash SHA-256 e is\_usable flag. La tabla cleaned\_jobs contiene texto limpio y normalizado con job\_id FK, title\_cleaned, description\_cleaned, combined\_text pre-computado y word\_count.

La tabla extracted\_skills registra habilidades identificadas mediante extraction\_id UUID, job\_id FK, skill\_text, extraction\_method, confidence\_score, esco\_uri y mapping\_method. La tabla esco\_skills funciona como taxonomía de referencia con esco\_uri PK, preferred\_label\_es, preferred\_label\_en, alt\_labels, skill\_type totalizando 14,174 registros.

La tabla skill\_embeddings almacena representaciones vectoriales con embedding\_id UUID, skill\_text UNIQUE, embedding vector[768], model\_name y created\_at. La tabla analysis\_results persiste resultados de clustering con analysis\_id UUID, analysis\_type, country, date\_range, parameters JSONB y results JSONB.

Todas las tablas derivadas mantienen referencia mediante foreign key hacia raw\_jobs, garantizando trazabilidad completa desde cualquier resultado hasta la oferta original.
