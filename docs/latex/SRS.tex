% ============================================================================
% ESPECIFICACIÓN DE REQUERIMIENTOS DE SOFTWARE (SRS)
% Observatorio de Demanda Laboral en Tecnología en Latinoamérica
% ============================================================================

\documentclass[11pt,oneside,letterpaper]{report}

% ============================================================================
% PAQUETES
% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[spanish,es-tabla]{babel}
\usepackage[letterpaper,top=3cm,bottom=3cm,left=3cm,right=3cm]{geometry}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{tocloft}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{array}
\usepackage{booktabs}
\usepackage[backend=biber,style=ieee,citestyle=numeric-comp,sorting=none]{biblatex}

% TikZ para diagramas
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, shadows, fit, shapes.multipart}

% ============================================================================
% CONFIGURACIÓN DE INTERLINEADO
% ============================================================================
\onehalfspacing

% ============================================================================
% CONFIGURACIÓN DE ENCABEZADOS Y PIE DE PÁGINA
% ============================================================================
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Pontificia Universidad Javeriana}
\fancyhead[R]{SRS - Observatorio de Demanda Laboral}
\fancyfoot[R]{Página \thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\fancypagestyle{plain}{%
  \fancyhf{}%
  \fancyhead[L]{Pontificia Universidad Javeriana}
  \fancyhead[R]{SRS - Observatorio de Demanda Laboral}
  \fancyfoot[R]{Página \thepage}
  \renewcommand{\headrulewidth}{0.4pt}
  \renewcommand{\footrulewidth}{0.4pt}
}

% ============================================================================
% CONFIGURACIÓN DE TÍTULOS
% ============================================================================
\titleformat{\chapter}[display]
{\normalfont\Large\bfseries\centering}
{}{0pt}{\Large}

\titlespacing*{\chapter}{0pt}{20pt}{20pt}

\titleformat{\section}
{\normalfont\large\bfseries}{\thesection}{1em}{}

\titleformat{\subsection}
{\normalfont\normalsize\bfseries}{\thesubsection}{1em}{}

\titleformat{\subsubsection}
{\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}

% ============================================================================
% INFORMACIÓN DEL DOCUMENTO
% ============================================================================
\newcommand{\proyectoTitulo}{Observatorio de Demanda Laboral en Tecnología en Latinoamérica}
\newcommand{\proyectoCodigo}{CIS2025CP08}
\newcommand{\autorUno}{Nicolas Francisco Camacho Alarcón}
\newcommand{\autorDos}{Alejandro Pinzón Fajardo}
\newcommand{\director}{Ing. Luis Gabriel Moreno Sandoval}
\newcommand{\anio}{2025}
\newcommand{\mes}{Noviembre}
\newcommand{\version}{Versión 2.1 - Fase 0 Implementada}
\newcommand{\fecha}{Noviembre 2025}

% ============================================================================
% DOCUMENTO
% ============================================================================
\begin{document}

% ============================================================================
% PORTADA
% ============================================================================
\begin{titlepage}
\centering

\vspace*{1cm}

{\Large\bfseries \proyectoTitulo\par}
\vspace{0.5cm}
{[Grupo 8]\par}

\vspace{2cm}

{\Large\bfseries ESPECIFICACIÓN DE REQUERIMIENTOS DE SOFTWARE\par}

\vspace{1cm}

{[\fecha]\par}
\vspace{0.5cm}
{[\version]\par}

\vspace{2cm}

\includegraphics[width=0.4\textwidth]{figures/logo-javeriana.png}

\vfill

{\large Autores:\par}
\vspace{0.5cm}
{\large \autorUno\par}
{\large \autorDos\par}

\vspace{1cm}

{\large Pontificia Universidad Javeriana\par}
{\large Facultad de Ingeniería\par}
{\large Bogotá, Colombia\par}
{\large \mes{} de \anio\par}

\end{titlepage}

% ============================================================================
% TABLA DE CONTENIDOS
% ============================================================================
\renewcommand{\contentsname}{CONTENIDO}
\tableofcontents
\newpage

% ============================================================================
% CAPÍTULO 1: INTRODUCCIÓN
% ============================================================================
\chapter{INTRODUCCIÓN}

\section{Propósito}

El presente documento tiene como propósito especificar de manera detallada los requerimientos funcionales y no funcionales del sistema denominado \textit{Observatorio de Demanda Laboral en Tecnología en Latinoamérica}, una herramienta de análisis automatizado orientada a procesar, extraer y segmentar habilidades tecnológicas desde portales de empleo en línea, mediante técnicas modernas de procesamiento de lenguaje natural (NLP), scraping, embeddings semánticos y clustering no supervisado.

Este documento está dirigido principalmente a los siguientes públicos:

\begin{itemize}
    \item El equipo de desarrollo del proyecto, compuesto por los estudiantes \autorUno{}, \autorDos{} y Daniel Vidal, como guía estructurada para la implementación y validación del sistema.

    \item El director del proyecto, \director{}, y los jurados evaluadores, como evidencia formal del entendimiento técnico y conceptual del producto a desarrollar.

    \item Otros actores académicos o institucionales interesados en replicar o adaptar el sistema en contextos similares, como universidades, centros de investigación o entidades públicas vinculadas al análisis del mercado laboral.
\end{itemize}

El documento cubre la totalidad del sistema propuesto, sin limitarse a un solo módulo o subsistema. Por tanto, especifica requerimientos relacionados con la adquisición de datos (scraping), su procesamiento semántico, análisis estadístico y segmentación por perfiles laborales, así como aspectos de validación, modularidad, documentación técnica y estándares de calidad.

La importancia de este documento radica en su papel como contrato técnico entre los actores involucrados, asegurando una visión compartida del comportamiento esperado del sistema, las restricciones existentes, los criterios de aceptación y los estándares metodológicos adoptados. Además, facilita la trazabilidad entre los objetivos definidos en la propuesta de grado y las funcionalidades implementadas en cada fase, garantizando coherencia metodológica, control de calidad y sostenibilidad del desarrollo.

\section{Alcance}

El sistema propuesto, titulado \textit{\proyectoTitulo}, tiene como propósito desarrollar una herramienta automatizada capaz de analizar la evolución de las habilidades tecnológicas demandadas en el mercado laboral digital, específicamente en los países de Colombia (CO), México (MX) y Argentina (AR). El sistema abarca desde la recolección periódica de datos a través de scraping en portales de empleo hasta el procesamiento semántico y la segmentación de perfiles laborales utilizando técnicas avanzadas de NLP y clustering no supervisado.

\textbf{Alcance geográfico:} Colombia (CO), México (MX), Argentina (AR)

\textbf{Fuentes de datos:} 11 portales de empleo incluyendo hiring.cafe, computrabajo, bumeran, elempleo.com, zonajobs, infojobs, entre otros.

\textbf{Taxonomía base:} El sistema utiliza una taxonomía unificada de \textbf{14,174 skills totales}, compuesta por:
\begin{itemize}
    \item \textbf{ESCO v1.1.0:} 13,939 skills (base europea de competencias laborales)
    \item \textbf{O*NET Hot Technologies:} 152 skills (tecnologías emergentes del sector IT)
    \item \textbf{Manual Curated Skills:} 83 skills (específicas para el mercado tech latinoamericano)
\end{itemize}

\textbf{Stack tecnológico:} Python 3.10+, Scrapy, spaCy, PostgreSQL, FAISS, embeddings E5 multilingües (intfloat/multilingual-e5-base, 768D), HDBSCAN, UMAP.

El producto incluirá las siguientes funcionalidades principales:

\begin{itemize}
    \item Extracción automatizada de vacantes desde 11 portales web, mediante spiders adaptables que operan respetando las normas de uso de cada sitio.

    \item Procesamiento y limpieza textual, incluyendo tokenización, lematización y detección de habilidades explícitas mediante técnicas híbridas de NER, expresiones regulares y modelos de lenguaje.

    \item Representación semántica de habilidades mediante embeddings multilingües (modelo E5 de 768 dimensiones) y reducción de dimensionalidad (UMAP).

    \item Mapeo de habilidades contra taxonomía ESCO mediante estrategia de tres capas: matching exacto, fuzzy matching y semantic matching con FAISS.

    \item Agrupamiento de perfiles mediante algoritmos robustos como HDBSCAN, que permitan segmentar la demanda en grupos funcionales coherentes.

    \item Visualización macro de resultados a través de gráficos interpretables y reportes estáticos que permitan identificar patrones emergentes sin requerir dashboards interactivos.

    \item Documentación metodológica y código reproducible, que permitan replicar o adaptar el sistema a otras regiones o sectores, bajo principios de ética, apertura y eficiencia computacional.
\end{itemize}

Este sistema no contempla el desarrollo de una interfaz web pública ni funcionalidades de tipo portal o dashboard interactivo, sino que prioriza la generación de reportes analíticos estáticos y reutilizables, como producto de validación académica y técnica.

El alcance funcional se circunscribe al dominio de las ofertas de empleo tecnológicas publicadas en español en los países mencionados, sin contemplar vacantes en otros idiomas ni otros sectores económicos. Sin embargo, el diseño modular del sistema permitirá su adaptación futura a nuevos contextos geográficos o temáticos.

\section{Definiciones, Acrónimos y Abreviaciones}

\subsection{Portales de empleo}
Son plataformas web donde empresas publican vacantes laborales y profesionales buscan oportunidades. En este proyecto se consideran fuentes como LinkedIn, Computrabajo, Bumeran, ZonaJobs e Indeed, que constituyen insumos primarios para los procesos de scraping y análisis.

\subsection{Web Scraping}
Técnica de recolección automatizada de datos desde páginas web, utilizando librerías como BeautifulSoup, Selenium o Playwright. Permite extraer de forma estructurada información relevante de las ofertas publicadas.

\subsection{Oferta laboral}
Se refiere al anuncio publicado por una organización donde se describe el perfil buscado, incluyendo título del cargo, funciones, requisitos y habilidades deseadas.

\subsection{Base de datos relacional (PostgreSQL)}
Sistema que organiza los datos recolectados en tablas interconectadas, facilitando su consulta, limpieza y posterior análisis mediante estructuras SQL.

\subsection{Normalización de datos}
Proceso de limpieza, estandarización y unificación de formatos para reducir ambigüedad, errores y duplicados, y mejorar la coherencia del análisis posterior.

\subsection{Expresiones regulares (Regex)}
Lenguaje sintáctico utilizado para identificar y extraer patrones textuales específicos (como frases que contengan habilidades o requisitos) en grandes volúmenes de texto.

\subsection{Named Entity Recognition (NER)}
Técnica de procesamiento de lenguaje natural (NLP) que identifica y clasifica entidades en un texto, como nombres de habilidades, empresas o tecnologías.

\subsection{Tokenización}
Consiste en dividir un texto en unidades mínimas llamadas ``tokens'' (palabras, signos u oraciones), facilitando el análisis lingüístico automatizado.

\subsection{Lematización}
Proceso que transforma las palabras a su forma canónica o raíz gramatical, permitiendo uniformar variaciones morfológicas del lenguaje.

\subsection{Stopwords}
Términos frecuentes sin valor informativo (como ``de'', ``por'', ``la''), comúnmente eliminados en tareas de procesamiento textual.

\subsection{Co-ocurrencia}
Medida estadística que indica la frecuencia con que dos o más términos aparecen juntos en un texto, útil para detectar relaciones semánticas.

\subsection{Bigramas y trigramas}
Secuencias de dos o tres palabras consecutivas utilizadas para capturar patrones de lenguaje más complejos que las palabras individuales.

\subsection{LLM (Large Language Models)}
Modelos de lenguaje de gran escala (como GPT o T5) entrenados sobre corpus masivos, capaces de generar texto, extraer conocimiento implícito y realizar razonamiento contextualizado.

\subsection{Prompt Engineering}
Diseño estratégico de instrucciones o ejemplos para guiar la salida de un LLM, crucial en tareas de extracción de habilidades o clasificación de ocupaciones.

\subsection{Few-shot learning}
Habilidad de los LLMs para realizar tareas complejas con pocos ejemplos, lo cual resulta clave cuando se carece de datasets etiquetados masivamente en español.

\subsection{Embeddings semánticos}
Representaciones numéricas de textos que capturan similitudes semánticas, permitiendo análisis cuantitativos y clustering. Ejemplos incluyen word2vec, BERT y E5.

\subsection{Embeddings multilingües}
Vectores entrenados para representar texto en múltiples idiomas en un mismo espacio semántico. Son esenciales para manejar contenido mixto español-inglés en ofertas laborales.

\subsection{UMAP (Reducción de dimensionalidad)}
Técnica que transforma espacios de alta dimensionalidad en representaciones más simples, conservando la estructura semántica subyacente para facilitar análisis y visualización.

\subsection{Clustering (HDBSCAN)}
Algoritmo no supervisado que detecta grupos naturales de observaciones (como habilidades o perfiles laborales) según su similitud semántica, sin requerir número de clusters predefinido.

\subsection{Taxonomía de habilidades (ESCO, CIUO-08, O*NET)}
Sistemas jerárquicos y normalizados de clasificación de habilidades y ocupaciones, fundamentales para anclar el análisis a estándares internacionales y mejorar interoperabilidad de los resultados.

\subsection{FAISS (Facebook AI Similarity Search)}
Biblioteca de código abierto para búsqueda eficiente de similitud en espacios vectoriales de alta dimensionalidad. El sistema utiliza FAISS IndexFlatIP para búsqueda exacta de vecinos más cercanos con producto interno, logrando velocidades de 30,147 consultas por segundo, aproximadamente 25 veces más rápido que PostgreSQL con pgvector.

\subsection{Estrategia de tres capas (Three-layer matching)}
Metodología implementada para mapear habilidades extraídas contra la taxonomía ESCO:
\begin{itemize}
    \item \textbf{Layer 1 - Exact Match:} Búsqueda exacta mediante SQL ILIKE con confianza 1.0
    \item \textbf{Layer 2 - Fuzzy Match:} Similitud difusa con fuzzywuzzy, threshold 0.92, confianza 0.92-1.0
    \item \textbf{Layer 3 - Semantic Match:} Búsqueda semántica con FAISS, threshold 0.87, confianza 0.87-1.0 (actualmente deshabilitado debido a limitaciones del modelo E5 con vocabulario técnico)
\end{itemize}

\subsection{Skills emergentes}
Habilidades extraídas de ofertas laborales que no pueden ser mapeadas a la taxonomía ESCO existente. Representan el 87.4\% de las skills extraídas y constituyen una señal valiosa sobre tendencias emergentes del mercado tech latinoamericano, no un fallo del sistema.

\subsection{Natural Language Processing (NLP)}
Conjunto de técnicas de inteligencia artificial, combinando modelos de lingüística computacional, machine learning y aprendizaje profundo, para poder procesar lenguaje humano.

\subsection{Python}
Lenguaje de programación ampliamente utilizado en ciencia de datos y NLP, por su sintaxis sencilla y librerías especializadas como scikit-learn, spaCy, transformers y pandas.

\section{Apreciación Global}

El presente documento de Especificación de Requerimientos del Software (SRS) tiene como objetivo presentar de manera estructurada y detallada los aspectos fundamentales del sistema ``\proyectoTitulo''. La organización del documento se ha realizado con el propósito de facilitar su comprensión tanto para usuarios técnicos como no técnicos, brindando una visión progresiva desde el contexto general hasta los requerimientos específicos del sistema.

El contenido del documento se distribuye de la siguiente manera:

\begin{itemize}
    \item En la \textbf{Sección 1}, se expone la introducción general del proyecto, incluyendo su propósito, alcance, definiciones clave, referencias utilizadas y una apreciación global de su contenido.

    \item La \textbf{Sección 2} describe de manera general los factores que afectan al producto, incluyendo su perspectiva, interfaces con otros sistemas y con el usuario, consideraciones de hardware y software, restricciones de memoria, operaciones del sistema y requerimientos de adaptación al entorno.

    \item La \textbf{Sección 3} presentará los requerimientos funcionales y no funcionales del sistema, detallando cada una de las funcionalidades esperadas, así como las restricciones y condiciones necesarias para su correcto funcionamiento.

    \item En las \textbf{Secciones 4 y 5}, se incluirán descripciones de cómo se piensan manejar los requerimientos mencionados anteriormente, así como el proceso de verificación y validación.

    \item Finalmente, se anexarán diagramas, tablas de trazabilidad y otros elementos que complementen la especificación del sistema.
\end{itemize}

Este documento servirá como base para el diseño, desarrollo, validación y evaluación del sistema propuesto, asegurando que todos los actores involucrados compartan una visión clara y consensuada de los objetivos, alcances y funcionalidades del software a implementar.

\section{Estado Actual de Implementación}

\textbf{Versión del sistema:} 2.1

\textbf{Última actualización:} Octubre 22, 2025

\textbf{Estado general:} Implementación en Progreso

\subsection{Fases Completadas}

\begin{itemize}
    \item \textbf{Fase 0 - Configuración Inicial:} ✅ COMPLETADA (100\%)
    \begin{itemize}
        \item Taxonomía de skills cargada: 14,174 skills (ESCO 13,939 + O*NET 152 + Manual 83)
        \item Embeddings generados: 14,133 embeddings únicos (768D, L2-normalized)
        \item Índice FAISS construido: 41.41 MB, 30,147 queries/segundo
        \item Tiempo total de ejecución: ~25 segundos
        \item Tests automatizados: 37 tests, 94.6\% pass rate
    \end{itemize}

    \item \textbf{Fase 1 - Recolección y Limpieza de Datos:} ✅ COMPLETADA (100\%)
    \begin{itemize}
        \item Total jobs scraped: 23,352 (hiring.cafe: 23,313, elempleo: 38, zonajobs: 1)
        \item Jobs limpios y utilizables: 23,188 (99.5\%)
        \item Jobs basura filtrados: 125 (0.5\%)
        \item Promedio palabras por job: 552
        \item Deduplicación implementada: SHA256 content hash
    \end{itemize}

    \item \textbf{Fase 2 - Extracción de Skills (Pipeline A):} ✅ IMPLEMENTADA
    \begin{itemize}
        \item Método Regex: 200+ patrones tecnológicos, 78-89\% precision
        \item Método NER: spaCy es\_core\_news\_sm + custom entity ruler
        \item Mapeo ESCO: Layer 1 (Exact) + Layer 2 (Fuzzy) activos
        \item Match rate actual: 12.6\% (esperado para taxonomías 2016-2017)
        \item Test con 100 jobs: 100\% success rate, 2,756 skills extraídas
    \end{itemize}
\end{itemize}

\subsection{Fases Pendientes}

\begin{itemize}
    \item \textbf{Fase 2 - Pipeline B (LLM-based):} ❌ NO IMPLEMENTADO
    \begin{itemize}
        \item Extracción con LLMs (GPT, Mistral, Llama)
        \item Comparación Pipeline A vs Pipeline B
    \end{itemize}

    \item \textbf{Módulo 6 - Clustering:} ❌ NO IMPLEMENTADO
    \begin{itemize}
        \item UMAP dimensionality reduction
        \item HDBSCAN clustering
        \item Análisis temporal de clusters
    \end{itemize}

    \item \textbf{Módulo 7 - Visualizaciones:} ❌ NO IMPLEMENTADO
    \begin{itemize}
        \item Gráficos interactivos (Plotly)
        \item Reportes analíticos estáticos
        \item Network analysis de co-ocurrencias
    \end{itemize}
\end{itemize}

\subsection{Decisiones Técnicas Importantes}

\begin{itemize}
    \item \textbf{Layer 3 Semantic Matching DESHABILITADO:} El modelo E5 multilingual demostró ser inadecuado para vocabulario técnico, generando matches absurdos (ej: "React" → "neoplasia" con score 0.82). Se mantendrá deshabilitado hasta implementar un modelo domain-specific o LLM-based classification.

    \item \textbf{Match rate 12.6\% es ACEPTABLE:} ESCO/O*NET son taxonomías tradicionales (2016-2017) que no cubren frameworks modernos. El 87.4\% de skills emergentes representa señal valiosa del mercado, no un fallo.

    \item \textbf{FAISS performance validada:} 30,147 q/s, 25x más rápido que PostgreSQL pgvector, con 100\% precision (IndexFlatIP exact search).
\end{itemize}

% ============================================================================
% CAPÍTULO 2: DESCRIPCIÓN GENERAL
% ============================================================================
\chapter{DESCRIPCIÓN GENERAL}

\section{Perspectiva del Producto}

El sistema propuesto, denominado \textit{Observatorio de Demanda Laboral en Tecnología en Latinoamérica}, corresponde a un producto completamente nuevo, diseñado desde cero con el fin de ofrecer una solución automatizada, académicamente robusta y técnicamente escalable para el análisis de habilidades tecnológicas demandadas en el mercado laboral digital. Si bien es novedosa la implementación y el diseño, se basa en múltiples componentes o sistemas similares ya propuestos en literatura Europea, Africana, y Estadounidense, lo que robustece la facilidad de su implementación.

Este sistema no reemplaza una herramienta previa ni se integra como módulo en un sistema existente; por el contrario, responde a una necesidad actual no cubierta de manera integral en los contextos académico y gubernamental latinoamericano: la escasez de plataformas automatizadas que permitan entender en profundidad la evolución semántica de las habilidades requeridas en el sector tecnológico, a partir de vacantes en línea publicadas en español.

La decisión de desarrollar este producto surge de una combinación de motivaciones técnicas y sociales:

\begin{itemize}
    \item Desde el punto de vista técnico, se busca aplicar metodologías avanzadas de procesamiento de lenguaje natural, extracción de entidades, embeddings multilingües y clustering semántico para lograr una segmentación coherente y útil de perfiles laborales.

    \item Desde una perspectiva social y estratégica, se pretende brindar herramientas de análisis que apoyen a universidades, centros de formación, entidades públicas y actores del ecosistema digital en la identificación temprana de brechas de habilidades, facilitando la toma de decisiones informadas en política educativa, empleabilidad y reconversión laboral.
\end{itemize}

El sistema propuesto se distingue de otros enfoques parciales por su carácter modular, reproducible y enfocado en la generación de conocimiento estratégico a partir de datos abiertos. Su desarrollo también permitirá validar técnicas emergentes de extracción e inferencia con LLMs en español, contribuyendo al cuerpo académico y técnico en ciencia de datos aplicada al empleo.

En síntesis, el Observatorio representa un aporte original y pertinente tanto desde el plano metodológico como desde su aplicabilidad social, al combinar scraping ético, NLP multilingüe y visualización analítica para abordar un problema real en el contexto latinoamericano.

\subsection{Interfaces con el sistema}

El sistema ``Observatorio de Demanda Laboral en Tecnología en Latinoamérica'' es un producto completamente nuevo y autónomo, por lo cual no mantiene interfaces directas con sistemas externos en tiempo de ejecución. Sin embargo, presenta una arquitectura modular interna donde cada componente se comunica con los demás a través de interfaces internas bien definidas.

El flujo funcional del sistema se organiza de manera secuencial y conectada, iniciando con el módulo de web scraping, el cual actúa como punto de entrada para la adquisición de datos desde portales de empleo. Cada uno de los módulos siguientes procesa la información recibida del anterior, generando una línea de procesamiento de datos estructurada y continua.

Las interfaces internas entre módulos incluyen:

\begin{itemize}
    \item \textbf{Scraping → Almacenamiento estructurado:} Cada spider obtiene vacantes desde portales como Computrabajo, Bumeran y elempleo.com, y deposita los datos en bruto en archivos .json o .csv, los cuales son posteriormente integrados a una base de datos PostgreSQL.

    \item \textbf{Almacenamiento → Procesamiento semántico:} Los textos extraídos son lematizados, tokenizados y limpiados utilizando librerías como spaCy, nltk y regex, para luego ser enviados al módulo de extracción de habilidades.

    \item \textbf{Procesamiento semántico → Enriquecimiento con LLMs:} Las habilidades explícitas e implícitas se extraen mediante NER, expresiones regulares y modelos de lenguaje como BETO, E5, o T5, ya sea de forma local o descargados desde Hugging Face.

    \item \textbf{Enriquecimiento → Embeddings y clustering:} Los textos enriquecidos se transforman en vectores utilizando SentenceTransformers y fastText, y se agrupan mediante algoritmos como HDBSCAN y reducción de dimensionalidad (UMAP).

    \item \textbf{Clustering → Visualización:} Finalmente, los resultados son representados en gráficos estáticos y reportes PDF generados con herramientas como matplotlib, Plotly, pandas y Jinja2.
\end{itemize}

Aunque el sistema hace uso intensivo de librerías de código abierto, estas no constituyen interfaces con sistemas externos, sino dependencias internas gestionadas como paquetes dentro del entorno local o virtual del proyecto (por ejemplo, vía pip o conda). No se consumen APIs externas, ni se establece comunicación en tiempo real con servicios web de terceros.

Este diseño modular y desacoplado permite mantener un alto grado de interoperabilidad y facilita la posibilidad de sustituir o extender cada módulo de forma independiente, sin afectar la funcionalidad del sistema completo.

\subsection{Interfaces con el usuario}

El sistema propuesto no cuenta con una interfaz gráfica pública ni con una aplicación de tipo portal web. En su lugar, la interacción con los usuarios se realizará por medio de interfaces técnicas de consola, notebooks ejecutables, scripts de configuración y reportes estáticos generados en PDF, HTML o gráficos .png/.svg.

A continuación, se describen las interfaces de usuario previstas, junto con sus características técnicas y funcionales:

\subsubsection{Terminal o Consola (CLI)}

\begin{itemize}
    \item \textbf{Propósito:} Interacción principal de los desarrolladores e investigadores con el sistema.
    \item \textbf{Acciones permitidas:} Ejecución de spiders, procesamiento de datos, entrenamiento de modelos, generación de reportes.
    \item \textbf{Requisitos técnicos:} Acceso a un entorno UNIX-like (Linux/macOS recomendado) o WSL en Windows; Python 3.10+ instalado.
    \item \textbf{Usabilidad:} Requiere conocimientos intermedios en línea de comandos. El entrenamiento estimado para familiarizarse con los comandos básicos es inferior a 1 hora para usuarios técnicos.
\end{itemize}

\subsubsection{Notebooks Jupyter}

\begin{itemize}
    \item \textbf{Propósito:} Validación de resultados, visualización exploratoria y pruebas modulares.
    \item \textbf{Acciones permitidas:} Carga de resultados del clustering, análisis gráfico, pruebas de embeddings, revisión de habilidades extraídas.
    \item \textbf{Requisitos técnicos:} Instalación local de jupyterlab o uso en plataforma cloud (ej. Google Colab). Se recomienda resolución mínima de 1366x768 para una visualización óptima.
    \item \textbf{Usabilidad:} Las notebooks están documentadas con celdas de texto y ejemplos reproducibles. Se espera un nivel básico de familiaridad con Python y Pandas por parte del usuario.
\end{itemize}

\subsubsection{Visualizaciones estáticas y reportes}

\begin{itemize}
    \item \textbf{Propósito:} Consulta de resultados finales en formato visual o tabular, por parte del equipo académico, directivo o institucional.
    \item \textbf{Tipos de salida:} Archivos .pdf, .png, .html o .md generados automáticamente desde scripts Python.
    \item \textbf{Requisitos técnicos:} Visualizador de PDF o navegador moderno actualizado (Chrome, Firefox, Edge). No se requiere conexión a internet tras la generación del archivo.
    \item \textbf{Usabilidad:} Interfaz pasiva. Los reportes están diseñados para facilitar la interpretación con títulos, leyendas y estructura clara.
\end{itemize}

\section{Funciones del Producto}

El sistema Observatorio de Demanda Laboral en Tecnología en Latinoamérica debe cumplir con una serie de funciones esenciales que permiten cubrir el ciclo completo de análisis automatizado de vacantes laborales. A continuación, se enumeran las principales funciones del producto:

\begin{enumerate}
    \item \textbf{Extracción automatizada de vacantes}
    \begin{itemize}
        \item Scraping periódico y configurable desde portales de empleo en español como Computrabajo, elempleo.com, Bumeran y LinkedIn.
        \item Filtros por país, cargo, modalidad, sector y fecha.
        \item Recolección estructurada de atributos como título del cargo, empresa, ubicación, modalidad, tecnologías mencionadas, y descripción completa.
    \end{itemize}

    \item \textbf{Preprocesamiento y limpieza textual}
    \begin{itemize}
        \item Tokenización, lematización y eliminación de ruido.
        \item Normalización de formatos y campos clave.
        \item Generación de bigramas y trigramas relevantes.
    \end{itemize}

    \item \textbf{Extracción de habilidades (explícitas e implícitas)}
    \begin{itemize}
        \item Detección mediante patrones regulares y NER.
        \item Enriquecimiento con LLMs multilingües para capturar habilidades implícitas y sinónimos contextuales.
        \item Clasificación en taxonomías como ESCO y CIUO.
    \end{itemize}

    \item \textbf{Vectorización y representación semántica}
    \begin{itemize}
        \item Embeddings mediante modelos como BETO, fastText y E5.
        \item Reducción de dimensionalidad con UMAP para visualización y agrupación.
    \end{itemize}

    \item \textbf{Clustering de perfiles laborales}
    \begin{itemize}
        \item Agrupamiento no supervisado mediante HDBSCAN.
        \item Evaluación con métricas de coherencia semántica y Silhouette Score.
        \item Identificación de segmentos funcionales de demanda tecnológica.
    \end{itemize}

    \item \textbf{Visualización y reporte}
    \begin{itemize}
        \item Generación de visualizaciones estáticas y reportes analíticos en CSV, PDF o HTML.
        \item Segmentación por país, portal y categoría tecnológica.
    \end{itemize}

    \item \textbf{Gestión y trazabilidad}
    \begin{itemize}
        \item Registro de logs, validaciones y errores.
        \item Documentación técnica accesible para usuarios académicos y replicadores.
        \item Modularidad para adaptarse a nuevos portales o países.
    \end{itemize}
\end{enumerate}

\section{Características de los Usuarios}

El sistema está diseñado para ser utilizado por distintos tipos de usuarios, cuyas características varían según su nivel de acceso, rol funcional, conocimientos técnicos y frecuencia de interacción. A continuación se describen las principales clases de usuario previstas:

\begin{table}[H]
\centering
\begin{tabular}{|p{4cm}|p{10cm}|}
\hline
\textbf{Característica} & \textbf{Descripción} \\
\hline
Nivel de seguridad o privilegios & Se establecen dos niveles principales de acceso:
\begin{itemize}
    \item \textbf{Administrador:} acceso completo al sistema, incluyendo configuración, ejecución y ajustes internos.
    \item \textbf{Analista:} acceso restringido a resultados, reportes y visualizaciones, sin modificar parámetros o lógica del sistema.
    \item Opcionalmente, se contempla un perfil de validador externo con acceso de solo lectura.
\end{itemize} \\
\hline
Roles & \textbf{Administrador técnico:} configura scraping, define filtros, ajusta modelos, ejecuta el pipeline completo.

\textbf{Investigador/Analista:} accede a resultados, visualiza reportes y comunica hallazgos sin intervenir en el procesamiento.

\textbf{Validador externo (opcional):} revisa calidad de extracción, validación semántica o consistencia de resultados con fines académicos o de control. \\
\hline
Nivel de estudios o experiencia técnica & El administrador debe tener formación en ingeniería, ciencia de datos o afines, con conocimientos sólidos en Python, NLP y manejo de entornos técnicos.

El analista requiere competencias básicas en análisis de datos, interpretación de visualizaciones y lectura de reportes técnicos.

El validador puede ser un docente, evaluador o experto externo sin conocimientos técnicos detallados. \\
\hline
Frecuencia de uso & El administrador accede típicamente una vez por semana o cuando se requiere actualizar scraping, modelos o ejecutar el sistema completo.

El analista accede cada dos semanas o mensualmente para consultar resultados y generar reportes.

El validador accede de forma puntual, en contextos de auditoría, revisión académica o validación de resultados. \\
\hline
\end{tabular}
\caption{Características de los usuarios del sistema}
\end{table}

\section{Restricciones}

El sistema propuesto presenta un conjunto de restricciones que condicionan su diseño, implementación y despliegue. Estas restricciones se clasifican en tres categorías principales: generales, de software y de hardware.

\subsection{Restricciones Generales}

\begin{itemize}
    \item \textbf{Alcance técnico-académico:} El sistema está diseñado con fines de investigación académica y validación técnica, no para uso comercial o despliegue masivo.

    \item \textbf{Idioma:} Todo el procesamiento textual está orientado a ofertas laborales en español, aunque se considera una posible presencia de términos en inglés.

    \item \textbf{Tolerancia a fallos:} Si bien el sistema cuenta con mecanismos de validación de datos y manejo básico de errores, no se implementarán estrategias avanzadas de tolerancia a fallos o disponibilidad continua.

    \item \textbf{Ejecución modular secuencial:} Las fases del sistema se ejecutarán en orden secuencial, sin requerimientos de paralelismo ni concurrencia en su versión inicial.
\end{itemize}

\subsection{Restricciones de Software}

\begin{itemize}
    \item \textbf{Versión de Python:} Python 3.10 o superior es requerido para compatibilidad con todas las librerías utilizadas.

    \item \textbf{Dependencia de librerías específicas:} El sistema depende del uso de bibliotecas especializadas:
    \begin{itemize}
        \item \textbf{Web Scraping:} Scrapy, BeautifulSoup, Selenium
        \item \textbf{NLP:} spaCy (es\_core\_news\_sm), transformers, SentenceTransformers
        \item \textbf{Embeddings:} intfloat/multilingual-e5-base (768D)
        \item \textbf{Búsqueda vectorial:} FAISS (Facebook AI Similarity Search)
        \item \textbf{Matching:} fuzzywuzzy (fuzzy string matching)
        \item \textbf{Clustering:} HDBSCAN, UMAP
        \item \textbf{Base de datos:} psycopg2 (PostgreSQL driver)
        \item \textbf{Data processing:} pandas, numpy
        \item \textbf{Visualización:} matplotlib, plotly, seaborn
    \end{itemize}

    \item \textbf{Base de datos:} PostgreSQL 13 o superior con soporte para arrays de tipo REAL[] para almacenamiento de embeddings de 768 dimensiones.

    \item \textbf{Modelo de embeddings:} El modelo \texttt{intfloat/multilingual-e5-base} debe ser descargado desde Hugging Face (tamaño aproximado: 1.1 GB).

    \item \textbf{Índice FAISS:} El archivo \texttt{esco.faiss} (41.41 MB) y su mapping asociado \texttt{esco\_mapping.pkl} (545 KB) deben estar disponibles en \texttt{data/embeddings/}.

    \item \textbf{Aceleración GPU (opcional):} CUDA/cuDNN para aceleración de generación de embeddings. El sistema es funcional con CPU pero 25x más lento.

    \item \textbf{Sistema operativo preferido:} Linux (recomendado), Windows 11 con WSL2, o macOS Monterey o superior.

    \item \textbf{Licencias de uso:} Se restringe el uso del sistema a entornos académicos bajo licencias open source o de uso investigativo.
\end{itemize}

\subsection{Restricciones de Hardware}

\begin{itemize}
    \item \textbf{Requisitos mínimos para Fase 0 y 1:}
    \begin{itemize}
        \item CPU: 4 núcleos (Intel i5, AMD Ryzen 5, o Apple M1 o superior)
        \item RAM: 8 GB mínimo (16 GB recomendado)
        \item Almacenamiento: 15 GB libres (base de datos, modelos, índices)
        \item Resolución de pantalla: 1280x800 mínimo
    \end{itemize}

    \item \textbf{Requisitos para generación de embeddings (Fase 0):}
    \begin{itemize}
        \item \textbf{Con GPU:} NVIDIA con soporte CUDA 11.0+, 6 GB VRAM mínimo. Velocidad: 721 skills/segundo
        \item \textbf{Sin GPU (CPU):} Intel i5/i7 o equivalente. Velocidad: ~30 skills/segundo (25x más lento)
        \item \textbf{Apple Silicon (MPS):} M1/M2/M3 con aceleración Metal. Velocidad: ~400 skills/segundo
    \end{itemize}

    \item \textbf{Requisitos para clustering (Módulo 6 - futuro):}
    \begin{itemize}
        \item RAM: 16 GB mínimo (32 GB recomendado para 23K jobs)
        \item HDBSCAN con 23,188 jobs requiere ~8 GB RAM disponible
        \item UMAP requiere ~4 GB RAM para reducción de dimensionalidad
    \end{itemize}

    \item \textbf{Almacenamiento de base de datos:}
    \begin{itemize}
        \item PostgreSQL con 23,188 jobs: ~2 GB
        \item Tabla skill\_embeddings (14,133 x 768D): ~400 MB
        \item Índices y tablas auxiliares: ~500 MB
        \item Total estimado: ~3 GB para datos + ~12 GB para modelos
    \end{itemize}

    \item \textbf{Acceso a internet:} Se requiere conexión estable para:
    \begin{itemize}
        \item Descarga inicial de modelos (intfloat/multilingual-e5-base: 1.1 GB)
        \item Instalación de librerías Python (~2 GB total)
        \item Web scraping de portales de empleo (bandwidth: ~10 MB/hora)
    \end{itemize}
\end{itemize}

\section{Supuestos y Dependencias}

Esta sección enumera factores externos y condiciones asumidas que podrían afectar el cumplimiento de los requerimientos definidos en la Sección 3.

\subsection{Suposiciones}

\begin{itemize}
    \item Se mantendrá acceso público y sin restricciones críticas a los portales de empleo definidos como fuente principal.

    \item Las estructuras HTML de dichas páginas no sufrirán cambios drásticos que impidan la funcionalidad de los spiders desarrollados.

    \item Se podrá ejecutar scraping bajo prácticas éticas, sin infringir condiciones explícitas de uso ni requerir autenticación compleja.

    \item Existirá conectividad a internet estable durante las fases de extracción y enriquecimiento de datos.

    \item Los modelos de lenguaje en español seleccionados estarán disponibles públicamente para descarga y uso local.

    \item Los LLMs empleados podrán ejecutarse en entornos locales de cómputo, sin requerir acceso continuo a APIs externas.

    \item Se contará con PostgreSQL funcional y correctamente configurado desde las fases iniciales del desarrollo.

    \item El equipo de desarrollo tendrá acceso constante al repositorio de código y a los entornos colaborativos definidos.

    \item El hardware utilizado por los desarrolladores cumple con los requerimientos mínimos establecidos.
\end{itemize}

\subsection{Dependencias}

\begin{itemize}
    \item \textbf{Velocidad de conexión a internet:} Afecta directamente los tiempos de scraping, descarga de modelos y ejecución de procesos remotos.

    \item \textbf{Disponibilidad y estabilidad de bibliotecas externas:} El sistema depende de librerías que podrían modificar sus versiones o comportamiento.

    \item \textbf{Funcionamiento adecuado del motor de base de datos:} La persistencia de datos depende de una base PostgreSQL operativa.

    \item \textbf{Funcionamiento de entornos de ejecución local:} Algunos modelos requerirán entornos específicos (compatibilidad CUDA, soporte de arquitectura x64).

    \item \textbf{Factores legales o contractuales externos:} Cambios en políticas de los portales web o en los lineamientos éticos institucionales podrían limitar la continuidad del scraping.

    \item \textbf{Capacidad de procesamiento local:} La ejecución de embeddings y clustering depende de la disponibilidad de memoria RAM suficiente y compatibilidad GPU.
\end{itemize}

% ============================================================================
% CAPÍTULO 3: REQUERIMIENTOS ESPECÍFICOS
% ============================================================================
\chapter{REQUERIMIENTOS ESPECÍFICOS}

Este capítulo detalla de manera exhaustiva los requerimientos funcionales y no funcionales del sistema, organizados según las categorías definidas en las secciones anteriores.

\section{Requerimientos de Interfaces Externas}

\subsection{Interfaces con el Usuario}

\begin{description}
    \item[REI-01] El sistema debe permitir la ejecución de scripts desde CLI para scraping y visualización.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Interfaz Usuario
        \item \textbf{Criterio:} Ejecución funcional por línea de comandos
    \end{itemize}

    \item[REI-02] El sistema debe incluir notebooks Jupyter con celdas documentadas.
    \begin{itemize}
        \item \textbf{Prioridad:} Media
        \item \textbf{Módulo:} Interfaz Usuario
        \item \textbf{Criterio:} Visualización y reproducción de notebooks
    \end{itemize}

    \item[REI-03] El sistema debe generar reportes en formato PDF, PNG y HTML.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Interfaz Usuario/Visualización
        \item \textbf{Criterio:} Visualización correcta de reportes generados
    \end{itemize}
\end{description}

\subsection{Interfaces con el Hardware}

\begin{description}
    \item[REI-04] El sistema debe operar en equipos sin GPU dedicada y mínimo 8 GB de RAM.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Interfaz Hardware
        \item \textbf{Criterio:} Ejecución sin errores en equipos personales
    \end{itemize}
\end{description}

\subsection{Interfaces con el Software}

\begin{description}
    \item[REI-05] El sistema debe conectarse a una base de datos PostgreSQL local.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Interfaz Software
        \item \textbf{Criterio:} Inserción y lectura desde PostgreSQL
    \end{itemize}

    \item[REI-06] El sistema debe acceder a portales web por HTTP/HTTPS para extraer datos.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Interfaz Comunicación
        \item \textbf{Criterio:} Scraping exitoso desde URLs definidas
    \end{itemize}
\end{description}

\section{Requerimientos Funcionales}

\subsection{Funcionalidad 1: Extracción de Vacantes (Scraping)}

\begin{description}
    \item[RF-01] El sistema debe extraer vacantes desde portales como Computrabajo, Bumeran y elempleo.com.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Scraping
        \item \textbf{Criterio:} Extracción visible y consistente de datos
    \end{itemize}
\end{description}

\subsection{Funcionalidad 2: Procesamiento de Texto}

\begin{description}
    \item[RF-02] El sistema debe almacenar las vacantes en PostgreSQL con deduplicación SHA256.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Almacenamiento
        \item \textbf{Criterio:} Consultas e inserciones validadas, duplicados detectados
        \item \textbf{Estado:} ✅ IMPLEMENTADO (23,352 jobs, dedup rate 0.5\%)
    \end{itemize}

    \item[RF-03] El sistema debe preprocesar el texto: limpieza HTML, tokenización, normalización.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Procesamiento NLP
        \item \textbf{Criterio:} Verificación de campos procesados, detección de jobs basura
        \item \textbf{Estado:} ✅ IMPLEMENTADO (23,188 jobs limpios, 99.5\% usable)
    \end{itemize}

    \item[RF-04] El sistema debe extraer habilidades explícitas mediante NER y Regex (Pipeline A).
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Extracción
        \item \textbf{Criterio:} Skills extraídas con método y confianza asociados
        \item \textbf{Métodos:}
        \begin{itemize}
            \item Regex: 200+ patrones tecnológicos (78-89\% precision)
            \item NER: spaCy + custom entity ruler (13\% precision con filtros)
        \end{itemize}
        \item \textbf{Estado:} ✅ IMPLEMENTADO (Test 100 jobs: 2,756 skills extraídas)
    \end{itemize}
\end{description}

\subsection{Funcionalidad 2.1: Mapeo contra Taxonomía ESCO}

\begin{description}
    \item[RF-04.1] El sistema debe cargar y mantener una taxonomía unificada de 14,174 skills.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Taxonomía (Fase 0)
        \item \textbf{Criterio:} ESCO v1.1.0 (13,939) + O*NET Hot Tech (152) + Manual Curated (83)
        \item \textbf{Estado:} ✅ IMPLEMENTADO
    \end{itemize}

    \item[RF-04.2] El sistema debe mapear skills extraídas contra ESCO usando estrategia de tres capas.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Matching
        \item \textbf{Criterio:} Layer 1 (Exact, SQL ILIKE, confidence 1.0) → Layer 2 (Fuzzy, threshold 0.92) → Layer 3 (Semantic, threshold 0.87, DESHABILITADO)
        \item \textbf{Estado:} ⚠️ PARCIALMENTE IMPLEMENTADO (Layer 1 + 2 activos, Layer 3 disabled)
        \item \textbf{Match rate actual:} 12.6\% (esperado para taxonomías 2016-2017)
    \end{itemize}

    \item[RF-04.3] El sistema debe identificar y rastrear skills emergentes no mapeadas.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Emergent Skills Tracking
        \item \textbf{Criterio:} Skills sin match ESCO almacenadas con frecuencia y contexto
        \item \textbf{Estado:} ✅ IMPLEMENTADO (87.4\% emergent rate en test de 100 jobs)
    \end{itemize}

    \item[RF-04.4] El sistema debe generar embeddings semánticos para todas las skills.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Embeddings (Fase 0)
        \item \textbf{Criterio:} Modelo intfloat/multilingual-e5-base (768D), L2-normalized
        \item \textbf{Performance:} 721 skills/segundo (GPU), 30 skills/segundo (CPU)
        \item \textbf{Estado:} ✅ IMPLEMENTADO (14,133 embeddings, 94.6\% test pass)
    \end{itemize}

    \item[RF-04.5] El sistema debe construir y mantener índice FAISS para búsqueda semántica.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} FAISS Index (Fase 0)
        \item \textbf{Criterio:} IndexFlatIP (exact search), 30,147 queries/segundo
        \item \textbf{Archivos:} data/embeddings/esco.faiss (41.41 MB), esco\_mapping.pkl (545 KB)
        \item \textbf{Estado:} ✅ IMPLEMENTADO (25x más rápido que PostgreSQL pgvector)
    \end{itemize}
\end{description}

\subsection{Funcionalidad 3: Representación Semántica}

\begin{description}
    \item[RF-05] El sistema debe generar representaciones semánticas (embeddings) y realizar clustering automático.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Agrupamiento
        \item \textbf{Criterio:} Clústeres coherentes generados
    \end{itemize}
\end{description}

\subsection{Funcionalidad 4: Visualización}

\begin{description}
    \item[RF-06] El sistema debe generar visualizaciones estáticas con gráficas de frecuencia de habilidades y comparativas.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Visualización
        \item \textbf{Criterio:} Visualizaciones exportadas exitosamente
    \end{itemize}
\end{description}

\section{Requerimientos de Desempeño}

\begin{description}
    \item[RD-01] El sistema debe extraer y almacenar vacantes desde múltiples portales de empleo.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Scraping
        \item \textbf{Criterio:} Mínimo 300 vacantes por país desde dos portales distintos
        \item \textbf{Performance actual:} 23,352 jobs scraped (hiring.cafe: 23,313, elempleo: 38, zonajobs: 1)
        \item \textbf{Estado:} ✅ CUMPLIDO (7,784\% sobre mínimo requerido)
    \end{itemize}

    \item[RD-02] El preprocesamiento textual debe ejecutarse sin errores críticos.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Procesamiento
        \item \textbf{Criterio:} Pipeline completado con success rate > 95\%
        \item \textbf{Performance actual:} 99.5\% usable rate (23,188/23,352 jobs)
        \item \textbf{Estado:} ✅ CUMPLIDO
    \end{itemize}

    \item[RD-03] La generación de embeddings debe completarse en tiempo razonable.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Embeddings (Fase 0)
        \item \textbf{Criterio:} Completar 14K skills en < 5 minutos con GPU
        \item \textbf{Performance actual:} 19.65 segundos para 14,133 skills (721 skills/seg)
        \item \textbf{Estado:} ✅ CUMPLIDO (15x mejor que requerimiento)
    \end{itemize}

    \item[RD-04] La búsqueda semántica con FAISS debe ser eficiente.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} FAISS Index
        \item \textbf{Criterio:} Mínimo 100 queries por segundo
        \item \textbf{Performance actual:} 30,147 queries/segundo (301x sobre requerimiento)
        \item \textbf{Estado:} ✅ CUMPLIDO (25x más rápido que PostgreSQL pgvector)
    \end{itemize}

    \item[RD-05] El sistema de extracción de skills debe procesar jobs sin fallos.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Extracción Pipeline A
        \item \textbf{Criterio:} Success rate > 95\% en procesamiento
        \item \textbf{Performance actual:} 100\% success rate en test de 100 jobs (2,756 skills extraídas)
        \item \textbf{Tiempo promedio:} 1.82 segundos por job
        \item \textbf{Estado:} ✅ CUMPLIDO
    \end{itemize}

    \item[RD-06] El matching contra ESCO debe completarse para todas las skills extraídas.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Matching (3-layer strategy)
        \item \textbf{Criterio:} Todas las skills procesadas por las 3 layers
        \item \textbf{Performance actual:} 12.6\% match rate (Layer 1: 5.4\%, Layer 2: 7.1\%, Layer 3: disabled)
        \item \textbf{Estado:} ⚠️ PARCIALMENTE CUMPLIDO (Layer 3 deshabilitado temporalmente)
        \item \textbf{Nota:} 87.4\% emergent skills es esperado para taxonomías 2016-2017
    \end{itemize}
\end{description}

\section{Restricciones de Diseño}

\begin{description}
    \item[RDZ-01] El sistema debe ejecutarse completamente en local, sin servicios web de pago.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Arquitectura General
        \item \textbf{Criterio:} Funciona sin acceso a servicios externos
    \end{itemize}

    \item[RDZ-02] La ejecución de modelos LLM se limitará a versiones descargables.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Enriquecimiento
        \item \textbf{Criterio:} Modelos configurados desde Hugging Face
    \end{itemize}

    \item[RDZ-03] El desarrollo deberá realizarse en Python 3.10 o superior.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Infraestructura
        \item \textbf{Criterio:} Repositorio sin dependencias privativas
    \end{itemize}

    \item[RDZ-04] No se implementará una interfaz gráfica interactiva.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Visualización
        \item \textbf{Criterio:} El sistema exporta reportes, no tiene frontend
    \end{itemize}

    \item[RDZ-05] El sistema debe ser modular.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Arquitectura General
        \item \textbf{Criterio:} Cada módulo puede lanzarse individualmente
    \end{itemize}
\end{description}

\section{Atributos del Sistema de Software (No funcionales)}

\subsection{Confiabilidad}

\begin{description}
    \item[NFA-01] El sistema debe producir resultados consistentes ante entradas iguales.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Todos
        \item \textbf{Criterio:} Repetición del proceso genera mismos resultados
    \end{itemize}

    \item[NFA-02] Se debe registrar el comportamiento del sistema mediante logs detallados.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Todos
        \item \textbf{Criterio:} Archivos de log por módulo
    \end{itemize}

    \item[NFA-03] En caso de interrupciones, los módulos deben permitir ser reiniciados.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Arquitectura General
        \item \textbf{Criterio:} Pipeline puede reiniciarse parcialmente
    \end{itemize}
\end{description}

\subsection{Disponibilidad}

\begin{description}
    \item[NFA-04] El sistema estará disponible para ejecución local en cualquier momento.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Infraestructura
        \item \textbf{Criterio:} Pipeline completo corre offline
    \end{itemize}

    \item[NFA-05] Scripts y notebooks deben estar organizados en GitHub.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Infraestructura
        \item \textbf{Criterio:} Repositorio contiene notebooks funcionales
    \end{itemize}
\end{description}

\subsection{Seguridad}

\begin{description}
    \item[NFA-06] Se evitará recolectar información personal.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Scraping
        \item \textbf{Criterio:} Ningún campo sensible almacenado
    \end{itemize}

    \item[NFA-07] Los spiders implementarán throttling.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Scraping
        \item \textbf{Criterio:} Tiempo entre requests configurable
    \end{itemize}

    \item[NFA-08] El código incluirá controles básicos de errores.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Todos
        \item \textbf{Criterio:} Pipeline continúa sin detenerse
    \end{itemize}
\end{description}

\subsection{Mantenibilidad}

\begin{description}
    \item[NFA-09] El sistema debe estar documentado a nivel de código.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Todos
        \item \textbf{Criterio:} Documentación presente en repositorio
    \end{itemize}

    \item[NFA-10] Cada módulo será independiente y versionado.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Arquitectura Modular
        \item \textbf{Criterio:} Se puede actualizar un módulo sin afectar demás
    \end{itemize}
\end{description}

\subsection{Portabilidad}

\begin{description}
    \item[NFA-11] El sistema debe poder ejecutarse en Linux, macOS o Windows con WSL.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Infraestructura
        \item \textbf{Criterio:} Se ejecuta en tres sistemas operativos
    \end{itemize}

    \item[NFA-12] Se debe proporcionar un archivo de entorno reproducible.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Infraestructura
        \item \textbf{Criterio:} Archivo requirements.txt ejecutado con éxito
    \end{itemize}
\end{description}

\section{Requerimientos de la Base de Datos}

\begin{description}
    \item[BD-01] El sistema debe utilizar PostgreSQL como sistema de gestión de base de datos.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Base de Datos
        \item \textbf{Criterio:} PostgreSQL instalado y configurado
    \end{itemize}

    \item[BD-02] La base de datos debe permitir el almacenamiento estructurado de vacantes.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Base de Datos
        \item \textbf{Criterio:} Contiene todos los campos definidos
    \end{itemize}

    \item[BD-03] Las relaciones deben seguir un modelo normalizado.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Base de Datos
        \item \textbf{Criterio:} Modelo ER respetado
    \end{itemize}

    \item[BD-04] La base de datos debe ser accesible desde Python.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Base de Datos
        \item \textbf{Criterio:} Scripts se conectan correctamente
    \end{itemize}

    \item[BD-05] El sistema debe poder crear automáticamente las tablas necesarias.
    \begin{itemize}
        \item \textbf{Prioridad:} Media
        \item \textbf{Módulo:} Base de Datos
        \item \textbf{Criterio:} Ejecución de init\_db.py genera tablas
    \end{itemize}

    \item[BD-06] Se debe garantizar la integridad de los datos.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Base de Datos
        \item \textbf{Criterio:} Restricciones UNIQUE y validaciones
    \end{itemize}

    \item[BD-07] La base de datos debe permitir exportar resultados.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Base de Datos
        \item \textbf{Criterio:} Exportación a CSV y JSON exitosa
    \end{itemize}

    \item[BD-08] La base de datos debe operar localmente.
    \begin{itemize}
        \item \textbf{Prioridad:} Alta
        \item \textbf{Módulo:} Base de Datos
        \item \textbf{Criterio:} Conexión vía localhost funcional
    \end{itemize}
\end{description}

% ============================================================================
% CAPÍTULO 4: PROCESO INGENIERÍA DE REQUERIMIENTOS
% ============================================================================
\chapter{PROCESO INGENIERÍA DE REQUERIMIENTOS}

La especificación de requerimientos del sistema \textit{Observatorio de Demanda Laboral en Tecnología en Latinoamérica} fue construida mediante un proceso iterativo, sistemático y colaborativo, guiado por buenas prácticas de ingeniería de software y ajustado al contexto académico y técnico del proyecto.

\section{Técnicas y Métodos Utilizados}

Para el levantamiento, análisis y validación de los requerimientos, se emplearon múltiples técnicas complementarias:

\begin{itemize}
    \item \textbf{Análisis de proyectos similares y literatura técnica:} Se revisaron más de una docena de fuentes académicas, artículos internacionales, repositorios en GitHub y proyectos institucionales.

    \item \textbf{Revisión documental interna:} Documentos clave como la Propuesta de Grado y el SPMP sirvieron como base estructural.

    \item \textbf{Descomposición funcional por casos de uso:} Se identificaron 12 casos de uso principales, cada uno articulado con uno o más módulos del sistema.

    \item \textbf{Sesiones de lluvia de ideas y revisión cruzada:} El equipo realizó sesiones colaborativas utilizando Google Docs y Notion.

    \item \textbf{Validación iterativa con el director del proyecto:} El borrador fue sometido a retroalimentación para mejorar redacción y consistencia.
\end{itemize}

\section{Trazabilidad y Consistencia}

Para garantizar la trazabilidad, cada requerimiento fue identificado mediante un código único según su tipo:
\begin{itemize}
    \item RFxx: Requerimiento Funcional
    \item RNFxx / NFAxx: Requerimiento No Funcional
    \item RDxx: Requerimiento de Desempeño
    \item RDZxx: Restricción de Diseño
    \item BDxx: Requerimiento de Base de Datos
\end{itemize}

Cada uno fue mapeado explícitamente a uno o más casos de uso y módulos funcionales, estableciendo una relación directa entre la funcionalidad planteada, su implementación esperada y su criterio de verificación.

Se procuró que todos los requerimientos cumplieran con las siguientes características esenciales:

\begin{itemize}
    \item Atómicos
    \item Correctos y completos
    \item No ambiguos y consistentes
    \item Verificables
    \item Modificables
    \item Trazables
    \item Priorizados
\end{itemize}

\section{Complementariedad con el SPMP}

Este proceso está alineado y complementa lo definido en la sección 6.3 del SPMP (Control de Requerimientos), donde se establece un mecanismo estructurado para la gestión de cambios. Cualquier modificación a los requerimientos deberá:

\begin{itemize}
    \item Ser documentada mediante una solicitud de cambio formal
    \item Incluir un análisis de impacto
    \item Actualizarse en la tabla de distribución de requerimientos
    \item Ser validada por el equipo completo y el director
\end{itemize}

% ============================================================================
% CAPÍTULO 5: PROCESO VERIFICACIÓN
% ============================================================================
\chapter{PROCESO VERIFICACIÓN}

El proceso de Verificación y Validación (V\&V) aplicado a esta Especificación de Requerimientos de Software (SRS) tiene como propósito asegurar que el documento refleja de forma precisa, clara y completa las necesidades funcionales y no funcionales del sistema.

\section{Verificación del Documento SRS}

Durante la construcción del SRS, se aplicaron las siguientes técnicas de verificación documental:

\begin{itemize}
    \item \textbf{Revisión cruzada interna:} Cada sección fue redactada por un integrante del equipo y verificada por otro.

    \item \textbf{Validación contra SPMP y Propuesta de Grado:} Se confirmó que todos los requerimientos respondieron al propósito, alcance y restricciones previamente acordados.

    \item \textbf{Construcción de matriz de trazabilidad preliminar:} Se elaboró una tabla que relaciona cada requerimiento con los casos de uso y módulos funcionales.
\end{itemize}

\section{Verificación de Requerimientos Individuales}

Cada requerimiento especificado será evaluado mediante al menos uno de los siguientes métodos de verificación:

\begin{table}[H]
\centering
\begin{tabular}{|p{3cm}|p{5cm}|p{6cm}|}
\hline
\textbf{Método} & \textbf{Descripción} & \textbf{Aplicación en el proyecto} \\
\hline
Inspección & Lectura técnica sistemática & Aplicada a todos los requerimientos durante la redacción \\
\hline
Análisis & Evaluación de consistencia lógica & Usada para requerimientos no funcionales \\
\hline
Prueba & Ejecución de scripts o notebooks & Requerimientos funcionales como scraping, extracción, clustering \\
\hline
Demostración & Presentación empírica del sistema & Generación de reportes con datos reales \\
\hline
\end{tabular}
\caption{Métodos de verificación de requerimientos}
\end{table}

\section{Validación del Sistema Completo}

La validación del sistema contempla un enfoque integral:

\begin{itemize}
    \item \textbf{Pruebas de extremo a extremo (E2E):} Simulación completa desde el scraping hasta la generación de reportes.

    \item \textbf{Casos de prueba por módulo:} Cada componente tendrá pruebas específicas.

    \item \textbf{Evaluación cualitativa de resultados semánticos:} Validación cruzada y revisión manual.

    \item \textbf{Validación externa:} Presentación de resultados al director y jurados.

    \item \textbf{Checklist de atributos no funcionales:} Verificación de confiabilidad, mantenibilidad, portabilidad, seguridad, disponibilidad y desempeño.
\end{itemize}

\section{Criterios de Aprobación}

Un requerimiento se considerará verificado y validado cuando cumpla con todos los siguientes criterios:

\begin{itemize}
    \item Su cumplimiento puede demostrarse mediante al menos uno de los métodos de V\&V
    \item Existe evidencia objetiva documentada
    \item No presenta ambigüedad ni contradicción
    \item Ha sido validado por al menos un miembro del equipo y aceptado en revisión final por el director
\end{itemize}

% ============================================================================
% ANEXOS
% ============================================================================
\chapter*{ANEXOS}
\addcontentsline{toc}{chapter}{ANEXOS}

\section*{Diagrama de Casos de Uso}
\addcontentsline{toc}{section}{Diagrama de Casos de Uso}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{diagrams/use-case-diagram.png}
\caption{Diagrama de casos de uso del sistema}
\end{figure}

\section*{Diagrama Modelo de Dominio}
\addcontentsline{toc}{section}{Diagrama Modelo de Dominio}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{diagrams/domain-model-diagram.png}
\caption{Diagrama del modelo de dominio del sistema}
\end{figure}

% ============================================================================
% REFERENCIAS
% ============================================================================
\chapter*{REFERENCIAS}
\addcontentsline{toc}{chapter}{REFERENCIAS}

\begin{enumerate}
    \item Orozco Puello \& Gómez Estrada, L. F. (2019). \textit{Proyecto de Grado II – Web Scraping} [Trabajo de grado, Universidad del Sinú Elías Bechara Zainúm].

    \item Rubio Arrubla, J. A. (2024). \textit{Demanda de habilidades tecnológicas: evidencia desde el mercado laboral colombiano} [Tesis de maestría, Universidad de los Andes].

    \item Lukauskas, M., Šarkauskaitė, V., Pilinkienė, V., \& Stundziene, A. (2023). Enhancing Skills Demand Understanding through Job Ad Segmentation Using NLP and Clustering Techniques. ResearchGate.

    \item Martínez Sánchez, J. C. (2024). Desajuste en el mercado laboral: análisis de los perfiles de candidatos y las ofertas de trabajo publicadas en internet. \textit{Revista del INEGI}, (44).

    \item Cárdenas Rubio, J. A., Guataquí Roa, J. C., \& Montaña Doncel, J. M. (2015). \textit{Metodología para el análisis de demanda laboral mediante datos de internet: caso Colombia}. Organización Internacional del Trabajo / CINTERFOR.

    \item Campos-Vázquez, R. M., \& Martínez Sánchez, J. C. (2024). Skills sought by companies in the Mexican labor market: An analysis of online job vacancies. \textit{Estudios Económicos De El Colegio De México}, 39(2), 243–278.

    \item Aguilera, S. O., \& Méndez, R. E. (2018). ¿Qué buscan los que buscan? Análisis De mercado laboral IT en Argentina. \textit{Revista Perspectivas}, 1(1), 15–30.

    \item Nguyen K., Zhang M., Montariol S., \& Bosselut A. (2024). Rethinking Skill Extraction in the Job Market Domain using Large Language Models. En \textit{Proceedings of the First Workshop on Natural Language Processing for Human Resources (NLP4HR 2024)} (pp. 27–42).

    \item Echeverría L., \& Rucci G. (2022). \textit{¿Qué suma la ciencia de datos a la identificación y anticipación de la demanda de habilidades?} Banco Interamericano de Desarrollo.

    \item Kavas H., Serra-Vidal M., \& Wanner L. (2025). Multilingual Skill Extraction for Job Vacancy–Job Seeker Matching in Knowledge Graphs. En \textit{Proceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK)}.
\end{enumerate}

\end{document}
