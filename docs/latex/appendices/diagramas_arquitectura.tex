\section*{APÉNDICE B: INFORMACIÓN ADICIONAL SOBRE ARQUITECTURA Y METODOLOGÍA}
\addcontentsline{toc}{section}{Apéndice B: Información Adicional sobre Arquitectura y Metodología}

Este apéndice complementa los Capítulos 4 (Metodología) y 5 (Diseño de Solución), proporcionando documentación técnica detallada sobre las decisiones arquitectónicas fundamentales y el flujo metodológico integrado del proyecto. Mientras el Capítulo 4 presenta el marco general de CRISP-DM adaptado al contexto del observatorio y el Capítulo 5 describe la arquitectura híbrida seleccionada mediante diagramas de vistas lógica, física y de procesos, este apéndice profundiza en las justificaciones técnicas que respaldaron cada decisión de diseño, las comparaciones cuantitativas entre alternativas arquitectónicas evaluadas, y el stack tecnológico completo especificando versiones exactas, configuraciones específicas y criterios de selección académica para cada componente.

La documentación incluye el detalle del flujo de transformación de datos a través del pipeline de siete etapas con estructuras de datos intermedias persistidas en PostgreSQL, las características técnicas específicas de la arquitectura híbrida que combina microservicios en capas con event-driven architecture mediante Redis y Celery, y las tablas exhaustivas del stack tecnológico organizadas por capa funcional con justificaciones académicas basadas en cinco criterios principales: licencias permisivas, madurez y estabilidad, documentación científica completa, reproducibilidad mediante control de versiones, y escalabilidad demostrada para el procesamiento de más de 30,000 ofertas laborales.

\subsection*{B.1. Detalles del Flujo Metodológico}

El diagrama de flujo metodológico presentado en el Capítulo 4 (Figura \ref{fig:flujo-metodologico}) integra las seis fases de CRISP-DM con las siete etapas del pipeline de software. Los ciclos iterativos entre las fases de Modelado y Evaluación permitieron mejora incremental del sistema mediante:

\begin{itemize}
    \item \textbf{Refinamiento de parámetros NER}: Ajuste de umbrales de confianza, expansión del EntityRuler con patrones ESCO, y calibración de filtros de stopwords técnicas.
    \item \textbf{Optimización de patrones regex}: Incremento de 47 a 548 patrones organizados en 18 categorías técnicas, incorporando variantes ortográficas y versiones numeradas.
    \item \textbf{Evolución de prompts LLM}: Iteración de diseño de prompts desde versión inicial de 50 líneas hasta versión final de 170 líneas con ejemplos contextuales, manejo de Spanglish, y esquema JSON validado.
    \item \textbf{Calibración de clustering}: Experimentación con 70+ configuraciones de HDBSCAN para determinar \texttt{min\_cluster\_size=12} y \texttt{min\_samples=3} óptimos.
\end{itemize}

\subsection*{B.2. Detalles del Pipeline de 7 Etapas}

El diagrama de arquitectura modular presentado en el Capítulo 5 (Figura \ref{fig:arquitectura-completa}) muestra el pipeline secuencial de 7 etapas. Información adicional sobre tecnologías específicas por módulo:

\begin{itemize}
    \item \textbf{Scraping}: Framework Scrapy 2.11 con Selenium 4.15 para contenido dinámico, delays adaptativos (2-5s), rotación de user-agents, y backoff exponencial.
    \item \textbf{Extraction}: spaCy \texttt{es\_core\_news\_lg} v3.7 con EntityRuler personalizado, 548 patrones regex en 18 categorías, latencia 50-80ms por documento.
    \item \textbf{LLM Processing}: Gemma 3 4B con cuantización Q4 (4-6 GB VRAM), temperatura 0.3, max\_tokens 3072, latencia 18s (P50).
    \item \textbf{ESCO Matching}: Taxonomía extendida 14,174 skills (13,939 ESCO v1.1.0 + 152 O*NET + 83 manuales), matching exact + fuzzy $\geq$0.92.
    \item \textbf{Embedding}: Modelo \texttt{intfloat/multilingual-e5-base} (768D, 278M parámetros), batch\_size=32, latencia $<$100ms/batch.
    \item \textbf{UMAP}: Parámetros \texttt{n\_neighbors=15}, \texttt{min\_dist=0.1}, \texttt{metric='cosine'}, reducción de 768D a 2-3D.
    \item \textbf{HDBSCAN}: Parámetros \texttt{min\_cluster\_size=12}, \texttt{min\_samples=3}, \texttt{cluster\_selection\_\allowbreak method='eom'}.
\end{itemize}

\subsection*{B.3. Flujo de Transformación de Datos}

El flujo de datos atraviesa las siguientes transformaciones principales desde la recolección inicial hasta la generación de visualizaciones finales, con estructuras de datos intermedias persistidas en PostgreSQL:

\begin{itemize}
    \item \textbf{raw\_jobs a cleaned\_jobs}: Normalización de encoding UTF-8, eliminación de HTML residual, detección de idioma (español/inglés/Spanglish), deduplicación SHA-256.
    \item \textbf{cleaned\_jobs a extracted\_skills}: Aplicación de Pipeline A (NER+Regex) con extracción de 27.6 skills promedio por oferta, 87.4\% emergent skills sin match ESCO.
    \item \textbf{cleaned\_jobs a enhanced\_skills}: Aplicación de Pipeline B (LLM) con prompt de 170 líneas, detección de hard skills (78.7\%) y soft skills (21.3\%).
    \item \textbf{extracted/enhanced\_skills a matched\_skills}: Normalización contra ESCO mediante 2 capas (exact + fuzzy), match rate 12.6\% baseline, 25\% con matcher mejorado.
    \item \textbf{matched\_skills a skill\_embeddings}: Vectorización con E5 Multilingual (768D), normalización L2, indexación FAISS para búsqueda rápida.
    \item \textbf{skill\_embeddings a umap\_projections}: Reducción dimensional con parámetros n\_neighbors=15, min\_dist=0.1, metric='cosine'.
    \item \textbf{umap\_projections a clusters}: Clustering HDBSCAN con min\_cluster\_size=12, min\_samples=3, identificación de 50 clusters principales.
\end{itemize}

\subsection*{B.4. Características de la Arquitectura Híbrida}

La arquitectura implementada combina tres patrones complementarios que operan coordinadamente para maximizar throughput y tolerancia a fallos. El primer patrón corresponde a microservicios en capas con separación física entre frontend (Next.js + React), API backend (FastAPI), y base de datos (PostgreSQL) con comunicación HTTP/REST, permitiendo desarrollo, despliegue y escalamiento independiente de cada componente. El segundo patrón implementa Event-Driven Architecture mediante cola de tareas distribuida con Redis como message broker y Celery como sistema de workers, habilitando procesamiento asíncrono de pipelines de extracción con paralelismo configurable. El tercer patrón estructura el procesamiento como pipelines modulares donde cada etapa (scraping, normalización, extracción, matching, clustering) se implementa como módulo Python independiente orquestado por Celery tasks, facilitando testing unitario y evolución incremental.

Esta arquitectura permite procesamiento paralelo de múltiples ofertas simultáneamente mediante workers Celery que operan en background sin bloquear la API web. La tolerancia a fallos se implementa mediante reintentos automáticos (max retries=3 configurado en tasks) y persistencia de estado en PostgreSQL permitiendo recuperación ante fallos de workers individuales sin pérdida de progreso. La escalabilidad horizontal se logra agregando workers Celery adicionales para incrementar throughput de procesamiento según demanda.

\subsection*{B.5. Stack Tecnológico Completo}

Las siguientes tablas documentan las decisiones tecnológicas fundamentales y su justificación académica y técnica, organizadas por capa funcional del sistema. Todas las tecnologías seleccionadas cumplen con cinco criterios principales: licencias permisivas (MIT, Apache 2.0, PostgreSQL, CC BY) permitiendo uso académico y potencial comercial futuro; madurez y estabilidad con versiones $\geq$ 2.0 y comunidades activas; documentación académica completa con publicaciones científicas revisadas por pares para componentes críticos; reproducibilidad mediante control de versiones de dependencias y semillas fijas para componentes estocásticos; y escalabilidad demostrada para el procesamiento de más de 30,000 ofertas laborales.

\begin{table}[H]
\centering
\caption{Stack Tecnológico: Infraestructura y Adquisición de Datos}
\label{tab:tech-stack-infra}
\begin{tabular}{|p{3.5cm}|p{3.5cm}|p{6.5cm}|}
\hline
\textbf{Componente} & \textbf{Tecnología} & \textbf{Justificación} \\
\hline
Base de datos & PostgreSQL 15+ con pgvector & Soporte JSONB para metadatos flexibles, extensión pgvector para vectores 768D, robustez ACID, particionamiento para escalabilidad. \\
\hline
Taxonomía & ESCO v1.1.0 (+ 235 ext.) & Cobertura 13,000+ skills con etiquetas español/inglés, extendida con 152 O*NET + 83 manual (total 14,174), estructura ontológica con URIs, respaldo institucional CE, licencia CC BY 4.0. \\
\hline
Framework scraping & Scrapy 2.11 + Selenium 4.15 & Arquitectura asíncrona (100+ req/min), manejo robusto de reintentos, middlewares extensibles, Selenium para JavaScript dinámico. \\
\hline
Modelo NLP español & spaCy 3.7 + es\_core\_news\_lg & Mejor modelo español disponible (97M parámetros), soporte EntityRuler para ESCO, optimizado CPU ($<$100ms/doc). \\
\hline
Lenguaje & Python 3.11+ & Ecosistema científico maduro (NumPy, pandas, scikit-learn), bibliotecas NLP referencia (spaCy, transformers), integración PostgreSQL. \\
\hline
Control versiones & Git + GitHub & Estándar industria, integración CI/CD (GitHub Actions), control issues/milestones, documentación Markdown, respaldo cloud. \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Stack Tecnológico: Procesamiento y Análisis}
\label{tab:tech-stack-analytics}
\begin{tabular}{|p{3.5cm}|p{3.5cm}|p{6.5cm}|}
\hline
\textbf{Componente} & \textbf{Tecnología} & \textbf{Justificación} \\
\hline
LLM extracción & Gemma 3 4B (cuantización Q4) & Modelo ligero de 4B parámetros, despliegue local sin APIs (privacidad), cuantización Q4 (4-6 GB memoria unificada), seleccionado por evaluación comparativa sobre 4 candidatos, ejecución con aceleración Metal en Apple Silicon. \\
\hline
Embeddings & intfloat/multilingual-e5-base & Estado del arte multilingüe (768D), contrastive learning en 100 idiomas, normalización L2 integrada, 278M parámetros ejecutables CPU ($<$100ms/batch). \\
\hline
Reducción dimensional & UMAP \cite{mcinnes2018umap} & Preserva estructura local y global (superior t-SNE), escalabilidad millones puntos, reproducibilidad con semilla fija, parámetros interpretables (n\_neighbors, min\_dist). \\
\hline
Clustering & HDBSCAN \cite{campello2013} & No requiere especificar k, identifica ruido automático, maneja densidades variables (nicho vs. mainstream), jerarquía multinivel, min\_cluster\_size=12 tras experimentación. \\
\hline
Cola de tareas & Redis 7 + Celery 5.3 & Message broker para Event-Driven Architecture, persistencia RDB/AOF, Celery para orquestación distribuida con retry automático (max\_retries=3), scheduling con Celery Beat. \\
\hline
Frontend web & Next.js 14 + React 18 & Server-Side Rendering para SEO, React para componentes interactivos, TailwindCSS para diseño responsivo, shadcn/ui para componentes UI, Recharts para visualizaciones. \\
\hline
API backend & FastAPI 0.104 + Pydantic & Framework async Python, validación automática con Pydantic, generación OpenAPI/Swagger, endpoints REST para CRUD y publicación de tareas asíncronas. \\
\hline
Contenedorización & Docker 24 + Docker Compose & Orquestación de 7 servicios (nginx, frontend, api, postgres, redis, celery\_beat, celery\_worker) \\
\hline
\end{tabular}
\end{table}
