# Gu√≠a de Scraping - Observatorio Demanda Laboral

## üìã Tabla de Contenidos
- [Inicio R√°pido](#inicio-r√°pido)
- [Scrapers Disponibles](#scrapers-disponibles)
- [Comandos Principales](#comandos-principales)
- [Configuraci√≥n Avanzada](#configuraci√≥n-avanzada)
- [Datos Actuales](#datos-actuales)

---

## üöÄ Inicio R√°pido

### Prerequisitos
```bash
# 1. Iniciar PostgreSQL
docker-compose up -d

# 2. Verificar que est√© corriendo
docker-compose ps
```

### Ejecutar Scraper (Modo Simple)
```bash
# Computrabajo Colombia - 5 p√°ginas (default)
python -m src.orchestrator run-once computrabajo -c CO

# Con m√°s p√°ginas
python -m src.orchestrator run-once computrabajo -c CO -p 50

# Con output verbose
python -m src.orchestrator run-once computrabajo -c CO -p 50 -v
```

---

## üìä Scrapers Disponibles

### Computrabajo (Optimizado ‚ú®)
Spider mejorado con:
- ‚úÖ Batch inserts (100 jobs/batch)
- ‚úÖ Deduplicaci√≥n SHA256
- ‚úÖ Empty page detection
- ‚úÖ Modo listing-only para m√°xima velocidad
- ‚úÖ Configuraci√≥n de concurrencia adaptativa

**Pa√≠ses soportados:** Colombia (CO), M√©xico (MX), Argentina (AR)

### Otros Scrapers
- `elempleo` - El Empleo (Colombia)
- `bumeran` - Bumeran (Argentina, Chile, Panam√°)
- `zonajobs` - ZonaJobs (Argentina)
- `occmundial` - OCC Mundial (M√©xico)
- `magneto` - Magneto (varios pa√≠ses)

Ver lista completa:
```bash
python -m src.orchestrator list-spiders
```

---

## ‚ö° Comandos Principales

### 1. Scraping B√°sico (Modo Detallado)
```bash
# Colombia - Modo FULL-DETAIL (completo pero lento)
python -m src.orchestrator run-once computrabajo -c CO -p 10
```

Este modo:
- Scrape p√°ginas de listado + p√°ginas de detalle
- Extrae descripci√≥n completa y requirements
- ~3s delay entre requests
- 1 concurrent request (anti-bot)

### 2. Scraping R√°pido (Modo Listing-Only) üöÄ
Para usar el modo listing-only (2x m√°s r√°pido), necesitas ejecutar directamente con Scrapy:

```bash
# Modo LISTING-ONLY (r√°pido)
scrapy crawl computrabajo \
  -a country=CO \
  -a listing_only=true \
  -a concurrent_requests=16 \
  -a download_delay=1.5
```

Este modo:
- Solo scrape p√°ginas de listado (sin detail pages)
- Extrae datos b√°sicos (t√≠tulo, empresa, location, preview)
- ~1.5s delay entre requests
- 16 concurrent requests
- **2x m√°s r√°pido** que modo detallado

### 3. Scraping Masivo (M√∫ltiples Ciudades/Keywords)
Para scraping masivo, usa el archivo de configuraci√≥n:

```python
# Crear archivo de configuraci√≥n
# scripts/colombia_massive_config.py
from src.orchestrator import run_scraper

cities = ['bogota-dc', 'medellin', 'cali', 'barranquilla']
keywords = ['sistemas', 'desarrollador', 'ingeniero']

for city in cities:
    for keyword in keywords:
        run_scraper('computrabajo', country='CO', city=city, keyword=keyword, max_pages=100)
```

---

## üîß Configuraci√≥n Avanzada

### Configurar Concurrencia

**Opci√≥n 1: Variables de entorno**
```bash
# Editar .env
SCRAPY_CONCURRENT_REQUESTS=16
SCRAPY_DOWNLOAD_DELAY=1.5
BATCH_INSERT_SIZE=100
```

**Opci√≥n 2: Custom settings en spider**
El spider de Computrabajo ajusta autom√°ticamente:
- **Listing-only mode:** 16 concurrent, 1.5s delay
- **Full-detail mode:** 1 concurrent, 3s delay

### Configurar Base de Datos
```bash
# Editar .env (si es necesario)
DB_HOST=127.0.0.1
DB_PORT=5433
DB_NAME=labor_observatory
DB_USER=labor_user
DB_PASSWORD=123456
```

### Par√°metros del Spider Computrabajo

```bash
scrapy crawl computrabajo \
  -a country=CO \              # Pa√≠s (CO, MX, AR)
  -a city=bogota-dc \          # Ciudad
  -a keyword=sistemas \        # Keyword de b√∫squeda
  -a listing_only=true \       # Modo r√°pido (true/false)
  -a concurrent_requests=16 \  # Concurrent requests
  -a download_delay=1.5        # Delay entre requests (s)
```

---

## üìà Datos Actuales

### Base de Datos
```sql
-- Verificar datos
SELECT portal, country, COUNT(*) as total_jobs
FROM raw_jobs
GROUP BY portal, country
ORDER BY total_jobs DESC;
```

**Estado actual (2025-10-19):**
- **25,063 jobs** totales
- **Computrabajo CO:** 23,729 jobs
- **Computrabajo MX:** 1,140 jobs
- **Elempleo CO:** 194 jobs

### Verificar √öltima Extracci√≥n
```bash
# PostgreSQL
docker-compose exec -T postgres psql -U labor_user -d labor_observatory -c "
SELECT
    portal,
    country,
    COUNT(*) as total_jobs,
    MAX(scraped_at) as last_scrape
FROM raw_jobs
GROUP BY portal, country
ORDER BY last_scrape DESC;"
```

---

## ‚ö†Ô∏è Rate Limiting

**Computrabajo detecta scraping masivo:**
- Si recibes **403 Forbidden**, espera 12-24 horas
- Reduce concurrencia (8 en vez de 16)
- Aumenta delay (2s en vez de 1.5s)
- Considera proxies (ver configuraci√≥n avanzada)

### Soluciones para Rate Limiting

1. **Esperar:** 12-24 horas entre runs masivos
2. **Reducir agresividad:**
   ```bash
   # M√°s conservador
   scrapy crawl computrabajo -a country=CO -a concurrent_requests=4 -a download_delay=3
   ```
3. **Usar Proxies:** Editar `src/scraper/middlewares.py` para configurar proxy rotation

---

## üìù Logs

```bash
# Ver logs en tiempo real
tail -f logs/scrapy.log

# Ver logs de orchestrator
tail -f logs/mass_scraping.log
```

---

## üéØ Ejemplos de Uso

### Ejemplo 1: Extracci√≥n Diaria (Incremental)
```bash
# Scraping ligero diario (primeras 5 p√°ginas, nuevos jobs)
python -m src.orchestrator run-once computrabajo -c CO -p 5
```

### Ejemplo 2: Extracci√≥n Semanal (Exhaustiva)
```bash
# Scraping profundo semanal (50 p√°ginas)
python -m src.orchestrator run-once computrabajo -c CO -p 50 -v
```

### Ejemplo 3: M√°xima Velocidad (Listing-Only)
```bash
# Para an√°lisis r√°pido o testing
scrapy crawl computrabajo -a country=CO -a listing_only=true -a concurrent_requests=16
```

---

## üõ†Ô∏è Troubleshooting

### Error: "Database error: value too long for type character varying(50)"
**Soluci√≥n:** Bug arreglado en √∫ltima versi√≥n. Si persiste, pull latest changes.

### Error: "403 Forbidden"
**Soluci√≥n:** Rate limiting activo. Esperar 12-24 horas o usar proxies.

### Error: "No jobs found"
**Causas posibles:**
- Keyword/ciudad sin resultados
- Website cambi√≥ estructura HTML
- Rate limiting silencioso

### Pipeline no guarda datos
**Verificar:**
```bash
# Check DB connection
docker-compose ps
docker-compose exec postgres psql -U labor_user -d labor_observatory -c "\dt"
```

---

## üìö Recursos Adicionales

- **Documentaci√≥n Scrapy:** https://docs.scrapy.org/
- **Orchestrator:** Ejecutar `python -m src.orchestrator --help`
- **Logs:** `logs/scrapy.log`, `logs/mass_scraping.log`

---

**√öltima actualizaci√≥n:** 2025-10-19
**Versi√≥n:** 2.0 (Arquitectura optimizada)
