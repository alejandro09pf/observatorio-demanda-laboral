# Observatorio de Demanda Laboral - FLUJO COMPLETO CORREGIDO

**Version:** 2.0 (Corregida)
**Date:** October 19, 2025
**Status:** FINAL - Responde todas las preguntas crÃ­ticas

---

## ğŸ¯ **Respuestas a Preguntas CrÃ­ticas**

### **Q1: Â¿En quÃ© momento se mapean las skills de AMBOS flows a ESCO/O*NET?**

**A:** DESPUÃ‰S de la extracciÃ³n de cada pipeline, ANTES de merge/comparaciÃ³n.

```
Pipeline A â†’ Extract â†’ Map to ESCO (2 layers) â†’ extracted_skills_A
Pipeline B â†’ Extract â†’ Map to ESCO (2 layers) â†’ extracted_skills_B

Luego: Compare A vs B
```

### **Q2: Â¿DÃ³nde entran los embeddings?**

**A:** En 2 momentos diferentes:

1. **Setup (ONE-TIME, Phase 0)**: Generate ESCO taxonomy embeddings (13,939 skills) + build FAISS index
2. **Mapping (RUNTIME, Module 4)**: Generate embedding for candidate skill â†’ FAISS search â†’ cosine similarity vs ESCO

**Note:** Los embeddings de skills individuales se generan en Phase 0 para ESCO y en Module 6 Step 6.1 para todas las skills extraÃ­das (para clustering)

### **Q3: Â¿QuÃ© hacemos si un LLM identifica una skill que NO estÃ¡ en ESCO/O*NET?**

**A:** 3-step process:

1. **Flag as emergent**: Mark skill as unmapped, save to `emergent_skills` table
2. **Track frequency**: Count how many jobs mention this skill
3. **Manual review**: High-frequency emergent skills â†’ add to custom taxonomy OR map to nearest ESCO

### **Q4: Â¿CÃ³mo comparar mÃºltiples LLMs?**

**A:** Run mÃºltiples LLMs en Pipeline B, compare resultados:

```
Pipeline B:
â”œâ”€â”€ Run GPT-3.5 â†’ skills_gpt35 â†’ Map to ESCO â†’ Save with llm_model='gpt-3.5'
â”œâ”€â”€ Run Mistral-7B â†’ skills_mistral â†’ Map to ESCO â†’ Save with llm_model='mistral'
â”œâ”€â”€ Run Llama-2 â†’ skills_llama â†’ Map to ESCO â†’ Save with llm_model='llama'
â””â”€â”€ Compare: Coverage, Confidence, ESCO mapping rate, Implicit skills
```

### **Q5: Â¿CÃ³mo comparamos A vs B sin LLM mediador?**

**A:** ComparaciÃ³n directa con mÃ©tricas objetivas + anÃ¡lisis cualitativo manual:

**Metrics:**
- Coverage: Skills Ãºnicas de A, B, y overlap
- ESCO mapping success rate
- Confidence scores distribution
- Explicit vs Implicit (solo B)

**Qualitative Analysis:**
- Manual review de skills Ãºnicas de cada pipeline
- Â¿CuÃ¡l encontrÃ³ skills mÃ¡s relevantes?
- Â¿QuÃ© skills perdiÃ³ cada mÃ©todo?

---

## ğŸ“Š **FLUJO COMPLETO CORREGIDO**

### **FASE 0: One-Time Setup (Run Once)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0.1: Load ESCO Taxonomy                    â”‚
â”‚     - 13,939 skills from ESCO database     â”‚
â”‚     - multilingual labels (ES + EN)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0.2: Generate ESCO Embeddings              â”‚
â”‚     Model: multilingual-e5-base (768D)     â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ for each ESCO skill:         â”‚       â”‚
â”‚     â”‚   text = label + description â”‚       â”‚
â”‚     â”‚   embedding = E5.encode(text)â”‚       â”‚
â”‚     â”‚   save to skill_embeddings   â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚     Output: 13,939 embeddings              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0.3: Build FAISS Index (CRITICAL)          â”‚
â”‚     âš ï¸ NOT optional - needed for Layer 2   â”‚
â”‚        semantic matching at scale          â”‚
â”‚                                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ import faiss                           â”‚ â”‚
â”‚ â”‚ import numpy as np                     â”‚ â”‚
â”‚ â”‚                                        â”‚ â”‚
â”‚ â”‚ # Load embeddings from DB              â”‚ â”‚
â”‚ â”‚ embeddings = load_esco_embeddings()    â”‚ â”‚
â”‚ â”‚ # Shape: (13,939 skills, 768 dims)     â”‚ â”‚
â”‚ â”‚                                        â”‚ â”‚
â”‚ â”‚ # Normalize for cosine similarity      â”‚ â”‚
â”‚ â”‚ faiss.normalize_L2(embeddings)         â”‚ â”‚
â”‚ â”‚                                        â”‚ â”‚
â”‚ â”‚ # Create IndexFlatIP (Inner Product)   â”‚ â”‚
â”‚ â”‚ index = faiss.IndexFlatIP(768)         â”‚ â”‚
â”‚ â”‚ index.add(embeddings)                  â”‚ â”‚
â”‚ â”‚                                        â”‚ â”‚
â”‚ â”‚ # Save index                           â”‚ â”‚
â”‚ â”‚ faiss.write_index(index,               â”‚ â”‚
â”‚ â”‚   'data/embeddings/esco.faiss')        â”‚ â”‚
â”‚ â”‚                                        â”‚ â”‚
â”‚ â”‚ # Also save mapping esco_uri â†’ index   â”‚ â”‚
â”‚ â”‚ np.save('data/embeddings/esco_uris.npy'â”‚ â”‚
â”‚ â”‚         esco_uris)                     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                             â”‚
â”‚ Why FAISS?                                  â”‚
â”‚   - PostgreSQL pgvector: ~5s per query     â”‚
â”‚   - FAISS IndexFlatIP: ~0.2s per query     â”‚
â”‚   - 25x speedup for semantic matching      â”‚
â”‚                                             â”‚
â”‚ Output Files:                               â”‚
â”‚   âœ… data/embeddings/esco.faiss (index)    â”‚
â”‚   âœ… data/embeddings/esco_uris.npy (map)   â”‚
â”‚   âœ… data/embeddings/esco_embeddings.npy   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ONE-TIME SETUP COMPLETE âœ…
(Run once, reuse for all 23K+ jobs)
```

---

### **FASE 1: Data Collection & Cleaning (DONE âœ…)**

#### **Module 1.1: Web Scraping Configuration**

**Input:**
- Target countries: `Colombia (CO)`, `Mexico (MX)`, `Argentina (AR)`
- Target portals: `hiring_cafe`, `computrabajo`, `bumeran`, `elempleo`, `zonajobs`, `infojobs`
- Scraping parameters: `limit`, `max_pages`, `country_code`

**Orchestrator Command:**
```bash
python -m src.orchestrator run-once hiring_cafe CO --limit 50000 --max-pages 1000
```

**Output:**
- Spider instances configured with portal-specific selectors
- Ready to execute async scraping

**Files:** `src/orchestrator.py`, `src/scraper/spiders/*.py`
**Status:** âœ… Complete

---

#### **Module 1.2: Web Navigation & HTML Download**

**Action:**
1. Execute spiders using **Scrapy** (async collection)
2. Use **Selenium** as fallback for JavaScript-rendered content (bumeran, zonajobs, clarin)
3. Navigate pagination (sorted by newest first when possible)
4. Download complete HTML of each job posting page

**Deduplication Strategy (During Scraping):**
- Track last 2 job IDs seen
- If 2 consecutive duplicates â†’ stop spider (all new jobs collected)

**Output:**
- Raw HTML responses for each job posting

**Current Stats:**
- hiring_cafe: 23,313 jobs
- elempleo: 38 jobs
- zonajobs: 1 job
- **Total:** 23,352 jobs scraped

---

#### **Module 1.3: HTML Parsing & Structured Extraction**

**Input:** Raw HTML responses

**Extracted Fields:**
- `title` - Job title
- `company` - Company name
- `description` - Full job description (HTML)
- `requirements` - Requirements section (HTML)
- `location` - Location/city
- `salary` - Salary range (if available)
- `contract_type` - Full-time/Part-time/Contract
- `posted_date` - Date published
- `url` - Original job posting URL

**Output:** Scrapy Items with structured data

**Files:** `src/scraper/spiders/*.py` (parse methods), `src/scraper/items.py`

---

#### **Module 1.4: Database Storage with SHA256 Deduplication**

**Deduplication Algorithm:**
```
1. Calculate content_hash = SHA256(title + description + requirements)
2. Check: SELECT job_id FROM raw_jobs WHERE content_hash = ?
3. If duplicate â†’ skip (log event)
4. If unique â†’ INSERT into raw_jobs
```

**Database Schema: raw_jobs**
```sql
raw_jobs (
    job_id UUID PRIMARY KEY,
    portal VARCHAR(50),           -- 'hiring_cafe', 'bumeran', etc
    country VARCHAR(2),            -- 'CO', 'MX', 'AR'
    url TEXT UNIQUE,
    title TEXT NOT NULL,
    company TEXT,
    description TEXT NOT NULL,    -- Raw HTML
    requirements TEXT,            -- Raw HTML (can be NULL)
    location TEXT,
    salary TEXT,
    contract_type VARCHAR(50),
    posted_date DATE,
    content_hash VARCHAR(64) UNIQUE,  -- SHA256 for deduplication
    scraped_at TIMESTAMP DEFAULT NOW(),

    -- Extraction tracking
    extraction_status VARCHAR(20) DEFAULT 'pending',  -- 'pending', 'processing', 'completed', 'failed'
    extraction_attempts INTEGER DEFAULT 0,
    extraction_completed_at TIMESTAMP,
    extraction_error TEXT,

    -- Data quality flags (added in migration 006)
    is_usable BOOLEAN DEFAULT TRUE,
    unusable_reason TEXT
)
```

**Files:** `src/scraper/pipelines.py`, `src/database/models.py`
**Status:** âœ… Complete

---

#### **Module 2.1: HTML Removal & Text Cleaning**

**Input:** `raw_jobs` table with HTML content

**Cleaning Process:**

**Step 2.1.1: HTML Tag Removal**
- Remove all `<tag>` elements
- Decode HTML entities (`&nbsp;` â†’ space, `&amp;` â†’ &)
- Preserve text content only

**Step 2.1.2: Text Normalization**
- Multiple whitespace â†’ single space
- Remove excessive punctuation (!!!, ???)
- Remove emojis and Unicode symbols
- Trim leading/trailing whitespace
- Keep accents (espaÃ±ol)
- Keep case (helps NER)
- Keep meaningful punctuation (-, /, +)

**Step 2.1.3: Junk Detection**

**Junk Patterns (Conservative):**
```
- Exact "test" (case-insensitive)
- Exact "demo"
- "002_Cand1" pattern (placeholder candidates)
- "Colombia Test 7" pattern (vendor test jobs)
- Description < 50 characters (extremely short)
```

**Action:**
- If junk detected â†’ UPDATE raw_jobs SET is_usable=FALSE, unusable_reason='...'
- Junk jobs are NOT deleted (preserved for audit)

**Step 2.1.4: Combined Text Generation**
```
Format: title_cleaned + "\n" + description_cleaned + "\n" + requirements_cleaned
```

**Step 2.1.5: Metadata Calculation**
- `combined_word_count` - Number of words in combined text
- `combined_char_count` - Number of characters

**Output: cleaned_jobs table**
```sql
cleaned_jobs (
    job_id UUID PRIMARY KEY REFERENCES raw_jobs(job_id),
    title_cleaned TEXT,
    description_cleaned TEXT,
    requirements_cleaned TEXT,
    combined_text TEXT NOT NULL,          -- Pre-computed for extraction
    cleaning_method VARCHAR(50) DEFAULT 'html_strip',
    cleaned_at TIMESTAMP DEFAULT NOW(),
    combined_word_count INTEGER,          -- ~552 words avg
    combined_char_count INTEGER
)
```

**Stats After Cleaning:**
- Total raw_jobs: 23,352
- Junk jobs (is_usable=FALSE): 125 (0.5%)
- Cleaned jobs: 23,188 (99.5%)
- Average combined_word_count: 552 words
- Average combined_char_count: ~3,000 characters

**extraction_ready_jobs VIEW:**
```sql
CREATE VIEW extraction_ready_jobs AS
SELECT
    r.job_id, r.portal, r.country, r.url, r.company, r.location,
    r.posted_date, r.scraped_at, r.extraction_status,
    c.title_cleaned, c.description_cleaned, c.requirements_cleaned,
    c.combined_text, c.combined_word_count, c.cleaned_at
FROM raw_jobs r
INNER JOIN cleaned_jobs c ON r.job_id = c.job_id
WHERE r.is_usable = TRUE
  AND r.extraction_status = 'pending';
```

**Files:** `scripts/clean_raw_jobs.py`, `src/database/migrations/006_add_cleaned_jobs_table.sql`
**Status:** âœ… Complete

---

**DATA READY FOR EXTRACTION âœ…**
- 23,188 clean, usable job postings
- All HTML removed, text normalized
- Combined text pre-computed for extraction
- Junk jobs filtered out

---

### **FASE 2: Parallel Skill Extraction**

```
                    cleaned_jobs.combined_text
                              |
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                   â–¼
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘  PIPELINE A       â•‘   â•‘  PIPELINE B       â•‘
        â•‘  (Traditional)    â•‘   â•‘  (LLM-based)      â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

### **PIPELINE A: Traditional Extraction (Regex + NER)**

```
Step 3A.1: Regex Pattern Matching
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: cleaned_jobs.combined_text           â”‚
â”‚                                             â”‚
â”‚ Patterns:                                   â”‚
â”‚   - Programming: Python, Java, JS, C++     â”‚
â”‚   - Frameworks: React, Django, Spring      â”‚
â”‚   - Databases: PostgreSQL, MongoDB         â”‚
â”‚   - Cloud: AWS, Azure, GCP                 â”‚
â”‚   - Tools: Git, Docker, Kubernetes         â”‚
â”‚                                             â”‚
â”‚ Output: List of regex-matched skills       â”‚
â”‚   [{skill: "Python", method: "regex",      â”‚
â”‚     confidence: 0.8, context: "..."}]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 3A.2: spaCy NER Processing with EntityRuler
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model: es_core_news_lg                      â”‚
â”‚                                             â”‚
â”‚ âœ… IMPLEMENTATION: Custom Entity Ruler     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ # Load spaCy + add EntityRuler         â”‚ â”‚
â”‚ â”‚ nlp = spacy.load("es_core_news_lg")    â”‚ â”‚
â”‚ â”‚ ruler = nlp.add_pipe("entity_ruler",   â”‚ â”‚
â”‚ â”‚                      before="ner")     â”‚ â”‚
â”‚ â”‚                                        â”‚ â”‚
â”‚ â”‚ # Load 13,939 ESCO skills as patterns â”‚ â”‚
â”‚ â”‚ patterns = []                          â”‚ â”‚
â”‚ â”‚ for skill in esco_skills:              â”‚ â”‚
â”‚ â”‚   patterns.append({                    â”‚ â”‚
â”‚ â”‚     "label": "SKILL",                  â”‚ â”‚
â”‚ â”‚     "pattern": skill.preferred_label   â”‚ â”‚
â”‚ â”‚   })                                   â”‚ â”‚
â”‚ â”‚                                        â”‚ â”‚
â”‚ â”‚ ruler.add_patterns(patterns)           â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                             â”‚
â”‚ Benefits:                                   â”‚
â”‚   âœ… Exact match for all ESCO skills       â”‚
â”‚   âœ… Higher recall (captures more skills)  â”‚
â”‚   âœ… No false positives on ESCO terms      â”‚
â”‚                                             â”‚
â”‚ Process:                                    â”‚
â”‚   doc = nlp(combined_text)                 â”‚
â”‚   for ent in doc.ents:                     â”‚
â”‚     if ent.label_ == "SKILL":              â”‚
â”‚       extract(ent)                         â”‚
â”‚                                             â”‚
â”‚ Output: List of NER-extracted skills       â”‚
â”‚   (includes both spaCy NER + EntityRuler)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 3A.3: Combine & Deduplicate
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Merge: regex_skills + ner_skills            â”‚
â”‚                                             â”‚
â”‚ Deduplicate:                                â”‚
â”‚   - Normalize: lowercase, strip whitespace â”‚
â”‚   - Group by normalized text               â”‚
â”‚   - Keep highest confidence score          â”‚
â”‚                                             â”‚
â”‚ Output: Unified list of candidate skills   â”‚
â”‚   (de-duplicated, sorted by confidence)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 3A.4: NO ESCO MAPPING YET
(Mapping happens in Module 4)
```

---

### **PIPELINE B: LLM-based Extraction**

```
Step 3B.1: LLM Selection & Comparison Strategy
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM OPTIONS TO COMPARE:                     â”‚
â”‚                                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Model Comparison Table                 â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ Model       â”‚ Cost â”‚ Speedâ”‚ F1  â”‚ ES? â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ GPT-3.5     â”‚ $0.50â”‚ Fast â”‚ 0.62â”‚ âœ…  â”‚ â”‚
â”‚ â”‚ GPT-4       â”‚$15.00â”‚ Slow â”‚ 0.68â”‚ âœ…  â”‚ â”‚
â”‚ â”‚ Mistral-7B  â”‚ $0   â”‚ Med  â”‚ 0.58â”‚ âœ…  â”‚ â”‚
â”‚ â”‚ Llama-3-8B  â”‚ $0   â”‚ Med  â”‚ 0.64â”‚ âœ…  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                             â”‚
â”‚ SELECTION CRITERIA:                         â”‚
â”‚   1. Cost (API vs local)                   â”‚
â”‚   2. Speed (jobs/second)                   â”‚
â”‚   3. F1-Score (from literature)            â”‚
â”‚   4. Spanish support                       â”‚
â”‚   5. **Gold Standard accuracy**            â”‚
â”‚                                             â”‚
â”‚ COMPARISON STRATEGY:                        â”‚
â”‚   âœ… Run multiple LLMs in parallel         â”‚
â”‚   âœ… Validate ALL against Gold Standard    â”‚
â”‚   âœ… Compare:                               â”‚
â”‚      - Precision/Recall vs Gold (300 jobs) â”‚
â”‚      - Distance to Silver Bullet (15K jobs)â”‚
â”‚      - Explicit vs Implicit skill coverage â”‚
â”‚      - Cost per 1M skills extracted        â”‚
â”‚                                             â”‚
â”‚ RECOMMENDED: Llama-3-8B                     â”‚
â”‚   Reason: Best balance (F1=0.64, free,     â”‚
â”‚           16GB VRAM, Spanish support)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 3B.2: Prompt Engineering
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt Template:                            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ You are an expert HR analyst.          â”‚ â”‚
â”‚ â”‚                                         â”‚ â”‚
â”‚ â”‚ Job: {title} at {company}              â”‚ â”‚
â”‚ â”‚ Description: {combined_text}           â”‚ â”‚
â”‚ â”‚                                         â”‚ â”‚
â”‚ â”‚ Extract ALL skills (explicit+implicit) â”‚ â”‚
â”‚ â”‚                                         â”‚ â”‚
â”‚ â”‚ Output JSON:                           â”‚ â”‚
â”‚ â”‚ {                                      â”‚ â”‚
â”‚ â”‚   "explicit_skills": [                 â”‚ â”‚
â”‚ â”‚     {"skill": "Python",                â”‚ â”‚
â”‚ â”‚      "category": "Programming",        â”‚ â”‚
â”‚ â”‚      "confidence": 0.95,               â”‚ â”‚
â”‚ â”‚      "evidence": "quoted text"}        â”‚ â”‚
â”‚ â”‚   ],                                   â”‚ â”‚
â”‚ â”‚   "implicit_skills": [                 â”‚ â”‚
â”‚ â”‚     {"skill": "Problem Solving",       â”‚ â”‚
â”‚ â”‚      "category": "Soft Skill",         â”‚ â”‚
â”‚ â”‚      "confidence": 0.8,                â”‚ â”‚
â”‚ â”‚      "evidence": "inferred from..."}   â”‚ â”‚
â”‚ â”‚   ]                                    â”‚ â”‚
â”‚ â”‚ }                                      â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 3B.3: LLM Inference (PER LLM)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FOR EACH LLM (GPT, Mistral, Llama):        â”‚
â”‚                                             â”‚
â”‚   response = llm.generate(prompt)          â”‚
â”‚   parsed = parse_json(response)            â”‚
â”‚                                             â”‚
â”‚   skills_from_llm_X = {                    â”‚
â”‚     'llm_model': 'gpt-3.5-turbo',          â”‚
â”‚     'explicit': parsed.explicit_skills,    â”‚
â”‚     'implicit': parsed.implicit_skills     â”‚
â”‚   }                                        â”‚
â”‚                                             â”‚
â”‚ Output: Skills from EACH LLM separately    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 3B.4: Compare LLM Results (OPTIONAL)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ IF running multiple LLMs:                   â”‚
â”‚                                             â”‚
â”‚ Compare:                                    â”‚
â”‚   - Coverage: Which found most skills?     â”‚
â”‚   - Confidence: Which has higher scores?   â”‚
â”‚   - Implicit: Which inferred more?         â”‚
â”‚                                             â”‚
â”‚ Select:                                     â”‚
â”‚   - Use BEST LLM results, OR               â”‚
â”‚   - MERGE all LLMs (union of skills)       â”‚
â”‚                                             â”‚
â”‚ Output: Selected/merged LLM skills         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 3B.5: NO ESCO MAPPING YET
(Mapping happens in Module 4)
```

---

### **MODULE 4: ESCO/O*NET Mapping (SHARED BY BOTH PIPELINES)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Skills from         â”‚    Skills from        â”‚
â”‚   Pipeline A          â”‚    Pipeline B         â”‚
â”‚   (Regex + NER)       â”‚    (LLM)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                       â”‚
            â”‚    BOTH GO THROUGH    â”‚
            â”‚    SAME MAPPING       â”‚
            â”‚    PROCESS            â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
```

#### **Layer 1: Direct & Fuzzy Matching**

```
Step 4.1: Exact Match
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FOR EACH candidate skill:                   â”‚
â”‚                                             â”‚
â”‚ Query ESCO database:                        â”‚
â”‚   SELECT esco_uri, preferred_label         â”‚
â”‚   FROM esco_skills                         â”‚
â”‚   WHERE LOWER(preferred_label_es) =        â”‚
â”‚         LOWER(candidate_skill)             â”‚
â”‚      OR LOWER(preferred_label_en) =        â”‚
â”‚         LOWER(candidate_skill)             â”‚
â”‚                                             â”‚
â”‚ IF match found:                            â”‚
â”‚   skill.esco_uri = match.esco_uri          â”‚
â”‚   skill.mapping_method = 'exact'           â”‚
â”‚   skill.mapping_confidence = 1.0           â”‚
â”‚   DONE âœ…                                   â”‚
â”‚                                             â”‚
â”‚ IF no match:                               â”‚
â”‚   Go to Step 4.2 (Fuzzy)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 4.2: Fuzzy Match
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ from fuzzywuzzy import fuzz                 â”‚
â”‚                                             â”‚
â”‚ best_match = None                          â”‚
â”‚ best_score = 0                             â”‚
â”‚                                             â”‚
â”‚ FOR EACH esco_skill in esco_database:      â”‚
â”‚   score = fuzz.ratio(                      â”‚
â”‚     normalize(candidate_skill),            â”‚
â”‚     normalize(esco_skill.label)            â”‚
â”‚   )                                        â”‚
â”‚                                             â”‚
â”‚   if score > best_score:                   â”‚
â”‚     best_match = esco_skill                â”‚
â”‚     best_score = score                     â”‚
â”‚                                             â”‚
â”‚ THRESHOLD = 85  # 85% similarity           â”‚
â”‚                                             â”‚
â”‚ IF best_score >= THRESHOLD:                â”‚
â”‚   skill.esco_uri = best_match.esco_uri     â”‚
â”‚   skill.mapping_method = 'fuzzy'           â”‚
â”‚   skill.mapping_confidence = best_score/100â”‚
â”‚   DONE âœ…                                   â”‚
â”‚                                             â”‚
â”‚ IF best_score < THRESHOLD:                 â”‚
â”‚   Go to Layer 2 (Semantic)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
```

#### **Layer 2: Semantic Matching with Embeddings**

```
Step 4.3: Generate Candidate Embedding
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load E5 model:                              â”‚
â”‚   model = SentenceTransformer(             â”‚
â”‚     'intfloat/multilingual-e5-base'        â”‚
â”‚   )                                        â”‚
â”‚                                             â”‚
â”‚ Generate embedding:                        â”‚
â”‚   candidate_embedding = model.encode(      â”‚
â”‚     candidate_skill,                       â”‚
â”‚     convert_to_numpy=True                  â”‚
â”‚   )                                        â”‚
â”‚                                             â”‚
â”‚ Normalize for cosine similarity:           â”‚
â”‚   candidate_embedding = (                  â”‚
â”‚     candidate_embedding /                  â”‚
â”‚     np.linalg.norm(candidate_embedding)    â”‚
â”‚   )                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 4.4: Similarity Search with FAISS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… USE FAISS (25x faster than PostgreSQL)  â”‚
â”‚                                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ # Load pre-built FAISS index           â”‚ â”‚
â”‚ â”‚ import faiss                           â”‚ â”‚
â”‚ â”‚ import numpy as np                     â”‚ â”‚
â”‚ â”‚                                        â”‚ â”‚
â”‚ â”‚ index = faiss.read_index(              â”‚ â”‚
â”‚ â”‚   'data/embeddings/esco.faiss'         â”‚ â”‚
â”‚ â”‚ )                                      â”‚ â”‚
â”‚ â”‚                                        â”‚ â”‚
â”‚ â”‚ # Load ESCO URI mapping                â”‚ â”‚
â”‚ â”‚ esco_uris = np.load(                   â”‚ â”‚
â”‚ â”‚   'data/embeddings/esco_uris.npy'      â”‚ â”‚
â”‚ â”‚ )                                      â”‚ â”‚
â”‚ â”‚                                        â”‚ â”‚
â”‚ â”‚ # Search for top 10 matches            â”‚ â”‚
â”‚ â”‚ k = 10                                 â”‚ â”‚
â”‚ â”‚ similarities, indices = index.search(  â”‚ â”‚
â”‚ â”‚   candidate_embedding.reshape(1, -1),  â”‚ â”‚
â”‚ â”‚   k                                    â”‚ â”‚
â”‚ â”‚ )                                      â”‚ â”‚
â”‚ â”‚                                        â”‚ â”‚
â”‚ â”‚ # Get best match                       â”‚ â”‚
â”‚ â”‚ best_idx = indices[0][0]               â”‚ â”‚
â”‚ â”‚ top_match = {                          â”‚ â”‚
â”‚ â”‚   'esco_uri': esco_uris[best_idx],     â”‚ â”‚
â”‚ â”‚   'similarity': float(similarities[0][0])â”‚ â”‚
â”‚ â”‚ }                                      â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                             â”‚
â”‚ Performance Comparison:                     â”‚
â”‚   FAISS IndexFlatIP: ~0.2s per skill       â”‚
â”‚   PostgreSQL pgvector: ~5s per skill       â”‚
â”‚   Speedup: 25x faster âš¡                    â”‚
â”‚                                             â”‚
â”‚ Why IndexFlatIP?                            â”‚
â”‚   - Exact nearest neighbor (no approx)     â”‚
â”‚   - Inner product = cosine sim (normalized)â”‚
â”‚   - No index building needed at runtime    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 4.5: Apply Threshold
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SEMANTIC_THRESHOLD = 0.85                   â”‚
â”‚                                             â”‚
â”‚ IF top_match.similarity >= THRESHOLD:      â”‚
â”‚   skill.esco_uri = top_match.esco_uri      â”‚
â”‚   skill.mapping_method = 'semantic'        â”‚
â”‚   skill.mapping_confidence = similarity    â”‚
â”‚   DONE âœ…                                   â”‚
â”‚                                             â”‚
â”‚ ELSE:                                      â”‚
â”‚   Go to Step 4.6 (Emergent Skill)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 4.6: Handle Unmapped Skills
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flag as EMERGENT SKILL:                     â”‚
â”‚                                             â”‚
â”‚ INSERT INTO emergent_skills (              â”‚
â”‚   skill_text,                              â”‚
â”‚   extraction_method,  -- 'ner'/'regex'/'llm'â”‚
â”‚   first_seen_job_id,                       â”‚
â”‚   occurrence_count,                        â”‚
â”‚   best_esco_match,    -- nearest match     â”‚
â”‚   best_similarity,    -- even if < 0.85    â”‚
â”‚   flag_reason,                             â”‚
â”‚   review_status       -- 'pending'         â”‚
â”‚ ) VALUES (...)                             â”‚
â”‚ ON CONFLICT (skill_text) DO UPDATE         â”‚
â”‚ SET occurrence_count = occurrence_count + 1â”‚
â”‚                                             â”‚
â”‚ skill.esco_uri = NULL                      â”‚
â”‚ skill.mapping_method = 'unmapped'          â”‚
â”‚ skill.flag = 'emergent_skill'              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **Save Mapped Skills**

```
Step 4.7: Save to Database
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INSERT INTO extracted_skills (              â”‚
â”‚   job_id,                                  â”‚
â”‚   skill_text,                              â”‚
â”‚   extraction_method,  -- 'regex'/'ner'/'llm'â”‚
â”‚   llm_model,          -- if from LLM       â”‚
â”‚   skill_type,         -- 'explicit'/'implicit'â”‚
â”‚   confidence_score,                        â”‚
â”‚   esco_uri,           -- NULL if unmapped  â”‚
â”‚   esco_label,                              â”‚
â”‚   mapping_method,     -- 'exact'/'fuzzy'/  â”‚
â”‚                       -- 'semantic'/'unmapped'â”‚
â”‚   mapping_confidence,                      â”‚
â”‚   evidence_text,      -- LLM reasoning     â”‚
â”‚   extracted_at                             â”‚
â”‚ ) VALUES (...)                             â”‚
â”‚                                             â”‚
â”‚ Output: Fully mapped skill database        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **MODULE 5: Pipeline Comparison (A vs B)**

```
Step 5.1: Extract Skills by Pipeline
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ -- Skills from Pipeline A                   â”‚
â”‚ SELECT * FROM extracted_skills              â”‚
â”‚ WHERE extraction_method IN ('regex', 'ner')â”‚
â”‚                                             â”‚
â”‚ -- Skills from Pipeline B                   â”‚
â”‚ SELECT * FROM extracted_skills              â”‚
â”‚ WHERE extraction_method LIKE 'llm%'        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 5.2: Calculate Comparison Metrics
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. COVERAGE ANALYSIS                        â”‚
â”‚    skills_A_only = skills_A - skills_B     â”‚
â”‚    skills_B_only = skills_B - skills_A     â”‚
â”‚    skills_both = skills_A âˆ© skills_B       â”‚
â”‚    overlap_ratio = len(both) / len(A âˆª B)  â”‚
â”‚                                             â”‚
â”‚ 2. ESCO MAPPING SUCCESS RATE                â”‚
â”‚    mapped_A = COUNT(esco_uri NOT NULL) / A â”‚
â”‚    mapped_B = COUNT(esco_uri NOT NULL) / B â”‚
â”‚                                             â”‚
â”‚ 3. CONFIDENCE DISTRIBUTION                  â”‚
â”‚    avg_conf_A = AVG(confidence_score)      â”‚
â”‚    avg_conf_B = AVG(confidence_score)      â”‚
â”‚                                             â”‚
â”‚ 4. EXPLICIT VS IMPLICIT (B only)            â”‚
â”‚    explicit_B = COUNT(skill_type='explicit')â”‚
â”‚    implicit_B = COUNT(skill_type='implicit')â”‚
â”‚                                             â”‚
â”‚ 5. EMERGENT SKILLS                          â”‚
â”‚    unmapped_A = COUNT(esco_uri IS NULL)    â”‚
â”‚    unmapped_B = COUNT(esco_uri IS NULL)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 5.3: Qualitative Analysis
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Export for manual review:                   â”‚
â”‚                                             â”‚
â”‚ 1. Top 50 skills unique to Pipeline A      â”‚
â”‚ 2. Top 50 skills unique to Pipeline B      â”‚
â”‚ 3. Skills with high confidence diff        â”‚
â”‚                                             â”‚
â”‚ Manual questions:                           â”‚
â”‚ - Are unique_B skills truly valuable?      â”‚
â”‚ - Did LLM infer useful implicit skills?    â”‚
â”‚ - Did NER/Regex miss obvious skills?       â”‚
â”‚ - Which pipeline is more comprehensive?    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 5.4: Compare Multiple LLMs (if applicable)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ IF ran multiple LLMs in Pipeline B:        â”‚
â”‚                                             â”‚
â”‚ Compare by llm_model:                       â”‚
â”‚   SELECT llm_model,                        â”‚
â”‚     COUNT(*) as skills_extracted,          â”‚
â”‚     AVG(confidence_score) as avg_conf,     â”‚
â”‚     COUNT(CASE WHEN esco_uri IS NOT NULL)  â”‚
â”‚       as mapped_count                      â”‚
â”‚   FROM extracted_skills                    â”‚
â”‚   WHERE extraction_method LIKE 'llm%'      â”‚
â”‚   GROUP BY llm_model                       â”‚
â”‚                                             â”‚
â”‚ Analyze:                                    â”‚
â”‚ - Which LLM found most skills?             â”‚
â”‚ - Which has best ESCO mapping rate?        â”‚
â”‚ - Which has highest confidence?            â”‚
â”‚ - Cost vs performance trade-off            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **MODULE 6: Skill Clustering & Temporal Analysis**

**IMPORTANT:** We cluster SKILLS, not jobs. This allows us to:
1. Identify skill profiles/families (e.g., "Frontend stack", "DevOps tools")
2. Track how skill clusters evolve over time
3. Discover emerging skill combinations

```
Step 6.1: Generate Skill Embeddings (Individual Skills)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âš ï¸ NOTE: ESCO embeddings (13,939 skills)   â”‚
â”‚          already generated in Phase 0      â”‚
â”‚                                             â”‚
â”‚ HERE: Generate embeddings for ALL extractedâ”‚
â”‚       skills (ESCO + emergent/unmapped)    â”‚
â”‚       for clustering analysis              â”‚
â”‚                                             â”‚
â”‚ FOR EACH unique skill extracted:            â”‚
â”‚                                             â”‚
â”‚   # Load E5 multilingual model             â”‚
â”‚   model = SentenceTransformer(             â”‚
â”‚     'intfloat/multilingual-e5-base'        â”‚
â”‚   )                                        â”‚
â”‚                                             â”‚
â”‚   # Generate 768D embedding                â”‚
â”‚   skill_embedding = model.encode(          â”‚
â”‚     skill_text,                            â”‚
â”‚     convert_to_numpy=True                  â”‚
â”‚   )                                        â”‚
â”‚                                             â”‚
â”‚   # Normalize for cosine similarity        â”‚
â”‚   skill_embedding = (                      â”‚
â”‚     skill_embedding /                      â”‚
â”‚     np.linalg.norm(skill_embedding)        â”‚
â”‚   )                                        â”‚
â”‚                                             â”‚
â”‚   # Save to DB                             â”‚
â”‚   INSERT INTO skill_embeddings (           â”‚
â”‚     skill_text, embedding_vector,          â”‚
â”‚     model_name, created_at                 â”‚
â”‚   ) VALUES (...)                           â”‚
â”‚                                             â”‚
â”‚ Result: N unique skills â†’ N embeddings     â”‚
â”‚         (768 dimensions each)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 6.2: UMAP Dimensionality Reduction (BEFORE Clustering)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âš ï¸ CRITICAL: Reduce BEFORE clustering      â”‚
â”‚                                             â”‚
â”‚ WHY? HDBSCAN performs poorly in high-dim   â”‚
â”‚      spaces (curse of dimensionality).     â”‚
â”‚      UMAP preserves local + global         â”‚
â”‚      structure better than PCA/t-SNE.      â”‚
â”‚                                             â”‚
â”‚ COMPARISON (from Paper 3):                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Method   â”‚ Speed   â”‚ Trustworthiness  â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ PCA      â”‚ Fast    â”‚ 0.72 (linear)    â”‚ â”‚
â”‚ â”‚ t-SNE    â”‚ Slow    â”‚ 0.85 (local)     â”‚ â”‚
â”‚ â”‚ UMAP     â”‚ Medium  â”‚ 0.91 (BEST)      â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                             â”‚
â”‚ UMAP reduces 768D â†’ 2D/3D while preserving â”‚
â”‚ both local clusters AND global topology.   â”‚
â”‚                                             â”‚
â”‚ Implementation:                             â”‚
â”‚   import umap                              â”‚
â”‚                                             â”‚
â”‚   reducer = umap.UMAP(                     â”‚
â”‚     n_components=2,      # 2D for viz      â”‚
â”‚     n_neighbors=15,      # local structure â”‚
â”‚     min_dist=0.1,        # cluster spacing â”‚
â”‚     metric='cosine'      # for embeddings  â”‚
â”‚   )                                        â”‚
â”‚                                             â”‚
â”‚   skill_embeddings_2d = reducer.fit_transform(â”‚
â”‚     skill_embeddings_768d                  â”‚
â”‚   )                                        â”‚
â”‚                                             â”‚
â”‚ Output: N skills Ã— 2 dimensions            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 6.3: HDBSCAN Clustering (AFTER Reduction)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âš ï¸ CRITICAL: Cluster on 2D UMAP output     â”‚
â”‚                                             â”‚
â”‚ Parameters (tuned for skill clustering):    â”‚
â”‚   import hdbscan                            â”‚
â”‚                                             â”‚
â”‚   clusterer = hdbscan.HDBSCAN(             â”‚
â”‚     min_cluster_size=50,   # Min skills    â”‚
â”‚     min_samples=10,        # Core density  â”‚
â”‚     metric='euclidean',    # On 2D UMAP    â”‚
â”‚     cluster_selection_method='eom'         â”‚
â”‚   )                                        â”‚
â”‚                                             â”‚
â”‚   cluster_labels = clusterer.fit_predict(  â”‚
â”‚     skill_embeddings_2d  # 2D, NOT 768D!  â”‚
â”‚   )                                        â”‚
â”‚                                             â”‚
â”‚ Output: Cluster labels for each skill      â”‚
â”‚   -1 = noise/outliers                      â”‚
â”‚   0, 1, 2, ... = cluster IDs               â”‚
â”‚                                             â”‚
â”‚ Save results:                               â”‚
â”‚   UPDATE extracted_skills                  â”‚
â”‚   SET cluster_id = %s, cluster_prob = %s   â”‚
â”‚   WHERE skill_text = %s                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
Step 6.4: Temporal Cluster Analysis
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Goal: Track how skill clusters change      â”‚
â”‚       over time (2018-2025)                 â”‚
â”‚                                             â”‚
â”‚ Analysis 1: Cluster growth/decline          â”‚
â”‚   SELECT                                    â”‚
â”‚     cluster_id,                            â”‚
â”‚     DATE_TRUNC('quarter', posted_date),    â”‚
â”‚     COUNT(DISTINCT job_id) as demand       â”‚
â”‚   FROM extracted_skills e                  â”‚
â”‚   JOIN raw_jobs r ON e.job_id = r.job_id  â”‚
â”‚   GROUP BY cluster_id, quarter             â”‚
â”‚   ORDER BY quarter, demand DESC            â”‚
â”‚                                             â”‚
â”‚ Analysis 2: Emerging clusters               â”‚
â”‚   - Identify clusters with demand spike    â”‚
â”‚   - Flag new clusters (appeared in 2024+)  â”‚
â”‚                                             â”‚
â”‚ Analysis 3: Dying clusters                  â”‚
â”‚   - Identify clusters with demand drop     â”‚
â”‚   - Mark as "obsolete skills"              â”‚
â”‚                                             â”‚
â”‚ Visualization:                              â”‚
â”‚   - Animated scatter plot (UMAP 2D)        â”‚
â”‚   - Color = cluster, size = demand         â”‚
â”‚   - Timeline slider (by quarter/year)      â”‚
â”‚   - "Replay" skill demand evolution        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **MODULE 7: SQL Analysis Queries & Visualizations**

#### **7.1: Top Skills Analysis**

**Query 1: Top 20 Most Demanded Skills Overall**
```sql
SELECT
    e.skill_text,
    es.preferred_label_es as esco_label,
    COUNT(DISTINCT e.job_id) as demand_count,
    COUNT(DISTINCT r.country) as countries_present,
    ROUND(AVG(e.confidence_score), 2) as avg_confidence,
    COUNT(CASE WHEN e.mapping_method = 'exact' THEN 1 END) as exact_matches,
    COUNT(CASE WHEN e.mapping_method = 'fuzzy' THEN 1 END) as fuzzy_matches,
    COUNT(CASE WHEN e.mapping_method = 'semantic' THEN 1 END) as semantic_matches
FROM extracted_skills e
INNER JOIN raw_jobs r ON e.job_id = r.job_id
LEFT JOIN esco_skills es ON e.esco_uri = es.esco_uri
WHERE r.is_usable = TRUE
GROUP BY e.skill_text, es.preferred_label_es
ORDER BY demand_count DESC
LIMIT 20;
```

**Query 2: Top Skills by Country**
```sql
SELECT
    r.country,
    e.skill_text,
    COUNT(*) as demand_count,
    ROUND(AVG(e.confidence_score), 2) as avg_confidence
FROM extracted_skills e
INNER JOIN raw_jobs r ON e.job_id = r.job_id
WHERE r.is_usable = TRUE
GROUP BY r.country, e.skill_text
ORDER BY r.country, demand_count DESC;
```

**Query 3: Temporal Skill Trends (Last 12 Months)**
```sql
SELECT
    DATE_TRUNC('month', r.posted_date) as month,
    e.skill_text,
    COUNT(DISTINCT e.job_id) as demand_count
FROM extracted_skills e
INNER JOIN raw_jobs r ON e.job_id = r.job_id
WHERE r.posted_date >= CURRENT_DATE - INTERVAL '12 months'
  AND r.is_usable = TRUE
GROUP BY month, e.skill_text
ORDER BY month DESC, demand_count DESC;
```

---

#### **7.2: Skill Co-occurrence Analysis**

**Query 4: Frequent Skill Pairs**
```sql
WITH skill_pairs AS (
    SELECT
        e1.skill_text as skill_1,
        e2.skill_text as skill_2,
        e1.job_id
    FROM extracted_skills e1
    INNER JOIN extracted_skills e2
        ON e1.job_id = e2.job_id
        AND e1.skill_text < e2.skill_text  -- Avoid duplicates
)
SELECT
    skill_1,
    skill_2,
    COUNT(*) as co_occurrence_count,
    ROUND(
        COUNT(*)::DECIMAL / (
            SELECT COUNT(DISTINCT job_id) FROM raw_jobs WHERE is_usable = TRUE
        ) * 100,
        2
    ) as percentage_of_jobs
FROM skill_pairs
GROUP BY skill_1, skill_2
ORDER BY co_occurrence_count DESC
LIMIT 50;
```

**Purpose:** Identify common skill combinations (e.g., Python + Django, React + TypeScript)

---

#### **7.3: Geographic Skill Distribution**

**Query 5: Skills Unique to Each Country**
```sql
-- Skills found ONLY in Colombia
SELECT
    e.skill_text,
    COUNT(*) as demand_count
FROM extracted_skills e
INNER JOIN raw_jobs r ON e.job_id = r.job_id
WHERE r.country = 'CO'
  AND r.is_usable = TRUE
  AND e.skill_text NOT IN (
      SELECT DISTINCT skill_text
      FROM extracted_skills e2
      INNER JOIN raw_jobs r2 ON e2.job_id = r2.job_id
      WHERE r2.country IN ('MX', 'AR')
  )
GROUP BY e.skill_text
ORDER BY demand_count DESC
LIMIT 20;

-- Repeat for MX and AR
```

**Query 6: Skill Demand by Portal**
```sql
SELECT
    r.portal,
    e.skill_text,
    COUNT(*) as demand_count
FROM extracted_skills e
INNER JOIN raw_jobs r ON e.job_id = r.job_id
WHERE r.is_usable = TRUE
GROUP BY r.portal, e.skill_text
ORDER BY r.portal, demand_count DESC;
```

---

#### **7.4: Cluster Analysis Queries**

**Query 7: Cluster Statistics**
```sql
SELECT
    cluster_id,
    COUNT(*) as job_count,
    COUNT(DISTINCT country) as countries,
    COUNT(DISTINCT portal) as portals,
    ROUND(AVG(cluster_probability), 2) as avg_probability,
    ARRAY_AGG(DISTINCT country) as country_list
FROM raw_jobs
WHERE cluster_id IS NOT NULL
  AND is_usable = TRUE
GROUP BY cluster_id
ORDER BY job_count DESC;
```

**Query 8: Top Skills per Cluster**
```sql
SELECT
    r.cluster_id,
    e.skill_text,
    COUNT(*) as skill_count,
    ROUND(AVG(e.confidence_score), 2) as avg_confidence
FROM extracted_skills e
INNER JOIN raw_jobs r ON e.job_id = r.job_id
WHERE r.cluster_id IS NOT NULL
  AND r.is_usable = TRUE
GROUP BY r.cluster_id, e.skill_text
ORDER BY r.cluster_id, skill_count DESC;
```

**Query 9: Cluster Temporal Evolution**
```sql
SELECT
    cluster_id,
    DATE_TRUNC('month', posted_date) as month,
    COUNT(*) as job_count
FROM raw_jobs
WHERE cluster_id IS NOT NULL
  AND is_usable = TRUE
  AND posted_date >= CURRENT_DATE - INTERVAL '12 months'
GROUP BY cluster_id, month
ORDER BY month, cluster_id;
```

---

#### **7.5: Pipeline Comparison Queries**

**Query 10: Pipeline A vs B Coverage**
```sql
-- Pipeline A (Regex + NER)
WITH pipeline_a AS (
    SELECT COUNT(DISTINCT skill_text) as unique_skills_a
    FROM extracted_skills
    WHERE extraction_method IN ('regex', 'ner')
),
-- Pipeline B (LLM)
pipeline_b AS (
    SELECT COUNT(DISTINCT skill_text) as unique_skills_b
    FROM extracted_skills
    WHERE extraction_method LIKE 'llm%'
),
-- Overlap
overlap AS (
    SELECT COUNT(DISTINCT e1.skill_text) as overlap_count
    FROM extracted_skills e1
    WHERE extraction_method IN ('regex', 'ner')
      AND EXISTS (
          SELECT 1 FROM extracted_skills e2
          WHERE e2.skill_text = e1.skill_text
            AND e2.extraction_method LIKE 'llm%'
      )
)
SELECT
    a.unique_skills_a,
    b.unique_skills_b,
    o.overlap_count,
    (a.unique_skills_a - o.overlap_count) as a_only,
    (b.unique_skills_b - o.overlap_count) as b_only,
    ROUND(o.overlap_count::DECIMAL / (a.unique_skills_a + b.unique_skills_b - o.overlap_count) * 100, 2) as jaccard_similarity
FROM pipeline_a a, pipeline_b b, overlap o;
```

---

#### **7.6: Emergent Skills Tracking**

**Query 11: Top Emergent Skills (Unmapped to ESCO)**
```sql
SELECT
    skill_text,
    occurrence_count,
    extraction_methods,
    best_esco_match,
    ROUND(best_similarity, 2) as similarity,
    first_seen_job_id,
    review_status
FROM emergent_skills
WHERE review_status = 'pending'
ORDER BY occurrence_count DESC
LIMIT 50;
```

**Purpose:** Identify new/emerging skills not in ESCO taxonomy that should be reviewed for inclusion

---

#### **7.7: Data Quality Metrics**

**Query 12: Extraction Success Rates**
```sql
SELECT
    portal,
    country,
    COUNT(*) as total_jobs,
    COUNT(CASE WHEN extraction_status = 'completed' THEN 1 END) as extracted,
    COUNT(CASE WHEN extraction_status = 'failed' THEN 1 END) as failed,
    ROUND(
        COUNT(CASE WHEN extraction_status = 'completed' THEN 1 END)::DECIMAL / COUNT(*) * 100,
        2
    ) as success_rate
FROM raw_jobs
WHERE is_usable = TRUE
GROUP BY portal, country
ORDER BY portal, country;
```

**Query 13: ESCO Mapping Success Rate**
```sql
SELECT
    extraction_method,
    COUNT(*) as total_skills,
    COUNT(CASE WHEN esco_uri IS NOT NULL THEN 1 END) as mapped,
    COUNT(CASE WHEN esco_uri IS NULL THEN 1 END) as unmapped,
    ROUND(
        COUNT(CASE WHEN esco_uri IS NOT NULL THEN 1 END)::DECIMAL / COUNT(*) * 100,
        2
    ) as mapping_success_rate
FROM extracted_skills
GROUP BY extraction_method
ORDER BY extraction_method;
```

---

#### **7.8: Visualization Types**

**1. Top Skills Bar Chart**
- X-axis: Skill name
- Y-axis: Demand count
- Color: ESCO category
- Data: Query 1

**2. Temporal Trend Line Chart**
- X-axis: Month
- Y-axis: Demand count
- Lines: Top 10 skills
- Data: Query 3

**3. Geographic Heat Map**
- Map of CO, MX, AR
- Color intensity: Skill demand per country
- Data: Query 2

**4. Skill Co-occurrence Network**
- Nodes: Skills
- Edges: Co-occurrence count
- Layout: Force-directed
- Data: Query 4

**5. Cluster Scatter Plot (UMAP 2D)**
- X, Y: UMAP coordinates
- Color: Cluster ID
- Size: Cluster probability
- Data: job_embeddings_reduced table

**6. Pipeline Comparison Venn Diagram**
- Circle A: Pipeline A skills
- Circle B: Pipeline B skills
- Overlap: Shared skills
- Data: Query 10

**Visualization Tools:**
- Plotly (interactive charts)
- Matplotlib/Seaborn (static charts)
- NetworkX (skill networks)
- Export formats: PNG, SVG, HTML, PDF

**Files:** `src/analyzer/visualizations.py`, `src/analyzer/report_generator.py`
**Status:** âŒ Not implemented

---

## ğŸ“‹ **Database Schema - ACTUALIZADO**

```sql
-- ============================================================
-- EXTRACTION & MAPPING TABLES
-- ============================================================

extracted_skills (
    extraction_id UUID PRIMARY KEY,
    job_id UUID REFERENCES raw_jobs(job_id),

    -- Extracted skill
    skill_text TEXT NOT NULL,
    extraction_method VARCHAR(50),  -- 'regex', 'ner', 'llm_explicit', 'llm_implicit'

    -- LLM-specific
    llm_model VARCHAR(100),          -- 'gpt-3.5-turbo', 'mistral-7b', 'llama-2-7b'
    skill_type VARCHAR(20),          -- 'explicit', 'implicit' (LLM only)
    evidence_text TEXT,              -- LLM reasoning

    -- Confidence
    confidence_score REAL,           -- 0.0 to 1.0

    -- ESCO Mapping (results from Module 4)
    esco_uri VARCHAR(255),           -- NULL if unmapped
    esco_label TEXT,
    mapping_method VARCHAR(50),      -- 'exact', 'fuzzy', 'semantic', 'unmapped'
    mapping_confidence REAL,         -- 0.0 to 1.0

    -- Metadata
    extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    UNIQUE(job_id, skill_text, extraction_method)
)

-- ============================================================
-- EMERGENT SKILLS (unmapped from Module 4)
-- ============================================================

emergent_skills (
    emergent_id UUID PRIMARY KEY,
    skill_text TEXT UNIQUE,

    -- Tracking
    first_seen_job_id UUID REFERENCES raw_jobs(job_id),
    occurrence_count INTEGER DEFAULT 1,
    extraction_methods TEXT[],       -- ['llm', 'ner', 'regex']

    -- Best attempt at mapping
    best_esco_match VARCHAR(255),    -- Nearest ESCO skill (even if < threshold)
    best_similarity REAL,            -- Similarity score

    -- Manual review
    flag_reason VARCHAR(100),
    review_status VARCHAR(20) DEFAULT 'pending',  -- 'pending', 'approved', 'rejected', 'mapped'
    reviewed_at TIMESTAMP,
    reviewer_notes TEXT,

    -- If approved, map to custom or ESCO
    custom_category VARCHAR(100),
    mapped_to_esco_uri VARCHAR(255),

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)

-- ============================================================
-- ESCO EMBEDDINGS (from Module 4 setup)
-- ============================================================

skill_embeddings (
    embedding_id UUID PRIMARY KEY,
    esco_uri VARCHAR(255) REFERENCES esco_skills(esco_uri),
    skill_text TEXT,

    -- Embedding
    embedding_vector REAL[768],      -- E5 multilingual embeddings
    model_name VARCHAR(100) DEFAULT 'multilingual-e5-base',
    embedding_dimension INTEGER DEFAULT 768,

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    UNIQUE(esco_uri, model_name)
)

-- ============================================================
-- JOB EMBEDDINGS (for clustering)
-- ============================================================

job_embeddings (
    job_id UUID PRIMARY KEY REFERENCES raw_jobs(job_id),

    -- Full embedding
    embedding_vector REAL[768],
    embedding_method VARCHAR(50),    -- 'skill_aggregation', 'full_text'
    model_name VARCHAR(100),

    -- Reduced for visualization
    umap_x REAL,
    umap_y REAL,
    umap_z REAL,                     -- NULL if 2D
    n_components INTEGER,            -- 2 or 3

    -- Cluster assignment
    cluster_id INTEGER,              -- NULL or -1 = noise
    cluster_probability REAL,

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)
```

---
