# ETL Strategy & Architecture Decisions

**Date:** October 19, 2025
**Status:** Design Complete, Implementation Ready
**Next Steps:** Implement cleaning script and update extraction pipeline

---

## ðŸ“Š Current Data Status

### Raw Data
- **Total jobs scraped:** 23,352
  - hiring_cafe/CO: 5,831
  - hiring_cafe/MX: 13,762
  - hiring_cafe/AR: 3,720
  - elempleo/CO: 38
  - zonajobs/AR: 1

- **Extraction status:** 0 jobs extracted (all pending)
- **Data quality issues identified:**
  - 6 jobs with empty descriptions (0.03%)
  - ~50-100 potential junk/test jobs (0.2-0.4%)
  - ~330 jobs missing requirements (1.4%)
  - HTML tags in descriptions/titles
  - Missing company names (~2%)

---

## ðŸŽ¯ Architecture Decisions

### âœ… DECISION 1: Separate `cleaned_jobs` Table

**Question:** Store cleaned data in raw_jobs columns OR separate table?

**Decision:** **Separate `cleaned_jobs` table**

**Rationale:**
- Clean separation of concerns (raw vs processed)
- Can rebuild cleaned_jobs without touching raw data
- Easier to version/iterate on cleaning logic
- Same `job_id` as foreign key to raw_jobs

**Implementation:**
```sql
CREATE TABLE cleaned_jobs (
    job_id UUID PRIMARY KEY REFERENCES raw_jobs(job_id),
    title_cleaned TEXT,
    description_cleaned TEXT,
    requirements_cleaned TEXT,
    combined_text TEXT NOT NULL,  -- For extraction
    cleaning_method VARCHAR(50),
    cleaned_at TIMESTAMP,
    combined_word_count INTEGER,
    combined_char_count INTEGER
);
```

---

### âœ… DECISION 2: Store Combined Clean Text

**Question:** Combine title+description+requirements at extraction time OR store pre-combined?

**Decision:** **Store `combined_text` in `cleaned_jobs` table**

**Rationale:**
- Avoid recomputing combination 23k+ times
- Faster extraction (read one field instead of concatenating 3)
- Consistent text input across all extraction methods
- Matches existing pipeline logic

**Format:**
```python
combined_text = f"{title_cleaned}\n{description_cleaned}\n{requirements_cleaned}"
```

---

### âœ… DECISION 3: Clean Title Too

**Question:** Should we clean the title field?

**Decision:** **YES, clean title**

**Rationale:**
- Titles can contain HTML: `<strong>Senior Developer</strong>`
- Titles can contain junk: `URGENTE!!! ðŸ’°ðŸ’°ðŸ’°`
- Title is part of combined_text for extraction
- Important for skill extraction (job titles contain skills)

**Cleaning steps:**
1. Remove HTML tags
2. Remove excessive punctuation (!!!, ???)
3. Remove emojis
4. Normalize whitespace
5. Keep original case (helps NER)

---

### âœ… DECISION 4: Junk Filtering Strategy

**Question:** How to handle test/junk jobs?

**Decision:** **Flag with `is_usable=FALSE`, don't delete**

**Rationale:**
- Preserve all scraped data
- Reversible (can manually review flagged jobs)
- Clear audit trail

**Implementation:**
```sql
-- Add to raw_jobs:
is_usable BOOLEAN DEFAULT TRUE
unusable_reason TEXT

-- Filter rules:
- description < 100 chars â†’ "Short description"
- title = "test" (exact) â†’ "Test job"
- title = "002_Cand1" pattern â†’ "Candidate placeholder"
- title = "Colombia Test 7" pattern â†’ "Vendor test job"
```

**Strict regex to avoid false positives:**
```python
JUNK_PATTERNS = [
    r'^test$',                    # Just "test"
    r'^demo$',                    # Just "demo"
    r'^\d{3}_cand',              # "002_Cand1"
    r'^(colombia|mexico|argentina)\s+(credo\s+)?test\s+\d+$'
]

# KEEP legitimate jobs:
# - "Test Automation Engineer" âœ…
# - "QA Tester" âœ…
# - "Software Development Engineer in Test" âœ…
```

---

### âœ… DECISION 5: FAISS for Embeddings

**Question:** Use pgvector OR FAISS OR both?

**Decision:** **FAISS for clustering, keep PostgreSQL for metadata**

**Rationale:**
- FAISS optimized for large-scale similarity search
- Better for clustering algorithms (UMAP + HDBSCAN)
- Loads all vectors into memory (fast)
- PostgreSQL stores metadata + skill text

**Implementation:**
```python
# skill_embeddings table (PostgreSQL):
skill_text, faiss_index_position, model_name, created_at

# FAISS index (file):
data/embeddings/skill_vectors.faiss

# Workflow:
1. Generate embeddings â†’ store in PostgreSQL
2. Export to numpy matrix â†’ build FAISS index
3. Use FAISS for similarity search + clustering
4. Query PostgreSQL for skill metadata
```

---

## ðŸ”„ Complete ETL Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: SCRAPING (âœ… DONE - 23,352 jobs)                    â”‚
â”‚                                                               â”‚
â”‚ Output: raw_jobs table                                       â”‚
â”‚ â”œâ”€â”€ title, description, requirements (with HTML)             â”‚
â”‚ â”œâ”€â”€ is_usable = TRUE (default)                               â”‚
â”‚ â””â”€â”€ extraction_status = 'pending'                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 2: ETL CLEANING (ðŸ”„ TO IMPLEMENT)                      â”‚
â”‚                                                               â”‚
â”‚ Script: scripts/clean_raw_jobs.py                            â”‚
â”‚                                                               â”‚
â”‚ For each job in raw_jobs:                                    â”‚
â”‚   1. Check if junk â†’ UPDATE is_usable=FALSE if junk          â”‚
â”‚   2. Clean title â†’ title_cleaned                             â”‚
â”‚   3. Clean description â†’ description_cleaned                 â”‚
â”‚   4. Clean requirements â†’ requirements_cleaned               â”‚
â”‚   5. Combine â†’ combined_text                                 â”‚
â”‚   6. INSERT INTO cleaned_jobs                                â”‚
â”‚                                                               â”‚
â”‚ Output: cleaned_jobs table (~23,250 usable jobs)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 3: SKILL EXTRACTION (ðŸ”„ TO UPDATE)                     â”‚
â”‚                                                               â”‚
â”‚ Read from: extraction_ready_jobs VIEW                        â”‚
â”‚ â”œâ”€â”€ Filters: is_usable=TRUE + has cleaned_jobs record        â”‚
â”‚ â””â”€â”€ Returns: job_id, combined_text                           â”‚
â”‚                                                               â”‚
â”‚ Process:                                                      â”‚
â”‚ 1. Extract skills from combined_text                         â”‚
â”‚ 2. Map to ESCO OR custom_skill_mappings                      â”‚
â”‚ 3. Store in extracted_skills                                 â”‚
â”‚ 4. UPDATE raw_jobs.extraction_status = 'completed'           â”‚
â”‚                                                               â”‚
â”‚ Output: extracted_skills table                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 4: EMBEDDING GENERATION (â³ FUTURE)                    â”‚
â”‚                                                               â”‚
â”‚ 1. Generate embeddings with E5 model                         â”‚
â”‚ 2. Store metadata in skill_embeddings (PostgreSQL)           â”‚
â”‚ 3. Build FAISS index for fast similarity search              â”‚
â”‚                                                               â”‚
â”‚ Output:                                                       â”‚
â”‚ â”œâ”€â”€ skill_embeddings table (metadata)                        â”‚
â”‚ â””â”€â”€ data/embeddings/skill_vectors.faiss (index file)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 5-6: CLUSTERING (â³ FUTURE)                            â”‚
â”‚                                                               â”‚
â”‚ 1. Load FAISS index                                          â”‚
â”‚ 2. UMAP dimensionality reduction (768D â†’ 2D/3D)              â”‚
â”‚ 3. HDBSCAN clustering                                        â”‚
â”‚ 4. Store in analysis_results                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“‹ Database Schema Changes

### Migration 006: `cleaned_jobs` table

**New table:**
```sql
cleaned_jobs (
    job_id UUID PRIMARY KEY â†’ raw_jobs(job_id),
    title_cleaned TEXT,
    description_cleaned TEXT,
    requirements_cleaned TEXT,
    combined_text TEXT NOT NULL,
    cleaning_method VARCHAR(50),
    cleaned_at TIMESTAMP,
    combined_word_count INTEGER,
    combined_char_count INTEGER
)
```

**New columns in raw_jobs:**
```sql
is_usable BOOLEAN DEFAULT TRUE,
unusable_reason TEXT
```

**New view:**
```sql
extraction_ready_jobs VIEW
-- Returns jobs ready for extraction:
-- WHERE is_usable=TRUE AND has cleaned_jobs record
```

**New function:**
```sql
get_cleaning_stats()
-- Returns ETL pipeline statistics
```

---

## ðŸ› ï¸ Text Cleaning Rules

### HTML Removal
```python
# Remove all HTML tags
text = re.sub(r'<[^>]+>', ' ', text)

# Decode HTML entities
text = html.unescape(text)  # &nbsp; â†’ space, &amp; â†’ &

# Examples:
"<div><b>Buscamos</b> desarrollador</div>" â†’ "Buscamos desarrollador"
"Empresa &amp; Asociados" â†’ "Empresa & Asociados"
```

### Whitespace Normalization
```python
# Multiple spaces â†’ single space
text = re.sub(r'\s+', ' ', text)

# Trim leading/trailing
text = text.strip()

# Examples:
"Buscamos    desarrollador" â†’ "Buscamos desarrollador"
"  Senior Developer  " â†’ "Senior Developer"
```

### Title-Specific Cleaning
```python
# Remove excessive punctuation
text = re.sub(r'([!?]){2,}', r'\1', text)

# Remove emojis (Unicode ranges)
text = re.sub(r'[\U00010000-\U0010ffff]', '', text)

# Examples:
"URGENTE!!! Desarrollador" â†’ "URGENTE! Desarrollador"
"Senior Dev ðŸ’°ðŸ’°" â†’ "Senior Dev"
```

### What We DON'T Change
- âœ… Keep accents: "Desarrollador" (important for Spanish NER)
- âœ… Keep case: "Senior Developer" (helps entity recognition)
- âœ… Keep punctuation: "Full-stack" (meaningful)
- âœ… Keep numbers: "5+ years" (meaningful)

---

## ðŸ“Š Expected Results

### After ETL Cleaning

```
Input: 23,352 raw jobs

Junk Detection:
â”œâ”€â”€ Empty description (< 100 chars): ~30 jobs
â”œâ”€â”€ Test jobs (exact match): ~20 jobs
â”œâ”€â”€ Candidate placeholders: ~10 jobs
â”œâ”€â”€ Total unusable: ~100 jobs (0.4%)
â””â”€â”€ Usable jobs: ~23,250 (99.6%)

Cleaning Results:
â”œâ”€â”€ Jobs with cleaned text: 23,250
â”œâ”€â”€ Average combined_text length: ~5,000 chars
â”œâ”€â”€ Average word count: ~700 words
â””â”€â”€ Ready for extraction: 23,250 jobs
```

### Storage Impact

```
Raw data size: 97 MB (raw_jobs)
Cleaned data size: ~80 MB (cleaned_jobs) [estimated]
Total database size: ~177 MB
Disk space increase: +82% (acceptable)
```

---

## ðŸŽ¯ Implementation Checklist

### âœ… Completed
1. [x] Analyze data quality issues
2. [x] Define ETL strategy and make architecture decisions
3. [x] Create SQL migration 006
4. [x] Run migration (cleaned_jobs table created)
5. [x] Document decisions and rationale

### ðŸ”„ Next Steps
1. [ ] Implement `scripts/clean_raw_jobs.py`
2. [ ] Run ETL on all 23k jobs
3. [ ] Verify cleaning results
4. [ ] Update extraction pipeline to use `extraction_ready_jobs` view
5. [ ] Test extraction on cleaned data
6. [ ] Document FAISS index implementation (Stage 4)

---

## ðŸ“š References

- Migration file: `src/database/migrations/006_add_cleaned_jobs_table.sql`
- Extraction pipeline: `src/extractor/pipeline.py` (needs update)
- ESCO matcher: `src/extractor/esco_matcher.py`
- Custom skills: `custom_skill_mappings` table

---

**Document Status:** Complete
**Last Updated:** 2025-10-19
**Ready for Implementation:** YES
