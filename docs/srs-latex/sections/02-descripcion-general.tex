\chapter{DESCRIPCIÓN GENERAL}

\section{Perspectiva del Producto}

El sistema propuesto, denominado Observatorio de Demanda Laboral en Tecnología en Latinoamérica, corresponde a un producto completamente nuevo, diseñado desde cero con el fin de ofrecer una solución automatizada, académicamente robusta y técnicamente escalable para el análisis de habilidades tecnológicas demandadas en el mercado laboral digital. Si bien es novedosa la implementación y el diseño, se basa en múltiples componentes o sistemas similares ya propuestos en literatura Europea, Africana, y Estadounidense, lo que robustece la facilidad de su implementación.

Este sistema no reemplaza una herramienta previa ni se integra como módulo en un sistema existente; por el contrario, responde a una necesidad actual no cubierta de manera integral en los contextos académico y gubernamental latinoamericano: la escasez de plataformas automatizadas que permitan entender en profundidad la evolución semántica de las habilidades requeridas en el sector tecnológico, a partir de vacantes en línea publicadas en español.

La decisión de desarrollar este producto surge de una combinación de motivaciones técnicas y sociales que se complementan mutuamente. Desde el punto de vista técnico, se busca aplicar metodologías avanzadas de procesamiento de lenguaje natural, extracción de entidades nombradas, embeddings multilingües y clustering semántico para lograr una segmentación coherente y útil de perfiles laborales emergentes en el sector tecnológico. Desde una perspectiva social y estratégica, se pretende brindar herramientas de análisis riguroso que apoyen a universidades, centros de formación, entidades públicas y actores del ecosistema digital en la identificación temprana de brechas de habilidades, facilitando así la toma de decisiones informadas en política educativa, mejoramiento de empleabilidad y diseño de programas de reconversión laboral.

El sistema propuesto se distingue de otros enfoques parciales por su carácter modular, reproducible y enfocado en la generación de conocimiento estratégico a partir de datos abiertos. Su desarrollo también permitirá validar técnicas emergentes de extracción e inferencia con modelos de lenguaje en español, contribuyendo al cuerpo académico y técnico en ciencia de datos aplicada al empleo. En síntesis, el Observatorio representa un aporte original y pertinente tanto desde el plano metodológico como desde su aplicabilidad social, al combinar scraping ético, procesamiento de lenguaje natural multilingüe y visualización analítica para abordar un problema real en el contexto latinoamericano.

\subsection{Interfaces con el sistema}

El sistema ``Observatorio de Demanda Laboral en Tecnología en Latinoamérica'' es un producto completamente nuevo y autónomo, por lo cual no mantiene interfaces directas con sistemas externos en tiempo de ejecución. Sin embargo, presenta una arquitectura modular interna donde cada componente se comunica con los demás a través de interfaces internas bien definidas.

El flujo funcional del sistema se organiza de manera secuencial y conectada, iniciando con el módulo de web scraping, el cual actúa como punto de entrada para la adquisición de datos desde portales de empleo. Cada uno de los módulos siguientes procesa la información recibida del anterior, generando una línea de procesamiento de datos estructurada y continua.

Las interfaces internas entre módulos siguen un flujo secuencial de procesamiento de datos. El módulo de scraping obtiene vacantes desde portales como Computrabajo, Bumeran y ElEmpleo, depositando los datos en bruto que posteriormente son integrados a PostgreSQL. El módulo de procesamiento semántico recibe estos textos y los normaliza mediante lematización, tokenización y limpieza utilizando librerías especializadas como spaCy y expresiones regulares. El módulo de extracción de habilidades procesa estos textos normalizados mediante técnicas de reconocimiento de entidades nombradas, expresiones regulares y modelos de lenguaje descargados desde Hugging Face. El módulo de embeddings transforma las habilidades extraídas en vectores de 768 dimensiones mediante SentenceTransformers. El módulo de clustering aplica reducción dimensional con UMAP seguido de agrupamiento con HDBSCAN. Finalmente, el módulo de visualización genera gráficos interactivos y reportes analíticos mediante matplotlib, Plotly y pandas, exportando resultados a formatos PDF, HTML y CSV, complementados por un dashboard web desarrollado con Next.js para consulta dinámica.

Aunque el sistema hace uso intensivo de librerías de código abierto, estas no constituyen interfaces con sistemas externos, sino dependencias internas gestionadas como paquetes dentro del entorno virtual del proyecto mediante pip. No se consumen APIs externas de pago, ni se establece comunicación en tiempo real con servicios web propietarios de terceros. Este diseño modular y desacoplado permite mantener un alto grado de interoperabilidad y facilita la posibilidad de sustituir o extender cada módulo de forma independiente, sin afectar la funcionalidad del sistema completo.

\subsection{Arquitectura del Sistema}

El sistema implementa una arquitectura híbrida que combina tres patrones arquitectónicos complementarios para satisfacer requisitos duales de latencia y procesamiento. El patrón Request/Response mediante microservicios en capas proporciona operaciones síncronas de baja latencia para consultas de usuarios a través de frontend Next.js y API FastAPI conectados a PostgreSQL. El patrón Event-Driven mediante comunicación Pub/Sub con Redis y Celery permite procesamiento asíncrono distribuido de tareas computacionalmente intensivas que pueden requerir minutos u horas, tales como scraping, extracción de habilidades con modelos de lenguaje, generación de embeddings y clustering. El patrón API Gateway implementado con Nginx actúa como punto único de entrada para todas las peticiones HTTP/HTTPS externas, proporcionando routing inteligente, terminación SSL/TLS y rate limiting.

El pipeline de procesamiento de datos se organiza en siete etapas especializadas siguiendo la metodología CRISP-DM adaptada para minería de textos: scraping automatizado desde 7 portales principales mediante Scrapy y Selenium, extracción de habilidades mediante reconocimiento de entidades nombradas con spaCy y expresiones regulares, enriquecimiento semántico opcional mediante modelos de lenguaje locales, generación de embeddings de 768 dimensiones con el modelo E5 Multilingual, reducción dimensional con UMAP preservando estructura topológica, clustering jerárquico basado en densidad con HDBSCAN sin requerir número de clusters predefinido, y visualización mediante dashboard web Next.js complementado con reportes estáticos en formatos PDF, HTML y CSV.

La especificación detallada de la arquitectura del sistema, incluyendo decisiones de diseño, justificaciones técnicas, diagramas de componentes, vistas arquitectónicas y patrones implementados, se documenta exhaustivamente en el Documento de Arquitectura del Software (SAD) que acompaña este SRS.

\subsection{Interfaces con el usuario}

El sistema proporciona múltiples interfaces de usuario orientadas a diferentes perfiles técnicos y necesidades analíticas. Además de interfaces técnicas de consola, notebooks ejecutables y reportes estáticos, se desarrolló un frontend web desplegado mediante Docker que presenta un dashboard interactivo con las estadísticas más importantes del observatorio.

A continuación, se describen las interfaces de usuario disponibles, junto con sus características técnicas y funcionales:

\subsubsection{Terminal o Consola}

La interfaz de línea de comandos constituye la interacción principal de los desarrolladores e investigadores con el sistema, permitiendo la ejecución de spiders, procesamiento de datos, entrenamiento de modelos y generación de reportes. Esta interfaz requiere acceso a un entorno UNIX-like, recomendándose Linux o macOS, aunque también es funcional en Windows mediante WSL. El sistema requiere Python 3.10 o superior instalado. La usabilidad de esta interfaz demanda conocimientos intermedios en línea de comandos, aunque el entrenamiento estimado para familiarizarse con los comandos básicos es inferior a una hora para usuarios técnicos.

\subsubsection{Notebooks Jupyter}

Las notebooks Jupyter sirven para validación de resultados, visualización exploratoria y pruebas modulares. Estas interfaces permiten la carga de resultados de clustering, análisis gráfico, pruebas de embeddings y revisión de habilidades extraídas. Los requisitos técnicos incluyen instalación local de JupyterLab o uso en plataforma cloud como Google Colab, recomendándose una resolución mínima de 1366x768 píxeles para visualización óptima. Las notebooks están documentadas con celdas de texto explicativo y ejemplos reproducibles, esperándose un nivel básico de familiaridad con Python y Pandas por parte del usuario.

\subsubsection{Dashboard Interactivo Web}

El dashboard web constituye la interfaz principal de visualización centralizada e interactiva de las estadísticas del observatorio, permitiendo consultas dinámicas y exploración de resultados sin requerir conocimientos técnicos avanzados. Esta interfaz permite navegación entre métricas clave, filtrado por país y portal, visualización de distribuciones de habilidades, consulta de tendencias temporales, exploración de clusters identificados y descarga de reportes generados. El despliegue se realiza mediante Docker Compose, requiriendo acceso vía navegador web moderno como Chrome, Firefox o Edge, recomendándose resolución mínima de 1920x1080 píxeles y conexión al servidor donde está desplegado el contenedor Docker. El frontend se desarrolló con Next.js conectado al backend FastAPI, que consulta PostgreSQL en tiempo real, todo desplegado de forma containerizada con Docker para garantizar portabilidad y reproducibilidad. La interfaz visual intuitiva está diseñada para usuarios no técnicos como directivos, analistas de políticas públicas, investigadores académicos y stakeholders institucionales, sin requerir conocimientos de programación ni línea de comandos.

\subsubsection{Visualizaciones estáticas y reportes}

Los reportes estáticos permiten la consulta de resultados finales en formato visual o tabular por parte del equipo académico, directivo o institucional. El sistema genera automáticamente archivos en formatos PDF, PNG, HTML y Markdown desde scripts Python especializados. Los requisitos técnicos se limitan a un visualizador de PDF o navegador moderno actualizado, sin requerir conexión a internet tras la generación del archivo. Los reportes están diseñados con títulos descriptivos, leyendas explicativas y estructura clara para facilitar la interpretación sin asistencia técnica.

\section{Funciones del Producto}

El sistema Observatorio de Demanda Laboral en Tecnología en Latinoamérica debe cumplir con una serie de funciones esenciales que permiten cubrir el ciclo completo de análisis automatizado de vacantes laborales. A continuación, se enumeran las principales funciones del producto:

\begin{enumerate}
    \item Extracción automatizada de vacantes
    \begin{itemize}
        \item Scraping periódico y configurable desde portales de empleo en español como Computrabajo, elempleo.com, Bumeran y LinkedIn.
        \item Filtros por país, cargo, modalidad, sector y fecha.
        \item Recolección estructurada de atributos como título del cargo, empresa, ubicación, modalidad, tecnologías mencionadas, y descripción completa.
    \end{itemize}

    \item Preprocesamiento y limpieza textual
    \begin{itemize}
        \item Tokenización, lematización y eliminación de ruido.
        \item Normalización de formatos y campos clave.
        \item Generación de bigramas y trigramas relevantes.
    \end{itemize}

    \item Extracción de habilidades (explícitas e implícitas)
    \begin{itemize}
        \item Detección mediante patrones regulares y NER.
        \item Enriquecimiento con LLMs multilingües para capturar habilidades implícitas y sinónimos contextuales.
        \item Clasificación en taxonomías como ESCO y CIUO.
    \end{itemize}

    \item Vectorización y representación semántica
    \begin{itemize}
        \item Embeddings mediante modelos como BETO, fastText y E5.
        \item Reducción de dimensionalidad con UMAP para visualización y agrupación.
    \end{itemize}

    \item Clustering de perfiles laborales
    \begin{itemize}
        \item Agrupamiento no supervisado mediante HDBSCAN.
        \item Evaluación con métricas de coherencia semántica y Silhouette Score.
        \item Identificación de segmentos funcionales de demanda tecnológica.
    \end{itemize}

    \item Visualización y reporte
    \begin{itemize}
        \item Generación de visualizaciones estáticas y reportes analíticos en CSV, PDF o HTML.
        \item Segmentación por país, portal y categoría tecnológica.
    \end{itemize}

    \item Gestión y trazabilidad
    \begin{itemize}
        \item Registro de logs, validaciones y errores.
        \item Documentación técnica accesible para usuarios académicos y replicadores.
        \item Modularidad para adaptarse a nuevos portales o países.
    \end{itemize}
\end{enumerate}

\section{Características de los Usuarios}

El sistema está diseñado para ser utilizado por distintos tipos de usuarios, cuyas características varían según su nivel de acceso, rol funcional, conocimientos técnicos y frecuencia de interacción. A continuación se describen las principales clases de usuario previstas:

\begin{table}[H]
\centering
\begin{tabular}{|p{4cm}|p{10cm}|}
\hline
\textbf{Característica} & \textbf{Descripción} \\
\hline
Nivel de seguridad o privilegios & Se establecen dos niveles principales de acceso:
\begin{itemize}
    \item Administrador: acceso completo al sistema, incluyendo configuración, ejecución y ajustes internos.
    \item Analista: acceso restringido a resultados, reportes y visualizaciones, sin modificar parámetros o lógica del sistema.
    \item Opcionalmente, se contempla un perfil de validador externo con acceso de solo lectura.
\end{itemize} \\
\hline
Roles & Administrador técnico: configura scraping, define filtros, ajusta modelos, ejecuta el pipeline completo.

Investigador/Analista: accede a resultados, visualiza reportes y comunica hallazgos sin intervenir en el procesamiento.

Validador externo (opcional): revisa calidad de extracción, validación semántica o consistencia de resultados con fines académicos o de control. \\
\hline
Nivel de estudios o experiencia técnica & El administrador debe tener formación en ingeniería, ciencia de datos o afines, con conocimientos sólidos en Python, NLP y manejo de entornos técnicos.

El analista requiere competencias básicas en análisis de datos, interpretación de visualizaciones y lectura de reportes técnicos.

El validador puede ser un docente, evaluador o experto externo sin conocimientos técnicos detallados. \\
\hline
Frecuencia de uso & El administrador accede típicamente una vez por semana o cuando se requiere actualizar scraping, modelos o ejecutar el sistema completo.

El analista accede cada dos semanas o mensualmente para consultar resultados y generar reportes.

El validador accede de forma puntual, en contextos de auditoría, revisión académica o validación de resultados. \\
\hline
\end{tabular}
\caption{Características de los usuarios del sistema}
\end{table}

\section{Restricciones}

El sistema propuesto presenta un conjunto de restricciones que condicionan su diseño, implementación y despliegue. Estas restricciones se clasifican en tres categorías principales: generales, de software y de hardware.

\subsection{Restricciones Generales}

\begin{itemize}
    \item Alcance técnico-académico: El sistema está diseñado con fines de investigación académica y validación técnica, no para uso comercial o despliegue masivo.

    \item Idioma: Todo el procesamiento textual está orientado a ofertas laborales en español, aunque se considera una posible presencia de términos en inglés.

    \item Tolerancia a fallos: Si bien el sistema cuenta con mecanismos de validación de datos y manejo básico de errores, no se implementarán estrategias avanzadas de tolerancia a fallos o disponibilidad continua.

    \item Ejecución modular secuencial: Las fases del sistema se ejecutarán en orden secuencial, sin requerimientos de paralelismo ni concurrencia en su versión inicial.
\end{itemize}

\subsection{Restricciones de Software}

\begin{itemize}
    \item Versión de Python: Python 3.10 o superior es requerido para compatibilidad con todas las librerías utilizadas.

    \item Dependencia de librerías específicas: El sistema depende del uso de bibliotecas especializadas:
    \begin{itemize}
        \item Web Scraping: Scrapy, BeautifulSoup, Selenium
        \item NLP: spaCy (es\_core\_news\_sm), transformers, SentenceTransformers
        \item Embeddings: intfloat/multilingual-e5-base (768D)
        \item Búsqueda vectorial: FAISS (Facebook AI Similarity Search)
        \item Matching: fuzzywuzzy (fuzzy string matching)
        \item Clustering: HDBSCAN, UMAP
        \item Base de datos: psycopg2 (PostgreSQL driver)
        \item Data processing: pandas, numpy
        \item Visualización: matplotlib, plotly, seaborn
    \end{itemize}

    \item Base de datos: PostgreSQL 15 o superior con soporte para arrays de tipo REAL[] para almacenamiento de embeddings de 768 dimensiones.

    \item Modelo de embeddings: El modelo \texttt{intfloat/multilingual-e5-base} debe ser descargado desde Hugging Face (tamaño aproximado: 1.1 GB).

    \item Índice FAISS: El archivo \texttt{esco.faiss} (41.41 MB) y su mapping asociado \texttt{esco\_mapping.pkl} (545 KB) deben estar disponibles en \texttt{data/embeddings/}.

    \item Aceleración GPU (opcional): CUDA/cuDNN para aceleración de generación de embeddings. El sistema es funcional con CPU pero 25x más lento.

    \item Sistema operativo preferido: Linux (recomendado), Windows 11 con WSL2, o macOS Monterey o superior.

    \item Licencias de uso: Se restringe el uso del sistema a entornos académicos bajo licencias open source o de uso investigativo.
\end{itemize}

\subsection{Restricciones de Hardware}

\begin{itemize}
    \item Requisitos mínimos para Fase 0 y 1:
    \begin{itemize}
        \item CPU: 4 núcleos (Intel i5, AMD Ryzen 5, o Apple M1 o superior)
        \item RAM: 8 GB mínimo (16 GB recomendado)
        \item Almacenamiento: 15 GB libres (base de datos, modelos, índices)
        \item Resolución de pantalla: 1280x800 mínimo
    \end{itemize}

    \item Requisitos para generación de embeddings (Fase 0):
    \begin{itemize}
        \item Con GPU: NVIDIA con soporte CUDA 11.0+, 6 GB VRAM mínimo. Velocidad: 721 skills/segundo
        \item Sin GPU (CPU): Intel i5/i7 o equivalente. Velocidad: ~30 skills/segundo (25x más lento)
        \item Apple Silicon (MPS): M1/M2/M3 con aceleración Metal. Velocidad: ~400 skills/segundo
    \end{itemize}

    \item Requisitos para clustering (Módulo 6 - futuro):
    \begin{itemize}
        \item RAM: 16 GB mínimo (32 GB recomendado para 23K jobs)
        \item HDBSCAN con 23,188 jobs requiere ~8 GB RAM disponible
        \item UMAP requiere ~4 GB RAM para reducción de dimensionalidad
    \end{itemize}

    \item Almacenamiento de base de datos:
    \begin{itemize}
        \item PostgreSQL con 23,188 jobs: ~2 GB
        \item Tabla skill\_embeddings (14,133 x 768D): ~400 MB
        \item Índices y tablas auxiliares: ~500 MB
        \item Total estimado: ~3 GB para datos + ~12 GB para modelos
    \end{itemize}

    \item Acceso a internet: Se requiere conexión estable para:
    \begin{itemize}
        \item Descarga inicial de modelos (intfloat/multilingual-e5-base: 1.1 GB)
        \item Instalación de librerías Python (~2 GB total)
        \item Web scraping de portales de empleo (bandwidth: ~10 MB/hora)
    \end{itemize}
\end{itemize}

\section{Supuestos y Dependencias}

Esta sección enumera factores externos y condiciones asumidas que podrían afectar el cumplimiento de los requerimientos definidos en la Sección 3.

\subsection{Suposiciones}

\begin{itemize}
    \item Se mantendrá acceso público y sin restricciones críticas a los portales de empleo definidos como fuente principal.

    \item Las estructuras HTML de dichas páginas no sufrirán cambios drásticos que impidan la funcionalidad de los spiders desarrollados.

    \item Se podrá ejecutar scraping bajo prácticas éticas, sin infringir condiciones explícitas de uso ni requerir autenticación compleja.

    \item Existirá conectividad a internet estable durante las fases de extracción y enriquecimiento de datos.

    \item Los modelos de lenguaje en español seleccionados estarán disponibles públicamente para descarga y uso local.

    \item Los LLMs empleados podrán ejecutarse en entornos locales de cómputo, sin requerir acceso continuo a APIs externas.

    \item Se contará con PostgreSQL funcional y correctamente configurado desde las fases iniciales del desarrollo.

    \item El equipo de desarrollo tendrá acceso constante al repositorio de código y a los entornos colaborativos definidos.

    \item El hardware utilizado por los desarrolladores cumple con los requerimientos mínimos establecidos.
\end{itemize}

\subsection{Dependencias}

\begin{itemize}
    \item Velocidad de conexión a internet: Afecta directamente los tiempos de scraping, descarga de modelos y ejecución de procesos remotos.

    \item Disponibilidad y estabilidad de bibliotecas externas: El sistema depende de librerías que podrían modificar sus versiones o comportamiento.

    \item Funcionamiento adecuado del motor de base de datos: La persistencia de datos depende de una base PostgreSQL operativa.

    \item Funcionamiento de entornos de ejecución local: Algunos modelos requerirán entornos específicos (compatibilidad CUDA, soporte de arquitectura x64).

    \item Factores legales o contractuales externos: Cambios en políticas de los portales web o en los lineamientos éticos institucionales podrían limitar la continuidad del scraping.

    \item Capacidad de procesamiento local: La ejecución de embeddings y clustering depende de la disponibilidad de memoria RAM suficiente y compatibilidad GPU.
\end{itemize}

% ============================================================================
