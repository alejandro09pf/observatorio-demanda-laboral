# Sistema de EvaluaciÃ³n de Pipelines

**Ãšltima actualizaciÃ³n:** 2025-11-05

---

## ğŸ“¦ QUÃ‰ SE IMPLEMENTÃ“

### MÃ³dulo Core: `src/evaluation/`

**1. `normalizer.py` (368 lÃ­neas)**
- NormalizaciÃ³n unificada de skills
- Diccionario canÃ³nico con 200+ tecnologÃ­as
- Maneja variantes: `postgres`â†’`PostgreSQL`, `js`â†’`JavaScript`, `k8s`â†’`Kubernetes`
- Blacklist de tÃ©rminos no-skills
- Singleton pattern para performance

**2. `metrics.py` (260 lÃ­neas)**
- CÃ¡lculo de mÃ©tricas: Precision, Recall, F1-Score, Accuracy
- Confusion Matrix: TP, FP, TN, FN
- Micro/Macro averaging
- Listas detalladas de errores (TP, FP, FN)

**3. `dual_comparator.py` (630 lÃ­neas)**
- **Modo 1:** ComparaciÃ³n vs Gold Standard
  - Texto normalizado (sin bias ESCO)
  - Post-mapeo ESCO (fairness - mismo cÃ³digo)
  - AnÃ¡lisis de impacto ESCO
- **Modo 2:** ComparaciÃ³n head-to-head de LLMs
  - Overlap (Jaccard similarity)
  - EstadÃ­sticas agregadas
- **Modo 3:** AnÃ¡lisis descriptivo sin gold standard
  - Stats bÃ¡sicas + cobertura ESCO

### Script: `scripts/evaluate_pipelines.py`

UN SOLO script que soporta 3 modos de evaluaciÃ³n.

---

## ğŸš€ CÃ“MO USAR

### **MODO 1: Gold Standard** (EvaluaciÃ³n Completa)

Compara pipelines contra el gold standard de 300 jobs anotados.

```bash
# Evaluar Pipeline A + mÃºltiples LLMs (todas las skills)
python scripts/evaluate_pipelines.py \
  --mode gold-standard \
  --pipelines pipeline-a llama-3.2-3b-instruct gemma-3-4b-instruct qwen2.5-3b-instruct

# Solo Pipeline A
python scripts/evaluate_pipelines.py \
  --mode gold-standard \
  --pipelines pipeline-a

# Solo LLMs
python scripts/evaluate_pipelines.py \
  --mode gold-standard \
  --pipelines llama-3.2-3b-instruct gemma-3-4b-instruct

# Evaluar solo hard skills
python scripts/evaluate_pipelines.py \
  --mode gold-standard \
  --pipelines pipeline-a llama-3.2-3b-instruct \
  --skill-type hard

# Evaluar hard y soft por separado (genera 2 reportes)
python scripts/evaluate_pipelines.py \
  --mode gold-standard \
  --pipelines pipeline-a llama-3.2-3b-instruct \
  --skill-type both
```

**Output:**
- `EVALUATION_REPORT_<timestamp>.md` - Reporte ejecutivo
- `comparison_pure_<timestamp>.csv` - MÃ©tricas sin ESCO
- `comparison_esco_<timestamp>.csv` - MÃ©tricas post-ESCO
- `evaluation_<timestamp>.json` - Datos completos

**Incluye:**
1. ComparaciÃ³n 1: Texto normalizado (sin mapeo ESCO)
2. ComparaciÃ³n 2: Post-mapeo ESCO (mismo cÃ³digo para todos)
3. AnÃ¡lisis de impacto ESCO (Î” F1, skills perdidas)
4. Skills emergentes identificadas (no en ESCO)

---

### **MODO 2: LLM Comparison** (Head-to-Head)

Compara mÃºltiples LLMs entre sÃ­ sin gold standard.

```bash
python scripts/evaluate_pipelines.py \
  --mode llm-comparison \
  --llm-models llama-3.2-3b gemma-3-4b qwen2.5-3b mistral-7b phi-3.5
```

**Output:**
- `LLM_COMPARISON_<timestamp>.md` - Reporte de comparaciÃ³n
- `llm_comparison_<timestamp>.json` - Datos completos

**Incluye:**
- Stats por modelo: total skills, unique skills, avg/job
- Overlap entre modelos (Jaccard similarity)
- Skills en comÃºn vs Ãºnicos

---

### **MODO 3: Descriptive** (AnÃ¡lisis sin Gold Standard)

Analiza un pipeline sin comparaciÃ³n.

```bash
# Analizar Pipeline A
python scripts/evaluate_pipelines.py \
  --mode descriptive \
  --pipeline pipeline-a

# Analizar un LLM
python scripts/evaluate_pipelines.py \
  --mode descriptive \
  --pipeline llama-3.2-3b-instruct
```

**Output:**
- `DESCRIPTIVE_<pipeline>_<timestamp>.md` - Reporte descriptivo
- `descriptive_<pipeline>_<timestamp>.json` - Datos

**Incluye:**
- Total jobs, skills extraÃ­dos, unique skills
- Avg skills/job
- Cobertura ESCO (% mapeados)
- Skills no mapeados (sample)

---

## ğŸ“Š ESTRUCTURA DE LOS REPORTES

### Reporte Gold Standard

```markdown
# EvaluaciÃ³n de Pipelines vs Gold Standard

## 1. ExtracciÃ³n Pura (Sin Mapeo ESCO)
| Pipeline           | Precision | Recall | F1-Score |
|--------------------|-----------|--------|----------|
| Pipeline A         | 0.85      | 0.78   | 0.81     |
| Pipeline B (Llama) | 0.88      | 0.82   | 0.85     |

**Ganador:** Pipeline B (F1=0.85)

## 2. Post-Mapeo ESCO
| Pipeline           | Precision | Recall | F1-Score | Cobertura ESCO |
|--------------------|-----------|--------|----------|----------------|
| Pipeline A         | 0.82      | 0.71   | 0.76     | 85%            |
| Pipeline B (Llama) | 0.85      | 0.75   | 0.80     | 89%            |

**Ganador:** Pipeline B (F1=0.80)

## 3. Impacto del Mapeo a ESCO
| Pipeline           | Î” F1     | Î” F1 (%) | Skills Perdidas |
|--------------------|----------|----------|-----------------|
| Pipeline A         | -0.0500  | -6.17%   | 4               |
| Pipeline B (Llama) | -0.0500  | -5.88%   | 3               |

## 4. Skills Emergentes (No en ESCO)
**Total:** 15

- Next.js
- Tailwind CSS
- FastAPI
- ...
```

---

## ğŸ¯ CASOS DE USO

### Caso 1: Evaluar Pipeline A vs mÃºltiples LLMs

**Objetivo:** Determinar si LLMs son mejores que NER+Regex

```bash
python scripts/evaluate_pipelines.py \
  --mode gold-standard \
  --pipelines pipeline-a llama-3.2-3b gemma-3-4b qwen2.5-3b qwen3-4b
```

**Resultado esperado:**
- ComparaciÃ³n P/R/F1 para cada pipeline
- Identificar cuÃ¡l extrae mejor
- Ver impacto de ESCO en cada uno

---

### Caso 2: Comparar LLMs entre sÃ­ (sin gold standard)

**Objetivo:** Ver quÃ© LLMs extraen mÃ¡s/menos skills, overlap

```bash
python scripts/evaluate_pipelines.py \
  --mode llm-comparison \
  --llm-models llama-3.2-3b gemma-3-4b qwen2.5-3b mistral-7b
```

**Resultado esperado:**
- Stats por LLM
- Overlap (Â¿extraen las mismas skills?)
- Identificar outliers

---

### Caso 3: AnÃ¡lisis descriptivo de Pipeline A

**Objetivo:** Ver stats generales sin comparaciÃ³n

```bash
python scripts/evaluate_pipelines.py \
  --mode descriptive \
  --pipeline pipeline-a
```

**Resultado esperado:**
- Total skills, unique, avg/job
- Cobertura ESCO
- Skills emergentes

---

## ğŸ”§ REQUISITOS

### Base de Datos

**Para Modo Gold Standard:**
- Tabla `gold_standard_annotations` con 300 jobs anotados âœ…
  - 7,848 skills totales (6,174 hard + 1,674 soft)
- Tabla `extracted_skills` con resultados de Pipeline A
- Tabla `enhanced_skills` con resultados de Pipeline B (por LLM)

**Para otros modos:**
- Solo `extracted_skills` o `enhanced_skills` segÃºn corresponda

### EjecuciÃ³n de Pipelines

Este sistema **NO ejecuta los pipelines**, solo los evalÃºa.

Debes ejecutar los pipelines usando tu mÃ©todo habitual (orchestrator, scripts, etc.)

---

## ğŸ“ METODOLOGÃA DE EVALUACIÃ“N DETALLADA

### Proceso de EvaluaciÃ³n en 3 Pasos

El sistema implementa un proceso de evaluaciÃ³n dual que permite responder tanto a la pregunta de **capacidad de extracciÃ³n** como de **estandarizaciÃ³n**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PASO 1: COMPARACIÃ“N PRE-ESCO                   â”‚
â”‚          (ValidaciÃ³n de capacidad de extracciÃ³n pura)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
         Gold Standard (texto)  â†â†’  Pipeline (texto)
                              â†“
              NormalizaciÃ³n + Text Matching
                              â†“
         MÃ©tricas: Precision, Recall, F1-Score
         âœ… Captura skills emergentes (Next.js, Tailwind)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PASO 2: MAPEO A ESCO (FAIRNESS)                â”‚
â”‚           (Todos los pipelines con MISMO cÃ³digo)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
         Re-mapear: Gold Standard â†’ ESCO URIs
         Re-mapear: Pipeline â†’ ESCO URIs
                              â†“
              ESCOMatcher3Layers (mismo para todos)
                              â†“
         âš ï¸  Skills emergentes se pierden (no mapean)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 PASO 3: COMPARACIÃ“N POST-ESCO                   â”‚
â”‚             (ValidaciÃ³n de estandarizaciÃ³n)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
         Gold ESCO URIs  â†â†’  Pipeline ESCO URIs
                              â†“
                    URI Matching Exacto
                              â†“
         MÃ©tricas: Precision, Recall, F1-Score
         + Cobertura ESCO (% mapeado)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PASO 4: ANÃLISIS DE IMPACTO                    â”‚
â”‚          (Trade-off: Flexibilidad vs EstandarizaciÃ³n)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
         Î” F1 = F1_post_esco - F1_pre_esco
         Skills Perdidas = Emergentes no mapeadas
         Skills Emergentes (listado completo)
```

---

### Â¿CÃ³mo funciona la validaciÃ³n PRE-ESCO?

**Pregunta comÃºn:** *"Estamos contrastando texto aleatorio con texto aleatorio?"*

**Respuesta:** No. Usamos normalizaciÃ³n inteligente + text matching:

#### 1. NormalizaciÃ³n CanÃ³nica (`normalizer.py`)

Diccionario con **200+ tecnologÃ­as** en forma canÃ³nica:

```python
CANONICAL_FORMS = {
    'python': 'Python',
    'javascript': 'JavaScript',
    'js': 'JavaScript',
    'postgres': 'PostgreSQL',
    'postgresql': 'PostgreSQL',
    'k8s': 'Kubernetes',
    'kubernetes': 'Kubernetes',
    # ... 200+ mappings
}
```

**Proceso:**
1. Gold Standard skill: `"postgres"` â†’ Normalizado: `"PostgreSQL"`
2. Pipeline extrae: `"PostgreSQL"` â†’ Normalizado: `"PostgreSQL"`
3. **Match exacto** âœ…

#### 2. Validez del Approach

**Â¿Es vÃ¡lido comparar texto normalizado sin ESCO?**

âœ… **SÃ, en el contexto de tech jobs:**

**Pros:**
- TecnologÃ­as tienen nombres estÃ¡ndar (Python, React, Docker)
- AmbigÃ¼edad es rara en avisos tech
- Captura skills emergentes que ESCO no tiene
- No sesga por coverage de ESCO

**Contras:**
- SinÃ³nimos raros pueden no matchear (ej: `Machine Learning` vs `ML`)
- SoluciÃ³n: diccionario canÃ³nico cubre casos comunes

**JustificaciÃ³n para tesis:**
- EvalÃºa **capacidad de extracciÃ³n** sin penalizar innovaciÃ³n
- Complementado por validaciÃ³n post-ESCO para estandarizaciÃ³n
- Permite identificar skills emergentes para actualizar ESCO

#### 3. Algoritmo de NormalizaciÃ³n (5 Pasos)

**CÃ³digo:** `normalizer.py` lÃ­neas 265-300

```python
def normalize(skill_text: str) -> str:
    """
    Normaliza un skill a su forma canÃ³nica.

    Pasos:
    1. Limpieza bÃ¡sica
    2. Remover acentos + lowercase (para lookup)
    3. Verificar blacklist
    4. Buscar en diccionario canÃ³nico
    5. Fallback: title case
    """
```

**PASO 1: Limpieza BÃ¡sica**
```python
normalized = skill_text.strip()
# "  python  " â†’ "python"
# "React.js " â†’ "React.js"
```

**PASO 2: Remover Acentos + Lowercase (para lookup)**
```python
normalized_for_lookup = remove_accents(normalized).lower()
# "ComunicaciÃ³n" â†’ "comunicacion"
# "PYTHON" â†’ "python"
# "K8s" â†’ "k8s"
```

**PASO 3: Verificar Blacklist** (descarta palabras que NO son skills)
```python
if normalized_for_lookup in BLACKLIST:
    return ""  # Descartada

# Blacklist = {"experiencia", "aÃ±os", "aÃ±o", "year", "meses", ...}
# "3 aÃ±os de experiencia" â†’ "" (descartada) âŒ
# "experiencia en Python" â†’ "" (descartada) âŒ
```

**PASO 4: Buscar en Diccionario CanÃ³nico** (193 mappings)
```python
if normalized_for_lookup in CANONICAL_NAMES:
    return CANONICAL_NAMES[normalized_for_lookup]

# Ejemplos:
# "python" â†’ "Python" âœ…
# "js" â†’ "JavaScript" âœ…
# "k8s" â†’ "Kubernetes" âœ…
# "postgres" â†’ "PostgreSQL" âœ…
# "react.js" â†’ "React" âœ…
# "nodejs" â†’ "Node.js" âœ…
# "liderazgo" â†’ "Liderazgo" âœ…
# "leadership" â†’ "Liderazgo" âœ… (mapea a espaÃ±ol)
```

**PASO 5: Fallback - Title Case** (si no estÃ¡ en diccionario)
```python
return to_title_case(clean_text(normalized))

# "machine learning" â†’ "Machine Learning"
# "data analysis" â†’ "Data Analysis"
# "resoluciÃ³n de problemas" â†’ "ResoluciÃ³n De Problemas"
```

**âš ï¸ LimitaciÃ³n del Fallback:**

Si una variante NO estÃ¡ en el diccionario canÃ³nico, puede NO matchear:

```python
# Ejemplo problemÃ¡tico:
Gold: "postgre sql" â†’ NO en diccionario â†’ Fallback: "Postgre Sql"
Pipeline: "postgres" â†’ En diccionario â†’ "PostgreSQL"

# Resultado: "Postgre Sql" â‰  "PostgreSQL" âŒ NO MATCH

# SoluciÃ³n: Agregar variante al diccionario
CANONICAL_NAMES = {
    'postgres': 'PostgreSQL',
    'postgresql': 'PostgreSQL',
    'postgre sql': 'PostgreSQL',  # â† Agregar
    'postgre': 'PostgreSQL',       # â† Agregar
}
```

---

### CÃ¡lculo de MÃ©tricas (Operaciones de Conjuntos)

**CÃ³digo:** `metrics.py` lÃ­neas 94-174

Una vez normalizadas las skills, se calculan las mÃ©tricas usando **operaciones de conjuntos**:

#### Paso 1: Operaciones de Conjuntos

```python
# Entrada:
gold_standard_skills = {"Python", "React", "PostgreSQL", "Docker"}
predicted_skills = {"Python", "React", "SQL", "Kubernetes"}

# OperaciÃ³n 1: IntersecciÃ³n (âˆ©)
TP = gold_standard_skills & predicted_skills
   = {"Python", "React"}  # Skills correctamente identificadas

# OperaciÃ³n 2: Diferencia (-)
FP = predicted_skills - gold_standard_skills
   = {"SQL", "Kubernetes"}  # Skills extraÃ­das pero NO en gold

FN = gold_standard_skills - predicted_skills
   = {"PostgreSQL", "Docker"}  # Skills del gold NO extraÃ­das

# Counts
tp = len(TP) = 2
fp = len(FP) = 2
fn = len(FN) = 2
```

#### Paso 2: FÃ³rmulas de MÃ©tricas

```python
# Precision: Â¿QuÃ© % de lo predicho es correcto?
Precision = TP / (TP + FP)
          = 2 / (2 + 2)
          = 2 / 4
          = 0.5000 (50%)

# Recall: Â¿QuÃ© % del gold se detectÃ³?
Recall = TP / (TP + FN)
       = 2 / (2 + 2)
       = 2 / 4
       = 0.5000 (50%)

# F1-Score: Media armÃ³nica de Precision y Recall
F1 = 2 * (Precision * Recall) / (Precision + Recall)
   = 2 * (0.5 * 0.5) / (0.5 + 0.5)
   = 2 * 0.25 / 1.0
   = 0.5000 (50%)
```

#### Paso 3: InterpretaciÃ³n

```python
Support = len(gold_standard_skills) = 4  # Total en gold
Predicted = len(predicted_skills) = 4     # Total predicho

# AnÃ¡lisis:
# - Precision 50% â†’ De 4 skills extraÃ­das, solo 2 son correctas
# - Recall 50% â†’ De 4 skills del gold, solo se detectaron 2
# - F1 50% â†’ Balance entre Precision y Recall
```

---

### Proceso Completo: De BD a MÃ©tricas

**Flujo end-to-end de evaluaciÃ³n PRE-ESCO:**

```
PASO 1: CARGA DESDE BASE DE DATOS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Gold Standard:
  SELECT job_id, skill_text FROM gold_standard_annotations
  â†’ {job_1: ["python", "REACT", "postgres"]}

Pipeline A:
  SELECT job_id, skill_text FROM extracted_skills
  WHERE extraction_method IN ('ner', 'regex')
  â†’ {job_1: ["Python", "react.js", "SQL"]}


PASO 2: NORMALIZACIÃ“N POR JOB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for job_id in common_jobs:
    # Gold
    raw_gold = ["python", "REACT", "postgres"]
    normalized_gold = normalize_list(raw_gold)
                    = ["Python", "React", "PostgreSQL"]

    # Pipeline
    raw_pipeline = ["Python", "react.js", "SQL"]
    normalized_pipeline = normalize_list(raw_pipeline)
                        = ["Python", "React", "SQL"]


PASO 3: AGREGACIÃ“N (todos los jobs)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
all_gold = set().union(*[normalized_gold_by_job[j] for j in jobs])
         = {"Python", "React", "PostgreSQL", ...}  # De todos los jobs

all_pipeline = set().union(*[normalized_pipeline_by_job[j] for j in jobs])
             = {"Python", "React", "SQL", ...}  # De todos los jobs


PASO 4: COMPARACIÃ“N (set operations)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TP = all_gold & all_pipeline        # IntersecciÃ³n
FP = all_pipeline - all_gold        # Solo en pipeline
FN = all_gold - all_pipeline        # Solo en gold


PASO 5: CÃLCULO DE MÃ‰TRICAS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Precision = TP / (TP + FP)
Recall = TP / (TP + FN)
F1 = 2 * (P * R) / (P + R)
```

**Ejemplo con 2 jobs:**

```
Job 1:
  Gold: ["Python", "React", "PostgreSQL"]
  Pipeline: ["Python", "React", "SQL"]

Job 2:
  Gold: ["JavaScript", "Docker", "Git"]
  Pipeline: ["JavaScript", "Docker", "Kubernetes"]

Agregado:
  all_gold = {"Python", "React", "PostgreSQL", "JavaScript", "Docker", "Git"}
  all_pipeline = {"Python", "React", "SQL", "JavaScript", "Docker", "Kubernetes"}

ComparaciÃ³n:
  TP = {"Python", "React", "JavaScript", "Docker"}  â† 4 correctos
  FP = {"SQL", "Kubernetes"}  â† 2 falsos positivos
  FN = {"PostgreSQL", "Git"}  â† 2 falsos negativos

MÃ©tricas:
  Precision = 4 / (4 + 2) = 4/6 = 0.6667 (66.67%)
  Recall = 4 / (4 + 2) = 4/6 = 0.6667 (66.67%)
  F1 = 0.6667 (66.67%)
```

---

### Â¿CÃ³mo se manejan las Skills Emergentes?

**Skills emergentes** = Skills NO en taxonomÃ­a ESCO (14,174 skills)

Ejemplos: `Next.js`, `Tailwind CSS`, `FastAPI`, `Svelte`, `Vite`

#### Flujo Completo:

**En PRE-ESCO:**
```
Gold Standard: ["Python", "Next.js", "PostgreSQL"]
Pipeline extrae: ["Python", "Next.js", "React"]

â†’ NormalizaciÃ³n:
  Gold: {"Python", "Next.js", "PostgreSQL"}
  Pipeline: {"Python", "Next.js", "React"}

â†’ Matching:
  TP: {"Python", "Next.js"}  â† Next.js cuenta como TP âœ…
  FN: {"PostgreSQL"}
  FP: {"React"}

â†’ Precision = 2/3 = 0.67
â†’ Recall = 2/3 = 0.67
```

**En MAPEO A ESCO:**
```
Gold: ["Python", "Next.js", "PostgreSQL"]
â†’ ESCO Mapper:
  "Python" â†’ http://data.europa.eu/esco/skill/abc123
  "Next.js" â†’ None (no existe en ESCO) âŒ
  "PostgreSQL" â†’ http://data.europa.eu/esco/skill/def456

Gold ESCO: {abc123, def456}  â† Next.js desaparece


Pipeline: ["Python", "Next.js", "React"]
â†’ ESCO Mapper:
  "Python" â†’ http://data.europa.eu/esco/skill/abc123
  "Next.js" â†’ None âŒ
  "React" â†’ http://data.europa.eu/esco/skill/ghi789

Pipeline ESCO: {abc123, ghi789}  â† Next.js desaparece
```

**En POST-ESCO:**
```
Gold ESCO: {abc123, def456}
Pipeline ESCO: {abc123, ghi789}

â†’ TP: {abc123} (Python)
â†’ FN: {def456} (PostgreSQL)
â†’ FP: {ghi789} (React)

â†’ Precision = 1/2 = 0.50
â†’ Recall = 1/2 = 0.50

âš ï¸ Next.js ya no afecta las mÃ©tricas (eliminado de ambos)
```

**En ANÃLISIS DE IMPACTO:**
```
Î” F1 = 0.50 - 0.67 = -0.17 (-25%)
Skills Perdidas = 1 (Next.js)
Skills Emergentes: ["Next.js"]
```

#### Reporte Final

```markdown
## 3. Impacto del Mapeo a ESCO
| Pipeline | Î” F1    | Skills Perdidas |
|----------|---------|-----------------|
| Pipeline | -0.17   | 1               |

## 4. Skills Emergentes (No en ESCO)
**Total:** 1

- Next.js
```

**InterpretaciÃ³n:**
- Pre-ESCO: F1=0.67 â†’ Pipeline extrae bien, incluyendo skills modernas
- Post-ESCO: F1=0.50 â†’ EstandarizaciÃ³n tiene costo (pierde Next.js)
- Trade-off: Flexibilidad vs EstandarizaciÃ³n explÃ­cito

---

### EvaluaciÃ³n por Tipo de Skill (Hard vs Soft)

El sistema soporta evaluar **hard skills** y **soft skills** por separado:

#### Uso:

```bash
# Evaluar todas las skills juntas (default)
python scripts/evaluate_pipelines.py \
  --mode gold-standard \
  --pipelines pipeline-a llama-3.2-3b-instruct \
  --skill-type all

# Evaluar solo hard skills
python scripts/evaluate_pipelines.py \
  --mode gold-standard \
  --pipelines pipeline-a llama-3.2-3b-instruct \
  --skill-type hard

# Evaluar solo soft skills
python scripts/evaluate_pipelines.py \
  --mode gold-standard \
  --pipelines pipeline-a llama-3.2-3b-instruct \
  --skill-type soft

# Evaluar ambas por separado (genera 2 reportes)
python scripts/evaluate_pipelines.py \
  --mode gold-standard \
  --pipelines pipeline-a llama-3.2-3b-instruct \
  --skill-type both
```

#### Output con `--skill-type both`:

Genera **2 reportes separados**:
- `EVALUATION_REPORT_hard_<timestamp>.md` - Solo hard skills
- `EVALUATION_REPORT_soft_<timestamp>.md` - Solo soft skills

Cada reporte incluye:
- ComparaciÃ³n Pre-ESCO (solo skills del tipo filtrado)
- ComparaciÃ³n Post-ESCO (solo skills del tipo filtrado)
- Impacto ESCO
- Skills emergentes del tipo

**Ejemplo:**

```markdown
# EvaluaciÃ³n de Pipelines vs Gold Standard (hard)

**Jobs evaluados:** 300
**Skill type:** hard
**Skills en gold standard:** 6,174

## 1. ExtracciÃ³n Pura (Sin Mapeo ESCO)
| Pipeline | Precision | Recall | F1-Score |
|----------|-----------|--------|----------|
| Pipeline A | 0.85 | 0.78 | 0.81 |
...
```

#### ImplementaciÃ³n:

El sistema filtra skills por tipo **antes** de la comparaciÃ³n:

```python
# En dual_comparator.py
filtered_gold = gold_standard.filter_by_type('hard')
filtered_pipeline = pipeline.filter_by_type('hard')

# Solo skills hard participan en mÃ©tricas
```

**Ventaja:** Permite analizar si los pipelines extraen mejor hard o soft skills.

---

## ğŸ’¡ DECISIONES DE DISEÃ‘O

### 1. Doble ComparaciÃ³n (Texto + ESCO)

**ComparaciÃ³n 1: Texto Normalizado**
- EvalÃºa capacidad de extracciÃ³n pura
- No penaliza skills emergentes (Next.js, Tailwind, etc.)
- Fairness: no sesga por ESCO

**ComparaciÃ³n 2: Post-Mapeo ESCO**
- Re-mapea todos los pipelines con el MISMO cÃ³digo
- Pipeline A se re-mapea (no usa su mapeo existente)
- EvalÃºa estandarizaciÃ³n
- Identifica skills no mapeables

**Â¿Por quÃ© ambas?**
- Responde 2 preguntas distintas:
  1. Â¿QuÃ© extrae mejor? (sin ESCO)
  2. Â¿QuÃ© estandariza mejor? (con ESCO)

### 2. NormalizaciÃ³n Unificada

**Problema:** Skills pueden tener variaciones (Python vs python vs PYTHON)

**SoluciÃ³n:** Diccionario canÃ³nico + reglas de normalizaciÃ³n

**AplicaciÃ³n:** Todos los pipelines se normalizan igual antes de comparar

### 3. Flexibilidad de Modos

**Problema:** No siempre tienes gold standard disponible

**SoluciÃ³n:** 3 modos independientes segÃºn quÃ© tengas disponible

---

## ğŸ“ ARCHIVOS DEL SISTEMA

```
src/evaluation/
â”œâ”€â”€ __init__.py              # Exports
â”œâ”€â”€ normalizer.py            # NormalizaciÃ³n unificada (368 lÃ­neas)
â”œâ”€â”€ metrics.py               # CÃ¡lculo de mÃ©tricas (260 lÃ­neas)
â””â”€â”€ dual_comparator.py       # Comparador multi-modo (630 lÃ­neas)

scripts/
â””â”€â”€ evaluate_pipelines.py    # Script Ãºnico de evaluaciÃ³n (400 lÃ­neas)

docs/
â”œâ”€â”€ EVALUATION_SYSTEM.md     # Esta documentaciÃ³n
â””â”€â”€ EVALUATION_IMPLEMENTATION_LOG.md  # Log detallado
```

**Total:** ~1,658 lÃ­neas de cÃ³digo + documentaciÃ³n

---

## âœ… CHECKLIST DE USO

### Antes de Evaluar

- [ ] Tienes gold standard en `gold_standard_annotations` (solo modo 1)
- [ ] Ejecutaste Pipeline A en los jobs que quieres evaluar
- [ ] Ejecutaste Pipeline B (LLMs) en los jobs que quieres evaluar
- [ ] Los jobs estÃ¡n en `extracted_skills` y/o `enhanced_skills`

### Ejecutar EvaluaciÃ³n

- [ ] Elegiste el modo apropiado (gold-standard, llm-comparison, descriptive)
- [ ] Especificaste los pipelines/LLMs correctos
- [ ] Verificaste el output directory

### DespuÃ©s de Evaluar

- [ ] Revisaste el reporte Markdown
- [ ] Verificaste las mÃ©tricas hacen sentido
- [ ] Exportaste CSV si necesitas procesar mÃ¡s

---

## â“ FAQ

**P: Â¿CÃ³mo ejecuto los pipelines?**
R: Este sistema NO ejecuta pipelines, solo los evalÃºa. Usa tu mÃ©todo habitual (orchestrator, scripts propios, etc.)

**P: Â¿Puedo evaluar sin gold standard?**
R: SÃ­, usa modo `llm-comparison` o `descriptive`

**P: Â¿QuÃ© es "Pipeline A"?**
R: Pipeline A = NER + Regex + ESCO. Sus resultados estÃ¡n en tabla `extracted_skills`

**P: Â¿CÃ³mo especifico un LLM?**
R: Usa el nombre del modelo como aparece en `llm_model` column de `enhanced_skills`

**P: Â¿El mapeo a ESCO es justo?**
R: SÃ­, todos los pipelines se re-mapean con el MISMO cÃ³digo (`ESCOMatcher3Layers`)

**P: Â¿Puedo comparar Pipeline A vs Pipeline A?**
R: No tiene sentido, pero tÃ©cnicamente funciona

**P: Â¿Los reportes se sobreescriben?**
R: No, cada ejecuciÃ³n crea archivos con timestamp Ãºnico

---

## ğŸ¯ PRÃ“XIMOS PASOS

Este sistema estÃ¡ **completo y listo para usar**.

Opcionales (si necesitas):
- Visualizaciones (grÃ¡ficos matplotlib/seaborn)
- AnÃ¡lisis por contexto (portal, paÃ­s, tipo de skill)
- Export a otros formatos
- IntegraciÃ³n con notebooks Jupyter

---

**Para cualquier duda, revisa el cÃ³digo - estÃ¡ documentado.**
