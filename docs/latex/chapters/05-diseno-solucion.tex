\chapter{DISEÑO DE LA SOLUCIÓN}

\section{Antecedentes Teóricos}

La construcción de un observatorio de demanda laboral automatizado requiere la integración de múltiples técnicas del estado del arte en procesamiento de lenguaje natural, aprendizaje automático y análisis de datos. Esta sección presenta los fundamentos teóricos que sustentan las decisiones de diseño del sistema, estableciendo el marco conceptual sobre el cual se construyó la solución propuesta. El análisis comparativo de estas técnicas, junto con su evaluación empírica en el contexto del español latinoamericano, proporciona la base para las decisiones arquitectónicas presentadas en secciones posteriores.

\subsection{Reconocimiento de Entidades Nombradas (NER)}

El Reconocimiento de Entidades Nombradas es una tarea fundamental de NLP que consiste en localizar y clasificar entidades en texto dentro de categorías predefinidas \cite{nadeau2007}. Los sistemas NER modernos se basan en modelos de aprendizaje supervisado, particularmente arquitecturas basadas en transformers \cite{devlin2019}. Para el dominio de habilidades técnicas, NER presenta ventajas teóricas significativas: alta precisión para entidades conocidas en datasets balanceados ($>$90\%), contextualización bidireccional para desambiguación, y eficiencia computacional con latencias de milisegundos por documento \cite{zhang2018}. Sin embargo, enfrenta limitaciones críticas: dependencia del vocabulario de entrenamiento, baja cobertura en dominios especializados, y la incapacidad de inferir habilidades implícitas no mencionadas explícitamente en el texto \cite{canete2020}.

En este proyecto se adoptó un enfoque híbrido que combina el modelo base \texttt{es\_core\_news\_lg} de spaCy con un EntityRuler personalizado poblado con 666 patrones de habilidades técnicas de la taxonomía ESCO, permitiendo reconocimiento directo de terminología técnica no presente en el modelo pre-entrenado \cite{decorte2023}. Los resultados experimentales revelaron que NER en este contexto específico presentó desempeño mixto: si bien aporta cobertura adicional de skills contextuales (mejora de +6.91pp F1 Pre-ESCO), introduce ruido que degrada precisión Post-ESCO (-6.64pp versus configuración Regex-Only). A pesar de este trade-off, NER se mantuvo en Pipeline A dado que incrementa recall detectando menciones contextuales que patrones deterministas omiten, como se documenta en la sección de pruebas de modelos.

\subsection{Extracción basada en Expresiones Regulares}

Las expresiones regulares (regex) son patrones de búsqueda basados en lenguajes formales que permiten identificar secuencias de caracteres con estructuras específicas \cite{friedl2006}. En el contexto de extracción de habilidades técnicas, regex fue particularmente efectiva para tecnologías con nomenclaturas estandarizadas: versiones numeradas (``Python 3.x'', ``Java 8+''), frameworks con sufijos convencionales (``.js'', ``SQL''), y acrónimos técnicos (``REST API'', ``CI/CD''). La implementación final consistió en 548 patrones organizados en 18 categorías técnicas (lenguajes de programación, frameworks, bases de datos, plataformas cloud, DevOps, ciencia de datos, entre otras). Sus ventajas incluyeron precisión determinística del 100\% en patrones bien definidos, transparencia y auditabilidad, velocidad extrema (microsegundos por documento), y ausencia de requisitos de entrenamiento. Las limitaciones principales fueron fragilidad ante variaciones ortográficas, mantenimiento manual intensivo de patrones, y ausencia de comprensión contextual \cite{chiticariu2013}.

\subsection{Extracción basada en Modelos de Lenguaje Grandes (LLMs)}

Los Large Language Models representaron un cambio de paradigma en NLP, basándose en arquitecturas transformer pre-entrenadas sobre corpus masivos mediante objetivos de modelado de lenguaje \cite{brown2020, touvron2023}. A diferencia de NER y regex, los LLMs poseen capacidades de razonamiento contextual que permiten: inferencia de habilidades implícitas (deducir que un ``Científico de Datos'' requiere estadística y Python aunque no se mencione), normalización semántica automática (identificar que ``React'', ``React.js'' y ``ReactJS'' son equivalentes), desambiguación contextual, adaptación al lenguaje informal y ``Spanglish'', y razonamiento explicable mediante prompt engineering \cite{zhang2022, wei2023, vilares2016}.

Sin embargo, presentaron limitaciones significativas: latencia 15-25x mayor que Pipeline A (tiempo típico de procesamiento de 15-25s por documento vs. 1s), no-determinismo con temperatura $>$ 0, alucinaciones que generaron habilidades incorrectas, alto costo computacional (GPUs 4-6GB o APIs de pago), y sesgo lingüístico hacia el inglés con rendimiento degradado en español técnico \cite{elazar2023, ji2023, banon2020}.

\subsection{Justificación del Enfoque Dual}

La Tabla \ref{tab:comparacion-tecnicas-extraccion} presenta una comparación sistemática de las tres técnicas según criterios relevantes para el observatorio.

\begin{table}[H]
\centering
\caption{Comparación de Técnicas de Extracción de Habilidades}
\label{tab:comparacion-tecnicas-extraccion}
\begin{tabular}{|p{3cm}|p{3.5cm}|p{3.5cm}|p{3.5cm}|}
\hline
\textbf{Criterio} & \textbf{Pipeline A (NER+Regex)} & \textbf{Regex Solo} & \textbf{LLMs (Gemma 3 4B)} \\
\hline
Precision Post-ESCO & 65.50\% & 86.36\% & \textbf{89.25\%} \\
\hline
Recall Post-ESCO & \textbf{81.25\%} & 73.08\% & 79.81\% \\
\hline
F1-Score Pre-ESCO & 24.98\% & 18.07\% & \textbf{46.23\%} \\
\hline
F1-Score Post-ESCO & 72.53\% & 79.17\% & \textbf{84.26\%} \\
\hline
Inferencia implícita & No soportado & No soportado & Sí (capacidad arquitectónica) \\
\hline
Latencia & 0.97 s/job & $\sim$0.32 s/job & 18s (P50), 15-25s (P25-P75) \\
\hline
Costo computacional & Bajo (CPU) & Muy bajo (CPU) & Alto (GPU 4-6 GB) \\
\hline
Mantenimiento & Alto (filtros) & Alto (patrones) & Bajo (prompt eng.) \\
\hline
Español LATAM & Medio & Alto & Alto \\
\hline
\end{tabular}
\end{table}

\textit{Nota: Los valores presentados en la Tabla \ref{tab:comparacion-tecnicas-extraccion} fueron obtenidos mediante evaluación experimental sobre el conjunto de datos gold standard de 300 ofertas laborales (100 por país: CO, MX, AR). El F1-Score Post-ESCO refleja el rendimiento después de normalización mediante el mismo código de mapeo ESCO para comparación justa. Ambos pipelines operan de forma independiente sobre el mismo texto de entrada.}

Dado que ninguna técnica individual satisface todos los requisitos, se implementaron dos pipelines en paralelo para comparación empírica: Pipeline A (NER + Regex) diseñado para procesamiento escalable del corpus completo mediante reglas determinísticas; y Pipeline B (LLMs) implementado experimentalmente sobre gold standard para evaluar enriquecimiento semántico y detección de habilidades implícitas. Ambos pipelines procesan las mismas ofertas de forma independiente, permitiendo comparación directa de sus capacidades. La evaluación experimental determinó que Pipeline B supera cuantitativamente a Pipeline A pero con mayor costo computacional, validando la necesidad de arquitectura dual que permita seleccionar el pipeline óptimo según el caso de uso: Pipeline A para análisis masivo y Pipeline B para validación cualitativa o detección de skills emergentes (evaluación comparativa detallada en el capítulo de Resultados) \cite{li2023}.

\subsection{Modelos de Lenguaje Grandes: Selección y Evaluación}

Habiendo establecido que el Pipeline B requirió capacidades de LLMs para inferencia de habilidades implícitas y normalización semántica, el siguiente desafío consistió en seleccionar un modelo específico que balanceara rendimiento lingüístico con restricciones computacionales del proyecto.

Se identificaron cuatro modelos LLM de código abierto como candidatos principales, evaluados según criterios de costo computacional, rendimiento en español latinoamericano, soporte multilingüe y capacidad de despliegue local sin dependencias de APIs comerciales.

\textbf{Candidatos Evaluados:}

Gemma 3 4B es un modelo ligero desarrollado por Google basado en la arquitectura Gemini. Con 4 mil millones de parámetros, está optimizado para despliegue eficiente en hardware limitado. Sus ventajas incluyen: tamaño compacto que permite ejecución en CPU o GPUs de gama media (4-6 GB VRAM con cuantización Q4), licencia permisiva Gemma para uso académico y comercial, tokenización eficiente heredada de la familia Gemini, y soporte multilingüe con cobertura de español latinoamericano. Las limitaciones principales son: menor capacidad de razonamiento complejo comparado con modelos más grandes, vocabulario técnico potencialmente limitado debido al tamaño reducido, y documentación aún en desarrollo al ser un modelo relativamente nuevo.

Llama 3 3B (Meta AI) es la versión compacta de la familia LLaMA 3, entrenado sobre un corpus masivo multilingüe con enfoque en eficiencia. Con 3 mil millones de parámetros, representa el balance extremo entre rendimiento y recursos computacionales. Ventajas: requisitos mínimos de memoria (3-4 GB VRAM con cuantización Q4), arquitectura optimizada con Grouped-Query Attention para inferencia rápida, tokenización eficiente de la familia LLaMA 3, licencia LLaMA 3 Community License permisiva para proyectos académicos, y latencia reducida ideal para procesamiento batch. Limitaciones: capacidad de razonamiento más limitada que modelos grandes, posible degradación en tareas complejas de inferencia, y menor cobertura de vocabulario técnico especializado.

Qwen 2.5 3B (Alibaba Cloud) es parte de la familia Qwen (Qianwen) optimizada para multilingüismo con énfasis en idiomas asiáticos y europeos. Con 3 mil millones de parámetros, destaca por su arquitectura de atención eficiente. Ventajas: precisión superior en tareas de extracción estructurada, tokenización eficiente multilingüe, requisitos moderados de memoria (3-4 GB VRAM con Q4), y licencia Apache 2.0 permisiva. Limitaciones: conservadurismo excesivo con recall bajo en contextos ambiguos, menor cobertura de vocabulario técnico latinoamericano, y tendencia a omitir skills implícitas.

La selección de estos cuatro modelos específicos se fundamentó en criterios técnicos y de disponibilidad de recursos. En primer lugar, se priorizaron modelos open-source dada la naturaleza académica del proyecto y la necesidad de reproducibilidad científica. En segundo lugar, se identificó a Llama 3 como referente base según los estudios del estado del arte analizados en el Capítulo 3, donde Herandi et al. (2024) demostraron la efectividad de LLMs fine-tuned para extracción de habilidades alcanzando F1-Score de 64.8\% sobre SkillSpan dataset \cite{herandi2024}, mientras que Kavas et al. (2024) validaron específicamente Llama-3 8B para contextos multilingües en Europa \cite{kavas2024}. Nguyen et al. (2024) documentaron que el prompting con LLMs, aunque flexible, requiere prompt engineering cuidadoso para tareas de extracción estructurada \cite{nguyen2024}. En tercer lugar, el rango de parámetros 3-4B se determinó mediante restricción de hardware disponible: un MacBook Air M4 base con 16 GB de memoria unificada permite ejecutar modelos de hasta 4.3B parámetros con cuantización Q4\_K\_M (4 bits por peso), requiriendo aproximadamente 2.4-2.8 GB de VRAM según la fórmula:

\begin{equation}
\text{VRAM} \approx \frac{\text{Parámetros} \times 4 \text{ bits}}{8 \text{ bits/byte}} + \text{overhead KV-cache}
\end{equation}

donde el overhead del KV-cache representa típicamente 15-20\% adicional. Modelos de 7B+ parámetros excederían los 5 GB de VRAM con cuantización Q4, comprometiendo estabilidad del sistema operativo durante inferencia. Adicionalmente, se consultaron rankings especializados: Hugging Face Open LLM Leaderboard (filtrado por modelos con evaluaciones en español) y LMArena (comparativa de modelos open-source en español) para identificar candidatos con desempeño reportado en tareas de NLP multilingüe.


Phi-3.5 Mini (Microsoft Research) es un modelo ultra-compacto de 3.8 mil millones de parámetros entrenado con datos sintéticos de alta calidad. Ventajas: arquitectura extremadamente eficiente con baja latencia, capacidad de razonamiento avanzado para su tamaño, soporte multilingüe, y licencia MIT. Limitaciones: inconsistencias en generación de JSON estructurado causando pérdidas durante parsing, menor robustez en instrucciones complejas comparado con modelos más grandes, y vocabulario técnico limitado en español.

La Tabla \ref{tab:comparacion-llms} resume la comparación cuantitativa entre los cuatro modelos evaluados.

\begin{table}[H]
\centering
\caption{Comparación de Large Language Models para Extracción de Habilidades}
\label{tab:comparacion-llms}
\begin{tabular}{|p{3.5cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
\hline
\textbf{Criterio} & \textbf{Gemma 3 4B} & \textbf{Llama 3.2 3B} & \textbf{Qwen 2.5 3B} & \textbf{Phi-3.5 Mini} \\
\hline
Parámetros & 4B & 3B & 3B & 3.8B \\
\hline
F1 Pre-ESCO & \textbf{46.23\%} & 39.7\% & 38.9\% & 35.2\% \\
\hline
Jobs evaluados & 299 & 10 & 10 & 10 \\
\hline
Skills/job prom. & 27.8 & 24.7 & 11.2 & 15.8 \\
\hline
Alucinaciones & \textbf{0} & 7 (DS) & 0 & 0 \\
\hline
JSON válido & 99\% & 90\% & 95\% & 60\% \\
\hline
Latencia (GPU) & 18s (P50) & 15.24s & N/D & N/D \\
\hline
VRAM (Q4) & 4-6 GB & 3-4 GB & 3-4 GB & 3-4 GB \\
\hline
Licencia & Gemma & LLaMA 3 CL & Apache 2.0 & MIT \\
\hline
\end{tabular}
\end{table}

\textit{Nota: Los cuatro modelos fueron evaluados comparativamente sobre el gold standard de 300 ofertas laborales anotadas manualmente (100 por país: CO, MX, AR). Gemma 3 4B procesó 299/300 ofertas completas (99.3\% cobertura; 2 jobs con mode collapse); Llama, Qwen y Phi procesaron 10 ofertas cada uno para comparación preliminar, siendo descartados por menor rendimiento y problemas técnicos. F1 Pre-ESCO mide rendimiento antes de normalización taxonómica. Alucinaciones (DS) indica skills de Data Science extraídas erróneamente. JSON válido mide porcentaje de respuestas con formato estructurado correcto.}

\textbf{Resultados de la Evaluación Comparativa:}

Se evaluaron los cuatro modelos candidatos (Gemma 3 4B, Llama 3.2 3B, Qwen 2.5 3B, Phi-3.5 Mini) mediante un experimento comparativo inicial sobre 10 ofertas laborales del gold standard. La evaluación midió Precision, Recall, F1-score para habilidades explícitas e implícitas, latencia promedio, throughput y presencia de alucinaciones. Basándose en estos resultados preliminares, Gemma 3 4B fue seleccionado como ganador y posteriormente procesó el conjunto completo de 300 ofertas laborales anotadas manualmente (100 por país: CO, MX, AR) para validación exhaustiva.

El gold standard se construyó con distribución balanceada por país y representación diversa de roles técnicos: Backend (33.33\%), QA (16.33\%), Frontend (14.00\%), DevOps (11.67\%), Data Science (9.00\%), y otros roles especializados (15.67\%). El protocolo de anotación contempló la revisión de cada oferta identificando tres aspectos principales: primero, habilidades técnicas explícitas (hard skills) mencionadas directamente en el texto, totalizando 6,174 anotaciones de 1,914 skills únicas; segundo, habilidades blandas (soft skills) inferibles del contexto del cargo, totalizando 1,674 anotaciones de 306 skills únicas; y tercero, mapeo manual a conceptos ESCO cuando aplicable. El gold standard resultante contiene 7,848 anotaciones totales con promedio de 26.16 skills por oferta (20.58 hard + 5.58 soft), distribución de seniority Senior (54\%), Mid (41\%), Junior (5\%), y cobertura idiomática de Español (83.33\%) e Inglés (16.67\%).

Los modelos fueron evaluados con prompt engineering específico para español latinoamericano, incluyendo instrucciones para manejar ``Spanglish'', ejemplos contextuales con ofertas reales, normalización con taxonomía ESCO, y solicitud de formato JSON estructurado. Los parámetros de inferencia utilizados fueron: temperatura 0.3 (balance entre determinismo y creatividad), max\_tokens 3072, y system prompt estandarizado.

Los resultados experimentales sobre las 300 ofertas determinaron la selección definitiva de Gemma 3 4B para el Pipeline B, fundamentado en cinco hallazgos principales: precisión superior, ausencia de alucinaciones sistemáticas detectadas en otros modelos, cobertura efectiva de habilidades implícitas, captura de tecnologías emergentes no presentes en ESCO, y robustez operacional con tasa de éxito superior al 99\%. La validación manual confirmó que Gemma genera outputs limpios sin fabricar skills, mientras que alternativas como Llama 3.2 presentaron alucinaciones sistemáticas en ofertas técnicas (métricas cuantitativas detalladas en el capítulo de Resultados).

El trade-off de latencia fue considerado aceptable para procesamiento batch, donde mayor tiempo de inferencia se compensa con eliminación de alucinaciones y mayor calidad semántica. La arquitectura final despliega Gemma 3 4B con cuantización Q4 (4-6 GB VRAM), temperatura 0.3, y max\_tokens 3072, ejecutándose localmente sin dependencias de APIs comerciales y garantizando privacidad de datos laborales sensibles.

\subsection{Embeddings Semánticos y Búsqueda de Similitud}

Habiendo establecido las técnicas de extracción de habilidades (NER, Regex, LLMs) y los modelos para procesamiento lingüístico (Gemma/Llama), el siguiente desafío arquitectónico consistió en normalizar las habilidades extraídas contra una taxonomía de referencia y agruparlas para descubrir patrones emergentes. Esta normalización es crítica para eliminar variantes sintácticas (``React'', ``React.js'', ``ReactJS''), mapear términos coloquiales a conceptos ESCO formales, y habilitar análisis comparativo entre países y portales.

En el observatorio, los embeddings semánticos cumplieron dos roles principales: primero, la normalización de habilidades extraídas contra taxonomía ESCO mediante búsqueda de similitud coseno; y segundo, el agrupamiento de habilidades para descubrir perfiles emergentes. El modelo \texttt{intfloat/\allowbreak multilingual-e5-base} fue seleccionado por su soporte multilingüe nativo (768D, 100 idiomas, entrenado con contrastive learning) \cite{wang2024}, normalización L2 integrada, tamaño compacto (278M parámetros ejecutables en CPU $<$100ms/batch), y licencia MIT.

\textbf{Limitaciones Empíricas y Decisión Arquitectónica:}

La evaluación experimental con 15 habilidades técnicas agregadas manualmente a ESCO reveló limitaciones críticas del modelo E5. Con threshold de similitud coseno = 0.75, se obtuvo tasa de matches correctos de 6.7\% (1/15) y falsos positivos de 93.3\% (14/15). Ejemplos de mapeos incorrectos: ``React'' se relacionó con ``neoplasia'' (0.828), ``Docker'' con ``Facebook'' (0.825), y ``Machine Learning'' con ``gas natural'' (0.825). El análisis de causa raíz identificó tres factores: entrenamiento en lenguaje natural vs. vocabulario técnico, confusión entre brand names y palabras comunes, y contaminación del espacio vectorial por 13,939 habilidades ESCO de dominios no técnicos.

La evaluación de thresholds demostró que no existe un valor que balancee precision y recall: thresholds bajos ($<$0.80) generan falsos positivos críticos, mientras que thresholds altos ($\geq$0.85) excluyen matches exactos. Con base en esta evidencia, se tomó la decisión de deshabilitar la capa de búsqueda semántica (Layer 3), operando el sistema con estrategia de dos capas: Layer 1 (Exact Match) mediante búsqueda SQL con confidence = 1.00, y Layer 2 (Fuzzy Match) con threshold = 0.85 para capturar variantes ortográficas.

Para búsqueda de similitud en clustering, se adoptó FAISS (Facebook AI Similarity Search) con índice IndexFlatIP (exact search con inner product). FAISS demostró performance superior: 30,147 queries/segundo, latencia 0.033ms por query, speedup 25x sobre PostgreSQL pgvector, superando ampliamente el objetivo de diseño (100 q/s) por un margen de 301x \cite{johnson2019}.

\subsection{Técnicas de Clustering No Supervisado}

Una vez que las habilidades fueron extraídas, normalizadas y representadas como embeddings vectoriales de 768 dimensiones, el objetivo final del observatorio consistió en descubrir patrones y perfiles laborales emergentes sin categorías predefinidas. El sistema integró dos algoritmos complementarios para proyección dimensional y agrupamiento basado en densidad.

\textbf{Configuración de UMAP:}

Para el observatorio se utilizó UMAP con los siguientes parámetros: \texttt{n\_neighbors=15} (balance entre estructura local y global), \texttt{min\_dist=0.1} (compactación de puntos cercanos), y \texttt{metric='cosine'} (métrica apropiada para embeddings normalizados). UMAP redujo vectores de 768 dimensiones a 2-3 dimensiones visualizables manteniendo propiedades semánticas.

\textbf{Configuración de HDBSCAN:}

Se seleccionó HDBSCAN sobre K-Means dado que el número de perfiles emergentes era desconocido a priori. Los parámetros de producción fueron: \texttt{min\_cluster\_size=12} (tamaño mínimo de clúster válido para interpretabilidad, determinado tras 70+ experimentos), \texttt{min\_samples=3} (puntos mínimos para núcleo de densidad), \texttt{cluster\_selection\_method='eom'} (Excess of Mass para clusters más estables), y \texttt{metric='euclidean'} (post-reducción dimensional con UMAP).

\subsection{Taxonomías de Habilidades Laborales}

Si bien las técnicas de extracción, embeddings y clustering constituyeron el núcleo analítico del observatorio, todas estas operaciones dependieron de un componente fundamental: una taxonomía de referencia que permitiera normalizar las habilidades extraídas del texto crudo y mapearlas a conceptos estandarizados. Sin esta normalización, habilidades equivalentes como ``React'', ``React.js'' y ``ReactJS'' hubieran sido tratadas como entidades distintas, fragmentando el análisis y degradando la calidad del clustering. La selección de la taxonomía apropiada requirió balancear cobertura, actualización, soporte multilingüe y accesibilidad. Se evaluaron tres alternativas principales.

\textbf{Alternativas Consideradas:}

O*NET (Occupational Information Network) es la taxonomía del Departamento de Trabajo de EE.UU. con 1,000+ ocupaciones y 20,000+ habilidades. Ventajas: altamente estructurada con relaciones jerárquicas, actualización periódica, y datos salariales asociados. Limitaciones: enfoque en mercado estadounidense, escasa cobertura de tecnologías emergentes, y ausencia de soporte multilingüe nativo. A pesar de sus limitaciones, se extrajeron 152 habilidades técnicas modernas (Hot Technologies) de O*NET para complementar ESCO, especialmente en áreas de tecnologías emergentes donde ESCO v1.1.0 presenta gaps de cobertura.

ESCO (European Skills, Competences, Qualifications and Occupations) es la taxonomía oficial de la Unión Europea con 13,000+ habilidades, 3,000+ ocupaciones, y soporte para 27 idiomas \cite{kavargyris2025}. Ventajas: etiquetas nativas en español e inglés, cobertura amplia de habilidades tecnológicas, estructura ontológica con URIs únicos, y respaldo institucional de la Comisión Europea garantizando mantenimiento. Limitaciones: actualización menos frecuente que el mercado tecnológico (v1.1.0 data de 2016-2017), y menor cobertura de frameworks JavaScript modernos.

Taxonomías propietarias (LinkedIn Skills, Burning Glass Technologies): Ventajas: actualizadas con mayor frecuencia, cobertura de tecnologías emergentes. Limitaciones críticas: acceso mediante API de pago, licencias restrictivas, y falta de transparencia metodológica.

\textbf{Decisión Final:}

Se seleccionó ESCO v1.1.0 como base taxonómica fundamentado en cuatro razones principales. En primer lugar, el soporte multilingüe nativo (español/inglés) eliminó la necesidad de traducción automática. En segundo lugar, la licencia Creative Commons BY 4.0 permitió uso académico y comercial sin restricciones. En tercer lugar, la estructura ontológica con URIs persistentes facilitó la integración con LLMs y sistemas de recomendación. En cuarto lugar, la cobertura de 13,939 habilidades resultó suficiente para establecer baseline. La taxonomía ESCO fue extendida con 152 habilidades técnicas de O*NET (Hot Technologies) y 124 habilidades agregadas manualmente identificadas en el análisis exploratorio, totalizando 14,215 skills. La taxonomía extendida se almacenó en PostgreSQL (tabla \texttt{esco\_skills}) con índices en \texttt{preferred\_label\_es}, \texttt{preferred\_label\_en} y \texttt{alt\_labels}, permitiendo búsquedas eficientes durante el matching.

Habiendo establecido las bases teóricas de las técnicas de extracción (NER, Regex, LLMs), modelos de lenguaje (Gemma, Llama), embeddings semánticos (E5 Multilingual), algoritmos de clustering (UMAP, HDBSCAN) y taxonomías de referencia (ESCO), la siguiente sección presentó la validación empírica de estas tecnologías sobre datos reales del mercado laboral latinoamericano. Las pruebas ejecutadas determinaron qué combinación de técnicas optimizó el balance entre precisión y cobertura para el contexto específico del español técnico, proporcionando evidencia cuantitativa que fundamentó las decisiones arquitectónicas del sistema.

\section{Pruebas de Modelos}

Los fundamentos teóricos presentados en la sección anterior establecieron las bases conceptuales para la selección de técnicas de extracción (NER, Regex, LLMs), tecnologías de embeddings (E5 Multilingual), y algoritmos de clustering (UMAP, HDBSCAN). Sin embargo, la viabilidad de estas técnicas en el contexto específico del español latinoamericano técnico requiere validación empírica con datos reales del dominio. Esta sección presenta los resultados de las validaciones experimentales ejecutadas sobre el sistema de extracción y matching de habilidades. A diferencia de benchmarks teóricos sobre datasets en inglés, estas pruebas se realizaron con ofertas laborales reales de portales latinoamericanos, proporcionando evidencia empírica específica del dominio que fundamentó decisiones arquitectónicas clave presentadas en la sección posterior.

\subsection{Conjunto de Datos}

El dataset del observatorio se compone de ofertas laborales tecnológicas recolectadas mediante web scraping de 8 portales principales en los países de Colmbia México y Argentina. El corpus final contiene 30,660 ofertas laborales únicas distribuidas como: México 17,835 (58.16\%), Colombia 9,479 (30.91\%), y Argentina 3,346 (10.93\%). Las ofertas abarcan el período de octubre 2018 a octubre 2025 (7 años de datos históricos), con 71.23\% de cobertura temporal (21,839 jobs con posted\_date válido). El trimestre más reciente (Q4 2025) representa 69\% del dataset total, reflejando la naturaleza dinámica del mercado laboral tecnológico.

Cada oferta contiene los siguientes campos estructurados: \texttt{job\_id} (UUID único), \texttt{portal} (origen de la oferta), \texttt{country} (CO/MX/AR), \texttt{title} (título del cargo), \texttt{company} (empresa), \texttt{description} (descripción detallada del cargo), \texttt{requirements} (requisitos y habilidades), \texttt{salary\_raw} (rango salarial cuando disponible), \texttt{contract\_type} (tipo de contrato), \texttt{remote\_type} (modalidad presencial/remota/híbrida), \texttt{posted\_date} (fecha de publicación), y \texttt{content\_hash} (hash SHA-256 para deduplicación).

\subsection{Construcción del Conjunto de Datos}

\textbf{Proceso de Scraping:}

La recolección de datos se ejecutó mediante Scrapy 2.11, un framework asíncrono de scraping en Python que permitió procesamiento concurrente de múltiples requests. La estrategia de extracción se adaptó al tipo de portal: para sitios con HTML estático o APIs accesibles, se utilizó \texttt{requests} directo obteniendo datos en formato JSON desde endpoints que alimentan las vistas frontend (más eficiente); para portales con contenido dinámico cargado mediante JavaScript (Bumeran, ZonaJobs), se integraron Selenium 4.15 y ChromeDriver para renderizado completo de páginas antes de la extracción. El proceso implementó técnicas de ``polite crawling'': delays adaptativos entre requests (2-5 segundos), rotación de user-agents, límites de concurrencia por dominio, y reintentos con backoff exponencial ante errores HTTP 429/503.

La deduplicación de ofertas se realizó mediante hashing SHA-256 del contenido normalizado (title + company + description limpiados), almacenando el hash en campo \texttt{content\_hash} con restricción UNIQUE en PostgreSQL. Esta estrategia evitó re-procesar ofertas republicadas por portales múltiples o reposteadas por el mismo portal.

\textbf{Limpieza y Normalización:}

Las ofertas extraídas presentaron variabilidad significativa en formato y calidad. Se aplicó un pipeline de limpieza: eliminación de caracteres HTML residuales (\texttt{<br>}, \texttt{\&nbsp;}), normalización de espacios múltiples y saltos de línea, conversión de encoding a UTF-8, y extracción de texto plano de campos con formato rich text. Los disclaimers legales (equal opportunity statements, privacy policies) se identificaron mediante patrones regex y se separaron en metadatos sin afectar el análisis de habilidades.

\subsection{Validación de Técnicas de Extracción (Pipeline A)}

Se evaluó el rendimiento de los dos métodos del Pipeline A (Regex y NER) sobre el gold standard completo de 300 ofertas laborales manualmente anotadas (100 por país: Colombia, México, Argentina).

\textbf{Resultados de Extracción con Expresiones Regulares:}

El módulo regex implementó 548 patrones organizados en 18 categorías técnicas para tecnologías con nomenclatura estructurada. Resultados sobre el gold standard de 300 ofertas: F1-Score solo regex Pre-ESCO 18.07\%, F1-Score Post-ESCO 79.17\%, precision 86.36\% (post-ESCO), recall 73.08\%, latencia $\sim$0.32 s/job (320 ms). Las skills detectadas correctamente incluyeron Python, React, AWS, Docker, PostgreSQL, Kubernetes, Git, JavaScript, SQL, FastAPI. Los falsos positivos pre-ESCO correspondieron a acrónimos ambiguos (``ML'' en contextos no técnicos) y nombres de empresas similares a tecnologías (``Oracle'' empresa vs. ``Oracle Database''), pero fueron efectivamente filtrados mediante matching con ESCO. Conclusión: regex demostró alta precision post-normalización (86.36\%) validando su uso como método base, con velocidad submilisegundos permitiendo procesamiento masivo.

\textbf{Resultados de Extracción con NER y spaCy:}

El módulo NER utilizó \texttt{es\_core\_news\_lg} con EntityRuler poblado con 666 patrones ESCO y múltiples filtros post-extracción (200+ stopwords NER, 60+ technical generic stopwords). Resultados sobre gold standard: Pipeline A (NER+Regex) alcanzó F1-Score Pre-ESCO 24.98\% y F1-Score Post-ESCO 72.53\%, con recall de 64.43\% y precision post-filtrado de 9.3\% (pre-ESCO), latencia 50-80ms. El análisis reveló que NER aportó +6.91pp de mejora Pre-ESCO respecto a regex solo, pero generó -6.64pp de degradación Post-ESCO debido a extracción de variantes textuales que no mapearon a ESCO. Dado que ESCO no cubre exhaustivamente el vocabulario técnico latinoamericano emergente, se priorizó optimizar el F1-Score Pre-ESCO (captura de habilidades reales mencionadas) sobre el Post-ESCO (normalización a taxonomía), por lo que NER se mantuvo activo en la arquitectura final para maximizar cobertura de detección.

\textbf{Estrategia Combinada y Matching con ESCO:}

La arquitectura final combinó ambos métodos con deduplicación posterior, logrando signal-to-noise ratio de 0.98 después de matching con ESCO. El matching contra taxonomía ESCO extendida (14,215 skills totales: 13,939 ESCO v1.1.0 + 152 O*NET + 124 agregadas manualmente) se ejecuta en dos capas: Layer 1 (exact match) mediante SQL \texttt{ILIKE} en \texttt{preferred\_label\_es/en} con confidence 1.00, y Layer 2 (fuzzy match) con \texttt{fuzzywuzzy} ratio $\geq$ 0.85 para variantes ortográficas con confidence = ratio/100.

\subsection{Evaluación del Sistema Completo con Gold Standard}

Se ejecutó un test end-to-end procesando 300 ofertas laborales del gold standard (100 por país: Colombia, México, Argentina) con el pipeline completo utilizando el matcher ESCO implementado de dos capas (exact + fuzzy). Resultados globales: jobs procesados exitosamente 300/300 (100\%), total skills extraídas 8,268 (27.6 skills/job promedio), skills matched con ESCO 1,038 (12.6\% match rate), emergent skills sin match 7,230 (87.4\%), latencia promedio 1.82 segundos/job. Posteriormente se desarrolló un matcher experimental mejorado con mapeos manuales curados que incrementó el match rate a aproximadamente 25\%, permitiendo identificar y cuantificar con mayor precisión las habilidades emergentes ausentes en ESCO v1.1.0. Sin embargo, esta versión experimental no fue integrada al pipeline productivo para evitar introducir sesgos en la comparación entre Pipeline A y Pipeline B, manteniendo condiciones de evaluación equitativas donde ambos pipelines utilizan el mismo matcher sin bias.

La distribución de matching por capa fue: Layer 1 (exact match) 149 skills (43.1\% del matched, confidence 1.00), Layer 2 (fuzzy match) 197 skills (56.9\% del matched, confidence 0.85-0.99). El análisis por país reveló variación: Colombia 15.3\% match rate (mayor proporción de stacks enterprise tradicionales: Java, .NET, Oracle, SAP), México 11.3\% (mayor adopción de frameworks modernos), Argentina 12.5\% (balance intermedio).

Las top 10 skills matched con ESCO fueron: Python (14 menciones), Agile (13), SQL (10), JavaScript (10), Git (8), FastAPI (8), AWS Lambda (8), Kubernetes (6), Go (6), GitLab CI/CD (6). Las top 10 emergent skills (sin match ESCO, verificadas manualmente) fueron: Data Science (67), HTML5 (35), CSS3 (21), Matplotlib (15), Dashboards (13), Data Modeling (9), Data Analysis (9), Flux (8), Relay (8), SOAP (8). Interpretación: las emergent skills confirman predominancia de conceptos amplios de análisis de datos (Data Science, Dashboards, Data Analysis) y versiones específicas de tecnologías (HTML5, CSS3) que ESCO v1.1.0 no distingue granularmente. Adicionalmente, se identificaron skills post-2022 de IA generativa (ChatGPT, LLM, Generative AI, Fine-tuning de LLMs) con frecuencias bajas pero alta relevancia estratégica.

El match rate de 12.6\% fue obtenido con el matcher baseline de dos capas (exact + fuzzy 0.92) operando sobre la taxonomía ESCO extendida de 14,215 skills (13,939 ESCO v1.1.0 + 152 O*NET + 124 agregadas manualmente). Este porcentaje es bajo pero esperado considerando dos factores principales. Primero, el corpus contiene alta frecuencia de conceptos amplios (Data Science, Data Analysis) y versiones específicas (HTML5, CSS3) que ESCO trata genéricamente sin distinguir granularidad. Segundo, el mercado laboral latinoamericano presenta alta demanda de tecnologías post-2022 (ChatGPT, Generative AI, LLM) ausentes en taxonomías europeas. Iteraciones posteriores con matcher enhanced de 4 capas incrementaron el coverage a $\sim$25\%, validando que las habilidades no matched (87.4\% baseline, 74.7\% enhanced) corresponden genuinamente a emergent skills y representan señal valiosa de innovación del mercado para análisis exploratorio posterior.

Estos resultados tienen implicaciones arquitectónicas importantes para el sistema. El alto porcentaje de emergent skills valida la decisión de implementar el Pipeline B con LLMs, ya que estas habilidades modernas probablemente corresponden a términos técnicos actuales que ESCO v1.1.0 no cubre pero que un LLM pre-entrenado puede reconocer contextualmente. Asimismo, la variación del match rate por país (CO 15.3\%, MX 11.3\%, AR 12.5\%) sugiere diferencias regionales en adopción tecnológica que deben considerarse en el análisis de clustering, potencialmente requiriendo ajuste de parámetros HDBSCAN por región. Finalmente, la alta frecuencia de conceptos amplios de datos (Data Science con 67 menciones) y skills de IA generativa post-2022 confirman la relevancia temporal del corpus y su alineación con tendencias del mercado tecnológico latinoamericano.

Para maximizar el valor analítico de las emergent skills, se propone un proceso de curación semi-automática que combinaría clustering semántico de las habilidades no matched mediante embeddings E5 y HDBSCAN, seguido de revisión manual de los clústeres más frecuentes. Los términos válidos y recurrentes (threshold de cinco o más menciones) se agregarían manualmente a la taxonomía ESCO local, mientras que los términos rechazados se documentarían con su justificación correspondiente. Este proceso iterativo permitiría que el match rate evolucione orgánicamente con el corpus, balanceando cobertura y control de calidad. En la implementación actual, las 124 habilidades agregadas manualmente fueron identificadas mediante análisis exploratorio inicial del corpus sin automatización del proceso de clustering.

\section{Arquitectura}

Las pruebas experimentales presentadas en la sección anterior proporcionaron evidencia empírica crítica que determinó las decisiones arquitectónicas del sistema. Los resultados demostraron que el enfoque dual NER+Regex alcanza precision de 78-95\% con latencias submilisegundos, que el matching con ESCO requiere expansión manual debido al bajo match rate inicial (12.6\%), y que los embeddings E5 presentan limitaciones para búsqueda semántica en vocabulario técnico especializado. Adicionalmente, las características del dominio  procesamiento batch de ofertas laborales sin requisitos de tiempo real, corpus objetivo de 600,000 ofertas, y recursos limitados propios de un proyecto académico restringieron las opciones arquitectónicas viables.

El sistema se diseñó como un observatorio automatizado end-to-end que integra siete etapas especializadas de procesamiento, desde la adquisición de ofertas laborales hasta la generación de visualizaciones analíticas. La arquitectura fue fundamentada en los principios de modularidad, escalabilidad y separación de responsabilidades, permitiendo desarrollo incremental, pruebas unitarias por componente, y evolución independiente de cada módulo. Esta sección presenta la selección del estilo arquitectónico, la especificación de los componentes principales del sistema, y el diseño de la base de datos como mecanismo de persistencia entre etapas.

\subsection{Modelo de Vistas Arquitectónicas}

La complejidad del sistema, que integra procesamiento síncrono de baja latencia para consultas de usuarios con procesamiento asíncrono distribuido de tareas computacionalmente intensivas, requiere múltiples perspectivas para su documentación completa. Se adoptó el Modelo 4+1 de Vistas Arquitectónicas \cite{kruchten1995}, el cual permite describir la arquitectura desde perspectivas complementarias enfocadas en las preocupaciones de diferentes stakeholders del proyecto.

Para este proyecto se documentan tres vistas principales: Vista Lógica (funcionalidad y componentes del sistema), Vista Física (topología de despliegue e infraestructura), y Vista de Procesos (comportamiento en tiempo de ejecución y concurrencia). Las siguientes subsecciones presentan cada vista arquitectónica, proporcionando una especificación completa de la arquitectura implementada.

\subsection{Vista Lógica: Componentes y Patrones Arquitectónicos}

La vista lógica describe la descomposición funcional del sistema en servicios especializados y los patrones arquitectónicos que gobiernan sus interacciones. El sistema implementa una arquitectura híbrida que combina tres patrones complementarios para satisfacer requisitos duales de latencia: operaciones síncronas de baja latencia (menos de 1 segundo) para consultas, y procesamiento asíncrono distribuido de tareas computacionalmente intensivas que pueden requerir minutos u horas.

\subsubsection{Patrón Híbrido Implementado}

El sistema integra tres patrones arquitectónicos fundamentales que trabajan de manera coordinada. El primer patrón corresponde al API Gateway, implementado mediante Nginx, que actúa como punto único de entrada para todas las peticiones HTTP/HTTPS externas. Este componente proporciona routing inteligente que enruta solicitudes hacia el servicio Frontend o API según la ruta solicitada, terminación SSL/TLS para gestión centralizada de certificados, rate limiting para protección contra abusos y auditoría de todas las peticiones del sistema.

El segundo patrón implementado son los Microservicios en Capas para comunicación Request/Response. Para operaciones que requieren respuesta inmediata, se estructura una arquitectura de tres capas: la capa de Presentación mediante Frontend con Next.js para renderizado y gestión de interfaz, la capa de Lógica de Negocio mediante API con FastAPI para endpoints REST y validación, y la capa de Persistencia mediante PostgreSQL para almacenamiento ACID. Este patrón se emplea para consultas de ofertas laborales, estadísticas agregadas, y operaciones que requieren latencias inferiores a 1 segundo.

El tercer patrón es la Event-Driven Architecture mediante comunicación Pub/Sub. Para operaciones computacionalmente intensivas, se implementa arquitectura orientada a eventos mediante el patrón Publisher/Subscriber. La API y Celery Beat actúan como publishers publicando tareas a una cola de mensajes gestionada por Redis, mientras que Celery Workers actúan como subscribers consumiendo estas tareas, ejecutando el procesamiento requerido, y persistiendo resultados en PostgreSQL. Este patrón se emplea para scraping automático de portales, extracción de habilidades con LLM en lotes, y clustering de habilidades.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{diagrams/VistaLogica.png}
\caption{Vista Lógica del Sistema. El diagrama presenta la arquitectura híbrida con sus tres patrones integrados: API Gateway (Nginx), Microservicios en Capas (Frontend, API, PostgreSQL) para operaciones síncronas, y Event-Driven Architecture (Redis, Celery) para procesamiento asíncrono distribuido.}
\label{fig:vista-logica}
\end{figure}

\subsubsection{Los Siete Servicios del Sistema}

La arquitectura se descompone en siete servicios especializados, cada uno con responsabilidades claramente delimitadas. El servicio NGINX actúa como API Gateway proporcionando el punto único de entrada HTTP/HTTPS, routing de peticiones hacia los servicios apropiados, terminación SSL/TLS, y compresión de respuestas. El servicio Frontend gestiona la interfaz de usuario mediante renderizado de páginas web con Server-Side Rendering utilizando Next.js, gestión de estado de aplicación, visualización de dashboards y estadísticas, y polling para monitoreo de tareas asíncronas.

El servicio API implementa la lógica de negocio del sistema mediante exposición de endpoints REST para operaciones CRUD, validación de datos de entrada, coordinación de servicios mediante publicación de tareas asíncronas, y consulta de estado de tareas en Redis. Este servicio implementa funcionalidad dual: Request/Response síncrono para consultas rápidas y Pub/Sub asíncrono para procesamiento batch. El servicio PostgreSQL proporciona persistencia ACID de datos estructurados, almacenamiento de embeddings de 768 dimensiones mediante extensión pgvector, garantía de trazabilidad mediante relaciones foreign key, y optimización de consultas mediante índices.

El servicio Redis cumple doble función como cola de mensajes para el patrón Pub/Sub actuando como broker de Celery, y como almacenamiento de resultados de tareas funcionando como backend de resultados de Celery. Adicionalmente provee cache de consultas frecuentes con TTL configurable y gestión de estado de tareas asíncronas. El servicio Celery Beat se encarga de la programación de tareas periódicas tipo cron tales como scraping cada 6 horas y limpieza diaria de datos antiguos, publicación de tareas programadas a la cola de Redis, y gestión de schedules de ejecución automática.

Finalmente, el servicio Celery Workers actúa como procesador distribuido mediante consumo de tareas de la cola de Redis, ejecución del pipeline CRISP-DM de procesamiento de datos, procesamiento de tareas en paralelo mediante múltiples workers escalables, persistencia de resultados en PostgreSQL, y reporte de progreso de tareas. Este servicio soporta escalamiento horizontal dinámico desde 1 hasta N workers sin requerir cambios de código.

\subsubsection{Justificación de la Arquitectura Híbrida}

La selección de una arquitectura híbrida se fundamentó en cinco razones principales: la dualidad de requisitos de latencia (consultas de usuarios requieren respuesta inmediata mientras que el procesamiento de datos requiere minutos u horas), escalabilidad horizontal selectiva (los workers pueden escalarse dinámicamente sin modificar código), simplicidad operativa con potencia de procesamiento (mantiene trazabilidad de sistemas modulares mientras obtiene paralelismo de sistemas distribuidos), optimización de recursos (Request/Response evita overhead para operaciones simples mientras que Event-Driven maximiza CPU para procesamiento intensivo), y madurez del ecosistema tecnológico (Celery y Redis es una combinación probada industrialmente con amplia documentación).

La Tabla \ref{tab:arch-comparison-hibrida} presenta la evaluación de tres estilos arquitectónicos considerados durante el diseño.

\begin{table}[H]
\centering
\caption{Comparación de Estilos Arquitectónicos Evaluados}
\label{tab:arch-comparison-hibrida}
\begin{tabular}{|p{3.2cm}|p{2.8cm}|p{3cm}|p{3.5cm}|}
\hline
\textbf{Criterio} & \textbf{Pipeline Lineal} & \textbf{Microservicios Puros} & \textbf{Arquitectura Híbrida} \\
\hline
Complejidad de implementación & Baja & Alta & Media \\
\hline
Escalabilidad horizontal & Limitada & Excelente & Excelente (workers) \\
\hline
Latencia de consultas & Alta (bloqueante) & Baja & Baja (menor a 1s) \\
\hline
Throughput de procesamiento & Bajo (secuencial) & Medio & Alto (paralelo) \\
\hline
Trazabilidad & Excelente & Media & Alta \\
\hline
Tolerancia a fallos & Baja & Alta & Alta \\
\hline
Time to market & Rápido & Lento & Medio \\
\hline
\end{tabular}
\end{table}

El pipeline lineal fue descartado por impedir paralelismo y limitar throughput. Los microservicios puros fueron descartados por introducir complejidad innecesaria para un proyecto académico con equipo de 2 desarrolladores. La arquitectura híbrida proporciona el balance óptimo entre capacidades técnicas y viabilidad operativa.

\subsection{Vista Física: Infraestructura y Despliegue}

La vista física describe el mapeo de componentes lógicos sobre infraestructura física, especificando hardware, contenedores Docker, configuración de red, y estrategia de despliegue. Esta sección detalla la topología de deployment que materializa la arquitectura lógica presentada anteriormente.

\subsubsection{Especificaciones del Servidor}

El sistema se despliega en un servidor único que ejecuta todos los servicios mediante contenedores Docker. Las especificaciones mínimas recomendadas incluyen un hardware o máquina virtual con CPU de 8 cores a 2.4 GHz (Intel Xeon o AMD EPYC), 16 GB de RAM DDR4, almacenamiento de 500 GB SSD con preferencia NVMe, red de 1 Gbps, y opcionalmente GPU NVIDIA con 8GB VRAM para aceleración de inferencia con LLM.

El software base requerido comprende Docker Engine versión 24.0 o superior, Docker Compose v2, y NVIDIA Container Toolkit en caso de utilizar GPU. La configuración de firewall establece puertos abiertos 80 para HTTP y 443 para HTTPS, manteniendo todos los demás puertos bloqueados externamente. La comunicación interna entre contenedores se realiza mediante red Docker.

\subsubsection{Contenedores Docker y Orquestación}

El sistema se compone de contenedores Docker orquestados mediante Docker Compose. La Tabla \ref{tab:contenedores-docker} presenta los contenedores principales con sus especificaciones de recursos.

\begin{table}[H]
\centering
\caption{Contenedores Docker del Sistema}
\label{tab:contenedores-docker}
\begin{tabular}{|p{3cm}|p{3cm}|p{2.5cm}|p{1.5cm}|p{1.5cm}|}
\hline
\textbf{Servicio} & \textbf{Imagen} & \textbf{Puerto} & \textbf{CPU} & \textbf{RAM} \\
\hline
nginx & nginx:alpine & 80, 443 & 1 & 1 GB \\
\hline
frontend & frontend:latest & 3000 & 1 & 1 GB \\
\hline
api & api:latest & 8000 & 1 & 1 GB \\
\hline
postgres & postgres:15 & 5433→5432 & 2 & 4 GB \\
\hline
redis & redis:7-alpine & 6379 & 1 & 2 GB \\
\hline
celery\_beat & celery:latest & N/A & 0.5 & 512 MB \\
\hline
celery\_worker & celery:latest & N/A & 2 & 4 GB \\
\hline
\end{tabular}
\end{table}

El servicio \texttt{celery\_worker} puede replicarse dinámicamente (1, 2, 4, 8, o N instancias) sin cambios de código mediante el comando \texttt{docker-compose up -d --scale celery\_worker=N}, permitiendo escalamiento horizontal según la carga de procesamiento.

\begin{figure}[H]
\centering
\makebox[\textwidth]{\includegraphics[width=1.25\textwidth]{diagrams/VistaFisica.png}}
\caption{Vista Física del Sistema. El diagrama muestra el servidor de producción con sus especificaciones de hardware, los contenedores Docker orquestados por Docker Compose, mapeo de puertos, red bridge interna, y volúmenes persistentes para PostgreSQL y Redis.}
\label{fig:vista-fisica}
\end{figure}

\subsubsection{Configuración de Red y Volúmenes}

Todos los contenedores se conectan a una red bridge interna (subnet 172.18.0.0/16) que proporciona resolución DNS automática por nombre de servicio y aislamiento de red (solo nginx expone puertos externamente). Los datos que deben persistir entre reinicios de contenedores se almacenan en volúmenes Docker: \texttt{postgres\_data} (100 GB para base de datos completa), \texttt{redis\_data} (10 GB para dumps y snapshots), y \texttt{logs\_data} (20 GB para logs de todos los servicios). Se implementa estrategia de backup diario de PostgreSQL mediante pg\_dump y snapshots semanales de volúmenes Docker.

\subsection{Vista de Procesos: Ejecución y Concurrencia}

La vista de procesos describe el comportamiento dinámico del sistema en tiempo de ejecución, abordando aspectos de concurrencia, distribución de procesamiento, throughput, y escalabilidad. Esta sección presenta el pipeline CRISP-DM que ejecutan los workers y la estrategia de escalamiento horizontal.

\subsubsection{Pipeline CRISP-DM y Flujo de Procesamiento}

El procesamiento de datos sigue la metodología CRISP-DM (Cross-Industry Standard Process for Data Mining) adaptada al dominio de análisis de mercado laboral. El pipeline se compone de 7 etapas secuenciales ejecutadas por los Celery Workers. La primera etapa de Scraping realiza recolección automatizada mediante Scrapy y Selenium desde 8 portales de empleo en 3 países, con tiempo estimado de 5 a 10 minutos por portal, implementando deduplicación mediante hash SHA-256 de título, empresa, ubicación y fecha.

La segunda etapa de Cleaning ejecuta normalización de texto, eliminación de HTML, y detección de idioma, generando como salida cleaned jobs con campo is\_usable en un tiempo de 50 a 100 milisegundos por oferta. La tercera etapa de Extraction realiza identificación de habilidades mediante Pipelines A basado en NER y Regex o Pipeline B basado en LLM. Estos pipelines son variantes experimentales de esta etapa, no arquitecturas separadas, con tiempo de procesamiento de 100 a 200 milisegundos para Pipeline A o 2 a 5 segundos para Pipeline B por oferta.

La cuarta etapa de Enhancement ejecuta normalización con LLM, inferencia de skills implícitas, y mapeo a taxonomía ESCO en un tiempo de 1 a 3 segundos por oferta. La quinta etapa de Embedding genera representaciones vectoriales de 768 dimensiones con modelo E5-multilingual en 100 milisegundos por lote de 32 skills. La sexta etapa de Clustering realiza reducción dimensional con UMAP transformando de 768 dimensiones a 2 o 3 dimensiones, seguida de clustering con HDBSCAN, requiriendo de 2 a 5 minutos para 10,000 skills. La séptima etapa de Visualization genera gráficos incluyendo scatter plots, heatmaps, y tendencias temporales, produciendo imágenes PNG y datos JSON para frontend en 10 a 30 segundos.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{diagrams/VistaProcesos.png}
\caption{Vista de Procesos del Sistema. El diagrama muestra el flujo completo de procesamiento desde el web scraping hasta la visualización, incluyendo los eventos de skill extraction, LLM processing, embedding generation, dimension reduction, clustering, y la orquestación del pipeline mediante Celery Workers con persistencia en PostgreSQL.}
\label{fig:vista-procesos}
\end{figure}

\subsubsection{Escalabilidad Horizontal de Workers}

La arquitectura permite escalamiento horizontal dinámico de workers sin modificar código ni reconfigurar servicios. Redis distribuye tareas equitativamente entre workers disponibles mediante load balancing automático. El impacto en throughput es aproximadamente lineal: con 1 worker, 100 tareas de 5 segundos requieren 500 segundos (8 min 20s); con 4 workers, el tiempo se reduce a 125 segundos (2 min 5s); con 8 workers, a 62.5 segundos (1 min 2s). El speedup real es aproximadamente 0.7×N debido a overhead de comunicación y contención de recursos.

El sistema implementa tolerancia a fallos: si un worker falla, Redis redistribuye tareas pendientes a workers restantes, y las tareas en ejecución se reintentan automáticamente en otro worker (hasta 3 reintentos con delay de 60 segundos).

\subsubsection{Gestión de Tareas Asíncronas}

El ciclo de vida de una tarea asíncrona comprende los siguientes estados: QUEUED (tarea publicada a Redis), PENDING (tarea en cola esperando worker disponible), STARTED (worker tomó la tarea y comenzó procesamiento), PROGRESS (worker reporta progreso: 20\%, 40\%, 60\%, etc.), y finalmente SUCCESS (tarea completada exitosamente con resultado disponible) o FAILURE (tarea falló con error almacenado). El frontend monitorea el progreso mediante polling cada 3 segundos al endpoint \texttt{/api/tasks/\{task\_id\}}, actualizando la interfaz de usuario con el estado y porcentaje de completitud.

\subsection{Diseño de la Base de Datos}

La base de datos actuó como columna vertebral del sistema, implementando el patrón de persistencia de pipeline donde cada etapa escribió sus resultados en tablas especializadas. Se seleccionó PostgreSQL 15+ por su soporte JSON nativo (JSONB) para almacenar metadatos flexibles, extensión pgvector para vectores de alta dimensionalidad, robustez transaccional (ACID), capacidad de particionamiento para escalabilidad, y licencia libre (PostgreSQL License).

\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{diagrams/DiagramaER.png}
\caption{Diagrama Entidad-Relación de la Base de Datos del Observatorio}
\label{fig:diagrama-er}
\end{figure}

El esquema (Figura \ref{fig:diagrama-er}) se estructura en torno a seis tablas principales que implementan el flujo completo del pipeline de procesamiento. La tabla central raw\_jobs almacena las ofertas laborales scrapeadas sin procesar, actuando como punto de partida para todas las transformaciones posteriores. A partir de esta tabla, el sistema genera dos flujos paralelos de enriquecimiento: extracted\_skills captura habilidades identificadas mediante Pipeline A utilizando NER y expresiones regulares, mientras que enhanced\_skills almacena el enriquecimiento semántico realizado por Pipeline B mediante LLM con capacidad de inferir habilidades implícitas.

Las habilidades extraídas por ambos pipelines se transforman posteriormente en representaciones vectoriales almacenadas en skill\_embeddings, la cual contiene vectores de 768 dimensiones generados con el modelo E5-multilingual y optimizados mediante índice IVFFlat para búsquedas de similitud. Los resultados de análisis avanzados se persisten en analysis\_results, incluyendo proyecciones UMAP, asignaciones de clusters HDBSCAN, y metadatos de configuración en formato JSONB. Finalmente, esco\_skills actúa como tabla de referencia conteniendo la taxonomía extendida de 14,215 habilidades compuesta por ESCO v1.1.0 (13,939 skills), O*NET Hot Technologies (152 skills), y habilidades agregadas manualmente (124 skills).

Todas las tablas derivadas mantienen referencia mediante llave foránea hacia raw\_jobs, garantizando trazabilidad completa desde cualquier resultado de análisis hasta la oferta laboral original que lo generó. La especificación detallada de campos, tipos de datos, índices optimizados, configuración de PostgreSQL para procesamiento batch, y estrategias de persistencia se documentan exhaustivamente en el Apéndice E.

Habiendo documentado la arquitectura del sistema mediante tres vistas complementarias (Lógica, Física y Procesos) que especifican los patrones arquitectónicos híbridos, los siete servicios especializados, la infraestructura de despliegue con Docker, el pipeline CRISP-DM de 7 etapas, y el diseño de base de datos con 6 tablas principales, la siguiente sección presenta las decisiones tecnológicas concretas que materializaron esta arquitectura. La selección de herramientas y tecnologías se fundamentó en criterios de madurez, licencias permisivas, escalabilidad comprobada y reproducibilidad científica, asegurando que cada componente arquitectónico contara con una implementación robusta y bien documentada.

\section{Herramientas y Tecnologías}

Las Tablas \ref{tab:tech-stack-infra} y \ref{tab:tech-stack-analytics} resumen las decisiones tecnológicas fundamentales y su justificación académica y técnica, organizadas por capa funcional del sistema.

\begin{table}[H]
\centering
\caption{Stack Tecnológico: Infraestructura y Adquisición de Datos}
\label{tab:tech-stack-infra}
\begin{tabular}{|p{3.5cm}|p{3.5cm}|p{6.5cm}|}
\hline
\textbf{Componente} & \textbf{Tecnología} & \textbf{Justificación} \\
\hline
Base de datos & PostgreSQL 15+ con pgvector & Soporte JSONB para metadatos flexibles, extensión pgvector para vectores 768D, robustez ACID, particionamiento para escalabilidad. \\
\hline
Taxonomía & ESCO v1.1.0 (+ 276 ext.) & Cobertura 13,000+ skills con etiquetas español/inglés, extendida con 152 O*NET + 124 manual (total 14,215), estructura ontológica con URIs, respaldo institucional CE, licencia CC BY 4.0. \\
\hline
Framework scraping & Scrapy 2.11 + Selenium 4.15 & Arquitectura asíncrona (100+ req/min), manejo robusto de reintentos, middlewares extensibles, Selenium para JavaScript dinámico. \\
\hline
Modelo NLP español & spaCy 3.7 + es\_core\_news\_lg & Mejor modelo español disponible (97M parámetros), soporte EntityRuler para ESCO, optimizado CPU ($<$100ms/doc). \\
\hline
Lenguaje & Python 3.11+ & Ecosistema científico maduro (NumPy, pandas, scikit-learn), bibliotecas NLP referencia (spaCy, transformers), integración PostgreSQL. \\
\hline
Control versiones & Git + GitHub & Estándar industria, integración CI/CD (GitHub Actions), control issues/milestones, documentación Markdown, respaldo cloud. \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Stack Tecnológico: Procesamiento y Análisis}
\label{tab:tech-stack-analytics}
\begin{tabular}{|p{3.5cm}|p{3.5cm}|p{6.5cm}|}
\hline
\textbf{Componente} & \textbf{Tecnología} & \textbf{Justificación} \\
\hline
LLM enriquecimiento & Gemma 3 4B / Llama 3 3B (Q4) & Modelos ligeros de 3-4B parámetros, despliegue local sin APIs (privacidad), cuantización Q4 (3-6 GB VRAM), selección por evaluación comparativa. \\
\hline
Embeddings & intfloat/multilingual-e5-base & Estado del arte multilingüe (768D), contrastive learning en 100 idiomas, normalización L2 integrada, 278M parámetros ejecutables CPU. \\
\hline
Índice similitud & FAISS IndexFlatIP & Velocidad 30,147 q/s (25x superior pgvector), búsqueda exacta (100\% recall), latencia 0.033ms, Facebook AI Research. \\
\hline
Reducción dimensional & UMAP \cite{mcinnes2018umap} & Preserva estructura local y global (superior t-SNE), escalabilidad millones puntos, reproducibilidad con semilla fija, parámetros interpretables. \\
\hline
Clustering & HDBSCAN \cite{campello2013} & No requiere especificar k, identifica ruido automático, maneja densidades variables (nicho vs. mainstream), jerarquía multinivel. \\
\hline
Orquestación & Typer CLI + APScheduler & Interface tipo Git con validación automática, scheduler 24/7, logging estructurado, integración cron/systemd. \\
\hline
\end{tabular}
\end{table}

Todas las tecnologías seleccionadas cumplen con cinco criterios principales. Primero, licencias permisivas (MIT, Apache 2.0, PostgreSQL, CC BY) permitiendo uso académico y potencial comercial futuro. Segundo, madurez y estabilidad con versiones $\geq$ 2.0 y comunidades activas. Tercero, documentación académica completa con publicaciones científicas revisadas por pares para componentes críticos. Cuarto, reproducibilidad mediante control de versiones de dependencias y semillas fijas para componentes estocásticos. Quinto, escalabilidad demostrada para el objetivo de 600,000 ofertas laborales mediante arquitectura de pipeline por lotes y particionamiento de base de datos.
