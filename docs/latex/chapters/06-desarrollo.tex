\chapter{DESARROLLO DE LA SOLUCIÓN}

\section{Implementación de la Infraestructura}

El sistema se implementó sobre PostgreSQL 15.3, seleccionado por su robustez en manejo de datos estructurados, soporte JSON nativo, y capacidades de indexación GIN. El esquema normalizado (3FN) utiliza seis tablas principales: \texttt{raw\_jobs} (ofertas crudas del scraping), \texttt{cleaned\_jobs} (ofertas normalizadas), \texttt{extracted\_\allowbreak skills} (Pipeline A), \texttt{enhanced\_\allowbreak skills} (Pipeline B), \texttt{gold\_\allowbreak standard\_\allowbreak annotations} (300 ofertas anotadas manualmente), y \texttt{esco\_skills} (14,215 skills de taxonomía ESCO extendida). Cada tabla de skills incluye metadatos de trazabilidad (\texttt{extraction\_\allowbreak method}, \texttt{llm\_model}, \texttt{esco\_uri}) permitiendo comparaciones sistemáticas entre pipelines.

Para optimizar consultas sobre 30,660 ofertas, se implementaron índices compuestos: B-tree en \texttt{(job\_id,\allowbreak\ extraction\_\allowbreak method)}, GIN en \texttt{skill\_text}, y B-tree en \texttt{posted\_date}. Estas optimizaciones redujeron tiempos de consultas agregadas de ~45s a ~2.3s.

La configuración de PostgreSQL se optimizó específicamente para procesamiento batch de grandes volúmenes de datos, priorizando throughput sobre latencia de consultas individuales. Se configuró memoria compartida de 4GB, memoria de trabajo de 256MB, y cache efectivo de 12GB según mejores prácticas para servidores orientados a procesamiento analítico. Se implementaron índices especializados tipo B-tree para comparaciones entre pipelines, GIN para búsquedas de texto completo, e IVFFlat para búsquedas de similitud vectorial. Las optimizaciones lograron reducir consultas agregadas de 45 segundos a 2.3 segundos e incrementar inserciones batch a 5,000 registros por segundo. Esta configuración permitió que el procesamiento del corpus completo de 30,660 ofertas completara en aproximadamente 6.2 horas, incluyendo todas las etapas del pipeline. La especificación completa de parámetros, índices y micro-benchmarks se documenta en el Apéndice E.

\subsection{Orquestación del Pipeline}

El orquestador implementó siete etapas modulares ejecutadas secuencialmente: scraping de ocho portales en tres países, limpieza y normalización, extracción mediante Pipeline A, extracción mediante Pipeline B, mapeo a taxonomía ESCO, generación de embeddings, clustering con UMAP y HDBSCAN, y análisis temporal. Esta arquitectura lineal se seleccionó sobre frameworks más complejos por su simplicidad de depuración y capacidad de reejecutar etapas individuales. Cada script registra progreso en logs estructurados con timestamps y métricas de performance.

El corpus se recolectó mediante scraping especializado por portal, adaptado a tres arquitecturas según las características de cada sitio: acceso directo a endpoints JSON/API para portales que exponen datos estructurados, combinación de \texttt{requests} con \texttt{BeautifulSoup4} para contenido HTML estático, y \texttt{selenium} para contenido JavaScript dinámico renderizado client-side. Se implementaron estrategias anti-bloqueo mediante delays aleatorios, rotación de User-Agent, y respeto de archivos \texttt{robots.txt}. El scraping completo recolectó 56,555 ofertas brutas durante aproximadamente 72 horas distribuidas en 15 días.

El pipeline de limpieza aplicó normalización de texto, eliminación de HTML residual, detección de idioma mediante \texttt{langdetect}, deduplicación mediante fuzzy matching y hash SHA-256, y validación de calidad descartando ofertas con contenido insuficiente. Del total scrapeado, se descartaron 25,895 ofertas por duplicación o calidad insuficiente, resultando en un dataset de 30,660 ofertas usables con texto normalizado, idioma identificado y metadata completa, cubriendo siete años de publicaciones laborales.

\section{Implementación de Sistemas de Extracción de Habilidades}

Se implementaron cuatro aproximaciones metodológicas para extracción automática de habilidades técnicas: Pipeline A (NER + Regex), Pipeline B (LLM), Pipeline A.1 (TF-IDF, descartado), y configuración Regex-Only para baseline.

\subsection{Pipeline A: NER y Expresiones Regulares}

Pipeline A constituye el método base de extracción de habilidades del observatorio, diseñado para identificar menciones explícitas de tecnologías mediante la combinación de Reconocimiento de Entidades Nombradas (NER) para detectar menciones contextuales y expresiones regulares (Regex) para capturar nomenclaturas estandarizadas. Esta arquitectura dual resuelve limitaciones complementarias: mientras NER con spaCy 3.5 y EntityRuler de 666 patrones ESCO captura tecnologías modernas en contexto pero falla con acrónimos técnicos, Regex con 548 patrones compilados en 18 categorías garantiza detección de nomenclaturas estructuradas pero omite menciones contextuales. La integración alcanza recall Post-ESCO de 81.25\% versus 73.08\% de Regex solo, procesando ofertas con latencia de 0.97 segundos, aproximadamente 18 veces más rápida que Pipeline B.

El flujo de procesamiento ejecuta ambas técnicas en paralelo sobre el mismo texto, combina resultados por unión, deduplica mediante normalización textual, y aplica diccionario canónico de 200 equivalencias que reduce 6,498 skills únicas a 3,200 formas estandarizadas. El control de calidad implementa filtrado multi-etapa: limpieza de HTML residual y validación de encoding en pre-procesamiento, aplicación de listas de stopwords NER y técnicos genéricos en post-extracción para eliminar falsos positivos, y validación cruzada mediante overlap NER-Regex que asigna mayor confianza a skills detectadas por ambos métodos. El pipeline logra cobertura de 98.7\% del corpus con promedio de 50.3 skills por oferta, de las cuales 12.6\% mapean a taxonomía ESCO mientras 87.4\% representan tecnologías emergentes sin estandarización oficial.

La evaluación utilizó métricas de Information Retrieval (Precision, Recall, F1-Score) debido a que la extracción de skills constituye un problema de multi-label retrieval en universo abierto donde la indefinición de True Negatives hace que Accuracy sea engañosa. El mapeo a taxonomía ESCO mejoró drásticamente todas las métricas validando la efectividad del matcher de dos capas para normalizar variantes léxicas. Pipeline A procesó el corpus completo de 30,660 ofertas en aproximadamente 8.3 horas, estableciéndose como baseline de alta cobertura complementado estratégicamente con Pipeline B para enriquecimiento semántico de subconjuntos donde se requiere mayor precisión contextual. Los resultados comparativos detallados se presentan en el Capítulo 7 (Resultados). La especificación técnica completa de Pipeline A, incluyendo arquitectura de componentes, patrones regex por categoría, flujo de integración detallado, estrategias de control de calidad, y metodología de evaluación exhaustiva, se documenta en el Apéndice F.

\subsection{Pipeline B: Modelos de Lenguaje Grandes}

Pipeline B constituye el método de enriquecimiento del observatorio, diseñado para complementar Pipeline A mediante extracción semánticamente consciente de habilidades implícitas, sinónimos contextuales, y competencias inferidas que no aparecen explícitamente mencionadas en el texto. Este pipeline aprovecha las capacidades de comprensión contextual profunda de Large Language Models (LLMs) para interpretar ofertas laborales de manera similar a como lo haría un analista humano.

La arquitectura de Pipeline B se diseñó con dos objetivos complementarios al Pipeline A: aumentar la cobertura de skills implícitas que expresiones regulares no pueden capturar, como inferir tecnologías cloud específicas a partir de descripciones genéricas, y mejorar la precisión mediante comprensión de contexto que reduce falsos positivos al distinguir términos ambiguos según el contexto laboral de la oferta. Sin embargo, dado el mayor costo computacional de LLMs con latencias típicas entre 15 y 25 segundos por oferta versus 0.97 segundos de Pipeline A, se implementó como pipeline de enriquecimiento estratégico aplicado a subconjuntos relevantes del corpus.

\subsubsection{Justificación del Enfoque LLM}

La decisión de implementar un pipeline basado en LLMs se fundamentó en cuatro limitaciones estructurales de Pipeline A que requerían comprensión semántica profunda. Primero, Pipeline A solo detecta skills mencionadas explícitamente mediante patrones léxicos, sin capacidad de inferir ``Git'' cuando una oferta solicita ``experiencia con control de versiones''. Segundo, fragmenta skills compuestas al detectar ``machine'' y ``learning'' como tokens separados en lugar de ``Machine Learning'' como concepto único. Tercero, carece de desambiguación contextual, procesando ``Python'' como tecnología incluso en ofertas de zoológicos que requieren conocimiento del reptil. Cuarto, no captura sinónimos contextuales como la equivalencia entre ``backend development'' y ``server-side programming''.

El enfoque LLM resuelve estas limitaciones mediante tres capacidades clave. La comprensión contextual permite interpretar ofertas considerando la semántica completa del texto en lugar de aplicar patrones sintácticos aislados, lo que habilita inferir ``Docker'' de ``experiencia en contenedorización'' sin requerir match textual directo. La desambiguación semántica utiliza el contexto de la oferta para distinguir tecnologías de homónimos (``Python'' + ``Django'' identifica lenguaje de programación; ``Python'' + ``zoo'' identifica animal). Más importante, los LLMs son capaces de identificar skills emergentes que aún no están codificadas en diccionarios estáticos: tecnologías recientes como ``Claude Code'', ``Cursor IDE'' o ``v0.dev'' son detectables por modelos pre-entrenados con conocimiento actualizado, mientras que Pipeline A requeriría actualización manual de sus expresiones regulares. Esta capacidad de capturar vocabulario emergente del mercado laboral representa la ventaja diferencial más significativa del enfoque LLM.

Los resultados cuantitativos validan esta arquitectura dual. Pipeline B con Gemma 3 4B alcanza 84.26\% F1 Post-ESCO versus 72.53\% de Pipeline A (+11.73pp mejora), extrayendo en promedio 27.8 skills por oferta con mayor relevancia contextual. Sin embargo, el costo computacional es significativamente superior: la mediana de latencia es 18.3 segundos por oferta (percentiles P25-P75: 15-25s), con media de 42.1 segundos inflada por ofertas extensas que contienen múltiples páginas de requisitos. La cuantización INT4 mediante \texttt{bitsandbytes} reduce requisitos de memoria de $\sim$16GB (FP16) a $\sim$4GB (INT4), permitiendo inferencia en GPUs consumer sin dependencia de APIs comerciales. Este balance entre calidad (+11.73pp F1) y latencia (18× más lento que Pipeline A) justifica la aplicación selectiva de Pipeline B a subconjuntos estratégicos del corpus que requieren análisis semántico profundo.

\subsubsection{Selección del Modelo}

Se evaluaron cuatro modelos de lenguaje open-source de tamaño intermedio que cumplían los requisitos de ejecución local con cuantización INT4: Gemma 3 4B Instruct de Google DeepMind, Llama 3.2 3B Instruct de Meta AI, Qwen 2.5 3B Instruct de Alibaba Cloud, y Phi-3.5 Mini Instruct de Microsoft Research. Todos los modelos se ejecutaron con cuantización INT4 mediante \texttt{bitsandbytes}, reduciendo requisitos de memoria de aproximadamente 16GB a 4GB para permitir inferencia local en hardware consumer. La evaluación preliminar sobre 10 ofertas laborales del gold standard analizó tanto métricas cuantitativas como comportamiento cualitativo considerando alucinaciones, consistencia de formato, y relevancia de extracciones.

Gemma 3 4B Instruct fue seleccionado como modelo final por su desempeño superior: alcanzó 46.23\% F1 Pre-ESCO extrayendo promedio de 27.8 skills por oferta, generó JSON válido en 99\% de casos sin alucinaciones sistemáticas, y exhibió latencia mediana de 18.3 segundos por oferta. Los modelos alternativos presentaron limitaciones significativas: Llama 3.2 3B mostró alucinaciones sistemáticas agregando tecnologías irrelevantes, Qwen 2.5 3B generó salidas no estructuradas en 40\% de casos requiriendo parsing manual, y Phi-3.5 Mini tuvo baja cobertura con solo 12.4 skills por oferta. El balance entre calidad de extracción, estabilidad de formato, y ausencia de alucinaciones justificó la selección de Gemma 3 4B como motor de Pipeline B, a pesar de su mayor latencia comparado con alternativas menos confiables.

\subsubsection{Arquitectura del Sistema}

La arquitectura de Pipeline B implementa procesamiento asíncrono con Gemma 3 4B cuantizado INT4 mediante \texttt{bitsandbytes}, permitiendo inferencia local con 4GB de memoria GPU. El flujo ejecuta prompt engineering estructurado que instruye al modelo a extraer skills técnicas en formato JSON, incluyendo tanto menciones explícitas como inferencias contextuales basadas en descripciones de responsabilidades. El sistema implementa validación de salida mediante Pydantic para garantizar schemas consistentes, retry logic con backoff exponencial para manejar fallos transitorios, y logging exhaustivo de latencias y tokens procesados para monitoreo de performance. El procesamiento batch aplica batch size de 1 oferta por inferencia debido a restricciones de memoria, resultando en throughput de aproximadamente 150-200 ofertas por hora en hardware consumer. La especificación técnica completa de Pipeline B, incluyendo arquitectura de componentes, diseño de prompts, estrategias de validación, manejo de errores, y evaluación exhaustiva, se documenta en el Apéndice F.

\subsection{Pipelines Alternativos Evaluados}

Pipeline A.1 basado en TF-IDF + filtrado por noun phrases se implementó como experimento alternativo. Utilizó \texttt{scikit-learn.\allowbreak Tfidf\allowbreak Vectorizer} (\texttt{ngram\_range=(1,3)}, \texttt{max\_\allowbreak features=10000}) extrayendo top-50 n-gramas por oferta, filtrados por part-of-speech con spaCy. Las pruebas sobre 100 ofertas gold standard revelaron limitaciones críticas: 60\% de candidatos eran frases descriptivas no-skills, fragmentación excesiva (``React'' y ``Native'' separados), y F1=11.69\%. Pipeline A.1 se descartó por performance inadecuado versus Pipeline A.

La configuración Regex-Only reutilizó los 548 patrones de Pipeline A eliminando NER, estableciendo baseline determinístico. Sobre el gold standard de 300 ofertas alcanzó F1 Post-ESCO de 79.17\% con precision 86.36\% y recall 73.08\%, procesando en menos de 1ms por oferta. Extrajo promedio de 35.2 skills por oferta versus 50.3 del pipeline combinado, confirmando que NER contribuye aproximadamente 30\% de detecciones adicionales. Los resultados validaron que regex solo proporciona precision superior pero menor recall Pre-ESCO al omitir menciones contextuales que NER captura.

\subsection{Pipeline B: Modelos de Lenguaje Grandes}

Pipeline B implementa extracción semántica mediante LLMs para complementar Pipeline A con comprensión contextual profunda. Se evaluaron cuatro modelos open-source de 3-4B parámetros ejecutables localmente con cuantización INT4: Gemma 3 4B Instruct (Google DeepMind), Llama 3.2 3B Instruct (Meta AI), Qwen 2.5 3B Instruct (Alibaba Cloud), y Phi-3.5 Mini Instruct (Microsoft Research). La Tabla~\ref{tab:llm_comparison} presenta los resultados de la evaluación comparativa sobre 10 ofertas laborales del gold standard.

\begin{table}[htbp]
\centering
\caption{Comparación de modelos LLM para extracción de habilidades}
\label{tab:llm_comparison}
\begin{tabular}{|l|c|c|p{6cm}|}
\hline
\textbf{Modelo} & \textbf{F1 Pre-ESCO} & \textbf{Skills/oferta} & \textbf{Observaciones clave} \\
\hline
Gemma 3 4B & 46.23\% & 27.8 & JSON válido (99\%), sin alucinaciones, latencia mediana 18.3s \\
\hline
Llama 3.2 3B & 39.7\% & --- & Alucinaciones sistemáticas (agregaba TensorFlow/Pandas en ofertas frontend) \\
\hline
Qwen 2.5 3B & 38.9\% & 10-12 & Extracciones conservadoras, alta precisión pero baja cobertura \\
\hline
Phi-3.5 Mini & 35.2\% & --- & Formato inconsistente (40\% no-JSON), requiere parser complejo \\
\hline
\end{tabular}
\end{table}

Como decisión final se seleccionó Gemma 3 4B Instruct como modelo de producción tras evaluar cuatro dimensiones críticas. Primero, alcanzó el mejor desempeño cuantitativo con 46.23\% F1 Pre-ESCO y 84.26\% F1 Post-ESCO, superando a Llama 3.2 por +6.5pp y a Qwen 2.5 por +7.3pp. Segundo, demostró estabilidad de formato con 99\% de respuestas en JSON válido (299/300 ofertas procesadas exitosamente), eliminando la necesidad de parsers heurísticos frágiles como los requeridos por Phi-3.5. Tercero, la revisión manual de 300 ofertas no detectó alucinaciones sistemáticas, contrastando con Llama 3.2 que agregaba skills de Data Science (TensorFlow, Pandas) en ofertas de desarrollo web tradicional sin componente analítico. Cuarto, la latencia mediana de 18.3 segundos por oferta (percentiles P25-P75: 15-25s) permitió procesar el gold standard completo en 3.5 horas, tiempo aceptable para pipelines de enriquecimiento no-interactivos.

Esta decisión prioriza el atributo de calidad de confiabilidad sobre maximización de métricas aisladas. Un pipeline de producción debe generar resultados predecibles sin alucinaciones que contaminen análisis agregados, incluso si esto implica sacrificar mejoras marginales de F1-Score. La arquitectura robusta de Gemma 3 4B (formato estructurado consistente + ausencia de sesgos evidentes) lo posiciona como fundamento confiable para el componente semántico del observatorio.

Pipeline B se implementó como sistema de tres módulos secuenciales: construcción de prompts estructurados con templates especializados, inferencia con Gemma 3 4B cuantizado INT4 generando respuestas JSON validadas mediante Pydantic, y parsing robusto con fallback regex para manejar respuestas mal formadas. El sistema procesa ofertas con latencia mediana de 17.66 segundos alcanzando 99\% de tasa de éxito en parsing JSON, registrando metadatos completos de inferencia y aplicando normalización canónica equivalente a Pipeline A para garantizar comparabilidad. La especificación completa del diseño de prompts, configuración de inferencia, y mecanismos de validación se documenta en el Apéndice F.

El flujo de procesamiento ejecuta seis etapas secuenciales con manejo robusto de errores: carga de ofertas desde PostgreSQL en batches de 50, construcción de prompts con truncamiento automático a 3800 tokens, inferencia secuencial con Gemma 3 4B bajo timeout de 120 segundos capturando excepciones CUDA, parsing con fallback regex, persistencia batch de resultados y metadatos, y logging de métricas agregadas cada 10 ofertas. El sistema procesó 299/300 ofertas del gold standard (99.7\% éxito) con distribución bimodal de latencias: mediana de 17.66 segundos para 80.6\% de casos típicos, media de 45.17 segundos inflada por 47 outliers (15.7\%) con ofertas extensas requiriendo hasta 254 segundos, totalizando 3.5 horas para el corpus completo. Los resultados comparativos de evaluación de Pipeline B versus los demás pipelines se presentan en el Capítulo 7 (Resultados).

\subsection{Pipeline A.1 (TF-IDF) y Regex-Only Baseline}

Pipeline A.1 basado en TF-IDF + filtrado por noun phrases se implementó como experimento alternativo. Utilizó \texttt{scikit-learn.\allowbreak Tfidf\allowbreak Vectorizer} (\texttt{ngram\_range=(1,3)}, \texttt{max\_\allowbreak features=10000}) extrayendo top-50 n-gramas por oferta, filtrados por part-of-speech con spaCy. Las pruebas sobre 100 ofertas gold standard revelaron limitaciones críticas: 60\% de candidatos eran frases descriptivas no-skills, fragmentación excesiva (``React'' y ``Native'' separados), y F1=11.69\%. \textbf{Pipeline A.1 se descartó} por performance inadecuado versus Pipeline A (F1=72.53\%).

La configuración Regex-Only reutilizó los 548 patrones de Pipeline A eliminando NER, estableciendo baseline determinístico. Sobre el gold standard de 300 ofertas alcanzó F1 Post-ESCO de 79.17% (precision 86.36%, recall 73.08%), procesando en $<$1ms/oferta. Extrajo promedio 35.2 skills/oferta versus 50.3 del pipeline combinado, confirmando que NER contribuye ~30\% de detecciones adicionales. Los resultados validaron que regex solo proporciona precision superior (+6.64pp F1 Post-ESCO vs. Pipeline A completo), pero menor recall Pre-ESCO (-6.91pp) al omitir menciones contextuales que NER captura.

\section{Implementación del Sistema de Mapeo a Taxonomía ESCO}

El mapeo de habilidades a taxonomía ESCO constituye una etapa crítica del observatorio, responsable de normalizar las extracciones de Pipeline A y Pipeline B a un vocabulario controlado estándar. Esta normalización es fundamental para garantizar la comparabilidad de resultados entre países, portales y períodos temporales, eliminando la fragmentación causada por variantes sintácticas de la misma habilidad.

Las extracciones crudas de Pipeline A y Pipeline B presentan alta fragmentación léxica debido a variantes ortográficas como las múltiples formas de referirse a React (``React'', ``React.js'', ``ReactJS'', ``react''), abreviaciones inconsistentes como JS versus JavaScript o K8s versus Kubernetes, diferencias idiomáticas entre español e inglés como ``Base de datos'' versus ``Database'' o ``Aprendizaje automático'' versus ``Machine Learning'', y niveles variables de especificidad como SQL versus PostgreSQL versus PostgreSQL 15. Sin normalización, estas variantes se tratarían como habilidades distintas en el análisis de clustering y tendencias, fragmentando artificialmente los resultados y degradando la calidad de las visualizaciones.

El sistema de mapeo a ESCO cumple tres funciones complementarias. Primero, realiza normalización léxica mapeando todas las variantes de una habilidad a un URI canónico ESCO único, consolidando por ejemplo las variantes ``React'', ``React.js'' y ``ReactJS'' bajo un mismo identificador \texttt{http://data.europa.eu/esco/skill/abc123}. Segundo, provee enriquecimiento semántico asociando a cada skill sus etiquetas preferidas bilingües, descripciones y relaciones jerárquicas, lo que permite análisis en español e inglés sin duplicar datos y habilita la exploración de jerarquías conceptuales como la relación de ``JavaScript'' con su categoría padre ``Programming Languages''. Tercero, facilita la identificación de skills emergentes detectando habilidades sin match en ESCO como señal de tecnologías nuevas, ejemplificado por tecnologías recientes como ``ChatGPT'', ``Tailwind CSS'' o ``Terraform'' que no están presentes en ESCO v1.1.0 publicada en 2022.

El sistema opera sobre una versión extendida de ESCO v1.1.0 que incorpora 13,939 habilidades oficiales de la Comisión Europea (98.1\% del total), complementadas con 152 habilidades técnicas de O*NET Skills del U.S. Department of Labor no cubiertas por ESCO (1.1\%), y 124 tecnologías modernas agregadas manualmente tras análisis exploratorio (0.9\%) que incluyen frameworks modernos como Next.js y Remix, herramientas DevOps como Terraform y ArgoCD, y tecnologías de AI/ML como LangChain y Prompt Engineering, totalizando 14,215 habilidades en la taxonomía extendida.

La implementación del matcher ESCO enfrentó dos desafíos técnicos principales. El primero fue balancear similitud ortográfica versus falsos positivos en fuzzy string matching, ejemplificado por casos problemáticos como ``Piano'' mapeando incorrectamente a ``tocar el piano''. El segundo fue la inadecuación de embeddings semánticos generalistas que producen matches incorrectos en vocabulario técnico, como ``Docker'' mapeando a ``Facebook''. El sistema final opera con arquitectura de tres capas secuenciales (exact match, fuzzy match, semantic match), con Layer 3 deshabilitada post-evaluación debido a limitaciones identificadas en embeddings multilingües generalistas para dominio técnico.

\subsection{Arquitectura ESCO\allowbreak Matcher\allowbreak 3Layers}

El sistema se diseñó como matcher de tres capas secuenciales con fallback en cascada: Layer 1 ejecuta matching exacto contra labels preferidos bilingües; Layer 2 aplica fuzzy string matching con umbral 0.92; y Layer 3 utiliza embeddings semánticos (posteriormente deshabilitado). Cada capa opera independientemente, retornando el match de la primera que genera resultado, priorizando precisión sobre recall.

La taxonomía se cargó desde \texttt{esco\_skills} con campos: \texttt{skill\_uri}, \texttt{preferred\_\allowbreak label\_en/es}, \texttt{alternative\_\allowbreak labels\_en/es}, \texttt{skill\_type}, y \texttt{description}. Se construyeron tres índices in-memory: (1) \textit{Exact index} como diccionario mapeando normalized labels a URIs (~42,000 entradas); (2) \textit{Fuzzy index} como lista ordenada de tuplas (label, URI); y (3) \textit{Semantic index} como matriz numpy 14,215×1024 de embeddings pre-computados. Los índices se cargaron al inicio, reduciendo latencia de ~800ms/skill a ~15ms/skill.

\subsection{Layer 1 y 2: Matching Exacto y Fuzzy}

Layer 1 implementó matching exacto case-insensitive contra labels bilingües. La normalización unificada aplicó: lowercase, eliminación de acentos (\texttt{unicodedata.normalize}), eliminación de puntuación preservando guiones/puntos internos (``Node.js''), y colapso de espacios. El lookup directo en \texttt{exact\_index} alcanzó 35-40\% de cobertura en Pipeline A y 40-45\% en Pipeline B, reflejando que LLMs generan ortografía más estandarizada.

Layer 2 aplicó fuzzy matching usando \texttt{fuzzywuzzy} con distancia de Levenshtein. La implementación inicial con \texttt{fuzz.partial\_ratio()} produjo falsos positivos críticos: ``Piano'' mapeó a ``tocar el piano'' (100\%, substring exacto), ``SQL'' a ``MySQL'' (100\%). Se reemplazó por \texttt{fuzz.ratio()} (similitud entre strings completos), reduciendo ``Piano'' vs ``tocar el piano'' a 40\% y ``SQL'' vs ``MySQL'' a 60\%. El umbral se configuró empíricamente en 0.92 tras evaluar 200 matches manuales: 0.85 generaba falsos positivos (``Java'' → ``JavaScript''), mientras 0.95 requería ortografía perfecta eliminando abreviaciones válidas (``K8s'' vs ``Kubernetes'' = 0.93).

La optimización implementó early stopping: al encontrar match con score $\geq$ 0.98, se detuvo la búsqueda. Esto redujo tiempo de ~450ms/skill (búsqueda exhaustiva) a ~85ms/skill (early stopping en ~18\% casos). Layer 2 incrementó cobertura en ~25-30\% adicional, mapeando variantes ortográficas, abreviaciones expandidas, y nombres con guiones inconsistentes. Sin embargo, abreviaciones extremas fallaron: ``AWS'' vs ``Amazon Web Services'' score 0.42, ``GCP'' vs ``Google Cloud Platform'' 0.35, ``ML'' vs ``Machine Learning'' 0.40.

\subsection{Layer 3: Embeddings Semánticos (Deshabilitado)}

Layer 3 implementó matching semántico con el modelo \path{paraphrase-multilingual-\_\allowbreak mpnet-base-v2} transformando skills a vectores de 768 dimensiones. Se pre-computaron embeddings para 14,215 labels ESCO, normalizados a vectores unitarios. Para cada skill sin match en Layers 1-2, se calculó similitud coseno contra matriz ESCO vía producto punto, retornando match si similitud $>$0.75 (~120ms/skill).

Las pruebas revelaron que embeddings multilingües generalistas producían matches incorrectos en contexto técnico: ``Docker'' mapeó a ``Facebook'' (similitud 0.82), ``REST'' a ``sleep'' (0.79), ``Python'' a ``snake programming'' (0.76). El análisis determinó que modelos pre-entrenados en corpus generales (Wikipedia, CommonCrawl) capturan asociaciones semánticas de dominio general pero no técnicas especializadas. Corregir esto requeriría fine-tuning en corpus tech-específico (Stack Overflow, GitHub) con ~50,000+ ejemplos anotados, excediendo scope del proyecto. Layer 3 se deshabilitó completamente.

El sistema final operó con Layers 1-2 (exact + fuzzy), alcanzando match rate de 12.6\% sobre el gold standard de 300 ofertas (1,038 de 8,268 skills extraídas). Se experimentó con un ESCO Matcher Enhanced que incorporaba matching más agresivo con \texttt{partial\_ratio} y reglas adicionales, logrando aumentar cobertura a ~25\%. Sin embargo, análisis cualitativo reveló incremento en falsos positivos (e.g., ``Europa'' $\to$ ``neuropatología'', ``Oferta'' $\to$ ``ofertas de empleo''), introduciendo sesgo no deseado. Se decidió no implementar esta versión enhanced, preservando el matcher conservador de 2 capas que prioriza precisión sobre recall. El match rate de 12.6\% refleja la naturaleza del mercado tech LATAM: aunque ESCO v1.1.0 incluye actualizaciones hasta 2023, la taxonomía europea no cubre completamente frameworks modernos emergentes (Next.js, Tailwind CSS, shadcn/ui) ni herramientas específicas de ecosistemas recientes (Vite, Bun, Astro), que representan 87.4\% de skills extraídas como emergentes sin mapeo ESCO. Estas skills emergentes se preservan en formato normalizado para análisis de tecnologías no estandarizadas y clustering Pre-ESCO.

El mapper se integró como etapa 5 del orquestador, procesando skills desde \texttt{extracted\_\allowbreak skills}, \texttt{enhanced\_\allowbreak skills}, y \texttt{gold\_\allowbreak standard\_\allowbreak annotations}. El procesamiento batch de ~15,000 skills únicas tomó ~6.5 minutos (~26ms/skill) aprovechando memoización: caché \texttt{skill\_text} $\to$ \texttt{esco\_uri} evitó remapear skills repetidas. Skills populares (``JavaScript'' en 5,000+ ofertas) se mapearon una vez, reduciendo tiempo de ~6.5h (sin caché) a ~6.5min (60× aceleración).

\section{Implementación del Sistema de Clustering de Habilidades}

El sistema de clustering de habilidades constituye un componente analítico central del observatorio, diseñado para descubrir familias semánticas de skills sin categorías predefinidas, analizar evolución temporal de perfiles tecnológicos, y detectar tecnologías emergentes mediante análisis no supervisado. Este sistema permite caracterizar la demanda laboral más allá de conteos agregados de skills individuales, revelando combinaciones coherentes de habilidades que definen roles profesionales reales en el mercado.

La arquitectura del clustering integra tres componentes complementarios que transforman texto de skills en agrupaciones semánticas interpretables, el primero siendo embeddings semánticos que capturan similitud entre skills en espacio vectorial de 768 dimensiones, el segundo reducción dimensional mediante UMAP que proyecta vectores de alta dimensión a espacio 2D preservando estructura local y global, y por ultimo clustering density-based con HDBSCAN que identifica automáticamente agrupaciones densas sin especificar número de clusters a priori.

El sistema se ejecutó en dos escenarios complementarios: Pre-ESCO que analiza texto normalizado de skills tal como fueron extraídas (preservando tecnologías emergentes sin mapeo ESCO), y Post-ESCO que opera sobre URIs estandarizados de taxonomía ESCO (consolidando variantes ortográficas para mayor coherencia). Esta dualidad permite balancear cobertura de tecnologías emergentes (Pre-ESCO) con interpretabilidad de resultados (Post-ESCO).

\subsection{Justificación del Enfoque de Clustering No Supervisado}

La decisión de implementar clustering no supervisado en lugar de categorización supervisada se fundamentó en las características dinámicas del mercado laboral tecnológico y las limitaciones de taxonomías predefinidas:

Los enfoques supervisados presentan limitaciones significativas que el clustering no supervisado resuelve. Las taxonomías tradicionales como O*NET SOC codes se actualizan cada 5-10 años y no capturan roles emergentes como AI/ML Engineer o DevOps Engineer, evidenciando obsolescencia de categorías predefinidas. Las jerarquías estáticas no reflejan el solapamiento natural de perfiles como Full-Stack Developer que combina competencias Backend y Frontend. La anotación manual de 30,660 ofertas requeriría 400-500 horas de trabajo especializado, representando un costo prohibitivo de supervisión manual. Adicionalmente, la categorización manual depende de interpretación subjetiva de roles laborales, introduciendo sesgo de anotadores que compromete la objetividad del análisis.

El enfoque no supervisado ofrece ventajas significativas para análisis de demanda laboral. Los datos revelan naturalmente agrupaciones mediante descubrimiento automático de patrones sin hipótesis a priori sobre roles existentes. La arquitectura se adapta a evolución temporal permitiendo que nuevos clusters emerjan automáticamente al procesar datos recientes, como el perfil ``Modern Frontend'' post-2022. HDBSCAN proporciona granularidad adaptativa con clusters de tamaños variables que capturan tanto roles mainstream como nichos especializados. El sistema identifica automáticamente skills atípicas o errores de extracción como outliers clasificados como ruido. Finalmente, la escalabilidad del enfoque permite agregar nuevas ofertas al corpus sin requerir re-entrenamiento supervisado.

La selección de componentes tecnológicos se fundamentó en criterios específicos del dominio. E5 Multilingual Embeddings se seleccionó por su soporte bilingüe español/inglés crítico para corpus LATAM, performance comprobado en benchmarks de similitud semántica, y tamaño intermedio de 278M parámetros que balancea calidad versus costo computacional. UMAP se eligió sobre t-SNE y PCA por su capacidad de preservar simultáneamente estructura local y global, ofrecer complejidad algorítmica favorable O(n log n) versus O(n²) de t-SNE, y garantizar proyecciones deterministas reproducibles. HDBSCAN se prefirió sobre K-Means por detectar automáticamente el número óptimo de clusters sin hiperparámetro k, identificar outliers como ruido en lugar de forzar asignación, y manejar clusters de formas arbitrarias sin asumir esfericidad.

Esta arquitectura responde a cuatro atributos de calidad del sistema. La adaptabilidad permite que el clustering no supervisado evolucione con el mercado laboral sin requerir actualización manual de categorías. La interpretabilidad se logra mediante proyecciones UMAP 2D que permiten visualización intuitiva de 768 dimensiones facilitando inspección manual de coherencia semántica. La escalabilidad con complejidad O(n log n) permite procesar el corpus completo de 30,660 ofertas en menos de 5 minutos. La reproducibilidad se garantiza mediante proyecciones deterministas con \texttt{random\_state} que aseguran resultados consistentes entre ejecuciones.

\subsection{Generación de Embeddings y Reducción UMAP}

El modelo \path{intfloat/multilingual-e5-base} transformó skills a vectores de 768 dimensiones capturando similitud semántica. Se seleccionó por su soporte multilingüe nativo para español e inglés, tamaño intermedio de 278M parámetros que balancea expresividad versus costo computacional, y desempeño comprobado en benchmarks de similitud semántica textual. El proceso operó en dos modos: Pre-ESCO embebió texto normalizado de 6,413 skills extraídas aplicando filtro de frecuencia mínima que redujo el corpus a 1,314 embeddings con densidad suficiente para clustering, mientras Post-ESCO embebió preferred labels ESCO resultando en 289 embeddings consolidados para el gold standard de 300 ofertas. Se aplicó prefijo ``query:'' según especificaciones del modelo E5. La generación batch procesó skills en lotes de 256 documentos en hardware consumer. Los embeddings se normalizaron a vectores unitarios y almacenaron en base de datos PostgreSQL con extensión pgvector para consultas de similitud eficientes.

UMAP (Uniform Manifold Approximation and Projection) redujo embeddings de 768D a 2D para visualización y clustering. Se seleccionó sobre t-SNE y PCA por su capacidad de preservar simultáneamente estructura local y global, ofrecer escalabilidad algorítmica favorable, y garantizar reproducibilidad determinista con semillas fijas. La configuración involucró el parámetro \texttt{n\_neighbors} para balancear estructura local versus global, \texttt{min\_dist=0.1} para controlar separación mínima entre puntos, \texttt{n\_components=2} para proyección bidimensional, y \texttt{metric=``cosine''} como medida de similitud. El grid search sobre \texttt{n\_neighbors} determinó que el valor 15 ofrecía el mejor balance al preservar agrupaciones semánticas coherentes mientras mantenía separación entre dominios mayores. La proyección UMAP de 1,314 skills completó en aproximadamente 5 segundos en CPU.

\subsection{Clustering HDBSCAN y Optimización}

HDBSCAN (Hierarchical Density-Based Spatial Clustering) identificó clusters sobre proyecciones UMAP 2D sin especificar número predefinido. Se seleccionó sobre K-Means por su capacidad de detectar automáticamente el número óptimo de clusters, identificar outliers como ruido, formar clusters de geometría arbitraria, y proporcionar jerarquía interpretable mediante dendrogramas. La configuración involucró parámetros \texttt{min\_cluster\_size} para controlar granularidad, \texttt{min\_samples} para robustez, y \texttt{metric=``euclidean''} como medida de distancia.

El grid search sobre \texttt{min\_cluster\_size} $\in \{5, 7, 10, 12, 15, 20\}$ evaluó múltiples criterios: número de clusters idealmente entre 50 y 200, porcentaje de ruido inferior al 25\%, Silhouette Score superior a 0.4 para datos Post-ESCO, e interpretabilidad manual de los grupos resultantes. Los experimentos revelaron un trade-off fundamental donde configuraciones bajas generaban más de 100 clusters muy específicos con alta fragmentación y Silhouette alrededor de 0.35-0.40, mientras que configuraciones altas producían pocos clusters gruesos con Silhouette entre 0.55-0.65 pero pérdida de granularidad útil para análisis práctico.

El análisis comparativo identificó UMAP n\_neighbors=15 + HDBSCAN min\_cluster\_size=12 como configuración óptima balanceando granularidad vs coherencia. Esta configuración operó sobre el gold standard de 300 ofertas, generando clustering diferenciado por pipeline: Pipeline A 300 Post-ESCO produjo 7 clusters sobre 289 skills únicas consolidadas (16.3\% ruido, Silhouette=0.398), Pipeline B 300 Post-ESCO generó 50 clusters sobre 1,618 skills (16.5\% ruido, Silhouette=0.348). Adicionalmente, el corpus completo de Pipeline A (30k ofertas) generó 53 clusters sobre 1,698 skills únicas ESCO (22.3\% ruido, Silhouette=0.456), demostrando escalabilidad del sistema a corpus de producción. La inspección manual confirmó coherencia semántica en top clusters: JavaScript/React ecosystem, Python/Data Science, Project Management, Cloud/DevOps (AWS/GCP), SQL/Databases.

\subsection{Comparación Pre-ESCO vs Post-ESCO}

El clustering se ejecutó en dos escenarios evaluando impacto del mapeo ESCO. Pre-ESCO operó sobre texto normalizado de skills sin consolidación: Pipeline A 300 generó 38 clusters sobre 1,314 skills (Silhouette=0.447, 25.7\% ruido), Pipeline B 300 produjo 34 clusters sobre 1,540 skills (Silhouette=0.234, 12.8\% ruido). La alta fragmentación se debe a variantes ortográficas formando micro-clusters separados: ``docker'', ``Docker'', ``docker-compose'' aparecen como puntos distintos en el espacio de embeddings, diluyendo densidad de clusters. El beneficio de Pre-ESCO es que captura skills emergentes sin mapeo ESCO (``ChatGPT'', ``Tailwind CSS'', ``Bun'') preservándolas en el análisis.

Post-ESCO procesó URIs ESCO consolidados: Pipeline A 300 generó 7 clusters sobre 289 skills (Silhouette=0.398, 16.3\% ruido), Pipeline B 300 produjo 50 clusters sobre 1,618 skills (Silhouette=0.348, 16.5\% ruido). La consolidación colapsa variantes ortográficas en puntos únicos fortaleciendo densidad de clusters, pero la baja cobertura ESCO (12.6\% Pipeline A, ~25\% Pipeline B) significa pérdida significativa de información: 87.4\% de skills emergentes desaparecen del análisis Post-ESCO. Los top clusters mostraron composición interpretable: JavaScript ecosystem, Python/Data Science, Cloud/DevOps, SQL/Databases. Se implementó análisis híbrido: clustering Post-ESCO para métricas cuantitativas sobre skills estandarizadas, complementado con análisis Pre-ESCO para tecnologías emergentes ausentes en taxonomía ESCO.

\subsection{Análisis Temporal}

Como extensión, se implementó módulo de análisis temporal para rastrear evolución de clusters sobre 21,839 ofertas fechadas (71.23\% del dataset), abarcando 29 trimestres desde Q4-2018 hasta Q4-2025. Sin embargo, la distribución temporal presenta alta concentración: 97.1\% de ofertas (21,216) corresponden a Q4-2025, reflejando el período intensivo de scraping reciente. Los trimestres anteriores (Q4-2018 a Q3-2025) contienen solo 623 ofertas dispersas, limitando análisis longitudinal robusto.

El módulo implementado permite análisis temporal mediante: (1) agrupación de ofertas por trimestre, (2) extracción de skills por período, (3) generación de embeddings E5, (4) proyección UMAP (\texttt{n\_neighbors=15}), (5) clustering HDBSCAN (\texttt{min\_cluster\_size=12}), y (6) tracking de consistencia de clusters entre períodos consecutivos (threshold: $\geq$60\% overlap en top-20 skills). El sistema genera visualizaciones temporales (heatmaps clusters × quarters, line charts de frecuencia) que permitirían identificar patrones de adopción tecnológica cuando se disponga de datos distribuidos temporalmente. La infraestructura está lista para análisis longitudinal futuro conforme el observatorio acumule ofertas distribuidas equitativamente a través de trimestres.

\subsection{Experimentación de Hiperparámetros y Trade-off Interpretabilidad vs. Métricas}

La optimización de UMAP+HDBSCAN requirió balancear métricas cuantitativas de calidad de clustering (Silhouette Score, Davies-Bouldin Index) con interpretabilidad práctica de los clusters resultantes para análisis del mercado laboral. Este balance no es trivial: configuraciones que maximizan métricas matemáticas frecuentemente producen clusterings inútiles para análisis humano, revelando una tensión fundamental entre optimización algorítmica y utilidad práctica.

Se realizaron 70+ experimentos documentados variando hiperparámetros UMAP (\texttt{n\_neighbors} $\in$ \{5, 10, 12, 15, 20, 30\}, \texttt{min\_dist} $\in$ \{0.05, 0.08, 0.1, 0.2\}) y HDBSCAN (\texttt{min\_cluster\_size} $\in$ \{2, 3, 4, 5, 8, 10, 12, 15, 20\}, \texttt{min\_samples} $\in$ \{1, 2, 3, 4, 5\}). Los experimentos documentaron el fenómeno del ``clustering cliff'': configuraciones con \texttt{min\_cluster\_size} $\leq$ 6 generaron 100-300 clusters con métricas excelentes (Silhouette $>$ 0.6, Davies-Bouldin $<$ 0.5) pero imposibles de interpretar manualmente; configuraciones con \texttt{min\_cluster\_size} $\geq$ 15 colapsaron a 2-10 clusters genéricos con baja utilidad analítica. El documento de pruebas (Capítulo 13) detalla la evaluación exhaustiva de configuraciones.

\textbf{Caso ilustrativo del problema (Experimento 8 vs. Experimento 15)}:

El experimento 8 con hiperparámetros finos (\texttt{n\_neighbors=5}, \texttt{min\_cluster\_size=3}) produjo 305 clusters con Silhouette Score = 0.618 (excelente según literatura), Davies-Bouldin = 0.439 (óptimo), y 16.2\% ruido. Sin embargo, la inspección manual reveló que los 305 clusters eran ininterpretables: ``Python+Flask'' formó un cluster separado de ``Python+Django'', ``JavaScript+React'' separado de ``JavaScript+Vue'', fragmentando artificialmente tecnologías relacionadas. Nombrar, categorizar y analizar 305 clusters excede la capacidad de procesamiento humano.

En contraste, el experimento 15 con hiperparámetros medios (\texttt{n\_neighbors=15}, \texttt{min\_cluster\_size=12}) generó 50 clusters con Silhouette Score = 0.348 (inferior al experimento 8), Davies-Bouldin = 0.687 (mayor pero aceptable), y 16.5\% ruido, pero 98\% de clusters (49/50) semánticamente coherentes e interpretables. Los clusters agruparon familias tecnológicas completas: ``Backend Python'' (Flask, Django, FastAPI, Celery), ``Frontend JavaScript'' (React, Vue, Angular, TypeScript), ``DevOps'' (Docker, Kubernetes, Jenkins, GitLab CI). Esta granularidad permitió análisis sistemático de 50 perfiles vs. imposibilidad de manejar 305.

Para balancear criterios se implementó función de scoring multi-criterio ponderando: granularidad (40\% del score, penalizando $<$30 o $>$200 clusters), Silhouette Score (30\%, recompensando $>$0.3), porcentaje de ruido (20\%, penalizando $>$25\%), e interpretabilidad manual (10\%, evaluada mediante inspección de top-10 clusters por coherencia temática). Este sistema formalizó la decisión de priorizar utilidad práctica sobre optimización matemática, justificando académicamente la selección de configuraciones con métricas numéricas moderadas pero alta interpretabilidad.

La configuración óptima seleccionada (\texttt{n\_neighbors=15}, \texttt{min\_cluster\_size=12}, \texttt{min\_samples=3}) representa el punto de equilibrio: genera 50-60 clusters interpretables por humanos, mantiene Silhouette $>$ 0.35 (aceptable), y limita ruido a 15-20\%. Esta decisión es consistente con investigaciones previas en clustering de dominios especializados, donde interpretabilidad del resultado es tan crítica como calidad métrica del agrupamiento. Los resultados cuantitativos de las configuraciones de clustering ejecutadas se presentan en el Capítulo 7 (Resultados).

El sistema de clustering implementado proporciona tres capacidades principales para el observatorio. Primero, descubre automáticamente 34-53 familias semánticas de skills según pipeline y configuración, revelando perfiles tecnológicos emergentes sin categorías predefinidas y capturando tanto ecosistemas amplios (JavaScript/React con 40-60 skills) como nichos especializados (DevOps tools, librerías ML). La granularidad adaptativa de HDBSCAN y detección de outliers (12-25\% ruido) actúan como mecanismo de filtrado de calidad sin supervisión manual. Segundo, las proyecciones UMAP 2D preservan topología semántica de 768 dimensiones permitiendo visualizaciones interpretables con reproducibilidad completa vía \texttt{random\_state}, crítico para documentación científica. Tercero, procesa el corpus completo (30,660 ofertas → 1,314 skills) en menos de 30 segundos con complejidad O(n log n), eliminando 400-500 horas de anotación manual y soportando actualización incremental sin reentrenar modelos.

El módulo de análisis temporal implementado permite rastrear evolución de clusters generando heatmaps y gráficos de demanda, pero enfrenta limitación actual: 93.5\% de datos se concentran en Q4-2025, con solo 5 quarters representados e insuficiente distribución histórica para análisis longitudinal robusto. La infraestructura está preparada para análisis temporal cuando el observatorio acumule datos distribuidos equitativamente mediante scraping continuo. El sistema caracteriza demanda laboral tecnológica en LATAM identificando composición de perfiles (JavaScript/React frontend, Python/Data Science analytics, Cloud/DevOps infrastructure) y detectando skills emergentes: 87.4\% de extracciones no mapean a ESCO v1.1.0, señalando tecnologías ganando tracción (Next.js, Tailwind CSS, Terraform) no formalizadas en estándares europeos.

\section{Creación del Gold Standard y Sistema de Evaluación}

Esta sección describe la construcción del dataset de referencia de 300 ofertas anotadas manualmente y el sistema de evaluación dual (Pre-ESCO y Post-ESCO) para comparar pipelines.

\subsection{Selección y Anotación del Gold Standard}

La evaluación rigurosa de los pipelines de extracción requirió construir un dataset de referencia de 300 ofertas laborales manualmente anotadas. Este gold standard debía balancear tres criterios fundamentales: representatividad del mercado laboral tecnológico latinoamericano, diversidad geográfica e idiomática, y viabilidad práctica de anotación manual exhaustiva. La construcción involucró un proceso iterativo de selección algorítmica y refinamiento manual que garantizó la calidad y coherencia del dataset final.

\subsubsection{Algoritmo de Selección Estratificada}

El algoritmo de selección operó mediante cuatro fases secuenciales automatizadas. La primera fase aplicó detección de idioma mediante patrones regex sobre el corpus completo de 56,555 ofertas scrapeadas, clasificándolas en español, inglés o contenido mixto. La segunda fase ejecutó pre-selección con filtros SQL estrictos que aplicaron restricciones de longitud mínima (1,200+ caracteres), inclusión de títulos técnicos (``developer'', ``engineer'', ``programador''), y exclusión explícita de roles no-software (``manager'', ``mechanical engineer'', ``cajero'', ``manufactura''), resultando en 7,102 candidatos tras deduplicación.

La tercera fase calculó un quality score de 0-100 para cada candidato, ponderando longitud de descripción, presencia de keywords técnicas, existencia de sección de requisitos estructurada, y penalizaciones por ruido HTML residual. Paralelamente se ejecutó clasificación automatizada de rol profesional en 8 categorías (Backend, Frontend, DevOps, Data Science, QA, Mobile, Fullstack, Security) y nivel de seniority (Junior, Mid, Senior) mediante análisis de título y descripción. La cuarta fase realizó selección estratificada estableciendo targets por país×idioma que reflejaran la distribución del mercado LATAM, ordenando candidatos por quality score descendente dentro de cada celda y seleccionando los top-ranked hasta completar los targets establecidos.

\subsubsection{Refinamiento Iterativo y Control de Calidad}

El proceso de refinamiento requirió 7 iteraciones para eliminar ofertas problemáticas detectadas mediante validación manual progresiva. Las iteraciones 4-6 removieron 76 ofertas mediante heurísticas automatizadas que identificaron duplicados con títulos repetidos, roles de ingeniería no-software (manufacturing, petroleum, químico, eléctrico), posiciones de sales/business development mal clasificadas, y roles ERP especializados fuera del alcance del estudio. Cada oferta removida se reemplazó con candidatos que pasaron filtros ultra-estrictos verificando exclusividad de desarrollo de software. La iteración 7, ejecutada durante el proceso de anotación manual, detectó 5 duplicados finales no capturados por hashes automatizados debido a similitud semántica alta (79.8\% overlap de vocabulario) pero Job IDs técnicamente distintos, los cuales fueron reemplazados inmediatamente para preservar la independencia estadística del dataset.

\subsubsection{Características del Dataset Final}

El dataset final de 300 ofertas presenta las siguientes características verificadas mediante queries SQL y análisis del archivo de anotaciones. La calidad se garantizó con 300 Job IDs únicos sin duplicados, 299 títulos únicos (solo un caso de ``DevOps Engineer'' duplicado con contenido diferente), y 100\% de roles verificados como desarrollo de software puro. La distribución geográfica alcanzó Colombia 40.7\%, México 37.7\%, y Argentina 21.7\%, cercana a los targets estratificados. La distribución idiomática fue español 80.7\% e inglés 19.3\%, reflejando el predominio del español en ofertas técnicas latinoamericanas.

La Tabla~\ref{tab:gold_standard_distribution} presenta la distribución de roles profesionales y niveles de seniority del dataset final:

\begin{table}[htbp]
\centering
\caption{Distribución de roles y seniority en gold standard (300 ofertas)}
\label{tab:gold_standard_distribution}
\begin{tabular}{lc|lc}
\hline
\textbf{Rol Profesional} & \textbf{\%} & \textbf{Nivel Seniority} & \textbf{\%} \\
\hline
Backend Developer & 34.3 & Senior & 54.0 \\
QA Engineer & 14.7 & Mid-level & 40.0 \\
Frontend Developer & 13.7 & Junior & 6.0 \\
DevOps Engineer & 12.3 & & \\
Data Scientist & 9.3 & & \\
Mobile Developer & 7.0 & & \\
Fullstack Developer & 4.7 & & \\
Security Engineer & 4.0 & & \\
\hline
\end{tabular}
\end{table}

El contenido textual presentó longitud promedio de 527 palabras por oferta (mediana 489, rango 119-2,447), reflejando variabilidad real del mercado desde descripciones concisas hasta especificaciones técnicas extensas.

\subsubsection{Protocolo de Anotación Manual}

Un único anotador con formación técnica en Ingeniería de Sistemas ejecutó la identificación manual de skills siguiendo un protocolo de formato atómico estricto. El protocolo requirió lectura exhaustiva de cada oferta, extracción de términos técnicos individuales sin construcciones compuestas, y clasificación en hard skills (tecnologías, lenguajes, frameworks, herramientas, metodologías) y soft skills (competencias transversales como comunicación, liderazgo, trabajo en equipo). El formato atómico prohibió construcciones compuestas como ``Python (pandas, numpy)'' o narrativas como ``Conocimientos de AWS y Azure'', exigiendo términos separados: ``Python'', ``Pandas'', ``NumPy'', ``AWS'', ``Azure''. Esta decisión de diseño simplificó la comparación automatizada mediante operaciones de conjuntos contra las extracciones de Pipeline A y Pipeline B.

El proceso de anotación completó las 300 ofertas produciendo 7,848 skills totales distribuidas en 6,174 hard skills (78.7\%) y 1,674 soft skills (21.3\%), con promedio de 26.2 skills por oferta. Aunque no se realizó medición de inter-annotator agreement al ser un único anotador, la validez del gold standard se garantizó mediante tres mecanismos: protocolo de anotación estricto documentado, ejemplos de calibración inicial sobre 15 ofertas piloto, y verificación post-anotación de consistencia de formato mediante scripts automatizados. El Apéndice B presenta ejemplos representativos de anotaciones ilustrando la diversidad geográfica e idiomática del dataset.

\subsection{Sistema de Evaluación Dual: Pre-ESCO y Post-ESCO}

El sistema implementó dos comparaciones independientes cuantificando capacidades complementarias: \textit{Pre-ESCO} evalúa capacidad de extracción pura comparando texto normalizado sin mapeo taxonómico, capturando skills emergentes ausentes en ESCO; \textit{Post-ESCO} evalúa capacidad de estandarización comparando URIs ESCO tras mapear todas las skills (gold standard y pipelines) con el mismo código \texttt{ESCO\allowbreak Matcher\allowbreak 3Layers}, eliminando sesgos ortográficos.

El componente Pre-ESCO utilizó módulo de normalización canónica con diccionario de 200+ tecnologías mapeando variantes a formas estándar (``js''/``javascript'' → ``JavaScript'', ``k8s'' → ``Kubernetes''). Las métricas se calcularon mediante operaciones de conjuntos: para cada job, se compararon skills gold normalizadas vs skills pipeline normalizadas, identificando True Positives (TP = intersección), False Positives (FP = predichas no en gold), y False Negatives (FN = en gold no predichas). Los valores agregados sobre 300 ofertas alimentaron fórmulas: Precision = TP/(TP+FP), Recall = TP/(TP+FN), F1 = 2×(P×R)/(P+R).

El componente Post-ESCO remapeó todas las skills usando \texttt{ESCO\allowbreak Matcher\allowbreak 3Layers} para garantizar fairness: Pipeline A se ignoró y remapeó desde texto normalizado igual que Pipeline B. Este diseño eliminó ventajas artificiales asegurando que diferencias Post-ESCO reflejaran calidad de extracción textual. Skills sin match ESCO se descartaron de la comparación Post-ESCO, cuantificándose separadamente como ``Skills Emergentes'' para análisis cualitativo.

Las 300 ofertas se procesaron por todos los pipelines: Pipeline A (NER+Regex completo), Pipeline A Regex-Only, Pipeline B con 4 LLMs (Gemma, Llama, Qwen, Phi), y Pipeline A.1 (TF-IDF, descartado por F1$<$12\%). Los outputs se almacenaron en tablas dedicadas facilitando queries de evaluación mediante joins con \texttt{gold\_\allowbreak standard\_\allowbreak annotations}.
