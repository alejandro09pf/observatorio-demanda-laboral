\chapter{METODOLOGÍA}

El desarrollo de este proyecto requirió la adopción de una metodología que permitiera gestionar eficazmente la complejidad inherente a un observatorio automatizado de demanda laboral basado en técnicas de inteligencia artificial. Se adoptó un enfoque metodológico híbrido que combinó la metodología CRISP-DM (Cross-Industry Standard Process for Data Mining) con principios de desarrollo modular iterativo. CRISP-DM proporcionó la estructura de seis fases secuenciales (comprensión del negocio, comprensión de los datos, preparación de los datos, modelado, evaluación, despliegue), mientras que el desarrollo modular permitió construir, probar y refinar cada componente de forma aislada hasta obtener resultados útiles como entrada para la fase siguiente.

\section{Selección de la Metodología}

La selección de CRISP-DM como marco metodológico principal se fundamentó en tres razones. Primero, CRISP-DM es el estándar de facto en proyectos de ciencia de datos y minería de datos, proporcionando un proceso estructurado que se alineó naturalmente con el flujo de trabajo del observatorio. Segundo, la metodología es explícitamente iterativa, permitiendo ciclos de refinamiento basados en evaluaciones empíricas, característica fundamental dado que el proyecto requirió múltiples iteraciones experimentales para alcanzar los objetivos de rendimiento establecidos. Tercero, CRISP-DM es independiente de la industria y adaptable a diferentes contextos, permitiendo su aplicación tanto al desarrollo de modelos de extracción de habilidades como al diseño de arquitectura de software.

Sin embargo, CRISP-DM no provee lineamientos específicos para la implementación iterativa de software de componentes especializados. Por esta razón, se complementó con principios de desarrollo modular iterativo, donde cada componente se desarrolló mediante ciclos repetidos de implementación, prueba, evaluación y refinamiento hasta obtener salidas de calidad suficiente para alimentar el componente siguiente. Esta estrategia redujo la complejidad del debugging y facilitó la reproducibilidad de experimentos, permitiendo que cada módulo evolucionara independientemente hasta alcanzar criterios de aceptación definidos antes de integrarlo al pipeline completo.

\section{Fases de CRISP-DM Aplicadas al Proyecto}

\subsection{Fase 1: Comprensión del Negocio}

Esta fase inicial se enfocó en definir los objetivos del proyecto desde la perspectiva del problema de mercado laboral en América Latina. Se realizó un análisis exhaustivo del contexto regional mediante revisión iterativa de literatura académica y reportes institucionales, refinando progresivamente la formulación del problema hasta identificar como desafío central la ausencia de herramientas automatizadas capaces de capturar habilidades técnicas emergentes. Este proceso iterativo de análisis y refinamiento permitió traducir el problema general en objetivos específicos medibles y criterios de éxito cuantitativos que guiarían las fases posteriores. La fase concluyó cuando se alcanzó consenso sobre la definición precisa del problema, el alcance del sistema, y las métricas de evaluación que determinarían el éxito del proyecto.

\subsection{Fase 2: Comprensión de los Datos}

La fase de comprensión de datos consistió en caracterizar iterativamente las fuentes disponibles, su estructura, calidad y limitaciones. Se realizaron múltiples rondas de análisis exploratorio de portales de empleo en Colombia, México y Argentina, comenzando con scraping manual de muestras pequeñas, identificando patrones y problemas, y expandiendo progresivamente el análisis hasta obtener caracterización completa. Este proceso iterativo reveló tres desafíos críticos: heterogeneidad estructural entre portales, variabilidad lingüística con mezcla de español e inglés técnico, e incompletitud de información estructurada en las ofertas.

Paralelamente, se evaluaron múltiples taxonomías de habilidades mediante ciclos de prueba y comparación, refinando criterios de selección hasta determinar que ESCO v1.1.0 proporcionaba la mejor cobertura multilingüe pese a sus limitaciones en tecnologías recientes. Estos hallazgos, obtenidos mediante experimentación sistemática, determinaron decisiones metodológicas fundamentales: la construcción de un gold standard manualmente anotado, la adopción de un enfoque dual de pipelines, y la implementación de evaluación dual Pre-ESCO y Post-ESCO. La fase concluyó cuando se obtuvo comprensión suficiente de los datos para diseñar estrategias robustas de preparación y modelado.

\subsection{Fase 3: Preparación de los Datos}

La preparación de datos se ejecutó mediante desarrollo iterativo de cada componente hasta obtener datos limpios y validados. El módulo de web scraping atravesó múltiples iteraciones para manejar la heterogeneidad de portales, comenzando con implementación simple para portales estáticos, identificando casos fallidos, agregando manejo de JavaScript dinámico, y refinando hasta lograr recolección robusta. Cada iteración se validó contra muestra de ofertas, identificando errores de parsing, ajustando selectores CSS, y expandiendo manejo de casos excepcionales.

El proceso de limpieza y normalización siguió ciclos similares: implementación inicial de reglas básicas, identificación de casos problemáticos mediante inspección manual de resultados, refinamiento de expresiones regulares, y validación hasta obtener texto limpio consistente. La deduplicación se desarrolló iterativamente probando diferentes estrategias de hashing hasta eliminar efectivamente ofertas republicadas sin falsos positivos. La construcción del gold standard requirió múltiples rondas de anotación, refinamiento del protocolo tras identificar inconsistencias, re-anotación de muestras para validar confiabilidad, y expansión progresiva hasta alcanzar las 300 ofertas con calidad validada. Cada componente iteró hasta producir salidas que cumplieran criterios de calidad definidos y fueran aptas como entrada para la fase de modelado.

\subsection{Fase 4: Modelado}

La fase de modelado consistió en el diseño, implementación y refinamiento iterativo prolongado de dos pipelines paralelos de extracción de habilidades. El Pipeline A inició con implementación baseline simple usando solo expresiones regulares, se evaluó contra gold standard identificando baja cobertura, se expandió agregando Reconocimiento de Entidades Nombradas, se identificaron nuevos problemas de ruido, se refinaron filtros de stopwords, se normalizaron variantes ortográficas, se ajustaron thresholds de matching, y se implementó deduplicación inteligente. Este proceso atravesó siete experimentos controlados, donde cada iteración partió de los resultados del experimento anterior, identificó debilidades específicas mediante análisis de errores, implementó mejoras dirigidas, y validó mejoras mediante métricas cuantitativas sobre gold standard fijo.

El Pipeline B siguió desarrollo iterativo similar, comenzando con selección de modelo mediante evaluación comparativa de múltiples candidatos sobre muestra pequeña, refinando progresivamente el prompt mediante análisis de errores hasta lograr balance entre exhaustividad y precisión. El mapeo a taxonomía ESCO iteró probando diferentes estrategias de búsqueda, identificando problemas de falsos positivos en búsqueda semántica, y ajustando umbrales hasta obtener normalización confiable. El clustering iteró sobre parámetros de UMAP y HDBSCAN hasta descubrir agrupamientos coherentes. Cada componente del modelado evolucionó mediante ciclos repetidos de implementación-evaluación-refinamiento hasta producir extracciones de calidad suficiente para la fase de evaluación formal.

\subsection{Fase 5: Evaluación}

La evaluación del sistema se estructuró en tres dimensiones ejecutadas iterativamente. La evaluación cuantitativa calculó métricas sobre el gold standard, identificó componentes con rendimiento subóptimo, y retroalimentó la fase de modelado con análisis de errores específicos que gatillaron nuevas iteraciones de refinamiento. La evaluación cualitativa analizó muestras de habilidades extraídas, validó que correspondían a tecnologías reales mediante verificación manual, identificó patrones de alucinaciones o ruido, y generó reglas adicionales para filtrado que se implementaron en iteraciones subsecuentes.

La evaluación de robustez midió latencia y throughput, identificó cuellos de botella en componentes específicos, motivó optimizaciones de código, y validó mejoras mediante mediciones repetidas. Los resultados de estas tres dimensiones retroalimentaron continuamente la fase de modelado en ciclos iterativos: cada ronda de evaluación identificaba debilidades específicas, se implementaban refinamientos dirigidos en el modelado, se re-evaluaba sobre gold standard fijo para medir mejora objetiva, y el ciclo se repetía hasta alcanzar criterios de rendimiento objetivo. Esta iteración sistemática entre Evaluación y Modelado fue fundamental para las mejoras incrementales logradas.

\subsection{Fase 6: Despliegue}

La fase de despliegue consolidó el sistema como herramienta operativa mediante desarrollo iterativo de componentes de orquestación y automatización. La interfaz de línea de comandos se desarrolló iterativamente agregando comandos básicos, validando usabilidad con usuarios, expandiendo funcionalidad de logging y manejo de errores, y refinando mensajes de validación hasta obtener interfaz robusta y usable. El scheduler de automatización iteró sobre configuraciones de periodicidad, manejo de fallos y reintentos, persistencia de estado entre ejecuciones, hasta lograr operación confiable sin supervisión.

La documentación técnica se construyó iterativamente durante todo el proyecto, comenzando con documentación inline de código, expandiendo a documentación de APIs y interfaces, agregando diagramas de arquitectura conforme el sistema evolucionaba, y consolidando en manual técnico completo. Cada componente de despliegue iteró mediante ciclos de implementación, prueba en condiciones reales, identificación de problemas operativos, y refinamiento hasta alcanzar estabilidad operativa. Esta fase materializó la transición de las cinco fases anteriores de CRISP-DM en un sistema operativo completo para monitoreo continuo del mercado laboral.

\section{Diagrama de Flujo Metodológico}

La Figura \ref{fig:flujo-metodologico} presenta una vista integrada del flujo metodológico completo del proyecto, combinando las seis fases de CRISP-DM con las siete etapas del pipeline de software y los ciclos iterativos de refinamiento. El diagrama ilustra cómo cada fase de CRISP-DM se tradujo en actividades concretas de desarrollo de software, y cómo los resultados de la fase de Evaluación retroalimentaron la fase de Modelado para generar versiones mejoradas de los pipelines de extracción.

\begin{figure}[H]
\centering
\includegraphics[width=0.90\textwidth]{diagrams/DiagramaMetodologia.png}
\caption{Flujo Metodológico del Proyecto: Integración de CRISP-DM con Pipeline de 7 Etapas}
\label{fig:flujo-metodologico}
\end{figure}
