\chapter{CONTEXTO DEL PROYECTO}

\section{Background}

Para comprender el diseño y la justificación de la solución desarrollada, fue necesario fundamentar el proyecto en una serie de conceptos clave provenientes de la ingeniería de sistemas, la ciencia de datos y, fundamentalmente, del Procesamiento de Lenguaje Natural (NLP). Estos conceptos no actúan de forma aislada, sino que se articulan en un flujo metodológico que va desde la adquisición de datos brutos hasta la generación de conocimiento estructurado sobre el mercado laboral.

El punto de partida del observatorio fue la recolección de datos a gran escala desde fuentes web públicas. Esta tarea se realizó mediante \textbf{Web Scraping}, una técnica de extracción automatizada de información desde el código HTML de las páginas web \cite{orozco2019}. En el contexto del mercado laboral, esta técnica ha demostrado ser fundamental para obtener datos de alta frecuencia y granularidad directamente de los portales de empleo, superando las limitaciones de las encuestas y los reportes institucionales, que suelen ser retrospectivos y de baja periodicidad \cite{cardenas2015, rubio2024}.

Una vez extraído el contenido textual de las ofertas laborales, el siguiente paso fue prepararlo para el análisis computacional. Esto implicó una serie de técnicas de preprocesamiento de texto, comenzando con la \textbf{Tokenización}, que consiste en segmentar el texto en unidades mínimas o ``tokens'' (generalmente palabras o signos de puntuación) \cite{nguyen2024}. Posteriormente, se aplicó la \textbf{Lematización}, un proceso que reduce las palabras a su forma base o raíz gramatical (lema), permitiendo agrupar variaciones morfológicas de un mismo término (por ejemplo, ``programar'', ``programando'' y ``programado'' se unifican bajo el lema ``programar'') \cite{echeverria2022}. Este paso es crucial para estandarizar el vocabulario y reducir la dispersión de los datos antes del análisis.

Con el texto limpio y normalizado, el núcleo del desafío consistió en la extracción de habilidades. Para ello, se empleó un enfoque híbrido. Por un lado, se utilizaron \textbf{Expresiones Regulares (Regex)}, un lenguaje de patrones sintácticos que permite identificar y extraer secuencias de texto muy específicas, como nombres de tecnologías o certificaciones con formatos predecibles \cite{lukauskas2023}. Por otro lado, y como método principal, se aplicó el \textbf{Reconocimiento de Entidades Nombradas (NER)}, una técnica de NLP diseñada para identificar y clasificar entidades en un texto, como nombres de personas, lugares o, en este caso, habilidades y competencias \cite{herandi2024}. El NER permitió pasar de una búsqueda basada en reglas a un sistema capaz de reconocer habilidades en contextos gramaticales complejos.

Para superar las limitaciones de la extracción de menciones explícitas, el proyecto incorporó el uso de \textbf{Large Language Models (LLMs)}. Estos modelos de lenguaje a gran escala, como GPT o Llama 3, entrenados sobre corpus masivos de texto, poseen capacidades de razonamiento contextual que permitieron abordar desafíos más complejos \cite{herandi2024}. A través del diseño de prompts específicos (\textbf{Prompt Engineering}), fue posible guiar a los LLMs para realizar tareas de enriquecimiento semántico, como la distinción entre habilidades explícitas (mencionadas textualmente) e implícitas (inferidas del contexto del cargo) \cite{nguyen2024}. Esta capacidad fue fundamental para obtener una visión más completa de los perfiles demandados.

Una vez extraídas y normalizadas, las habilidades debían ser representadas de una forma que permitiera su análisis cuantitativo. Para ello, se utilizaron \textbf{Embeddings Semánticos}, que son representaciones vectoriales (numéricas) de palabras o frases en un espacio de alta dimensionalidad. La propiedad fundamental de estos embeddings es que la distancia entre dos vectores en ese espacio refleja la similitud semántica entre los textos que representan \cite{kavas2024}. Dado que las ofertas laborales en América Latina a menudo contienen términos técnicos en inglés (``Spanglish''), fue crucial el uso de \textbf{Embeddings Multilingües}, modelos entrenados para que textos con el mismo significado en diferentes idiomas tengan representaciones vectoriales cercanas en el mismo espacio semántico \cite{echeverria2022}.

Finalmente, para descubrir patrones y estructuras latentes en el conjunto de datos, se aplicó un pipeline de análisis no supervisado. Debido a que los embeddings son vectores de muy alta dimensionalidad (ej. 768 dimensiones), lo que dificulta la efectividad de muchos algoritmos (la ``maldición de la dimensionalidad''), primero se aplicó una técnica de \textbf{Reducción de Dimensionalidad} como \textbf{UMAP (Uniform Manifold Approximation and Projection)}. UMAP es un algoritmo no lineal que reduce el número de dimensiones preservando tanto la estructura local como global de los datos, lo que lo hace superior a métodos lineales como PCA para visualizar relaciones semánticas complejas \cite{lukauskas2023}. Sobre los datos ya en un espacio de baja dimensionalidad, se aplicó un algoritmo de \textbf{Clustering} basado en densidad como \textbf{HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)}. A diferencia de métodos como K-Means, HDBSCAN no requiere que se especifique el número de clústeres de antemano y es capaz de identificar grupos de formas arbitrarias y, crucialmente, de separar los puntos que no pertenecen a ningún grupo como ``ruido'' \cite{lukauskas2023}. Esta secuencia metodológica, inspirada en la literatura de vanguardia, fue la que permitió la identificación automática de ``ecosistemas de habilidades'' y perfiles laborales emergentes.

\section{Context Analysis}

El desafío de extraer, analizar y comprender la demanda de habilidades a partir de fuentes de datos no estructuradas, como las ofertas de empleo en línea, ha sido abordado desde múltiples frentes en la literatura académica y aplicada. Si bien el objetivo es común ---traducir texto en conocimiento accionable sobre el mercado laboral---, las aproximaciones metodológicas varían significativamente en su complejidad, escalabilidad y profundidad semántica. Para posicionar adecuadamente la contribución de este proyecto, fue necesario realizar un análisis crítico de las soluciones existentes a nivel global, las cuales se pueden agrupar en tres grandes líneas de trabajo:

\begin{itemize}
    \item \textbf{Enfoques Regionales en América Latina:} Un conjunto de estudios pioneros en la región que validaron el uso de técnicas de web scraping para la recolección de datos, pero cuyo análisis se fundamentó en métodos de Procesamiento de Lenguaje Natural (NLP) tradicionales, como el análisis léxico y el emparejamiento basado en reglas.

    \item \textbf{La Frontera de la Extracción con Large Language Models (LLMs):} Investigaciones de vanguardia a nivel internacional que exploraron el uso de modelos de lenguaje a gran escala, tanto en modalidades de prompting (sin re-entrenamiento) como de fine-tuning (con re-entrenamiento), para lograr una extracción de habilidades con mayor capacidad semántica e inferencial.

    \item \textbf{Pipelines Semánticos y Descubrimiento No Supervisado:} Arquitecturas de análisis completas que, más allá de la extracción, integran embeddings semánticos, técnicas de reducción de dimensionalidad y algoritmos de clustering para descubrir patrones y perfiles laborales emergentes directamente desde los datos.
\end{itemize}

El siguiente análisis demostrará que, si bien cada una de estas líneas ha aportado herramientas y hallazgos valiosos, ninguna de ellas, de forma aislada, resolvía de manera integral los desafíos metodológicos, geográficos y lingüísticos que presenta el mercado laboral tecnológico en América Latina. Esta fragmentación en el estado del arte fue la que justificó la necesidad de una solución sintética y adaptada, como la que se desarrolló en este proyecto.

\subsection{Enfoques Regionales: Caracterización del Mercado con Métodos Léxicos}

La primera línea de trabajo relevante para este proyecto comprende un conjunto de estudios pioneros desarrollados en América Latina. Estos trabajos fueron fundamentales porque validaron el uso de portales de empleo en línea como una fuente de datos rica y de alta frecuencia para el análisis del mercado laboral, pero se caracterizaron por emplear metodologías de procesamiento de texto basadas en análisis léxico, frecuencias de términos y reglas manuales.

El estudio más completo y reciente en este ámbito fue el de Rubio Arrubla (2024) para el mercado colombiano. Este trabajo construyó una base de datos masiva mediante web scraping del portal elempleo.com para el periodo 2018-2023. Su principal aporte fue la caracterización cuantitativa del impacto de la pandemia, demostrando un cambio estructural en la demanda de habilidades. Metodológicamente, el estudio implementó una tipología propia de habilidades tecnológicas y clasificó las vacantes utilizando un algoritmo de emparejamiento de texto basado en la descomposición de textos en n-gramas y el cálculo de puntajes de similitud contra la Clasificación Internacional Uniforme de Ocupaciones (CIUO) \cite{rubio2024}. Si bien esta aproximación permitió extraer tendencias valiosas, su dependencia de la coincidencia léxica representó una limitación fundamental, ya que el método perdía eficiencia a medida que aumentaba el número de palabras en los títulos al no poder capturar el contexto general \cite{rubio2024}.

De forma análoga, el trabajo de Aguilera y Méndez (2018) para el contexto argentino se centró en el sector de Tecnologías de la Información (TI), extrayendo datos de portales como ZonaJobs y Bumeran. Su análisis se apoyó en técnicas de minería de texto, específicamente en el análisis de frecuencias y el uso de bigramas, para identificar las tecnologías y roles más demandados \cite{aguilera2018}. Sin embargo, para estandarizar el vocabulario informal de las ofertas, los autores tuvieron que construir una lista de palabras clave de forma semi-manual, lo que limita la escalabilidad del sistema y su capacidad para adaptarse a la aparición de nuevas tecnologías no contempladas inicialmente \cite{aguilera2018}.

Para el caso de México, la investigación de Martínez Sánchez (2024) propuso un enfoque innovador al combinar datos de encuestas oficiales con información obtenida mediante scraping. Su análisis se basó en la frecuencia de términos y en una tipología manual para segmentar las habilidades, arrojando luz sobre el desajuste entre oferta y demanda, pero sin incluir un procesamiento avanzado y automatizado del lenguaje natural \cite{martinez2024}.

En conjunto, estos estudios regionales fueron cruciales para establecer la viabilidad de la recolección de datos, pero, desde una perspectiva metodológica, expusieron una brecha fundamental compartida: su dependencia de la correspondencia léxica explícita. Al basarse en frecuencias de palabras, n-gramas o listas de términos predefinidos, estos sistemas eran metodológicamente frágiles ante la ambigüedad y la variabilidad del lenguaje natural. Más allá de las limitaciones académicas individuales, esta carencia de infraestructura analítica ha sido reconocida a nivel institucional. El Banco Interamericano de Desarrollo (BID) ha señalado la falta de pipelines de análisis modernos y automatizados en la región, destacando que la mayoría de los sistemas existentes, si bien articulan el scraping, todavía se basan en reglas fijas o mapeos manuales y no han incorporado técnicas de embeddings ni de NLP avanzado \cite{echeverria2022}. Esta constatación institucional refuerza la conclusión de que existía un vacío sistémico: la ausencia de una solución que superara los enfoques léxicos para proporcionar un análisis semántico, dinámico y escalable de la demanda de habilidades en América Latina.

\subsection{La Frontera de la Extracción: El Uso de Large Language Models (LLMs)}

Paralelamente a los enfoques regionales, una segunda línea de investigación a nivel internacional ha explorado el uso de Large Language Models (LLMs) para superar las limitaciones de los métodos léxicos. Estos trabajos representan la frontera del estado del arte en extracción semántica, mostrando tanto el potencial transformador de los modelos de lenguaje de gran escala como las complejidades prácticas de su aplicación en dominios especializados como el mercado laboral.

Una de las primeras aproximaciones en este campo fue la de Nguyen et al. (2024), quienes investigaron el uso de LLMs de propósito general, como GPT-3.5 y GPT-4, en una modalidad de prompting sin re-entrenamiento (few-shot learning). Su metodología consistió en proporcionar al modelo una instrucción y unos pocos ejemplos de extracción de habilidades dentro del propio prompt. Experimentaron con dos formatos de salida: uno de extracción directa, donde el modelo devolvía una lista de habilidades (``EXTRACTION-STYLE''), y otro de etiquetado, donde el modelo reescribía la oración original encerrando las habilidades entre etiquetas especiales (``NER-STYLE'') \cite{nguyen2024}. Sus hallazgos fueron reveladores: aunque los LLMs no lograron igualar la precisión (medida con el F1-score) de los modelos supervisados tradicionales, demostraron una capacidad superior para interpretar frases sintácticamente complejas o ambiguas, como aquellas donde múltiples habilidades están conectadas por conjunciones \cite{nguyen2024}. Sin embargo, el estudio también advirtió sobre las limitaciones inherentes a este enfoque, principalmente la inconsistencia en los formatos de salida y el riesgo de ``alucinaciones'', donde el modelo genera entidades que no corresponden a habilidades reales \cite{nguyen2024}.

Tomando estas limitaciones como punto de partida, el trabajo de Herandi et al. (2024) representó la siguiente evolución lógica: el fine-tuning o re-entrenamiento específico de un LLM para la tarea. En su investigación, tomaron el modelo LLaMA 3 8B y lo ajustaron utilizando el dataset de referencia SkillSpan \cite{herandi2024}. Su principal innovación fue el diseño de un formato de salida estructurado en JSON que no solo extraía la habilidad (skill\_span), sino también el contexto textual que la rodeaba. Este enfoque les permitió alcanzar un rendimiento que superó el estado del arte (SOTA), logrando un F1-score total de 64.8\%, superior tanto a los modelos supervisados previos como a los LLMs utilizados mediante prompting \cite{herandi2024}. Más importante aún, su método garantizó la consistencia y la auditabilidad de los resultados, resolviendo uno de los mayores problemas prácticos de los LLMs.

A pesar de su sofisticación técnica, estos estudios de vanguardia comparten una limitación crucial que fue central para la justificación de este proyecto: fueron desarrollados y validados casi exclusivamente en contextos anglosajones y sobre datasets en idioma inglés. El trabajo de Herandi et al. (2024), por ejemplo, se fundamentó íntegramente en el dataset SkillSpan, que, como su nombre indica, contiene únicamente ofertas de empleo en inglés. Esta dependencia del idioma inglés evidenció un claro vacío geográfico y lingüístico en la aplicación de técnicas de NLP avanzadas para el análisis del mercado laboral. En conclusión, si bien los LLMs representan la tecnología de punta para la extracción de habilidades, su aplicación efectiva no es trivial. El prompting simple resulta insuficiente en términos de precisión y consistencia \cite{nguyen2024}, y las metodologías de fine-tuning de alto rendimiento, aunque superiores, estaban limitadas por la barrera del idioma de los datos de entrenamiento disponibles \cite{herandi2024}. Esto subrayó la necesidad de un proyecto que no solo aplicara estas técnicas avanzadas, sino que las adaptara y validara para la realidad lingüística y contextual del español en América Latina.

\subsection{Pipelines Semánticos y Descubrimiento No Supervisado}

La tercera línea de investigación relevante se centra en arquitecturas de análisis completas que van más allá de la simple extracción de entidades para estructurar los datos y descubrir patrones latentes de manera no supervisada. Estos sistemas se enfocan en responder preguntas sobre cómo se agrupan las habilidades y cómo evolucionan los perfiles laborales, en lugar de solo identificar menciones individuales.

El trabajo de Lukauskas et al. (2023) es el pilar fundamental de esta aproximación. Su investigación en el mercado laboral de Lituania propuso y validó empíricamente un pipeline de extremo a extremo que se ha convertido en una referencia metodológica. El flujo comenzaba con la extracción de las secciones de ``Requisitos'' de las ofertas de empleo mediante expresiones regulares (Regex). A continuación, el texto extraído era vectorizado utilizando un modelo basado en BERT (Sentence Transformers) para generar embeddings semánticos de 384 dimensiones. Conscientes de la ``maldición de la dimensionalidad'', los autores compararon cinco métodos de reducción de dimensionalidad, concluyendo que UMAP (Uniform Manifold Approximation and Projection) ofrecía los mejores resultados al preservar la estructura local y global de los datos de manera más efectiva que alternativas como PCA o t-SNE. Finalmente, sobre los datos ya reducidos, aplicaron y compararon una batería de algoritmos de clustering, demostrando que HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) fue el más eficaz por su capacidad para identificar clústeres de formas y densidades variables y manejar el ruido de manera robusta. El gran aporte de este estudio fue, por tanto, proporcionar una validación empírica para la secuencia completa Regex → Embeddings BERT → UMAP → HDBSCAN como una metodología de vanguardia para el descubrimiento automático y no supervisado de perfiles laborales coherentes.

En una línea complementaria, enfocada en la estandarización, se encuentra la herramienta open-source ESCOX, presentada por Kavargyris et al. (2025). ESCOX fue diseñada para operacionalizar el mapeo semántico de texto no estructurado contra las taxonomías ESCO e ISCO. Su arquitectura se basa en el uso de un modelo Sentence Transformer pre-entrenado (all-MiniLM-L6-v2) para generar embeddings tanto del texto de entrada como de todas las entidades de ESCO. Posteriormente, calcula la similitud del coseno entre el texto de entrada y cada entidad de la taxonomía, devolviendo aquellas que superan un umbral predefinido. El valor de ESCOX reside en su practicidad, eficiencia y su naturaleza de código abierto, ofreciendo una solución accesible para la estandarización de habilidades. Sin embargo, sus propios autores reconocen la limitación de su enfoque: al ser un método basado en embeddings pre-entrenados sin fine-tuning, su precisión es inherentemente menor que la de modelos más avanzados y especializados, como los basados en arquitecturas Transformer con re-entrenamiento específico para el dominio.

En conclusión, el estado del arte al inicio de este proyecto mostraba que ya existían, por separado, pipelines robustos para el análisis no supervisado y el descubrimiento de perfiles \cite{lukauskas2023}, así como herramientas prácticas para la estandarización semántica \cite{kavargyris2025}. No obstante, estas capacidades no se habían integrado en una solución única que también incorporara la potencia inferencial de los LLMs de última generación (como los explorados por Herandi et al., 2024) en un flujo coherente. Más importante aún, ninguna de estas arquitecturas avanzadas había sido desarrollada, adaptada o validada para el contexto específico del mercado laboral en América Latina y las particularidades lingüísticas del español en la región.

\subsection{Análisis Comparativo y Valor Agregado de la Solución Propuesta}

El análisis del contexto revela un panorama de investigación rico pero fragmentado, donde ninguna solución existente abordaba de manera integral los desafíos del mercado laboral tecnológico en América Latina. Ante esta realidad, el sistema desarrollado en este proyecto no se posicionó como una alternativa incremental, sino como una síntesis estratégica que articuló las fortalezas de las distintas líneas de investigación para crear una solución metodológicamente superior y contextualmente relevante.

El proyecto partió de los aprendizajes de los estudios regionales, adoptando su enfoque en la recolección de datos masivos a través de web scraping como una fuente válida y de alta frecuencia para caracterizar el mercado \cite{aguilera2018, martinez2024, rubio2024}. Sin embargo, reemplazó conscientemente su análisis léxico, propenso a errores y de limitada profundidad, con la potencia semántica e inferencial de los Large Language Models (LLMs). Para ello, se inspiró en la investigación internacional de vanguardia, tanto en la exploración del prompting para manejar frases ambiguas \cite{nguyen2024}, como en la implementación de técnicas de fine-tuning para alcanzar un rendimiento de última generación \cite{herandi2024}.

Además, la solución no se detuvo en la simple extracción de habilidades, sino que buscó estructurar el conocimiento descubierto. Para lograrlo, integró el robusto pipeline de análisis no supervisado validado empíricamente por Lukauskas et al. (2023), combinando embeddings, UMAP y HDBSCAN para la identificación automática de clústeres de competencias. Crucialmente, todo el sistema fue diseñado desde su concepción para adaptarse a la realidad lingüística y de datos de América Latina, un vacío metodológico dejado por la investigación internacional, que se ha centrado casi exclusivamente en datasets en inglés \cite{herandi2024}.

Finalmente, el valor agregado más significativo del proyecto residió en su arquitectura comparativa (Pipeline A vs. Pipeline B). Este diseño dual no solo permitió aprovechar lo mejor de los métodos tradicionales y de los LLMs, sino que introdujo un marco de validación empírica que aporta un rigor científico del que carecen muchas aplicaciones prácticas. Al contrastar sistemáticamente un método transparente y auditable (Pipeline A) contra un modelo semántico avanzado (Pipeline B), el sistema no solo generó resultados, sino que también proveyó una medida de la fiabilidad y el valor agregado de cada enfoque, constituyendo una contribución novedosa y completa al campo de los observatorios laborales automatizados.
