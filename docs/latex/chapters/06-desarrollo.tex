\chapter{DESARROLLO DE LA SOLUCIÓN}

\section{Implementación de la Infraestructura}

El sistema se implementó sobre PostgreSQL 15.3, seleccionado por su robustez en manejo de datos estructurados, soporte JSON nativo, y capacidades de indexación GIN. El esquema normalizado (3FN) utiliza seis tablas principales: \texttt{raw\_jobs} (ofertas crudas del scraping), \texttt{cleaned\_jobs} (ofertas normalizadas), \texttt{extracted\_\allowbreak skills} (Pipeline A), \texttt{enhanced\_\allowbreak skills} (Pipeline B), \texttt{gold\_\allowbreak standard\_\allowbreak annotations} (300 ofertas anotadas manualmente), y \texttt{esco\_skills} (14,215 skills de taxonomía ESCO extendida). Cada tabla de skills incluye metadatos de trazabilidad (\texttt{extraction\_\allowbreak method}, \texttt{llm\_model}, \texttt{esco\_uri}) permitiendo comparaciones sistemáticas entre pipelines.

La configuración de PostgreSQL se ajustó para procesamiento batch de grandes volúmenes de datos. Se implementaron índices especializados: B-tree para comparaciones entre pipelines, GIN para búsquedas de texto completo, e IVFFlat para búsquedas de similitud vectorial sobre embeddings de 768 dimensiones mediante la extensión pgvector.

\subsection{Orquestación del Pipeline}

El orquestador implementó seis etapas modulares ejecutadas secuencialmente: scraping de siete portales en tres países, limpieza y normalización, extracción mediante Pipeline A o Pipeline B, mapeo a taxonomía ESCO, generación de embeddings, y clustering con UMAP y HDBSCAN. Esta arquitectura lineal se seleccionó sobre frameworks más complejos por su simplicidad de depuración y capacidad de reejecutar etapas individuales.

La orquestación se implementó mediante Celery como sistema distribuido de workers con Redis como message broker. Los Celery workers ejecutan tareas asíncronas de procesamiento (scraping, extracción, clustering) sin bloquear la API web, permitiendo procesamiento batch de miles de ofertas en background. El sistema incluye Celery Beat como scheduler que automatiza tareas recurrentes programadas: respaldos automáticos diarios de PostgreSQL mediante \texttt{pg\_dump}, scraping periódico de portales configurables por frecuencia, y limpieza de tareas completadas. La configuración \texttt{max\_retries=3} en las task definitions proporciona tolerancia básica a fallos transitorios, mientras que el estado de ejecución persiste en Redis permitiendo monitoreo en tiempo real desde el frontend. Cada script registra progreso en logs estructurados con timestamps y métricas de performance.

El corpus se recolectó mediante scraping especializado por portal, adaptado a tres arquitecturas según las características de cada sitio: acceso directo a endpoints JSON/API para portales que exponen datos estructurados, combinación de \texttt{requests} con \texttt{BeautifulSoup4} para contenido HTML estático, y \texttt{selenium} para contenido JavaScript dinámico renderizado client-side. Se implementaron estrategias anti-bloqueo mediante delays aleatorios, rotación de User-Agent, y respeto de archivos \texttt{robots.txt}. El scraping completo recolectó 56,555 ofertas brutas durante aproximadamente 72 horas distribuidas en 15 días.

El pipeline de limpieza aplicó normalización de texto, eliminación de HTML residual, detección de idioma mediante \texttt{langdetect}, deduplicación mediante fuzzy matching y hash SHA-256, y validación de calidad descartando ofertas con contenido insuficiente. Del total scrapeado, se descartaron 25,895 ofertas por duplicación o calidad insuficiente, resultando en un dataset de 30,660 ofertas usables con texto normalizado, idioma identificado y metadata completa, cubriendo siete años de publicaciones laborales.

\section{Implementación de Sistemas de Extracción de Habilidades}

Se implementaron cuatro aproximaciones metodológicas para extracción automática de habilidades técnicas: Pipeline A (NER + Regex), Pipeline B (LLM), Pipeline A.1 (TF-IDF, descartado), y configuración Regex-Only para baseline.

\subsection{Pipeline A: NER y Expresiones Regulares}

Pipeline A constituye el método base de extracción de habilidades del observatorio, diseñado para identificar menciones explícitas de tecnologías mediante la combinación de Reconocimiento de Entidades Nombradas (NER) para detectar menciones contextuales y expresiones regulares (Regex) para capturar nomenclaturas estandarizadas. Esta arquitectura dual resuelve limitaciones complementarias: mientras NER con spaCy 3.5 y EntityRuler de 666 patrones ESCO captura tecnologías modernas en contexto pero falla con acrónimos técnicos, Regex con 548 patrones compilados en 18 categorías garantiza detección de nomenclaturas estructuradas pero omite menciones contextuales. La integración alcanza recall Post-ESCO de 81.25\% versus 73.08\% de Regex solo, procesando ofertas con latencia de 0.97 segundos, aproximadamente 18 veces más rápida que Pipeline B.

El flujo de procesamiento ejecuta ambas técnicas en paralelo sobre el mismo texto, combina resultados por unión, deduplica mediante normalización textual, y aplica diccionario canónico de 200 equivalencias para consolidar variantes ortográficas (e.g., ``JavaScript''/``js'', ``k8s''/``kubernetes''). El control de calidad implementa filtrado multi-etapa: limpieza de HTML residual y validación de encoding en pre-procesamiento, aplicación de listas de stopwords NER y técnicos genéricos en post-extracción para eliminar falsos positivos, y validación cruzada mediante overlap NER-Regex que asigna mayor confianza a skills detectadas por ambos métodos. El pipeline logra cobertura de 98.7\% del corpus con promedio de 50.3 skills por oferta, de las cuales 12.6\% mapean a taxonomía ESCO mientras 87.4\% representan tecnologías emergentes sin estandarización oficial.

La evaluación utilizó métricas de Information Retrieval (Precision, Recall, F1-Score) debido a que la extracción de skills constituye un problema de multi-label retrieval en universo abierto donde la indefinición de True Negatives hace que Accuracy sea engañosa. El mapeo a taxonomía ESCO mejoró drásticamente todas las métricas validando la efectividad del matcher de dos capas para normalizar variantes léxicas. Pipeline A procesó el corpus completo de 30,660 ofertas en aproximadamente 8.3 horas, estableciéndose como baseline de alta cobertura complementado estratégicamente con Pipeline B para enriquecimiento semántico de subconjuntos donde se requiere mayor precisión contextual. Los resultados comparativos detallados se presentan en el Capítulo 7 (Resultados). La especificación técnica completa de Pipeline A, incluyendo arquitectura de componentes, patrones regex por categoría, flujo de integración detallado, estrategias de control de calidad, y metodología de evaluación exhaustiva, se documenta en el Apéndice F.

\subsection{Pipeline B: Modelos de Lenguaje Grandes}

Pipeline B constituye el método de enriquecimiento del observatorio, diseñado para complementar Pipeline A mediante extracción semánticamente consciente de habilidades implícitas, sinónimos contextuales, y competencias inferidas que no aparecen explícitamente mencionadas en el texto. Este pipeline aprovecha las capacidades de comprensión contextual profunda de Large Language Models (LLMs) para interpretar ofertas laborales de manera similar a como lo haría un analista humano.

La arquitectura de Pipeline B se diseñó con dos objetivos complementarios al Pipeline A: aumentar la cobertura de skills implícitas que expresiones regulares no pueden capturar, como inferir tecnologías cloud específicas a partir de descripciones genéricas, y mejorar la precisión mediante comprensión de contexto que reduce falsos positivos al distinguir términos ambiguos según el contexto laboral de la oferta. Sin embargo, dado el mayor costo computacional de LLMs con latencias típicas entre 15 y 25 segundos por oferta versus 0.97 segundos de Pipeline A, se implementó como pipeline de enriquecimiento estratégico aplicado a subconjuntos relevantes del corpus.

\subsubsection{Justificación del Enfoque LLM}

La decisión de implementar un pipeline basado en LLMs se fundamentó en cuatro limitaciones estructurales de Pipeline A que requerían comprensión semántica profunda. Primero, Pipeline A solo detecta skills mencionadas explícitamente mediante patrones léxicos, sin capacidad de inferir ``Git'' cuando una oferta solicita ``experiencia con control de versiones''. Segundo, fragmenta skills compuestas al detectar ``machine'' y ``learning'' como tokens separados en lugar de ``Machine Learning'' como concepto único. Tercero, carece de desambiguación contextual, procesando ``Python'' como tecnología incluso en ofertas de zoológicos que requieren conocimiento del reptil. Cuarto, no captura sinónimos contextuales como la equivalencia entre ``backend development'' y ``server-side programming''.

El enfoque LLM resuelve estas limitaciones mediante tres capacidades clave. La comprensión contextual permite interpretar ofertas considerando la semántica completa del texto en lugar de aplicar patrones sintácticos aislados, lo que habilita inferir ``Docker'' de ``experiencia en contenedorización'' sin requerir match textual directo. La desambiguación semántica utiliza el contexto de la oferta para distinguir tecnologías de homónimos (``Python'' + ``Django'' identifica lenguaje de programación; ``Python'' + ``zoo'' identifica animal). Más importante, los LLMs son capaces de identificar skills emergentes que aún no están codificadas en diccionarios estáticos: tecnologías recientes como ``Claude Code'', ``Cursor IDE'' o ``v0.dev'' son detectables por modelos pre-entrenados con conocimiento actualizado, mientras que Pipeline A requeriría actualización manual de sus expresiones regulares. Esta capacidad de capturar vocabulario emergente del mercado laboral representa la ventaja diferencial más significativa del enfoque LLM.

Como se detallará en el Capítulo de Resultados, la evaluación cuantitativa demuestra que Pipeline B con Gemma 3 4B alcanza desempeño superior (84.26\% F1 Post-ESCO) comparado con Pipeline A (72.53\%), validando la efectividad del enfoque basado en LLMs. Sin embargo, el costo computacional es significativamente superior, con latencias de procesamiento aproximadamente 18 veces más lentas que Pipeline A. La cuantización INT4 mediante \texttt{bitsandbytes} reduce requisitos de memoria de $\sim$16GB (FP16) a $\sim$4GB (INT4), permitiendo inferencia local en hardware consumer sin dependencia de APIs comerciales. Este balance entre calidad y latencia justifica la aplicación selectiva de Pipeline B a subconjuntos estratégicos del corpus que requieren análisis semántico profundo.

\subsubsection{Selección del Modelo}

Se evaluaron cuatro modelos de lenguaje open-source de tamaño intermedio que cumplían los requisitos de ejecución local con cuantización INT4: Gemma 3 4B Instruct de Google DeepMind, Llama 3.2 3B Instruct de Meta AI, Qwen 2.5 3B Instruct de Alibaba Cloud, y Phi-3.5 Mini Instruct de Microsoft Research. Todos los modelos se ejecutaron con cuantización INT4 mediante \texttt{bitsandbytes}, reduciendo requisitos de memoria de aproximadamente 16GB a 4GB para permitir inferencia local en hardware consumer. La evaluación preliminar sobre 10 ofertas laborales del gold standard analizó tanto métricas cuantitativas como comportamiento cualitativo considerando alucinaciones, consistencia de formato, y relevancia de extracciones.

La Tabla~\ref{tab:llm_comparison} presenta los resultados de la evaluación comparativa:

\begin{table}[htbp]
\centering
\caption{Comparación de modelos LLM para extracción de habilidades}
\label{tab:llm_comparison}
\begin{tabular}{|l|c|c|p{6cm}|}
\hline
\textbf{Modelo} & \textbf{F1 Pre-ESCO} & \textbf{Skills/oferta} & \textbf{Observaciones clave} \\
\hline
Gemma 3 4B & 46.23\% & 27.8 & JSON válido (99\%), sin alucinaciones, latencia mediana 18.3s \\
\hline
Llama 3.2 3B & 39.7\% & --- & Alucinaciones sistemáticas (agregaba TensorFlow/Pandas en ofertas frontend) \\
\hline
Qwen 2.5 3B & 38.9\% & 10-12 & Extracciones conservadoras, alta precisión pero baja cobertura \\
\hline
Phi-3.5 Mini & 35.2\% & --- & Formato inconsistente (40\% no-JSON), requiere parser complejo \\
\hline
\end{tabular}
\end{table}

Gemma 3 4B Instruct fue seleccionado como modelo de producción tras evaluar cuatro dimensiones críticas. Primero, alcanzó el mejor desempeño cuantitativo con 46.23\% F1 Pre-ESCO y 84.26\% F1 Post-ESCO, superando a Llama 3.2 por +6.5pp y a Qwen 2.5 por +7.3pp. Segundo, demostró estabilidad de formato con 99\% de respuestas en JSON válido (299/300 ofertas procesadas exitosamente), eliminando la necesidad de parsers heurísticos frágiles como los requeridos por Phi-3.5. Tercero, la revisión manual de 300 ofertas no detectó alucinaciones sistemáticas, contrastando con Llama 3.2 que agregaba skills de Data Science (TensorFlow, Pandas) en ofertas de desarrollo web tradicional sin componente analítico. Cuarto, la latencia mediana de 18.3 segundos por oferta (percentiles P25-P75: 15-25s) permitió procesar el gold standard completo en 3.5 horas, tiempo aceptable para pipelines de enriquecimiento no-interactivos.

Esta decisión prioriza el atributo de calidad de confiabilidad sobre maximización de métricas aisladas. Un pipeline de producción debe generar resultados predecibles sin alucinaciones que contaminen análisis agregados, incluso si esto implica sacrificar mejoras marginales de F1-Score. La arquitectura robusta de Gemma 3 4B (formato estructurado consistente + ausencia de sesgos evidentes) lo posiciona como fundamento confiable para el componente semántico del observatorio.

\subsubsection{Arquitectura del Sistema}

La arquitectura de Pipeline B implementa procesamiento asíncrono con Gemma 3 4B cuantizado INT4 mediante \texttt{bitsandbytes}, permitiendo inferencia local con 4GB de memoria GPU. El sistema se implementó como tres módulos secuenciales: construcción de prompts estructurados con templates especializados, inferencia con Gemma 3 4B cuantizado INT4 generando respuestas JSON validadas mediante Pydantic, y parsing robusto con fallback regex para manejar respuestas mal formadas.

El flujo de procesamiento ejecuta seis etapas secuenciales con manejo robusto de errores: carga de ofertas desde PostgreSQL en batches de 50, construcción de prompts con truncamiento automático a 3800 tokens, inferencia secuencial con Gemma 3 4B bajo timeout de 120 segundos capturando excepciones CUDA, parsing con fallback regex, persistencia batch de resultados y metadatos, y logging de métricas agregadas cada 10 ofertas. El sistema procesó 299/300 ofertas del gold standard (99.7\% éxito) con distribución bimodal de latencias: mediana de 17.66 segundos para 80.6\% de casos típicos, media de 45.17 segundos inflada por 47 outliers (15.7\%) con ofertas extensas requiriendo hasta 254 segundos, totalizando 3.5 horas para el corpus completo. El procesamiento batch aplica batch size de 1 oferta por inferencia debido a restricciones de memoria, logrando normalización canónica equivalente a Pipeline A para garantizar comparabilidad. La especificación técnica completa de Pipeline B, incluyendo diseño de prompts, configuración de inferencia, mecanismos de validación y evaluación exhaustiva, se documenta en el Apéndice F.

\subsection{Pipelines Alternativos Evaluados}

Pipeline A.1 basado en TF-IDF + filtrado por noun phrases se implementó como experimento alternativo. Utilizó \texttt{scikit-learn.\allowbreak Tfidf\allowbreak Vectorizer} (\texttt{ngram\_range=(1,3)}, \texttt{max\_\allowbreak features=10000}) extrayendo top-50 n-gramas por oferta, filtrados por part-of-speech con spaCy. Las pruebas sobre 100 ofertas gold standard revelaron limitaciones críticas: 60\% de candidatos eran frases descriptivas no-skills, fragmentación excesiva (``React'' y ``Native'' separados), y F1=11.69\%. \textbf{Pipeline A.1 se descartó} por performance inadecuado versus Pipeline A (F1=72.53\%).

La configuración Regex-Only reutilizó los 548 patrones de Pipeline A eliminando NER, estableciendo baseline determinístico. Sobre el gold standard de 300 ofertas alcanzó F1 Post-ESCO de 79.17\% (precision 86.36\%, recall 73.08\%), procesando en $<$1ms/oferta. Extrajo promedio 35.2 skills/oferta versus 50.3 del pipeline combinado, confirmando que NER contribuye ~30\% de detecciones adicionales. Los resultados validaron que regex solo proporciona precision superior (+6.64pp F1 Post-ESCO vs. Pipeline A completo), pero menor recall Pre-ESCO (-6.91pp) al omitir menciones contextuales que NER captura. Los resultados comparativos de evaluación de todos los pipelines (Pipeline A, Pipeline B, Regex-Only, Pipeline A.1) se presentan en el Capítulo 7 (Resultados).

\section{Implementación del Sistema de Mapeo a Taxonomía ESCO}

El sistema de mapeo normaliza las habilidades extraídas por Pipeline A y Pipeline B contra la taxonomía ESCO v1.1.0, consolidando variantes ortográficas (``React''/``React.js''/``ReactJS''), diferencias idiomáticas (``Database''/``Base de datos''), y abreviaciones (``JS''/``JavaScript'', ``K8s''/``Kubernetes'') bajo URIs canónicos únicos. Las skills sin correspondencia ESCO se preservan como emergentes para análisis de tecnologías no estandarizadas.

El sistema opera sobre una versión extendida de ESCO v1.1.0 que incorpora 13,939 habilidades oficiales de la Comisión Europea (98.1\% del total), complementadas con 152 habilidades técnicas de O*NET Skills del U.S. Department of Labor no cubiertas por ESCO (1.1\%), y 124 tecnologías modernas agregadas manualmente tras análisis exploratorio (0.9\%) que incluyen frameworks modernos como Next.js y Remix, herramientas DevOps como Terraform y ArgoCD, y tecnologías de AI/ML como LangChain y Prompt Engineering, totalizando 14,215 habilidades en la taxonomía extendida.

La implementación del matcher ESCO enfrentó dos desafíos técnicos principales. El primero fue balancear similitud ortográfica versus falsos positivos en fuzzy string matching, ejemplificado por casos problemáticos como ``Piano'' mapeando incorrectamente a ``tocar el piano''. El segundo fue la inadecuación de embeddings semánticos generalistas que producen matches incorrectos en vocabulario técnico, como ``Docker'' mapeando a ``Facebook''. El sistema final opera con arquitectura de tres capas secuenciales (exact match, fuzzy match, semantic match), con Layer 3 deshabilitada post-evaluación debido a limitaciones identificadas en embeddings multilingües generalistas para dominio técnico.

\subsection{Arquitectura ESCO\allowbreak Matcher\allowbreak 3Layers}

El sistema se diseñó como matcher de tres capas secuenciales con fallback en cascada: Layer 1 ejecuta matching exacto contra labels preferidos bilingües; Layer 2 aplica fuzzy string matching con umbral 0.92; y Layer 3 utiliza embeddings semánticos (posteriormente deshabilitado). Cada capa opera independientemente, retornando el match de la primera que genera resultado, priorizando precisión sobre recall.

La taxonomía se cargó desde \texttt{esco\_skills} con campos: \texttt{skill\_uri}, \texttt{preferred\_\allowbreak label\_en/es}, \texttt{alternative\_\allowbreak labels\_en/es}, \texttt{skill\_type}, y \texttt{description}. Se construyeron tres índices in-memory: (1) \textit{Exact index} como diccionario mapeando normalized labels a URIs (~42,000 entradas); (2) \textit{Fuzzy index} como lista ordenada de tuplas (label, URI); y (3) \textit{Semantic index} como matriz numpy 14,215×1024 de embeddings pre-computados. Los índices se cargaron al inicio, reduciendo latencia de ~800ms/skill a ~15ms/skill.

\subsection{Layer 1 y 2: Matching Exacto y Fuzzy}

Layer 1 implementó matching exacto case-insensitive contra labels bilingües. La normalización unificada aplicó: lowercase, eliminación de acentos (\texttt{unicodedata.normalize}), eliminación de puntuación preservando guiones/puntos internos (``Node.js''), y colapso de espacios. El lookup directo en \texttt{exact\_index} alcanzó 35-40\% de cobertura en Pipeline A y 40-45\% en Pipeline B, reflejando que LLMs generan ortografía más estandarizada.

Layer 2 aplicó fuzzy matching usando \texttt{fuzzywuzzy} con distancia de Levenshtein. La implementación inicial con \texttt{fuzz.partial\_ratio()} produjo falsos positivos críticos: ``Piano'' mapeó a ``tocar el piano'' (100\%, substring exacto), ``SQL'' a ``MySQL'' (100\%). Se reemplazó por \texttt{fuzz.ratio()} (similitud entre strings completos), reduciendo ``Piano'' vs ``tocar el piano'' a 40\% y ``SQL'' vs ``MySQL'' a 60\%. El umbral se configuró empíricamente en 0.92 tras evaluar 200 matches manuales: thresholds bajos generaban falsos positivos (``Java'' → ``JavaScript''), mientras 0.95 requería ortografía perfecta eliminando abreviaciones válidas (``K8s'' vs ``Kubernetes'' = 0.93).

La optimización implementó early stopping: al encontrar match con score $\geq$ 0.98, se detuvo la búsqueda. Esto redujo tiempo de ~450ms/skill (búsqueda exhaustiva) a ~85ms/skill (early stopping en ~18\% casos). Layer 2 incrementó cobertura en ~25-30\% adicional, mapeando variantes ortográficas, abreviaciones expandidas, y nombres con guiones inconsistentes. Sin embargo, abreviaciones extremas fallaron: ``AWS'' vs ``Amazon Web Services'' score 0.42, ``GCP'' vs ``Google Cloud Platform'' 0.35, ``ML'' vs ``Machine Learning'' 0.40.

\subsection{Layer 3: Embeddings Semánticos (Deshabilitado)}

Layer 3 implementó matching semántico con el modelo \path{intfloat/multilingual-e5-base} transformando skills a vectores de 768 dimensiones. Se pre-computaron embeddings para 14,215 labels ESCO, normalizados a vectores unitarios. Para cada skill sin match en Layers 1-2, se calculó similitud coseno contra matriz ESCO vía producto punto, retornando match si similitud $>$0.75 (~120ms/skill).

Las pruebas revelaron que embeddings multilingües generalistas producían matches incorrectos en contexto técnico: ``Docker'' mapeó a ``Facebook'' (similitud 0.82), ``REST'' a ``sleep'' (0.79), ``Python'' a ``snake programming'' (0.76). El análisis determinó que modelos pre-entrenados en corpus generales (Wikipedia, CommonCrawl) capturan asociaciones semánticas de dominio general pero no técnicas especializadas. Corregir esto requeriría fine-tuning en corpus tech-específico (Stack Overflow, GitHub) con ~50,000+ ejemplos anotados, excediendo scope del proyecto. Layer 3 se deshabilitó completamente.

El sistema final operó con Layers 1-2 (exact + fuzzy), alcanzando match rate de 12.6\% sobre el gold standard de 300 ofertas (1,038 de 8,268 skills extraídas). Se experimentó con un ESCO Matcher Enhanced que incorporaba matching más agresivo con \texttt{partial\_ratio} y reglas adicionales, logrando aumentar cobertura a ~25\%. Sin embargo, análisis cualitativo reveló incremento en falsos positivos (e.g., ``Europa'' $\to$ ``neuropatología'', ``Oferta'' $\to$ ``ofertas de empleo''), introduciendo sesgo no deseado. Adicionalmente, aunque esta versión enhanced se aplicaría uniformemente a ambos pipelines, su comportamiento más permisivo favorece intrínsecamente el descubrimiento de más matches ESCO, lo cual sesgaría la comparación experimental al beneficiar desproporcionadamente al pipeline cuyas características de extracción se alinean mejor con mayor cobertura ESCO, comprometiendo la neutralidad metodológica requerida para la evaluación comparativa. Se decidió no implementar esta versión enhanced, preservando el matcher conservador de 2 capas aplicado uniformemente a todos los pipelines, priorizando precisión sobre recall e integridad de la comparación. El match rate de 12.6\% refleja la naturaleza del mercado tech LATAM: aunque ESCO v1.1.0 incluye actualizaciones hasta 2023, la taxonomía europea no cubre completamente frameworks modernos (Next.js, Tailwind CSS), herramientas DevOps recientes (Terraform, ArgoCD), ni tecnologías de IA post-2022 (LangChain, Prompt Engineering). Las 87.4\% de skills sin mapeo ESCO incluyen tanto tecnologías genuinamente emergentes como ruido residual de extracción y variantes ortográficas extremas que el matcher conservador rechazó intencionalmente. Estas skills se preservan en formato normalizado para análisis Pre-ESCO, requiriendo validación cualitativa posterior para distinguir innovaciones reales de artefactos de extracción.

El mapper se integró como etapa 5 del orquestador, procesando skills desde \texttt{extracted\_\allowbreak skills}, \texttt{enhanced\_\allowbreak skills}, y \texttt{gold\_\allowbreak standard\_\allowbreak annotations}. El procesamiento aprovecha memoización mediante caché \texttt{skill\_text} $\to$ \texttt{esco\_uri} que evita remapear skills repetidas, optimizando significativamente el procesamiento de skills populares como ``JavaScript'' que aparece en miles de ofertas.

\section{Implementación del Sistema de Clustering de Habilidades}

El sistema de clustering de habilidades constituye un componente analítico central del observatorio, diseñado para descubrir familias semánticas de skills sin categorías predefinidas, analizar evolución temporal de perfiles tecnológicos, y detectar tecnologías emergentes mediante análisis no supervisado. Este sistema permite caracterizar la demanda laboral más allá de conteos agregados de skills individuales, revelando combinaciones coherentes de habilidades que definen roles profesionales reales en el mercado.

La arquitectura del clustering integra tres componentes complementarios que transforman texto de skills en agrupaciones semánticas interpretables, el primero siendo embeddings semánticos que capturan similitud entre skills en espacio vectorial de 768 dimensiones, el segundo reducción dimensional mediante UMAP que proyecta vectores de alta dimensión a espacio 2D preservando estructura local y global, y por ultimo clustering density-based con HDBSCAN que identifica automáticamente agrupaciones densas sin especificar número de clusters a priori.

El sistema se ejecutó en dos escenarios complementarios: Pre-ESCO que analiza texto normalizado de skills tal como fueron extraídas (preservando tecnologías emergentes sin mapeo ESCO), y Post-ESCO que opera sobre URIs estandarizados de taxonomía ESCO (consolidando variantes ortográficas para mayor coherencia). Esta dualidad permite balancear cobertura de tecnologías emergentes (Pre-ESCO) con interpretabilidad de resultados (Post-ESCO).

\subsection{Justificación del Enfoque de Clustering No Supervisado}

La decisión de implementar clustering no supervisado en lugar de categorización supervisada se fundamentó en las características dinámicas del mercado laboral tecnológico y las limitaciones de taxonomías predefinidas \cite{lukauskas2023}:

Los enfoques supervisados presentan limitaciones significativas que el clustering no supervisado resuelve. Las taxonomías tradicionales como O*NET SOC codes se actualizan cada 5-10 años y no capturan roles emergentes como AI/ML Engineer o DevOps Engineer, evidenciando obsolescencia de categorías predefinidas. Las jerarquías estáticas no reflejan el solapamiento natural de perfiles como Full-Stack Developer que combina competencias Backend y Frontend. La anotación manual de 30,660 ofertas requeriría 400-500 horas de trabajo especializado, representando un costo prohibitivo de supervisión manual. Adicionalmente, la categorización manual depende de interpretación subjetiva de roles laborales, introduciendo sesgo de anotadores que compromete la objetividad del análisis.

El enfoque no supervisado ofrece ventajas significativas para análisis de demanda laboral. Los datos revelan naturalmente agrupaciones mediante descubrimiento automático de patrones sin hipótesis a priori sobre roles existentes. La arquitectura se adapta a evolución temporal permitiendo que nuevos clusters emerjan automáticamente al procesar datos recientes, como el perfil ``Modern Frontend'' post-2022. HDBSCAN proporciona granularidad adaptativa con clusters de tamaños variables que capturan tanto roles mainstream como nichos especializados. El sistema identifica automáticamente skills atípicas o errores de extracción como outliers clasificados como ruido. Finalmente, la escalabilidad del enfoque permite agregar nuevas ofertas al corpus sin requerir re-entrenamiento supervisado.

La selección de componentes tecnológicos se fundamentó en criterios específicos del dominio. E5 Multilingual Embeddings se seleccionó por su soporte bilingüe español/inglés crítico para corpus LATAM, performance comprobado en benchmarks de similitud semántica, y tamaño intermedio de 278M parámetros que balancea calidad versus costo computacional. UMAP se eligió sobre t-SNE y PCA por su capacidad de preservar simultáneamente estructura local y global, ofrecer complejidad algorítmica favorable O(n log n) versus O(n²) de t-SNE, y garantizar proyecciones deterministas reproducibles. HDBSCAN se prefirió sobre K-Means por detectar automáticamente el número óptimo de clusters sin hiperparámetro k, identificar outliers como ruido en lugar de forzar asignación, y manejar clusters de formas arbitrarias sin asumir esfericidad.

Esta arquitectura responde a cuatro atributos de calidad del sistema. La adaptabilidad permite que el clustering no supervisado evolucione con el mercado laboral sin requerir actualización manual de categorías. La interpretabilidad se logra mediante proyecciones UMAP 2D que permiten visualización intuitiva de 768 dimensiones facilitando inspección manual de coherencia semántica. La escalabilidad con complejidad O(n log n) permite procesar el corpus completo de 30,660 ofertas en menos de 5 minutos. La reproducibilidad se garantiza mediante proyecciones deterministas con \texttt{random\_state} que aseguran resultados consistentes entre ejecuciones.

\subsection{Generación de Embeddings y Reducción UMAP}

El modelo \path{intfloat/multilingual-e5-base} transformó skills a vectores de 768 dimensiones capturando similitud semántica. Se seleccionó por su soporte multilingüe nativo para español e inglés, tamaño intermedio de 278M parámetros que balancea expresividad versus costo computacional, y desempeño comprobado en benchmarks de similitud semántica textual. El proceso operó en dos modos: Pre-ESCO embebió texto normalizado de 6,413 skills extraídas aplicando filtro de frecuencia mínima que redujo el corpus a 1,314 embeddings con densidad suficiente para clustering, mientras Post-ESCO embebió preferred labels ESCO resultando en 289 embeddings consolidados para el gold standard de 300 ofertas. La generación batch procesó skills en lotes de 256 documentos en hardware consumer. Los embeddings se normalizaron a vectores unitarios y almacenaron en base de datos PostgreSQL con extensión pgvector para consultas de similitud eficientes.

UMAP (Uniform Manifold Approximation and Projection) redujo embeddings de 768D a 2D para visualización y clustering. Se seleccionó sobre t-SNE y PCA por su capacidad de preservar simultáneamente estructura local y global, ofrecer escalabilidad algorítmica favorable, y garantizar reproducibilidad determinista con semillas fijas. La configuración involucró el parámetro \texttt{n\_neighbors} para balancear estructura local versus global, \texttt{min\_dist=0.1} para controlar separación mínima entre puntos, \texttt{n\_components=2} para proyección bidimensional, y \texttt{metric=``cosine''} como medida de similitud. El grid search sobre \texttt{n\_neighbors} determinó que el valor 15 ofrecía el mejor balance al preservar agrupaciones semánticas coherentes mientras mantenía separación entre dominios mayores.

\subsection{Clustering HDBSCAN y Optimización}

HDBSCAN (Hierarchical Density-Based Spatial Clustering) identificó clusters sobre proyecciones UMAP 2D sin especificar número predefinido. Se seleccionó sobre K-Means por su capacidad de detectar automáticamente el número óptimo de clusters, identificar outliers como ruido, formar clusters de geometría arbitraria, y proporcionar jerarquía interpretable mediante dendrogramas. La configuración involucró parámetros \texttt{min\_cluster\_size} para controlar granularidad, \texttt{min\_samples} para robustez, y \texttt{metric=``euclidean''} como medida de distancia.

El grid search sobre \texttt{min\_cluster\_size} $\in \{5, 7, 10, 12, 15, 20\}$ evaluó múltiples criterios: número de clusters idealmente entre 50 y 200, porcentaje de ruido inferior al 25\%, Silhouette Score superior a 0.4 para datos Post-ESCO, e interpretabilidad manual de los grupos resultantes. Los experimentos revelaron un trade-off fundamental donde configuraciones bajas generaban más de 100 clusters muy específicos con alta fragmentación y Silhouette alrededor de 0.35-0.40, mientras que configuraciones altas producían pocos clusters gruesos con Silhouette entre 0.55-0.65 pero pérdida de granularidad útil para análisis práctico.

El análisis comparativo identificó UMAP n\_neighbors=15 + HDBSCAN min\_cluster\_size=12 como configuración óptima balanceando granularidad vs coherencia. Los resultados cuantitativos detallados del clustering, incluyendo métricas por configuración, composición de clusters identificados, y análisis de coherencia semántica, se documentan en el Capítulo de Resultados y en el Documento de Pruebas (Capítulo 13).

\subsection{Comparación Pre-ESCO vs Post-ESCO}

El clustering se ejecutó en dos escenarios evaluando impacto del mapeo ESCO: Pre-ESCO que opera sobre texto normalizado de skills sin consolidación, preservando tecnologías emergentes sin mapeo ESCO, y Post-ESCO que procesa URIs ESCO consolidados, colapsando variantes ortográficas. La comparación cuantitativa detallada de métricas (número de clusters, Silhouette Score, porcentaje de ruido) entre ambos escenarios para Pipeline A y Pipeline B se presenta en el Capítulo de Resultados. Se implementó análisis híbrido: clustering Post-ESCO para métricas cuantitativas sobre skills estandarizadas, complementado con análisis Pre-ESCO para tecnologías emergentes ausentes en taxonomía ESCO.

\subsection{Análisis Temporal}

Como extensión, se implementó la infraestructura base para análisis temporal futuro de evolución de clusters. El módulo permite análisis longitudinal mediante: (1) agrupación de ofertas por trimestre, (2) extracción de skills por período, (3) generación de embeddings E5, (4) proyección UMAP (\texttt{n\_neighbors=15}), (5) clustering HDBSCAN (\texttt{min\_cluster\_size=12}), y (6) tracking de consistencia de clusters entre períodos consecutivos (threshold: $\geq$60\% overlap en top-20 skills). El sistema genera visualizaciones temporales (heatmaps clusters × quarters, line charts de frecuencia) para identificar patrones de adopción tecnológica.

Sin embargo, el dataset actual no permite análisis temporal robusto: de las 21,839 ofertas fechadas (71.23\% del dataset), el 97.1\% (21,216 ofertas) corresponden a Q4-2025, reflejando el período intensivo de scraping reciente, mientras que los 28 trimestres anteriores (Q4-2018 a Q3-2025) contienen solo 623 ofertas dispersas. La infraestructura queda implementada para análisis longitudinal futuro conforme el observatorio acumule ofertas distribuidas equitativamente a través de trimestres.

\subsection{Experimentación de Hiperparámetros y Trade-off Interpretabilidad vs. Métricas}

La optimización de UMAP+HDBSCAN requirió balancear métricas cuantitativas de calidad de clustering (Silhouette Score, Davies-Bouldin Index) con interpretabilidad práctica de los clusters resultantes para análisis del mercado laboral. Se realizaron más de 70 experimentos variando hiperparámetros UMAP (\texttt{n\_neighbors}, \texttt{min\_dist}) y HDBSCAN (\texttt{min\_cluster\_size}, \texttt{min\_samples}), documentando el fenómeno del ``clustering cliff'': configuraciones que maximizan métricas matemáticas frecuentemente producen clusterings con cientos de micro-clusters imposibles de interpretar manualmente, mientras que configuraciones conservadoras colapsan a pocos clusters genéricos con baja utilidad analítica.

Para balancear estos criterios se implementó función de scoring multi-criterio ponderando granularidad, Silhouette Score, porcentaje de ruido, e interpretabilidad manual. La configuración óptima seleccionada (\texttt{n\_neighbors=15}, \texttt{min\_cluster\_size=12}, \texttt{min\_samples=3}) genera 50-60 clusters interpretables, mantiene Silhouette aceptable, y limita ruido al 15-20\%. Esta decisión prioriza utilidad práctica sobre optimización matemática, consistente con investigaciones en clustering de dominios especializados. La evaluación exhaustiva de las 70+ configuraciones experimentales, incluyendo casos ilustrativos comparativos y resultados cuantitativos detallados, se documenta en el Documento de Pruebas (Capítulo 13) y en el Capítulo de Resultados.

\section{Creación del Gold Standard y Sistema de Evaluación}

Esta sección describe la construcción del dataset de referencia de 300 ofertas anotadas manualmente y el sistema de evaluación dual (Pre-ESCO y Post-ESCO) para comparar pipelines.

\subsection{Selección y Anotación del Gold Standard}

La evaluación rigurosa de los pipelines de extracción requirió construir un dataset de referencia de 300 ofertas laborales manualmente anotadas. Este gold standard debía balancear tres criterios fundamentales: representatividad del mercado laboral tecnológico latinoamericano, diversidad geográfica e idiomática, y viabilidad práctica de anotación manual exhaustiva. La construcción involucró un proceso iterativo de selección algorítmica y refinamiento manual que garantizó la calidad y coherencia del dataset final.

\subsubsection{Algoritmo de Selección Estratificada}

El algoritmo de selección operó mediante cuatro fases secuenciales automatizadas. La primera fase aplicó detección de idioma mediante patrones regex sobre el corpus completo de 56,555 ofertas scrapeadas, clasificándolas en español, inglés o contenido mixto. La segunda fase ejecutó pre-selección con filtros SQL estrictos que aplicaron restricciones de longitud mínima (1,200+ caracteres), inclusión de títulos técnicos (``developer'', ``engineer'', ``programador''), y exclusión explícita de roles no-software (``manager'', ``mechanical engineer'', ``cajero'', ``manufactura''), resultando en 7,102 candidatos tras deduplicación.

La tercera fase calculó un quality score de 0-100 para cada candidato, ponderando longitud de descripción, presencia de keywords técnicas, existencia de sección de requisitos estructurada, y penalizaciones por ruido HTML residual. Paralelamente se ejecutó clasificación automatizada de rol profesional en 8 categorías (Backend, Frontend, DevOps, Data Science, QA, Mobile, Fullstack, Security) y nivel de seniority (Junior, Mid, Senior) mediante análisis de título y descripción. La cuarta fase realizó selección estratificada estableciendo targets por combinación de país e idioma que reflejaran la distribución del mercado LATAM, ordenando candidatos por quality score descendente dentro de cada subgrupo y seleccionando los top-ranked hasta completar los targets establecidos.

\subsubsection{Refinamiento Iterativo y Control de Calidad}

El proceso de refinamiento requirió 7 iteraciones para eliminar ofertas problemáticas detectadas mediante validación manual progresiva. Las iteraciones 4-6 removieron 76 ofertas mediante heurísticas automatizadas que identificaron duplicados con títulos repetidos, roles de ingeniería no-software (manufacturing, petroleum, químico, eléctrico), posiciones de sales/business development mal clasificadas, y roles ERP especializados fuera del alcance del estudio. Cada oferta removida se reemplazó con candidatos que pasaron filtros ultra-estrictos verificando exclusividad de desarrollo de software. La iteración 7, ejecutada durante el proceso de anotación manual, detectó 5 duplicados finales no capturados por hashes automatizados debido a similitud semántica alta (79.8\% overlap de vocabulario) pero Job IDs técnicamente distintos, los cuales fueron reemplazados inmediatamente para preservar la independencia estadística del dataset.

\subsubsection{Características del Dataset Final}

El dataset final de 300 ofertas presenta las siguientes características verificadas mediante queries SQL y análisis del archivo de anotaciones. La calidad se garantizó con 300 Job IDs únicos sin duplicados, 299 títulos únicos (solo un caso de ``DevOps Engineer'' duplicado con contenido diferente), y 100\% de roles verificados como desarrollo de software puro. La distribución geográfica alcanzó Colombia 40.7\%, México 37.7\%, y Argentina 21.7\%, cercana a los targets estratificados. La distribución idiomática fue español 80.7\% e inglés 19.3\%, reflejando el predominio del español en ofertas técnicas latinoamericanas.

La Tabla~\ref{tab:gold_standard_distribution} presenta la distribución de roles profesionales y niveles de seniority del dataset final:

\begin{table}[htbp]
\centering
\caption{Distribución de roles y seniority en gold standard (300 ofertas)}
\label{tab:gold_standard_distribution}
\begin{tabular}{lc|lc}
\hline
\textbf{Rol Profesional} & \textbf{\%} & \textbf{Nivel Seniority} & \textbf{\%} \\
\hline
Backend Developer & 34.3 & Senior & 54.0 \\
QA Engineer & 14.7 & Mid-level & 40.0 \\
Frontend Developer & 13.7 & Junior & 6.0 \\
DevOps Engineer & 12.3 & & \\
Data Scientist & 9.3 & & \\
Mobile Developer & 7.0 & & \\
Fullstack Developer & 4.7 & & \\
Security Engineer & 4.0 & & \\
\hline
\end{tabular}
\end{table}

El contenido textual presentó longitud promedio de 527 palabras por oferta (mediana 489, rango 119-2,447), reflejando variabilidad real del mercado desde descripciones concisas hasta especificaciones técnicas extensas.

\subsubsection{Protocolo de Anotación Manual}

Un único anotador con formación técnica en Ingeniería de Sistemas ejecutó la identificación manual de skills siguiendo un protocolo de formato atómico estricto. El protocolo requirió lectura exhaustiva de cada oferta, extracción de términos técnicos individuales sin construcciones compuestas, y clasificación en hard skills (tecnologías, lenguajes, frameworks, herramientas, metodologías) y soft skills (competencias transversales como comunicación, liderazgo, trabajo en equipo). El formato atómico prohibió construcciones compuestas como ``Python (pandas, numpy)'' o narrativas como ``Conocimientos de AWS y Azure'', exigiendo términos separados: ``Python'', ``Pandas'', ``NumPy'', ``AWS'', ``Azure''. Esta decisión de diseño simplificó la comparación automatizada mediante operaciones de conjuntos contra las extracciones de Pipeline A y Pipeline B.

El proceso de anotación completó las 300 ofertas produciendo 7,848 skills totales distribuidas en 6,174 hard skills (78.7\%) y 1,674 soft skills (21.3\%), con promedio de 26.2 skills por oferta. Aunque no se realizó medición de inter-annotator agreement al ser un único anotador, la validez del gold standard se garantizó mediante tres mecanismos: protocolo de anotación estricto documentado, ejemplos de calibración inicial sobre 15 ofertas piloto validadas por una reclutadora profesional que solicitó permanecer anónima (quien aprobó el nivel de granularidad y exhaustividad de la extracción manual), y verificación post-anotación de consistencia de formato mediante scripts automatizados. El Apéndice B presenta ejemplos representativos de anotaciones ilustrando la diversidad geográfica e idiomática del dataset.

\subsection{Sistema de Evaluación Dual: Pre-ESCO y Post-ESCO}

El sistema implementó dos comparaciones independientes cuantificando capacidades complementarias: \textit{Pre-ESCO} evalúa capacidad de extracción pura comparando texto normalizado sin mapeo taxonómico, capturando skills emergentes ausentes en ESCO; \textit{Post-ESCO} evalúa capacidad de estandarización comparando URIs ESCO tras mapear todas las skills (gold standard y pipelines) con el mismo código \texttt{ESCO\allowbreak Matcher\allowbreak 3Layers}, eliminando sesgos ortográficos.

El componente Pre-ESCO utilizó módulo de normalización canónica con diccionario de 200+ tecnologías mapeando variantes a formas estándar (``js''/``javascript'' → ``JavaScript'', ``k8s'' → ``Kubernetes''). Las métricas se calcularon mediante operaciones de conjuntos: para cada job, se compararon skills gold normalizadas vs skills pipeline normalizadas, identificando True Positives (TP = intersección), False Positives (FP = predichas no en gold), y False Negatives (FN = en gold no predichas). Los valores agregados sobre 300 ofertas alimentaron fórmulas: Precision = TP/(TP+FP), Recall = TP/(TP+FN), F1 = 2×(P×R)/(P+R).

El componente Post-ESCO remapeó todas las skills usando \texttt{ESCO\allowbreak Matcher\allowbreak 3Layers} para garantizar fairness: Pipeline A se ignoró y remapeó desde texto normalizado igual que Pipeline B. Este diseño eliminó ventajas artificiales asegurando que diferencias Post-ESCO reflejaran calidad de extracción textual. Skills sin match ESCO se descartaron de la comparación Post-ESCO, cuantificándose separadamente como ``Skills Emergentes'' para análisis cualitativo.

Las 300 ofertas se procesaron por todos los pipelines: Pipeline A (NER+Regex completo), Pipeline A Regex-Only, Pipeline B con 4 LLMs (Gemma, Llama, Qwen, Phi), y Pipeline A.1 (TF-IDF, descartado por F1$<$12\%). Los outputs se almacenaron en tablas dedicadas facilitando queries de evaluación mediante joins con \texttt{gold\_\allowbreak standard\_\allowbreak annotations}.

\section{Implementación del Frontend Interactivo}

El observatorio implementó un frontend web con Next.js 14, React 18 y TypeScript que proporciona acceso interactivo a las 30,660 ofertas procesadas y habilidades extraídas mediante sistema de filtrado multidimensional. La arquitectura utiliza Next.js App Router con componentes client-side, comunicación con backend FastAPI mediante Axios, fetching paralelo de datos, y estilos Tailwind CSS con paleta de colores contextual que distingue visualmente skills técnicas, soft skills, elementos ESCO y métodos de extracción.

\subsection{Dashboard Principal}

El dashboard presenta estadísticas de cobertura en tiempo real con filtrado simultáneo por país (Colombia, México, Argentina), método de extracción (Manual, Pipeline A, Pipeline B, NER, Regex), estado del empleo, tipo de habilidad y estado de mapeo ESCO. Muestra métricas clave del corpus, preview del Top 15 de habilidades más demandadas con barras de progreso, distribución geográfica y desglose de métodos de extracción. Las estadísticas se actualizan dinámicamente conforme el usuario aplica filtros.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/DashboardFront.png}
\caption{Dashboard Principal del Observatorio mostrando estadísticas agregadas, Top 15 de habilidades más demandadas, distribución geográfica y sistema de filtrado multidimensional.}
\label{fig:dashboard-front}
\end{figure}

\subsection{Módulo de Exploración de Habilidades}

El módulo combina tres modos de visualización: Top (20, 50 o 100 habilidades más frecuentes), Todos (paginación de 50 resultados), y Búsqueda (texto completo con paginación). Cada skill muestra frecuencia absoluta, porcentaje de aparición, tipo con badge de color, badge ESCO cuando aplica, y barra de progreso visual. El sistema de paginación incluye navegación por números de página y muestra el rango actual de resultados.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/SkillsFront.png}
\caption{Módulo de Exploración de Habilidades con búsqueda de texto completo, filtrado multidimensional y tres modos de visualización (Top, Todos, Búsqueda).}
\label{fig:skills-front}
\end{figure}

\subsection{Sistema de Consulta de Ofertas Laborales}

La sección de empleos permite filtrado por país, portal de origen y estado de procesamiento, además de búsqueda de texto completo. Los resultados se presentan en tabla paginada (20 empleos por página) con título, empresa, ubicación, portal y fecha de publicación. Cada empleo lleva a página de detalle individual que muestra descripción completa, requisitos, metadata (salario, contrato, modalidad remota) y todas las habilidades extraídas con filtrado propio por tipo, método de extracción y estado de mapeo ESCO.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/JobAddViewFront.png}
\caption{Vista Detallada de Oferta Laboral mostrando descripción completa, metadata y habilidades extraídas con filtrado por tipo, método de extracción y estado de mapeo ESCO.}
\label{fig:job-detail-front}
\end{figure}

\subsection{Panel de Administración}

El panel de administración proporciona monitoreo en tiempo real del sistema Celery mostrando workers activos, tareas en ejecución y salud del sistema. Presenta estadísticas de Pipeline A y Pipeline B incluyendo jobs procesados, completados, pendientes, fallidos, skills extraídas y tasa de completitud visualizada con barra de progreso. Muestra el schedule de Celery Beat con tareas programadas automáticamente, implementa auto-refresh cada 10 segundos, y maneja gracefully casos donde Celery no está disponible.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/PanelAdminFront.png}
\caption{Panel de Administración y Monitoreo presentando estado del sistema Celery, estadísticas de Pipeline A y Pipeline B, y schedule de tareas automáticas con auto-refresh cada 10 segundos.}
\label{fig:admin-panel-front}
\end{figure}

El frontend implementa además funcionalidades avanzadas de visualización de distribuciones geográficas, análisis detallado de habilidades individuales con skills co-ocurrentes, y módulo completo de clustering con selección de configuraciones y visualización de resultados. Las capturas de pantalla adicionales de estas funcionalidades se presentan en el Apéndice G.
