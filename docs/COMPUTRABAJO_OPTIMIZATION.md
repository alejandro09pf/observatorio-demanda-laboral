# Computrabajo Spider Optimization Guide

## üéØ Resumen

El spider de Computrabajo ha sido completamente optimizado con una **estrategia dual** que ofrece dos opciones seg√∫n tu caso de uso:

- **OPCI√ìN A (Scrapy Optimizado)**: Conservador, para scraping continuo y de bajo riesgo
- **OPCI√ìN B (Requests Masivo)**: Agresivo, para scraping masivo inicial o catch-up

---

## üìä Comparaci√≥n de Opciones

| Caracter√≠stica | OPCI√ìN A: Scrapy | OPCI√ìN B: Requests |
|---|---|---|
| **Throughput** | ~500-1,000 jobs/hora | ~2,000-5,000 jobs/hora |
| **Riesgo de Ban** | MUY BAJO | MEDIO (controlado) |
| **Uso recomendado** | Scraping diario, mantenimiento | Scraping masivo inicial, catch-up |
| **Complejidad** | Media | Alta |
| **Concurrencia** | Secuencial (1 request) | 4-8 workers paralelos |
| **Anti-bot evasion** | ‚úÖ Avanzado | ‚úÖ Avanzado |
| **Proxies** | ‚úÖ Soportado | ‚ö†Ô∏è Requiere implementaci√≥n manual |
| **Delays** | Inteligentes (3s base + 10-15s entre p√°ginas) | Distribuido por worker (3-5s) |
| **Paginaci√≥n** | Ilimitada | Hasta max_pages configurado |
| **Fingerprinting** | ‚úÖ Din√°mico (60+ UAs, headers variables) | ‚úÖ Din√°mico (60+ UAs, headers variables) |
| **Auto-stop en ban** | ‚úÖ S√≠ | ‚úÖ S√≠ |
| **Deduplicaci√≥n** | Content hash en DB | Content hash en DB |

---

## üöÄ OPCI√ìN A: Scrapy Optimizado (Conservador)

### Mejoras Implementadas

#### 1. **Pool de User-Agents Extendido (60+)**
```
config/user_agents.txt ahora contiene:
- Chrome (Windows, macOS, Linux)
- Firefox (Windows, macOS, Linux)
- Safari (macOS)
- Edge (Windows, macOS)
- Opera, Brave, Vivaldi
- M√∫ltiples versiones (117-122)
```

#### 2. **Browser Fingerprinting Avanzado**
- **Accept-Language** din√°mico (10 variantes para LATAM)
- **Sec-CH-UA headers** modernos (Client Hints)
- **Sec-Fetch headers** realistas
- **Referer tracking** inteligente:
  - 30% chance de venir de Google
  - Detail pages usan listing como referer
- **DNT header** aleatorio (75% sin DNT, 25% con DNT)
- **Cache-Control** variable

#### 3. **Delays Inteligentes**
```python
DOWNLOAD_DELAY = 3s (base)
+ 10-15s entre p√°ginas (como HiringCafe)
+ AutoThrottle habilitado
```

#### 4. **Paginaci√≥n Ilimitada**
- **Removido el l√≠mite de 5 p√°ginas**
- Detecci√≥n de 3 p√°ginas vac√≠as consecutivas ‚Üí stop
- Detecci√≥n de alto porcentaje de duplicados (>80%) ‚Üí advertencia

#### 5. **Configuraci√≥n Anti-Ban**
```python
CONCURRENT_REQUESTS = 1  # Secuencial
HTTPCACHE_ENABLED = False  # No cache de errores 429
RETRY_HTTP_CODES = [429, 403, 500, 502, 503, 504, 408]
AUTOTHROTTLE_ENABLED = True
```

### Uso

#### V√≠a Orchestrator (Recomendado)
```bash
python -m src.orchestrator run-once computrabajo -c CO -l 1000 -v
```

#### Directamente con Scrapy
```bash
cd src/scraper
scrapy crawl computrabajo -a country=CO -a keyword=sistemas -a city=bogota-dc -s SETTINGS_MODULE=scraper.settings
```

### Throughput Esperado
- **~500-1,000 jobs/hora** (dependiendo de conexi√≥n y latencia)
- **Riesgo de ban**: MUY BAJO
- **Recomendado para**: Scraping diario, ejecutar cada 24-48 horas

---

## ‚ö° OPCI√ìN B: Requests Masivo (Agresivo)

### Caracter√≠sticas

#### 1. **Threading con Workers Independientes**
- 4-8 workers paralelos (configurable)
- Cada worker tiene:
  - Sesi√≥n HTTP independiente
  - User-Agent √∫nico
  - Accept-Language √∫nico
  - Rate limiting individual

#### 2. **Rate Limiting Distribuido**
```python
# Por worker:
- 3-5 segundos entre requests
- Tracking de tiempo por sesi√≥n
- Auto-stop global en errores 429/403
```

#### 3. **Scraping Completo**
- Extrae listing page + detail page
- Content hash deduplication en DB
- Parsea descripci√≥n, requirements, metadata

#### 4. **Auto-Stop en Bans**
```python
if response.status_code in [429, 403]:
    stop_scraping.set()  # Detiene TODOS los workers
```

### Uso

#### B√°sico (4 workers, 50 p√°ginas)
```bash
python scripts/scrape_computrabajo_requests.py CO bogota-dc sistemas
```

#### Con configuraci√≥n custom
```bash
# 6 workers, 100 p√°ginas
python scripts/scrape_computrabajo_requests.py CO bogota-dc desarrollador --workers 6 --max-pages 100

# M√©xico
python scripts/scrape_computrabajo_requests.py MX distrito-federal ingeniero --workers 4 --max-pages 50

# Argentina
python scripts/scrape_computrabajo_requests.py AR capital-federal programador --workers 4 --max-pages 50
```

### Throughput Esperado
- **~2,000-5,000 jobs/hora** (con 4-6 workers)
- **Riesgo de ban**: MEDIO (auto-stop en detecci√≥n)
- **Recomendado para**:
  - Scraping masivo inicial
  - Catch-up despu√©s de downtime
  - Colecci√≥n r√°pida de datos hist√≥ricos

### ‚ö†Ô∏è Precauciones

1. **No ejecutar 24/7**: Usar solo para catch-up o scraping inicial
2. **Monitorear logs**: Si ves errores 429/403, detener y esperar
3. **Empezar con pocos workers**: Probar con 2-3 workers primero
4. **Aumentar gradualmente**: Si funciona bien, subir a 4-6 workers

---

## üîß Configuraci√≥n Avanzada

### Fingerprints Configuration
Editar `config/fingerprints.yaml` para ajustar:

```yaml
portal_configs:
  computrabajo:
    use_sec_fetch: true
    use_sec_ch_ua: true
    use_referer: true
    use_dnt: true
    randomize_accept_language: true
    session_cookies: true
```

### User-Agents
Editar `config/user_agents.txt` para agregar m√°s UAs:
```
# Chrome - Windows
Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...
```

### Proxies (Opcional)
Editar `config/proxies.yaml`:

```yaml
portal_config:
  computrabajo:
    pool: 'residential'  # Usar proxies residenciales
    rotation_strategy: 'per_request'
    timeout: 20
    max_retries: 3

proxy_pools:
  residential:
    proxies:
      - http://user:pass@proxy1.com:8080
      - http://user:pass@proxy2.com:8080
```

---

## üìà M√©tricas de √âxito

### Monitorear
- **Duplicate rate por p√°gina**: Si >80%, casi todos los jobs ya est√°n en DB
- **Empty pages consecutivas**: 3 = fin natural del scraping
- **HTTP status codes**: 200 = OK, 429/403 = rate limited
- **Jobs scraped vs found**: Alta conversi√≥n = buen scraping

### Logs a Buscar
```bash
‚úÖ New job found       # Job nuevo detectado
‚è≠Ô∏è Skipping duplicate  # Job duplicado (ya en DB)
‚ö†Ô∏è High duplicate rate # >80% duplicados en p√°gina
üö´ Rate limited        # Bloqueado (detener scraping)
```

---

## üéì Recomendaciones

### Para Uso Diario (Mantenimiento)
```bash
# OPCI√ìN A: Scrapy conservador
python -m src.orchestrator run-once computrabajo -c CO -l 500 -v

# Ejecutar cada 24-48 horas
# Baja carga, bajo riesgo
```

### Para Catch-Up Masivo (Inicial o Post-Downtime)
```bash
# OPCI√ìN B: Requests agresivo
python scripts/scrape_computrabajo_requests.py CO bogota-dc sistemas --workers 4 --max-pages 100

# Ejecutar una vez, esperar 24 horas antes de repetir
# Alta carga, riesgo medio pero controlado
```

### Para M√∫ltiples Pa√≠ses/Ciudades
```bash
# Script para ejecutar secuencialmente
python scripts/scrape_computrabajo_requests.py CO bogota-dc sistemas --workers 3 --max-pages 50
sleep 3600  # Esperar 1 hora
python scripts/scrape_computrabajo_requests.py CO medellin sistemas --workers 3 --max-pages 50
sleep 3600
python scripts/scrape_computrabajo_requests.py MX distrito-federal desarrollador --workers 3 --max-pages 50
```

---

## üîç Troubleshooting

### Problema: Rate Limited (HTTP 429)
**Soluci√≥n**:
1. Detener el scraping inmediatamente
2. Esperar 2-4 horas
3. Reiniciar con OPCI√ìN A (Scrapy conservador)
4. Si persiste, reducir workers o agregar proxies

### Problema: Forbidden (HTTP 403)
**Soluci√≥n**:
1. Verificar que fingerprint middleware est√° activo
2. Revisar logs para ver qu√© headers est√°n siendo enviados
3. Probar con proxies residenciales
4. Esperar 24 horas antes de reintentar

### Problema: No encuentra jobs
**Soluci√≥n**:
1. Verificar el selector CSS: `response.css("article")`
2. Abrir la URL en navegador y verificar estructura HTML
3. Puede que Computrabajo haya cambiado su dise√±o
4. Actualizar selectores en el spider

### Problema: Alto porcentaje de duplicados
**No es un problema**:
- Es normal despu√©s de scraping inicial
- Significa que la mayor√≠a de jobs ya est√°n en DB
- Sistema funcionando correctamente

---

## üìö Archivos Modificados/Creados

```
‚úÖ config/user_agents.txt              # 60+ user agents
‚úÖ config/fingerprints.yaml            # Configuraci√≥n de fingerprinting
‚úÖ src/scraper/middlewares.py          # BrowserFingerprintMiddleware (nuevo)
‚úÖ src/scraper/spiders/computrabajo_spider.py  # Optimizaciones
‚úÖ src/scraper/settings.py             # Habilitar fingerprinting
‚úÖ scripts/scrape_computrabajo_requests.py     # Script masivo (nuevo)
‚úÖ docs/COMPUTRABAJO_OPTIMIZATION.md   # Esta documentaci√≥n
```

---

## üéâ Resultados Esperados

### Antes (Spider Original)
- Max 5 p√°ginas = ~100-150 jobs
- Delays de 1.5s (muy agresivo)
- 8 User-Agents b√°sicos
- Headers est√°ticos
- Alto riesgo de ban

### Despu√©s (Optimizado)
- **OPCI√ìN A**: Paginaci√≥n ilimitada, ~500-1,000 jobs/hora, riesgo MUY BAJO
- **OPCI√ìN B**: Hasta max_pages configurado, ~2,000-5,000 jobs/hora, riesgo MEDIO
- 60+ User-Agents variados
- Headers din√°micos y realistas
- Fingerprinting avanzado
- Auto-stop inteligente
- Deduplicaci√≥n en DB

### Comparado con HiringCafe
- **HiringCafe**: API REST directa, 100 jobs/request, throughput ~3,000 jobs/hora
- **Computrabajo OPCI√ìN A**: HTML scraping, ~30 jobs/page + detail, throughput ~500-1,000 jobs/hora
- **Computrabajo OPCI√ìN B**: HTML scraping paralelo, throughput ~2,000-5,000 jobs/hora

**Conclusi√≥n**: Computrabajo nunca ser√° tan r√°pido como HiringCafe (API vs HTML), pero ahora es **10-50x m√°s eficiente** que antes.

---

## ü§ù Contribuciones

Si encuentras formas de optimizar a√∫n m√°s:
1. Agregar m√°s User-Agents a `config/user_agents.txt`
2. Mejorar selectores CSS si cambian
3. Implementar proxies autom√°ticos
4. Optimizar parseo para extraer m√°s del listing (menos detail requests)

---

**Autores**: Nicol√°s Francisco Camacho Alarc√≥n y Alejandro Pinz√≥n
**Fecha**: Octubre 2025
**Versi√≥n**: 1.0
