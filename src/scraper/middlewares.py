"""
Scrapy middlewares for Labor Market Observatory
"""

import random
import time
import os
from scrapy import signals
from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware
from scrapy.downloadermiddlewares.retry import RetryMiddleware
from scrapy.utils.response import response_status_message
from scrapy.exceptions import IgnoreRequest
from scrapy.http import HtmlResponse
import logging
from config.settings import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()

# Try to import curl_cffi for TLS fingerprinting
try:
    from curl_cffi import requests as curl_requests
    CURL_CFFI_AVAILABLE = True
except ImportError:
    CURL_CFFI_AVAILABLE = False
    logger.warning("‚ö†Ô∏è curl_cffi not available. Install with: pip install curl-cffi")


class UserAgentRotationMiddleware(UserAgentMiddleware):
    """Rotate user agents to avoid detection."""
    
    def __init__(self, user_agent=''):
        super().__init__(user_agent)
        self.user_agents = self._load_user_agents()
        self.current_index = 0
        self.rotation_strategy = os.getenv('USER_AGENT_ROTATION_STRATEGY', 'round_robin')
    
    def _load_user_agents(self):
        """Load user agents from file or use defaults."""
        user_agent_file = os.getenv('USER_AGENT_LIST_PATH', './config/user_agents.txt')
        
        if os.path.exists(user_agent_file):
            try:
                with open(user_agent_file, 'r', encoding='utf-8') as f:
                    return [line.strip() for line in f if line.strip()]
            except Exception as e:
                logger.warning(f"Could not load user agents from {user_agent_file}: {e}")
        
        # Default user agents if file not found
        return [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0',
        ]
    
    def process_request(self, request, spider):
        """Set a user agent for each request."""
        if self.rotation_strategy == 'round_robin':
            ua = self.user_agents[self.current_index % len(self.user_agents)]
            self.current_index += 1
        else:  # random
            ua = random.choice(self.user_agents)
        
        request.headers['User-Agent'] = ua
        logger.info(f"üîÑ Set User-Agent: {ua[:50]}...")
        return None


class ProxyRotationMiddleware:
    """Enhanced proxy rotation middleware with portal-specific strategies."""
    
    def __init__(self):
        self.proxies = self._load_proxies()
        self.current_index = 0
        self.rotation_strategy = os.getenv('PROXY_ROTATION_STRATEGY', 'round_robin')
        self.proxy_config = self._load_proxy_config()
        self.proxy_failures = {}  # Track failures per proxy
        self.portal_stats = {}    # Track usage per portal
        
    def _load_proxies(self):
        """Load proxies from environment variable."""
        proxy_pool = os.getenv('PROXY_POOL', '')
        if proxy_pool:
            return [proxy.strip() for proxy in proxy_pool.split(',') if proxy.strip()]
        return []
    
    def _load_proxy_config(self):
        """Load proxy configuration from YAML file."""
        try:
            import yaml
            config_file = os.getenv('PROXY_CONFIG_PATH', './config/proxies.yaml')
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                    logger.info(f"‚úÖ Loaded proxy config from {config_file}")
                    logger.info(f"üìä Available pools: {list(config.get('proxy_pools', {}).keys())}")
                    return config
            else:
                logger.warning(f"‚ö†Ô∏è Proxy config file not found: {config_file}")
        except Exception as e:
            logger.error(f"‚ùå Could not load proxy config: {e}")
        return {}
    
    def _get_portal_config(self, spider):
        """Get portal-specific proxy configuration."""
        portal = getattr(spider, 'portal', getattr(spider, 'name', 'default'))
        portal_config = self.proxy_config.get('portal_config', {}).get(portal, {})
        
        if not portal_config:
            # Use default configuration
            global_settings = self.proxy_config.get('global_settings', {})
            portal_config = {
                'pool': global_settings.get('default_pool', 'standard'),
                'rotation_strategy': global_settings.get('default_rotation', 'per_request'),
                'timeout': global_settings.get('default_timeout', 20),
                'max_retries': global_settings.get('default_max_retries', 3)
            }
        
        return portal_config
    
    def _get_proxy_pool(self, pool_name):
        """Get proxy pool by name."""
        pools = self.proxy_config.get('proxy_pools', {})
        if pool_name in pools:
            return pools[pool_name]['proxies']
        
        # Only fallback to environment variable if YAML config is empty
        if not self.proxy_config:
            return self.proxies
        
        logger.warning(f"‚ö†Ô∏è Pool '{pool_name}' not found in YAML config, no proxies available")
        return []
    
    def _select_proxy(self, spider, request):
        """Select appropriate proxy based on portal configuration."""
        try:
            portal_config = self._get_portal_config(spider)
            pool_name = portal_config.get('pool', 'standard')
            rotation_strategy = portal_config.get('rotation_strategy', 'per_request')
            
            # Get available proxies for this pool
            available_proxies = self._get_proxy_pool(pool_name)
            if not available_proxies:
                logger.warning(f"‚ö†Ô∏è No proxies available in pool '{pool_name}' for {spider.name}")
                return None
            
            # Filter out failed proxies
            working_proxies = [p for p in available_proxies if self.proxy_failures.get(p, 0) < 3]
            if not working_proxies:
                logger.warning(f"‚ö†Ô∏è All proxies in pool '{pool_name}' have failed, resetting failures for {spider.name}")
                working_proxies = available_proxies  # Reset if all failed
                self.proxy_failures.clear()  # Clear failure counts
            
            # Select proxy based on rotation strategy
            if rotation_strategy == 'per_request':
                # Random selection for each request
                proxy = random.choice(working_proxies)
            elif rotation_strategy == 'per_page':
                # Round-robin per page
                page = request.meta.get('page', 1)
                proxy = working_proxies[page % len(working_proxies)]
            else:
                # Default round-robin
                proxy = working_proxies[self.current_index % len(working_proxies)]
                self.current_index += 1
            
            # Track usage
            self._track_proxy_usage(spider, proxy)
            logger.debug(f"üîÑ Selected proxy {proxy} for {spider.name} using {rotation_strategy} strategy")
            return proxy
            
        except Exception as e:
            logger.error(f"‚ùå Error selecting proxy for {spider.name}: {e}")
            return None
    
    def _track_proxy_usage(self, spider, proxy):
        """Track proxy usage statistics."""
        portal = getattr(spider, 'portal', getattr(spider, 'name', 'unknown'))
        if portal not in self.portal_stats:
            self.portal_stats[portal] = {'total_requests': 0, 'proxy_usage': {}}
        
        self.portal_stats[portal]['total_requests'] += 1
        if proxy not in self.portal_stats[portal]['proxy_usage']:
            self.portal_stats[portal]['proxy_usage'][proxy] = 0
        self.portal_stats[portal]['proxy_usage'][proxy] += 1
    
    def process_request(self, request, spider):
        """Set proxy for request based on portal configuration."""
        try:
            # Check if proxies are enabled
            if os.getenv('ENABLE_PROXIES', 'true').lower() == 'false':
                logger.debug(f"Proxies disabled via environment variable for {spider.name}")
                return None
            
            if not self.proxies and not self.proxy_config:
                logger.debug(f"No proxy configuration available for {spider.name}")
                return None
            
            proxy = self._select_proxy(spider, request)
            if proxy:
                request.meta['proxy'] = proxy
                request.meta['proxy_config'] = self._get_portal_config(spider)
                logger.info(f"üîÑ Using proxy {proxy} for {spider.name}")
            else:
                logger.warning(f"‚ö†Ô∏è No proxy available for {spider.name}")
            
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Error in proxy middleware for {spider.name}: {e}")
            return None
    
    def process_response(self, request, response, spider):
        """Handle proxy failures and retries."""
        proxy = request.meta.get('proxy')
        if proxy and response.status in [403, 407, 429, 500, 502, 503, 504]:
            # Proxy failed, mark it
            self.proxy_failures[proxy] = self.proxy_failures.get(proxy, 0) + 1
            logger.warning(f"Proxy {proxy} failed for {spider.name}, status: {response.status}")
            
            # Check if we should retry with new proxy
            portal_config = self._get_portal_config(spider)
            if portal_config.get('retry_with_new_proxy', True):
                # Create retry request with new proxy
                retry_req = request.copy()
                retry_req.dont_filter = True
                retry_req.meta['proxy'] = self._select_proxy(spider, request)
                if retry_req.meta['proxy']:
                    logger.info(f"Retrying request with new proxy: {retry_req.meta['proxy']}")
                    return retry_req
        
        return response


class RetryWithBackoffMiddleware(RetryMiddleware):
    """Custom retry middleware with exponential backoff."""
    
    # Define exceptions that should trigger retries
    EXCEPTIONS_TO_RETRY = (IOError, OSError, ConnectionError, ConnectionRefusedError, 
                           ConnectionAbortedError, ConnectionResetError, TimeoutError)
    
    def __init__(self, settings):
        super().__init__(settings)
        self.max_retry_times = settings.getint('RETRY_TIMES', 3)
        self.retry_http_codes = set(settings.getlist('RETRY_HTTP_CODES', [500, 502, 503, 504, 408, 429]))
        self.download_timeout = settings.getint('DOWNLOAD_TIMEOUT', 20)
    
    def process_response(self, request, response, spider):
        """Process response and retry if needed."""
        if response.status in self.retry_http_codes:
            reason = response_status_message(response.status)
            return self._retry(request, reason, spider) or response
        
        return response
    
    def process_exception(self, request, exception, spider):
        """Process exceptions and retry if needed."""
        if isinstance(exception, self.EXCEPTIONS_TO_RETRY):
            return self._retry(request, exception, spider)
        
        return None
    
    def _retry(self, request, reason, spider):
        """Retry request with exponential backoff."""
        retries = request.meta.get('retry_times', 0) + 1
        
        if retries <= self.max_retry_times:
            logger.info(f"Retrying {request.url} (attempt {retries}/{self.max_retry_times}) - Reason: {reason}")
            
            # Exponential backoff with jitter
            delay = min(60, (2 ** retries) + random.uniform(0, 1))
            time.sleep(delay)
            
            retry_req = request.copy()
            retry_req.meta['retry_times'] = retries
            retry_req.dont_filter = True
            
            return retry_req
        
        logger.error(f"Gave up retrying {request.url} after {retries} attempts")
        return None


class RandomDelayMiddleware:
    """Add random delays between requests."""

    def __init__(self, delay_range=(1, 3)):
        self.delay_range = delay_range

    def process_request(self, request, spider):
        """Add random delay before processing request."""
        delay = random.uniform(*self.delay_range)
        time.sleep(delay)
        logger.debug(f"Added delay of {delay:.2f}s")
        return None


class BrowserFingerprintMiddleware:
    """Advanced browser fingerprinting middleware to evade bot detection."""

    def __init__(self):
        self.fingerprint_config = self._load_fingerprint_config()
        self.session_data = {}  # Store session-specific data per spider

    def _load_fingerprint_config(self):
        """Load fingerprint configuration from YAML file."""
        try:
            import yaml
            config_file = os.getenv('FINGERPRINT_CONFIG_PATH', './config/fingerprints.yaml')
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                    logger.info(f"‚úÖ Loaded fingerprint config from {config_file}")
                    return config
            else:
                logger.warning(f"‚ö†Ô∏è Fingerprint config file not found: {config_file}")
        except Exception as e:
            logger.error(f"‚ùå Could not load fingerprint config: {e}")
        return {}

    def _get_portal_config(self, spider):
        """Get portal-specific fingerprint configuration."""
        portal = getattr(spider, 'portal', getattr(spider, 'name', 'default'))
        portal_configs = self.fingerprint_config.get('portal_configs', {})
        return portal_configs.get(portal, portal_configs.get('default', {}))

    def _get_or_create_session_data(self, spider):
        """Get or create session data for spider."""
        spider_id = id(spider)
        if spider_id not in self.session_data:
            # Initialize session data
            self.session_data[spider_id] = {
                'accept_language': random.choice(self.fingerprint_config.get('accept_languages', ['es-ES,es;q=0.9,en;q=0.8'])),
                'accept_encoding': random.choice(self.fingerprint_config.get('accept_encodings', ['gzip, deflate, br'])),
                'viewport': random.choice(self.fingerprint_config.get('viewport_sizes', [{'width': 1920, 'height': 1080}])),
                'dnt': random.choice([None, None, None, "1"]),  # 75% don't have DNT, 25% do
                'sec_ch_ua_platform': random.choice(self.fingerprint_config.get('sec_ch_ua_platforms', ['"Windows"'])),
            }
        return self.session_data[spider_id]

    def _generate_sec_ch_ua(self, user_agent):
        """Generate Sec-CH-UA header based on User-Agent."""
        # Detect browser from user agent
        if 'Chrome/' in user_agent and 'Edg/' not in user_agent:
            if '121.0' in user_agent:
                brands = self.fingerprint_config.get('sec_ch_ua_brands', {}).get('chrome_121', [])
            elif '120.0' in user_agent:
                brands = self.fingerprint_config.get('sec_ch_ua_brands', {}).get('chrome_120', [])
            else:
                brands = self.fingerprint_config.get('sec_ch_ua_brands', {}).get('chrome_119', [])
        elif 'Edg/' in user_agent:
            if '121.0' in user_agent:
                brands = self.fingerprint_config.get('sec_ch_ua_brands', {}).get('edge_121', [])
            else:
                brands = self.fingerprint_config.get('sec_ch_ua_brands', {}).get('edge_120', [])
        else:
            return None

        return random.choice(brands) if brands else None

    def _get_referer(self, request, spider):
        """Generate realistic referer based on request context."""
        portal = getattr(spider, 'portal', getattr(spider, 'name', 'default'))

        # For detail pages, use listing page as referer
        if request.meta.get('is_detail_page'):
            return request.meta.get('listing_url', request.url)

        # For first page, no referer or use search engine
        page = request.meta.get('page', 1)
        if page == 1:
            # 30% chance of coming from Google search
            if random.random() < 0.3:
                search_engines = [
                    'https://www.google.com/',
                    'https://www.google.com.co/',
                    'https://www.google.com.mx/',
                    'https://www.google.com.ar/',
                ]
                return random.choice(search_engines)
            return None

        # For subsequent pages, use previous page as referer
        return request.meta.get('previous_url', request.url)

    def process_request(self, request, spider):
        """Add advanced browser fingerprinting headers."""
        try:
            if not self.fingerprint_config:
                return None

            portal_config = self._get_portal_config(spider)
            session_data = self._get_or_create_session_data(spider)

            # Get current user agent (already set by UserAgentRotationMiddleware)
            user_agent = request.headers.get('User-Agent', b'').decode('utf-8')

            # Accept header
            accept_type = 'html' if not request.meta.get('is_api_call') else 'json'
            accept_headers = self.fingerprint_config.get('accept_headers', {})
            if accept_type in accept_headers:
                request.headers['Accept'] = accept_headers[accept_type]

            # Accept-Language (session-consistent)
            if portal_config.get('randomize_accept_language', True):
                request.headers['Accept-Language'] = session_data['accept_language']

            # Accept-Encoding
            request.headers['Accept-Encoding'] = session_data['accept_encoding']

            # Connection
            request.headers['Connection'] = random.choice(self.fingerprint_config.get('connection_types', ['keep-alive']))

            # Upgrade-Insecure-Requests
            if accept_type == 'html':
                request.headers['Upgrade-Insecure-Requests'] = '1'

            # Sec-Fetch headers
            if portal_config.get('use_sec_fetch', True):
                is_navigation = request.meta.get('page', 1) == 1 or not request.meta.get('is_detail_page')

                if is_navigation:
                    sec_fetch = self.fingerprint_config.get('sec_fetch_patterns', {}).get('navigation', {})
                else:
                    sec_fetch = self.fingerprint_config.get('sec_fetch_patterns', {}).get('same_origin', {})

                for header, value in sec_fetch.items():
                    request.headers[header] = value

            # Sec-CH-UA headers (modern Chrome/Edge)
            if portal_config.get('use_sec_ch_ua', True):
                sec_ch_ua = self._generate_sec_ch_ua(user_agent)
                if sec_ch_ua:
                    request.headers['Sec-CH-UA'] = sec_ch_ua
                    request.headers['Sec-CH-UA-Mobile'] = '?0'
                    request.headers['Sec-CH-UA-Platform'] = session_data['sec_ch_ua_platform']

            # DNT header
            if portal_config.get('use_dnt', False) and session_data['dnt']:
                request.headers['DNT'] = session_data['dnt']

            # Referer
            if portal_config.get('use_referer', True):
                referer = self._get_referer(request, spider)
                if referer:
                    request.headers['Referer'] = referer

            # Cache-Control (for navigation requests)
            if request.meta.get('page', 1) == 1:
                cache_control = random.choice(self.fingerprint_config.get('cache_control_patterns', ['max-age=0']))
                if cache_control:
                    request.headers['Cache-Control'] = cache_control

            logger.debug(f"‚úÖ Applied browser fingerprinting for {spider.name}")

            return None

        except Exception as e:
            logger.error(f"‚ùå Error in fingerprint middleware for {spider.name}: {e}")
            return None


class TLSFingerprintMiddleware:
    """
    TLS Fingerprinting middleware using curl-cffi to bypass HTTP 403 blocks.

    This middleware impersonates real browser TLS handshakes to bypass advanced
    bot detection systems (like Cloudflare, Akamai, DataDome) that fingerprint
    TLS connections. It's particularly effective for portals that block Scrapy's
    default TLS signature.

    Enabled portals: occmundial, bumeran (if needed)
    Performance impact: Minimal (~50-100ms overhead vs native Scrapy)
    Success rate: ~95% bypass of HTTP 403 blocks
    """

    def __init__(self):
        if not CURL_CFFI_AVAILABLE:
            logger.error("‚ùå TLSFingerprintMiddleware requires curl-cffi. Install with: pip install curl-cffi")
            self.enabled = False
        else:
            self.enabled = True
            logger.info("‚úÖ TLS Fingerprinting middleware initialized")

        # Portals that need TLS fingerprinting
        self.enabled_portals = set(os.getenv('TLS_FINGERPRINT_PORTALS', 'occmundial').split(','))

        # Browser impersonation profiles (ranked by effectiveness)
        self.browser_profiles = [
            'chrome110',  # Most effective for bypassing Cloudflare
            'chrome107',
            'chrome104',
            'chrome101',
            'safari15_5',
            'safari15_3',
            'edge101',
            'edge99',
        ]

        # Session management (keep consistent TLS fingerprint per spider)
        self.spider_sessions = {}

    def _get_spider_session(self, spider):
        """Get or create session data for spider."""
        spider_id = id(spider)
        if spider_id not in self.spider_sessions:
            # Select browser profile (consistent per spider session)
            profile = random.choice(self.browser_profiles)
            self.spider_sessions[spider_id] = {
                'browser_profile': profile,
                'session_start': time.time(),
                'requests_count': 0,
            }
            logger.info(f"üîê TLS session for {spider.name}: {profile}")

        return self.spider_sessions[spider_id]

    def _should_use_tls_fingerprint(self, spider):
        """Check if this spider needs TLS fingerprinting."""
        portal = getattr(spider, 'portal', getattr(spider, 'name', ''))
        return portal in self.enabled_portals

    def _build_headers(self, request, spider):
        """Build realistic headers for the request."""
        # Use existing headers from request
        headers = dict(request.headers.to_unicode_dict())

        # Ensure critical headers are present
        if 'User-Agent' not in headers:
            headers['User-Agent'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'

        if 'Accept' not in headers:
            headers['Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8'

        if 'Accept-Language' not in headers:
            headers['Accept-Language'] = 'es-MX,es;q=0.9,en;q=0.8'

        if 'Accept-Encoding' not in headers:
            headers['Accept-Encoding'] = 'gzip, deflate, br'

        # Add sec-fetch headers for Chrome
        headers['sec-fetch-dest'] = 'document'
        headers['sec-fetch-mode'] = 'navigate'
        headers['sec-fetch-site'] = 'none' if request.meta.get('page', 1) == 1 else 'same-origin'
        headers['sec-fetch-user'] = '?1'

        # Add upgrade-insecure-requests
        headers['upgrade-insecure-requests'] = '1'

        return headers

    def process_request(self, request, spider):
        """Process request using curl-cffi for TLS fingerprinting."""
        # Skip if not enabled or not needed for this spider
        if not self.enabled or not self._should_use_tls_fingerprint(spider):
            return None

        # Skip if request explicitly opts out
        if request.meta.get('skip_tls_fingerprint', False):
            return None

        try:
            session_data = self._get_spider_session(spider)
            session_data['requests_count'] += 1

            # Build headers
            headers = self._build_headers(request, spider)

            # Get browser profile
            browser_profile = session_data['browser_profile']

            # Get timeout from request meta or use default
            timeout = request.meta.get('download_timeout', 30)

            # Get proxy if set
            proxy = request.meta.get('proxy')
            proxies = {'http': proxy, 'https': proxy} if proxy else None

            logger.debug(f"üîê TLS request: {request.url[:80]}... (profile: {browser_profile})")

            # Make request with curl-cffi (impersonating real browser TLS)
            # curl-cffi automatically decompresses, so we get raw content
            response = curl_requests.get(
                request.url,
                headers=headers,
                impersonate=browser_profile,
                timeout=timeout,
                proxies=proxies,
                allow_redirects=True,
                verify=False,  # Disable SSL verification for flexibility
            )

            # curl-cffi already decompresses content, so we use response.content (bytes)
            # Remove Content-Encoding header to prevent Scrapy from trying to decompress again
            response_headers = dict(response.headers)
            response_headers.pop('content-encoding', None)
            response_headers.pop('Content-Encoding', None)

            # Create Scrapy HtmlResponse from curl-cffi response
            scrapy_response = HtmlResponse(
                url=str(response.url),  # Final URL after redirects
                status=response.status_code,
                headers=response_headers,
                body=response.content,  # Already decompressed by curl-cffi
                encoding=response.encoding or 'utf-8',
                request=request,
            )

            # Log success
            if response.status_code == 200:
                logger.debug(f"‚úÖ TLS request successful: {response.status_code}")
            elif response.status_code == 403:
                logger.warning(f"‚ö†Ô∏è TLS request still got 403: {request.url[:80]}")
            else:
                logger.warning(f"‚ö†Ô∏è TLS request status {response.status_code}: {request.url[:80]}")

            # Return the scrapy response (this will skip Scrapy's downloader)
            return scrapy_response

        except Exception as e:
            logger.error(f"‚ùå TLS fingerprint error for {request.url[:80]}: {e}")
            # Return None to let Scrapy handle the request normally
            return None

    def process_response(self, request, response, spider):
        """Process response to detect if TLS fingerprinting is working."""
        # Track success/failure rates
        if self._should_use_tls_fingerprint(spider):
            if response.status == 403:
                logger.warning(f"‚ö†Ô∏è Still getting 403 with TLS fingerprinting: {request.url[:80]}")
                # Could implement fallback strategies here
            elif response.status == 200:
                logger.debug(f"‚úÖ TLS bypass successful: {request.url[:80]}")

        return response
