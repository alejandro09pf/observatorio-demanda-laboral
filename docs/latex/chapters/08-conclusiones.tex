\chapter{CONCLUSIONES Y TRABAJO FUTURO}

Este trabajo diseñó, implementó y validó un sistema completo de observatorio de demanda laboral para América Latina, comparando tres enfoques de extracción de habilidades técnicas: métodos basados en reglas y reconocimiento de entidades nombradas (Pipeline A), métodos estadísticos con TF-IDF y n-gramas (Pipeline A.1), y modelos de lenguaje grandes (Pipeline B). La evaluación rigurosa sobre un gold standard de 7,848 anotaciones manuales demostró la superioridad de Pipeline B, estableciendo métricas cuantitativas para la comparación de enfoques de extracción de habilidades en ofertas laborales.

\section{Hallazgos Principales}

\subsection{Superioridad de Modelos de Lenguaje Grandes}

Los resultados experimentales presentados en el Capítulo 7 demuestran de manera concluyente que los modelos de lenguaje grandes superan a métodos tradicionales basados en reglas y reconocimiento de entidades nombradas. Pipeline B alcanzó un F1-Score post-ESCO de 84.26\%, superando en 11.73 puntos porcentuales a Pipeline A (72.53\%), lo que representa una mejora relativa del 16.2\%. En términos de precisión, Pipeline B obtuvo 89.25\% versus 65.50\% de Pipeline A, evidenciando una mejora relativa del 36.3\%. Esta superioridad se mantiene consistentemente incluso en evaluación pre-ESCO, donde Pipeline B alcanzó casi el doble de rendimiento que métodos tradicionales.

Más allá de las métricas cuantitativas, los LLMs demuestran robustez cualitativa ante variaciones de ortografía, nomenclatura y lenguaje. Mientras que métodos basados en reglas y expresiones regulares requieren actualización manual constante para capturar variantes como ``React.js''/``ReactJS''/``React JS'', errores tipográficos como ``Javascrpt'' o ``Kuberentes'', y nomenclaturas alternativas como ``K8s'' para Kubernetes, los modelos de lenguaje comprenden estas variaciones sin modificación de sus parámetros. Esta adaptabilidad elimina el mantenimiento continuo de diccionarios y patrones que caracteriza a sistemas basados en NER y regex, reduciendo significativamente el esfuerzo operativo de largo plazo.

\subsection{Detección de Habilidades Emergentes}

Los resultados confirman que los LLMs detectan habilidades emergentes ausentes en taxonomías estáticas como ESCO. El 59.5\% de habilidades extraídas por Pipeline B carecen de equivalente en ESCO v1.1.0, identificándose 4,945 habilidades emergentes de 8,301 extraídas en total. Estas incluyen tecnologías modernas como SAM (AWS Serverless Application Model), CDK (Cloud Development Kit), React Hooks y Kubernetes Custom Resource Definitions, que aparecen con frecuencia significativa en múltiples ofertas, validando demandas reales del mercado. Este hallazgo confirma la limitación inherente de taxonomías estáticas que se actualizan cada 2-3 años, mientras el mercado tecnológico evoluciona en ciclos de 6-12 meses.

\subsection{Inferencia de Habilidades Implícitas}

Los LLMs demostraron capacidad de inferir habilidades implícitas a partir del contexto de las ofertas laborales. Esta comprensión contextual permite a los modelos identificar tecnologías y competencias que no aparecen explícitamente mencionadas, sino que se infieren de las responsabilidades descritas. Por ejemplo, al leer "diseñar arquitecturas escalables para millones de usuarios", el modelo puede inferir conocimientos en sistemas distribuidos y cloud computing aunque estos términos no aparezcan textualmente.

Esta capacidad de comprensión contextual representa una ventaja cualitativa fundamental respecto a métodos sintácticos tradicionales, que se limitan a detectar menciones explícitas mediante patrones léxicos.

\section{Contribuciones del Trabajo}

Los hallazgos anteriores se sustentan en un conjunto de contribuciones metodológicas, técnicas, empíricas y prácticas que este trabajo aporta al campo de extracción automática de habilidades en ofertas laborales. Estas contribuciones se organizan en cuatro dimensiones complementarias que abarcan desde fundamentos metodológicos hasta aplicaciones prácticas inmediatas.

\subsection{Contribuciones Metodológicas}

Este trabajo desarrolló la primera evaluación rigurosa de modelos de lenguaje grandes versus métodos tradicionales para extracción de habilidades en español latinoamericano, llenando un vacío en la literatura que se ha concentrado principalmente en inglés y mercados europeos o estadounidenses.

La metodología de evaluación dual (pre-ESCO + post-ESCO) constituye una contribución metodológica clave. Esta separación permite comparar la capacidad de extracción pura de cada pipeline independientemente de su capacidad de normalización a taxonomías, evitando confundir ambas dimensiones en una única métrica compuesta.

El gold standard de 7,848 anotaciones manuales con clasificación de habilidades técnicas, junto con el sistema de normalización canónica de 193 mapeos tecnológicos validados, constituye un recurso reutilizable para investigaciones futuras en el dominio de análisis de mercado laboral latinoamericano.

\subsection{Contribuciones Técnicas}

Se implementó un sistema completo end-to-end operativo que integra scraping, limpieza, extracción, mapeo a taxonomías, generación de embeddings y clustering semántico. La arquitectura de scraping incorpora 7 scrapers especializados que recolectan ofertas de portales regionales en Colombia, México y Argentina, manejando tanto sitios estáticos como dinámicos con JavaScript.

El componente de mapeo a ESCO se optimizó con dos capas: exact matching para coincidencias directas y fuzzy matching con threshold 0.92 para variantes ortográficas, reduciendo significativamente falsos positivos respecto a thresholds más permisivos. El sistema detecta y etiqueta habilidades emergentes sin equivalente en ESCO, preservando información de demanda tecnológica actual.

Pipeline A evolucionó mediante 7 experimentos iterativos que mejoraron F1 post-ESCO de 23.45\% inicial a 72.53\% final, demostrando que la experimentación sistemática produce mejoras sustanciales incluso en enfoques basados en reglas. El clustering semántico UMAP+HDBSCAN organizó más de 30,000 habilidades en 53 clusters coherentes, revelando agrupaciones naturales de tecnologías relacionadas. Todo el sistema está disponible como código abierto.

\subsection{Contribuciones Empíricas}

El trabajo identifica empíricamente limitaciones concretas de ESCO como taxonomía oficial para el dominio tecnológico latinoamericano. Se documenta sesgo europeo en la selección de ocupaciones y habilidades, reflejando prioridades del mercado laboral europeo que no necesariamente coinciden con las dinámicas regionales latinoamericanas. La granularidad resulta inconsistente entre categorías: algunas áreas tecnológicas presentan descomposición excesivamente detallada mientras otras permanecen agregadas en términos genéricos.

La desactualización tecnológica constituye otra limitación crítica observada. Herramientas y frameworks modernos ampliamente demandados en el mercado (React Hooks, Serverless Framework, infrastructure-as-code con Terraform) carecen de representación en ESCO v1.1.0, reflejando el rezago inherente a taxonomías que se actualizan cada 2-3 años. Estas observaciones empíricas contribuyen a la discusión sobre la necesidad de taxonomías dinámicas actualizadas frecuentemente o sistemas híbridos que combinen vocabularios controlados con detección automática de términos emergentes.

\subsection{Contribuciones Prácticas}

Desde la perspectiva de aplicabilidad inmediata, el observatorio genera artefactos concretos utilizables por diversos actores del ecosistema tecnológico. La base de datos de 30,660 ofertas laborales recolectadas de Colombia, México y Argentina está disponible para análisis, proporcionando un corpus representativo del mercado laboral regional que puede informar decisiones de política educativa, estrategias de contratación empresarial y planificación de trayectorias profesionales.

El sistema genera visualizaciones de clustering semántico y perfiles de habilidades técnicas que facilitan la exploración intuitiva de las agrupaciones identificadas, permitiendo a usuarios no técnicos identificar patrones de demanda sin requerir conocimientos especializados de análisis de datos. Arquitecturalmente, la infraestructura modular diseñada soporta escalamiento futuro a millones de ofertas mediante batch processing optimizado, asegurando que la solución técnica actual pueda evolucionar con el crecimiento del proyecto.

\section{Limitaciones Identificadas}

Si bien las contribuciones descritas son sustanciales, la honestidad académica requiere reconocer limitaciones del trabajo realizado. Estas limitaciones se organizan en tres dimensiones: el sistema implementado, los datos recolectados y la evaluación realizada. Identificar estos aspectos explícitamente facilita que trabajos futuros puedan abordarlos sistemáticamente.

\subsection{Limitaciones del Sistema}

La principal limitación operativa de Pipeline B es su velocidad de procesamiento. Con una mediana de 18 segundos por oferta, el sistema requiere aproximadamente 15-25 segundos por documento versus 1-2 segundos de Pipeline A. Esta diferencia limita la aplicabilidad en escenarios de tiempo real donde se requiere respuesta inmediata.

Adicionalmente, una tasa de error del 0.7\% (2 de 300 ofertas) experimentó \textit{mode collapse}, donde el modelo LLM entró en ciclos de repetición infinita que requirieron intervención manual. Aunque la frecuencia es baja, estos casos evidencian fragilidad ocasional en la generación.

En el componente de mapeo ESCO, persisten falsos positivos en fuzzy matching. Un ejemplo emblemático es el mapeo erróneo de ``REST'' (arquitectura de APIs) a ``restaurar dentaduras'' en la taxonomía odontológica de ESCO. Aunque el threshold 0.92 mitiga significativamente este problema comparado con valores más permisivos, no lo elimina por completo.

Adicionalmente, existe un desafío metodológico en la evaluación del matcher: una cantidad significativa de habilidades extraídas no se mapean correctamente a ESCO debido a incompatibilidades semánticas o granularidad inadecuada. Si estas habilidades se normalizaran manualmente para forzar mapeos a ESCO, se introduciría sesgo favorable hacia el pipeline que generó extracciones más compatibles con la estructura de ESCO (potencialmente favoreciendo métodos basados en regex/NER que extraen términos más convencionales). Este sesgo comprometería la validez de la comparación entre pipelines, por lo que se optó por aceptar habilidades sin mapeo como información válida sobre demanda tecnológica emergente.

\subsection{Limitaciones del Dataset}

El gold standard de 300 ofertas, si bien estadísticamente significativo, podría ampliarse para análisis más robustos con mayor diversidad de casos. La cobertura temporal presenta desbalance con mayor concentración en años recientes (2020-2025), lo que limita la capacidad de análisis de evolución histórica de largo plazo.

Geográficamente, el dataset se restringe a Colombia, México y Argentina, excluyendo otros mercados latinoamericanos importantes como Perú, Chile y Ecuador. La arquitectura de scraping actual incorpora 7 scrapers especializados, pero omite actores relevantes del mercado como LinkedIn, Indeed completo y Glassdoor, introduciendo potencial sesgo hacia portales regionales tradicionales.

\subsection{Limitaciones de Evaluación}

Las anotaciones manuales del gold standard provienen de un único anotador, introduciendo potencial sesgo subjetivo en la definición de qué constituye una habilidad relevante. Este es un desafío inherente a tareas de anotación sin consenso objetivo establecido en la literatura.

El análisis temporal de evolución de demanda de habilidades fue documentado conceptualmente pero no ejecutado completamente debido a la falta de datos históricos suficientes en el corpus recolectado, que concentra la mayor parte de ofertas en el período 2020-2025.

\section{Lecciones Aprendidas}

Las limitaciones identificadas, junto con los aciertos técnicos y metodológicos del proyecto, generan un conjunto de lecciones aprendidas valiosas. Estas lecciones son aplicables tanto a proyectos similares de NLP aplicado como al desarrollo de sistemas de análisis del mercado laboral en general. Se organizan en tres categorías: metodológicas, tecnológicas y arquitecturales.

\subsection{Iteración Sistemática y Evaluación Dual}

Una lección fundamental del proyecto es que la iteración sistemática basada en evidencia produce mejoras sustanciales incluso en enfoques aparentemente simples. Pipeline A evolucionó de un F1 inicial de 23.45\% a 72.53\% final (49 puntos porcentuales de mejora) y de Recall de 30\% a 81.25\% a través de 7 experimentos controlados. Cada iteración identificó debilidades específicas mediante análisis de errores, informando el diseño de la siguiente versión.

La separación metodológica entre extracción pura (pre-ESCO) y normalización (post-ESCO) fue esencial para este proceso. Sin esta distinción, hubiera sido imposible determinar si las fallas provenían de la etapa de detección de habilidades o de la etapa de mapeo a taxonomía, llevando potencialmente a optimizaciones en componentes incorrectos.

\subsection{Eficiencia de Modelos Pequeños y Limitaciones de Taxonomías}

El proyecto demostró que modelos LLM de 4B parámetros (Gemma 3 4B) compiten efectivamente con alternativas más grandes sin requerir infraestructura costosa. Esta observación tiene implicaciones importantes para la democratización de capacidades avanzadas de NLP, permitiendo a instituciones con recursos limitados implementar soluciones competitivas en hardware consumer.

Respecto a las taxonomías oficiales, ESCO resultó útil para normalización post-extracción, proporcionando identificadores estables y descripciones estandarizadas de habilidades. Sin embargo, su insuficiencia para cubrir tecnologías emergentes confirma la necesidad de sistemas híbridos que combinen taxonomías establecidas con detección dinámica de habilidades, en lugar de depender exclusivamente de vocabularios controlados estáticos.

\subsection{Valor del \textit{Gold Standard} y Comparación Multi-Modelo}

Las 7,848 anotaciones manuales constituyen el activo más valioso del proyecto, habilitando evaluación rigurosa y cuantitativa de los diferentes enfoques. Sin este dataset de referencia, la comparación entre pipelines habría dependido de evaluación cualitativa subjetiva o métricas indirectas poco concluyentes.

La comparación sistemática de 4 modelos LLM diferentes (Gemma, Llama, Qwen, Phi) resultó esencial para decisión informada. Las diferencias observadas en precisión, recall y velocidad de inferencia no eran predecibles a priori, validando la necesidad de experimentación empírica versus adopción de modelos basada únicamente en popularidad o reputación.

\subsection{Decisiones Arquitecturales y Tecnológicas}

El desarrollo del orquestador central mediante CLI unificada reemplazó más de 100 scripts dispersos, simplificando operación y mantenimiento del sistema. Esta decisión arquitectural redujo significativamente la complejidad operativa y facilitó la reproducibilidad de experimentos.

La búsqueda semántica mediante embeddings E5 multilingual y FAISS falló para vocabulario técnico, generando falsos positivos inaceptables. Esta experiencia evidencia que modelos de embeddings generalistas no siempre capturan adecuadamente jerga especializada, requiriendo validación experimental en cada dominio de aplicación.

\section{Trabajo Futuro}

Las contribuciones realizadas y las limitaciones identificadas abren múltiples líneas de investigación y desarrollo. Estas oportunidades se organizan por horizonte temporal, desde extensiones incrementales de corto plazo hasta proyectos de investigación académica de mayor alcance. Las prioridades reflejan tanto el potencial de impacto como la viabilidad técnica de cada línea.

\subsection{Extensiones de Corto Plazo}

La primera línea de trabajo futuro inmediato es la completación del análisis temporal de demanda de habilidades. Este análisis generará heatmaps de evolución trimestral desde 2015 hasta 2025, cuantificando tendencias estacionales y ciclos de adopción tecnológica en la región.

Una segunda extensión valiosa es la evaluación de LLMs más grandes (Llama 3.3 70B, GPT-4o, Claude 3.5 Sonnet) para validar si el incremento en capacidad del modelo justifica el mayor costo computacional. Esta comparación cuantificará la relación costo-beneficio entre modelos pequeños y grandes en la tarea específica de extracción de habilidades.

Finalmente, la ampliación de cobertura geográfica a Perú, Chile, Uruguay y Ecuador con al menos 1,000 ofertas por país diversificará la representatividad regional del observatorio, permitiendo análisis comparativos de demanda laboral entre diferentes mercados latinoamericanos.

\subsection{Desarrollo de Mediano Plazo}

En el mediano plazo, el fine-tuning de un LLM específico (Gemma o Llama) utilizando las 7,848 anotaciones del gold standard puede mejorar la precisión de extracción y reducir alucinaciones específicas del dominio. Este entrenamiento supervisado podría capturar patrones particulares del lenguaje de ofertas laborales latinoamericanas que los modelos generalistas no optimizan.

El desarrollo de una API pública con endpoints REST transformaría el observatorio de herramienta de investigación a plataforma de servicio. Esta API permitiría extracción de habilidades en tiempo real e integración con sistemas de terceros, habilitando aplicaciones como análisis automático de CVs o sugerencias de capacitación personalizadas.

Complementariamente, un dashboard interactivo implementado con React y D3.js democratizaría el acceso a los resultados del observatorio. Esta interfaz permitiría a usuarios no técnicos explorar visualizaciones de tendencias, clusters y perfiles de habilidades sin requerir conocimientos de consultas SQL o análisis de datos.

\subsection{Proyectos de Largo Plazo}

Un proyecto ambicioso de largo plazo es la creación de una taxonomía dinámica latinoamericana actualizada mensualmente mediante agregación automática de habilidades emergentes. Esta taxonomía regional reemplazaría la dependencia de ESCO europea, reflejando las particularidades del mercado laboral tecnológico latinoamericano y actualizándose a velocidades compatibles con la evolución del sector.

El desarrollo de modelos de series temporales para predicción de demanda futura de habilidades con 3-6 meses de anticipación constituye otra línea valiosa. Estas predicciones podrían informar decisiones curriculares de instituciones educativas y programas de capacitación empresarial, permitiendo ajustes proactivos antes que las brechas de habilidades se materialicen.

Un sistema de matching bidireccional oferta-candidato basado en embeddings de habilidades representa una aplicación directa con valor comercial inmediato. Este sistema compararía perfiles semánticos de ofertas y candidatos más allá de coincidencias léxicas superficiales, identificando compatibilidades basadas en proximidad en el espacio de embeddings.

\section{Reflexión Final}

Este trabajo demuestra empíricamente la viabilidad y superioridad de los modelos de lenguaje grandes para extracción de habilidades técnicas en el contexto latinoamericano. La mejora de 16.2\% en F1-Score respecto a métodos tradicionales, combinada con la capacidad de detectar 59.5\% de habilidades emergentes ausentes en taxonomías oficiales, establece fundamentos cuantitativos sólidos para esta conclusión.

Los resultados obtenidos sientan las bases para un observatorio laboral dinámico con aplicaciones prácticas inmediatas. Instituciones académicas pueden usar los datos de habilidades emergentes para actualizar currículos de formación en tecnología. Empresas pueden identificar tendencias de contratación y diseñar programas de capacitación interna basados en demanda real del mercado. Desarrolladores pueden orientar sus trayectorias profesionales hacia habilidades con alta demanda validada empíricamente.

La democratización del acceso mediante código open-source y el uso de modelos locales de 4B parámetros ejecutables en hardware consumer es particularmente relevante para el contexto latinoamericano. Instituciones con recursos limitados pueden implementar soluciones similares sin depender de infraestructura costosa o APIs comerciales, contribuyendo al desarrollo tecnológico regional mediante herramientas accesibles.

Desde la perspectiva de Ingeniería de Sistemas, el proyecto demuestra que la integración efectiva de tecnologías heterogéneas produce sistemas robustos para problemas reales. El observatorio combina web scraping distribuido, procesamiento de lenguaje natural, modelos de lenguaje grandes, bases de datos relacionales y clustering no supervisado en una arquitectura modular escalable. El sistema procesa actualmente 30,660 ofertas laborales de tres países, pero la arquitectura soporta escalamiento a millones de ofertas y decenas de países sin cambios fundamentales en el diseño.

A diferencia de observatorios europeos o estadounidenses, este sistema captura particularidades del mercado latinoamericano: idioma español con mezcla de inglés técnico, portales de empleo regionales específicos, y dinámicas económicas propias de la región. Esta contextualización geográfica es esencial para que los resultados sean relevantes y accionables para actores locales.

Finalmente, la demostración cuantitativa de superioridad de LLMs, la identificación honesta de limitaciones, y la documentación exhaustiva del sistema establecen un precedente metodológico para futuros trabajos en el área. Este proyecto demuestra que la combinación rigurosa de métodos tradicionales de NLP, modelos de lenguaje grandes y evaluación sistemática con gold standard produce sistemas interpretables y de alto rendimiento aplicables a problemas reales del mercado laboral latinoamericano.
