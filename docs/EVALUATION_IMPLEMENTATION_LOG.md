# Sistema de Evaluaci√≥n de Pipelines - Log de Implementaci√≥n

**Fecha de Inicio:** 2025-11-05
**Autores:** Nicol√°s Francisco Camacho Alarc√≥n y Alejandro Pinz√≥n
**Objetivo:** Evaluar y comparar Pipeline A (NER+Regex) vs Pipeline B (LLMs) contra Gold Standard

---

## üìã CONTEXTO DEL PROYECTO

### Estado Actual de la Base de Datos

**Gold Standard:**
- ‚úÖ 300 jobs seleccionados (`raw_jobs.is_gold_standard = TRUE`)
- ‚úÖ 7,848 skills anotados manualmente (`gold_standard_annotations`)
- Formato: `skill_text` capitalizado (Python, JavaScript, PostgreSQL, etc.)

**Pipeline A (NER + Regex + ESCO):**
- ‚úÖ C√≥digo implementado en `src/extractor/`
- ‚ùå NO ejecutado en los 300 jobs gold standard
- Tabla: `extracted_skills` (actualmente vac√≠a)

**Pipeline B (LLM Direct Extraction):**
- ‚úÖ C√≥digo implementado en `src/llm_processor/`
- ‚úÖ 9 LLMs configurados (Gemma 3, Llama 3.2, Qwen 2.5, Qwen3, Mistral, Phi, DeepSeek)
- ‚ùå NO ejecutado en los 300 jobs gold standard
- Tabla: `enhanced_skills` (actualmente vac√≠a)

**Infraestructura ESCO:**
- ‚úÖ 14,174 skills en taxonom√≠a (ESCO + O*NET + Manual)
- ‚úÖ Embeddings generados (768D, multilingual-e5-base)
- ‚úÖ √çndice FAISS construido (30,147 q/s)
- ‚úÖ ESCOMatcher3Layers con 3 capas (exact, fuzzy, semantic)

---

## üéØ ESTRATEGIA DE EVALUACI√ìN ACORDADA

### Decisi√≥n Clave: Doble Comparaci√≥n

**PROBLEMA IDENTIFICADO:**
- Pipeline A ya mapea a ESCO durante extracci√≥n
- Pipeline B no mapea a ESCO (solo extrae texto)
- Gold Standard probablemente no tiene ESCO URIs
- Skills emergentes (Next.js, Tailwind, FastAPI) no est√°n en ESCO

**SOLUCI√ìN ACORDADA:**
Realizar **2 comparaciones independientes**:

1. **Comparaci√≥n 1: Extracci√≥n Pura (Texto Normalizado)**
   - Eval√∫a capacidad de extracci√≥n SIN bias de ESCO
   - No penaliza skills emergentes
   - Completamente justo entre pipelines

2. **Comparaci√≥n 2: Post-Mapeo ESCO (Estandarizaci√≥n)**
   - Mapear TODOS los pipelines con el MISMO c√≥digo (`ESCOMatcher3Layers`)
   - Re-mapear Pipeline A (no usar su mapeo existente)
   - Eval√∫a qu√© pipeline produce skills m√°s estandarizables
   - Calcula impacto de ESCO en m√©tricas

### Normalizaci√≥n Unificada

**Reglas de normalizaci√≥n:**
1. Tecnolog√≠as conocidas: Diccionario can√≥nico (PostgreSQL, JavaScript, React, AWS)
2. Variantes comunes: postgres‚ÜíPostgreSQL, js‚ÜíJavaScript, k8s‚ÜíKubernetes
3. Fallback: Title case (primera letra may√∫scula)
4. Remover espacios extras, caracteres especiales

**Aplicaci√≥n:**
- Normalizar Gold Standard antes de comparar
- Normalizar Pipeline A (ignorar esco_uri existente)
- Normalizar Pipeline B

---

## üìä M√âTRICAS A CALCULAR

### M√©tricas Principales (Ambas Comparaciones)

**Por Pipeline:**
- **Precision:** % de skills extra√≠dos que est√°n en gold standard
- **Recall:** % de skills del gold standard que fueron detectados
- **F1-Score:** Media arm√≥nica de Precision y Recall
- **False Positives:** Skills extra√≠dos que NO est√°n en gold standard
- **False Negatives:** Skills en gold standard que NO fueron detectados

**M√©tricas Adicionales (Comparaci√≥n 2):**
- **Cobertura ESCO:** % de skills que se pudieron mapear a ESCO
- **Skills Perdidas:** Skills que se perdieron en el mapeo
- **Delta F1:** Diferencia de F1 entre Comparaci√≥n 1 y 2
- **Skills Emergentes:** Skills modernas no en ESCO

### An√°lisis por Contexto (Pipeline B)

**Por portal:** bumeran, computrabajo, elempleo, etc.
**Por pa√≠s:** CO, MX, AR
**Por tipo de skill:** lenguajes, frameworks, cloud, databases, soft skills
**Por longitud de job ad:** corto (< 500 palabras), medio (500-1500), largo (> 1500)
**Por complejidad:** n√∫mero de skills en gold standard (simple: < 20, complejo: > 40)

### An√°lisis de Rendimiento (Pipeline B)

**Velocidad:** segundos/job
**Tokens:** generados y consumidos
**Memoria:** uso de RAM durante inferencia

---

## üèóÔ∏è ARQUITECTURA DEL SISTEMA

### Estructura de Directorios

```
src/evaluation/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ normalizer.py              # Normalizaci√≥n unificada de skills
‚îú‚îÄ‚îÄ metrics.py                 # Precision, Recall, F1, Confusion Matrix
‚îú‚îÄ‚îÄ dual_comparator.py         # Comparaci√≥n dual (texto + ESCO)
‚îú‚îÄ‚îÄ pipeline_evaluator.py      # Eval√∫a un pipeline individual
‚îú‚îÄ‚îÄ llm_evaluator.py           # Compara LLMs individuales
‚îú‚îÄ‚îÄ context_analyzer.py        # An√°lisis por contexto
‚îî‚îÄ‚îÄ report_generator.py        # Genera reportes MD/CSV/JSON

scripts/
‚îú‚îÄ‚îÄ run_pipeline_a_gold_standard.py      # Ejecuta Pipeline A en 300 jobs
‚îú‚îÄ‚îÄ run_pipeline_b_gold_standard.py      # Ejecuta Pipeline B (por modelo)
‚îú‚îÄ‚îÄ evaluate_extraction_pipelines.py     # Script principal de evaluaci√≥n
‚îî‚îÄ‚îÄ generate_evaluation_reports.py       # Genera reportes finales

data/reports/
‚îú‚îÄ‚îÄ EVALUATION_REPORT.md                 # Reporte ejecutivo principal
‚îú‚îÄ‚îÄ pipeline_comparison_pure.csv         # Comparaci√≥n 1 (texto)
‚îú‚îÄ‚îÄ pipeline_comparison_esco.csv         # Comparaci√≥n 2 (ESCO)
‚îú‚îÄ‚îÄ llm_ranking.csv                      # Ranking de LLMs
‚îú‚îÄ‚îÄ llm_context_analysis.csv             # Performance por contexto
‚îú‚îÄ‚îÄ esco_impact_analysis.csv             # An√°lisis de impacto ESCO
‚îî‚îÄ‚îÄ charts/                              # Gr√°ficos PNG
    ‚îú‚îÄ‚îÄ f1_comparison_pure.png
    ‚îú‚îÄ‚îÄ f1_comparison_esco.png
    ‚îú‚îÄ‚îÄ context_heatmap.png
    ‚îú‚îÄ‚îÄ confusion_matrix_*.png
    ‚îî‚îÄ‚îÄ speed_vs_quality.png
```

---

## üöÄ PLAN DE EJECUCI√ìN

### FASE 0: Preparaci√≥n (30 min)
- [x] An√°lisis de c√≥digo existente
- [x] Definici√≥n de estrategia de evaluaci√≥n
- [ ] Crear documento de memoria persistente
- [ ] Crear estructura de directorios

### FASE 1: Normalizaci√≥n Unificada (30 min)
- [ ] Implementar `src/evaluation/normalizer.py`
- [ ] Diccionario can√≥nico de tecnolog√≠as
- [ ] Funciones de normalizaci√≥n
- [ ] Tests unitarios de normalizaci√≥n

### FASE 2: Ejecutar Pipelines (1-2 horas)
- [ ] Script: `run_pipeline_a_gold_standard.py`
- [ ] Ejecutar Pipeline A en 300 jobs gold standard
- [ ] Script: `run_pipeline_b_gold_standard.py`
- [ ] Ejecutar Pipeline B con 4 LLMs:
  - [ ] Llama 3.2 3B Instruct
  - [ ] Gemma 3 4B Instruct
  - [ ] Qwen 2.5 3B Instruct
  - [ ] Qwen3 4B
- [ ] Verificar datos en BD

### FASE 3: Sistema de M√©tricas (1 hora)
- [ ] Implementar `src/evaluation/metrics.py`
- [ ] Precision, Recall, F1
- [ ] Confusion Matrix
- [ ] False Positives/Negatives detallados

### FASE 4: Comparador Dual (2 horas)
- [ ] Implementar `src/evaluation/dual_comparator.py`
- [ ] Comparaci√≥n 1: Texto normalizado
- [ ] Comparaci√≥n 2: Post-mapeo ESCO
- [ ] An√°lisis de impacto ESCO
- [ ] Identificaci√≥n de skills perdidas

### FASE 5: An√°lisis de Contexto (1-2 horas)
- [ ] Implementar `src/evaluation/context_analyzer.py`
- [ ] An√°lisis por portal
- [ ] An√°lisis por pa√≠s
- [ ] An√°lisis por tipo de skill
- [ ] An√°lisis de rendimiento (velocidad, tokens)

### FASE 6: Reportes y Visualizaciones (2 horas)
- [ ] Implementar `src/evaluation/report_generator.py`
- [ ] Reporte principal en Markdown
- [ ] Tablas CSV comparativas
- [ ] Gr√°ficos:
  - [ ] Barras: F1-Score por pipeline
  - [ ] Heatmap: LLM √ó Contexto
  - [ ] Scatter: Precision vs Recall
  - [ ] Scatter: Velocidad vs Calidad
  - [ ] Confusion Matrices
  - [ ] Stacked bar: Tipos de skills

### FASE 7: Validaci√≥n y Ajustes (1 hora)
- [ ] Revisar resultados
- [ ] Validar m√©tricas
- [ ] Ajustar visualizaciones
- [ ] Documentar hallazgos

---

## üéì LLMs SELECCIONADOS PARA EVALUACI√ìN

**Criterio de selecci√≥n:** 2-4 modelos peque√±os (2-4B), descargados, variedad de familias

1. **Llama 3.2 3B Instruct** (Meta)
   - Tama√±o: 2.1 GB
   - Context: 128K tokens
   - Fortaleza: Multilingual, reasoning

2. **Gemma 3 4B Instruct** (Google)
   - Tama√±o: 2.8 GB
   - Context: 8K tokens
   - Fortaleza: √öltima generaci√≥n, balance velocidad/calidad

3. **Qwen 2.5 3B Instruct** (Alibaba)
   - Tama√±o: 2.1 GB
   - Context: 32K tokens
   - Fortaleza: Structured outputs

4. **Qwen3 4B** (Alibaba)
   - Tama√±o: 2.5 GB
   - Context: 32K tokens
   - Fortaleza: √öltima generaci√≥n (Abril 2025), hybrid reasoning

---

## üìù FORMATO DE RESULTADOS ESPERADOS

### Ejemplo de Output

```markdown
# Evaluaci√≥n de Pipelines de Extracci√≥n

## 1. Comparaci√≥n de Extracci√≥n Pura

### M√©tricas Generales
| Pipeline           | Precision | Recall | F1-Score | Skills/Job |
|--------------------|-----------|--------|----------|------------|
| Pipeline A         | 0.85      | 0.78   | 0.81     | 24.3       |
| Pipeline B (Llama) | 0.88      | 0.82   | 0.85     | 26.1       |
| Pipeline B (Gemma) | 0.83      | 0.79   | 0.81     | 23.8       |
| Pipeline B (Qwen2.5)| 0.86     | 0.80   | 0.83     | 25.2       |
| Pipeline B (Qwen3) | 0.87      | 0.81   | 0.84     | 25.8       |

**Conclusi√≥n:** Pipeline B (Llama) extrae mejor (+4.9% F1)

---

## 2. Comparaci√≥n Post-Mapeo ESCO

### M√©tricas con ESCO
| Pipeline           | Precision | Recall | F1-Score | Cobertura ESCO |
|--------------------|-----------|--------|----------|----------------|
| Pipeline A         | 0.82      | 0.71   | 0.76     | 85%            |
| Pipeline B (Llama) | 0.85      | 0.75   | 0.80     | 89%            |
| Pipeline B (Gemma) | 0.80      | 0.73   | 0.76     | 85%            |

**Conclusi√≥n:** Pipeline B (Llama) sigue siendo mejor (+5.3% F1)

---

## 3. Impacto del Mapeo a ESCO

| Pipeline           | Œî F1-Score | Skills Perdidas | Skills Emergentes |
|--------------------|------------|-----------------|-------------------|
| Pipeline A         | -6.2%      | 4 skills        | Next.js, Tailwind |
| Pipeline B (Llama) | -5.9%      | 3 skills        | FastAPI, Svelte   |

**Conclusi√≥n:** Mapeo a ESCO reduce F1 ~6% por p√©rdida de skills emergentes

---

## 4. An√°lisis por Contexto (Pipeline B)

### Performance por Pa√≠s
| LLM    | CO F1  | MX F1  | AR F1  | Avg F1 |
|--------|--------|--------|--------|--------|
| Llama  | 0.86   | 0.85   | 0.84   | 0.85   |
| Gemma  | 0.82   | 0.81   | 0.80   | 0.81   |

### Performance por Portal
| LLM    | bumeran | computrabajo | elempleo | Avg |
|--------|---------|--------------|----------|-----|
| Llama  | 0.87    | 0.84         | 0.85     | 0.85|
| Gemma  | 0.83    | 0.80         | 0.81     | 0.81|
```

---

## üî¨ PREGUNTAS DE INVESTIGACI√ìN

### Preguntas Principales

1. **¬øPipeline A (NER+Regex) o Pipeline B (LLM) extrae mejor?**
   - Hip√≥tesis: LLM extrae mejor (mayor recall, detecta impl√≠citos)

2. **¬øCu√°l LLM funciona mejor para extracci√≥n de skills?**
   - Hip√≥tesis: Modelos m√°s grandes (Qwen3 4B, Llama 3.2 3B) > peque√±os

3. **¬øEl mapeo a ESCO penaliza skills emergentes?**
   - Hip√≥tesis: S√≠, ~5-10% reducci√≥n de F1 por skills no en ESCO

4. **¬øEl contexto afecta la performance de LLMs?**
   - Hip√≥tesis: S√≠, algunos LLMs mejores para ciertos tipos de jobs

### Preguntas Secundarias

5. ¬øQu√© tipos de skills extrae mejor cada pipeline? (lenguajes vs frameworks vs soft skills)
6. ¬øCu√°l es el trade-off velocidad vs calidad en Pipeline B?
7. ¬øHay skills que ning√∫n pipeline detecta correctamente? (blind spots)
8. ¬øLos LLMs generan m√°s false positives o false negatives?

---

## üìä DECISIONES DE DISE√ëO

### Decisi√≥n 1: Comparaci√≥n Dual (Texto + ESCO)
**Fecha:** 2025-11-05
**Raz√≥n:** Evaluar extracci√≥n pura sin penalizar skills emergentes, luego evaluar estandarizaci√≥n
**Alternativas consideradas:** Solo comparar v√≠a ESCO URIs (descartado por injusto)
**Impacto:** Permite ver impacto real de ESCO en m√©tricas

### Decisi√≥n 2: Re-mapear Pipeline A a ESCO
**Fecha:** 2025-11-05
**Raz√≥n:** Garantizar fairness - mismo c√≥digo de mapeo para ambos pipelines
**Alternativas consideradas:** Usar mapeo existente de Pipeline A (descartado por injusto)
**Impacto:** Comparaci√≥n completamente justa

### Decisi√≥n 3: Normalizaci√≥n Case-Insensitive + Diccionario
**Fecha:** 2025-11-05
**Raz√≥n:** Gold Standard usa capitalizaci√≥n correcta, pipelines pueden variar
**Alternativas consideradas:** Fuzzy matching (descartado, demasiado permisivo)
**Impacto:** Elimina diferencias triviales (Python vs python vs PYTHON)

### Decisi√≥n 4: 4 LLMs para Evaluaci√≥n Inicial
**Fecha:** 2025-11-05
**Raz√≥n:** Probar sistema con subset antes de expandir a los 9 modelos
**Selecci√≥n:** Llama 3.2 3B, Gemma 3 4B, Qwen 2.5 3B, Qwen3 4B
**Criterio:** Variedad de familias, tama√±os 2-4B, ya descargados
**Impacto:** Validaci√≥n r√°pida, puede expandirse despu√©s

---

## üêõ PROBLEMAS Y SOLUCIONES

### Problema 1: Pipeline A deduplica con lowercase pero guarda original
**Fecha:** 2025-11-05
**S√≠ntoma:** Puede tener "python", "Python", "PYTHON" como 3 entries
**Causa:** L√≠nea 231 en `extractor/pipeline.py` usa `lower()` para deduplicar pero guarda `skill.skill_text` original
**Soluci√≥n:** Normalizar antes de guardar en BD
**Estado:** Pendiente de implementar en scripts de ejecuci√≥n

### Problema 2: Pipeline B no mapea a ESCO
**Fecha:** 2025-11-05
**S√≠ntoma:** Tabla `enhanced_skills` vac√≠a, no hay esco URIs
**Causa:** Pipeline B solo extrae, no mapea (dise√±o intencional)
**Soluci√≥n:** Mapear en Comparaci√≥n 2 usando `ESCOMatcher3Layers`
**Estado:** Por implementar en `dual_comparator.py`

---

## üìà RESULTADOS Y HALLAZGOS

### Ejecuci√≥n Pipeline A
**Fecha:** [Pendiente]
**Jobs procesados:** 0/300
**Skills extra√≠dos:** 0
**Tiempo:** -
**Notas:** -

### Ejecuci√≥n Pipeline B - Llama 3.2 3B
**Fecha:** [Pendiente]
**Jobs procesados:** 0/300
**Skills extra√≠dos:** 0
**Tiempo:** -
**Notas:** -

### Ejecuci√≥n Pipeline B - Gemma 3 4B
**Fecha:** [Pendiente]
**Jobs procesados:** 0/300
**Skills extra√≠dos:** 0
**Tiempo:** -
**Notas:** -

### Comparaci√≥n 1: Extracci√≥n Pura
**Fecha:** [Pendiente]
**Pipeline ganador:** -
**Delta F1:** -
**Hallazgos clave:** -

### Comparaci√≥n 2: Post-ESCO
**Fecha:** [Pendiente]
**Pipeline ganador:** -
**Cobertura ESCO promedio:** -
**Skills emergentes identificadas:** -
**Hallazgos clave:** -

---

## üîó REFERENCIAS

### C√≥digo Relevante
- `src/extractor/pipeline.py` - Pipeline A implementation
- `src/extractor/esco_matcher_3layers.py` - ESCO matcher (exact, fuzzy, semantic)
- `src/llm_processor/pipeline.py` - Pipeline B implementation
- `src/llm_processor/model_registry.py` - LLM configurations

### Documentaci√≥n
- `docs/CORRECTED_COMPLETE_FLOW.md` - Flujo completo del sistema
- `docs/PIPELINE_B_IMPLEMENTATION.md` - Implementaci√≥n Pipeline B
- `data/gold_standard/README.md` - Descripci√≥n del gold standard

### Papers de Referencia
- ESCO Taxonomy v1.1.0 - European Skills Classification
- O*NET Hot Technologies - US Labor Department

---

## ‚úÖ CHECKLIST DE PROGRESO

### Preparaci√≥n
- [x] An√°lisis de c√≥digo existente
- [x] Definici√≥n de estrategia
- [ ] Documento de memoria persistente
- [ ] Estructura de directorios

### Implementaci√≥n
- [ ] Normalizer
- [ ] Metrics calculator
- [ ] Dual comparator
- [ ] Context analyzer
- [ ] Report generator

### Ejecuci√≥n
- [ ] Pipeline A en 300 jobs
- [ ] Pipeline B (Llama) en 300 jobs
- [ ] Pipeline B (Gemma) en 300 jobs
- [ ] Pipeline B (Qwen 2.5) en 300 jobs
- [ ] Pipeline B (Qwen3) en 300 jobs

### An√°lisis
- [ ] Comparaci√≥n 1 (texto)
- [ ] Comparaci√≥n 2 (ESCO)
- [ ] An√°lisis de contexto
- [ ] An√°lisis de rendimiento
- [ ] Identificar skills emergentes

### Documentaci√≥n
- [ ] Reporte principal
- [ ] Tablas CSV
- [ ] Gr√°ficos
- [ ] Conclusiones
- [ ] Recomendaciones

---

**√öltima actualizaci√≥n:** 2025-11-05
**Pr√≥ximo paso:** Implementar normalizer y crear estructura de directorios


---

## üìù PROGRESO DE IMPLEMENTACI√ìN

### Sesi√≥n: 2025-11-05

#### FASE 0: Preparaci√≥n ‚úÖ COMPLETADA
- [x] An√°lisis de c√≥digo existente
- [x] Definici√≥n de estrategia de evaluaci√≥n (doble comparaci√≥n)
- [x] Creaci√≥n de documento de memoria persistente
- [x] Creaci√≥n de estructura de directorios

#### FASE 1: Normalizaci√≥n Unificada ‚úÖ COMPLETADA
**Archivo:** `src/evaluation/normalizer.py` (368 l√≠neas)

**Implementado:**
- Diccionario can√≥nico con 200+ tecnolog√≠as
  - Lenguajes: Python, JavaScript, Java, C++, Go, Rust, etc.
  - Frameworks: React, Vue.js, Django, Flask, Spring Boot, etc.
  - Databases: PostgreSQL, MySQL, MongoDB, Redis, etc.
  - Cloud: AWS, Azure, GCP, Heroku, etc.
  - DevOps: Docker, Kubernetes, Jenkins, Terraform, etc.
- Normalizaci√≥n de variantes: postgres‚ÜíPostgreSQL, js‚ÜíJavaScript, k8s‚ÜíKubernetes
- Blacklist de t√©rminos no-skills: experiencia, a√±os, salario, etc.
- Funciones de limpieza y title case
- Singleton pattern para performance

**Decisiones:**
- Case-insensitive matching con diccionario
- Preservar acr√≥nimos (AWS, API, SQL)
- Remover acentos para matching
- Fallback a Title Case para skills desconocidas

#### FASE 2: Sistema de M√©tricas ‚úÖ COMPLETADA
**Archivo:** `src/evaluation/metrics.py` (260 l√≠neas)

**Implementado:**
- Clase `MetricsCalculator` con m√©todos:
  - `calculate()`: Precision, Recall, F1, Accuracy
  - `calculate_aggregate()`: Micro-averaging para m√∫ltiples jobs
  - `calculate_per_job()`: M√©tricas individuales por job
  - `calculate_macro_average()`: Macro-averaging
- Clase `ConfusionMatrix`: TP, FP, TN, FN
- Clase `EvaluationMetrics`: Container de todas las m√©tricas
- Funciones helper: `calculate_metrics()`, `compare_pipelines()`, `print_metrics()`

**M√©tricas calculadas:**
- Precision: TP / (TP + FP)
- Recall: TP / (TP + FN)
- F1-Score: 2 * (P * R) / (P + R)
- Accuracy: (TP + TN) / Total
- Listas detalladas de TP, FP, FN

#### FASE 3: Comparador Dual ‚úÖ COMPLETADA
**Archivo:** `src/evaluation/dual_comparator.py` (430 l√≠neas)

**Implementado:**
- Clase `DualPipelineComparator`:
  - `load_gold_standard()`: Carga anotaciones manuales
  - `load_pipeline_a()`: Carga extracted_skills
  - `load_pipeline_b()`: Carga enhanced_skills por modelo
  - `compare_pure_text()`: Comparaci√≥n 1 (texto normalizado)
  - `compare_post_esco()`: Comparaci√≥n 2 (post-mapeo ESCO)
  - `analyze_esco_impact()`: An√°lisis de impacto
  - `run_dual_comparison()`: Orquestador principal
- Dataclasses:
  - `PipelineData`: Container de skills por job
  - `ComparisonResult`: Resultado de comparaci√≥n
  - `DualComparisonReport`: Reporte completo

**Caracter√≠sticas:**
- Fairness: Re-mapea Pipeline A a ESCO (no usa mapeo existente)
- Normalizaci√≥n unificada para todos
- Identifica skills emergentes (perdidas en mapeo ESCO)
- Calcula cobertura ESCO por pipeline
- An√°lisis de impacto: Œî F1, Œî Precision, Œî Recall

#### FASE 4: Scripts de Ejecuci√≥n ‚úÖ COMPLETADA

**Script 1:** `scripts/run_pipeline_a_gold_standard.py` (200 l√≠neas)
- Ejecuta Pipeline A (NER + Regex + ESCO) en 300 jobs gold standard
- Normaliza skills antes de guardar (fairness)
- Guarda en tabla `extracted_skills`
- Flags: `--limit N`, `--dry-run`
- Estad√≠sticas: total jobs, skills extra√≠dos, tiempo promedio

**Script 2:** `scripts/run_pipeline_b_gold_standard.py` (220 l√≠neas)
- Ejecuta Pipeline B (LLM) en 300 jobs gold standard
- Soporte para cualquier modelo del registry
- Normaliza skills antes de guardar
- Guarda en tabla `enhanced_skills` con `llm_model`
- Flags: `--model <name>`, `--limit N`, `--dry-run`, `--list-models`
- Tracking de velocidad por job

**Script 3:** `scripts/evaluate_extraction_pipelines.py` (280 l√≠neas)
- Script principal de evaluaci√≥n
- Usa `DualPipelineComparator` para comparar
- Soporta m√∫ltiples LLMs simult√°neos
- Genera 3 tipos de reportes:
  1. Markdown: `EVALUATION_REPORT.md` (reporte ejecutivo)
  2. CSV: `pipeline_comparison_pure.csv`, `pipeline_comparison_esco.csv`
  3. JSON: `evaluation_results.json` (datos completos)
- Flags: `--llm-models <names>`, `--pipeline-a-only`, `--output-dir`

#### M√≥dulo de Evaluaci√≥n Creado
**Estructura:**
```
src/evaluation/
‚îú‚îÄ‚îÄ __init__.py           (Exports principales)
‚îú‚îÄ‚îÄ normalizer.py         (368 l√≠neas) ‚úÖ
‚îú‚îÄ‚îÄ metrics.py            (260 l√≠neas) ‚úÖ
‚îî‚îÄ‚îÄ dual_comparator.py    (430 l√≠neas) ‚úÖ
```

**Total de c√≥digo:** ~1,060 l√≠neas implementadas

---

## üöÄ PR√ìXIMOS PASOS

### Ejecuci√≥n Inmediata

1. **Ejecutar Pipeline A** (5-10 min):
```bash
python scripts/run_pipeline_a_gold_standard.py
```

2. **Ejecutar Pipeline B con 4 LLMs** (1-2 horas total):
```bash
# Llama 3.2 3B (~15-20 min)
python scripts/run_pipeline_b_gold_standard.py --model llama-3.2-3b-instruct

# Gemma 3 4B (~15-20 min)
python scripts/run_pipeline_b_gold_standard.py --model gemma-3-4b-instruct

# Qwen 2.5 3B (~15-20 min)
python scripts/run_pipeline_b_gold_standard.py --model qwen2.5-3b-instruct

# Qwen3 4B (~15-20 min)
python scripts/run_pipeline_b_gold_standard.py --model qwen3-4b
```

3. **Generar Evaluaci√≥n** (1-2 min):
```bash
python scripts/evaluate_extraction_pipelines.py \
  --llm-models llama-3.2-3b-instruct gemma-3-4b-instruct qwen2.5-3b-instruct qwen3-4b
```

### Tareas Pendientes (Opcionales)

- [ ] Context analyzer (an√°lisis por portal, pa√≠s, tipo de skill)
- [ ] Report generator con gr√°ficos (matplotlib/seaborn)
- [ ] An√°lisis de rendimiento (velocidad vs calidad)
- [ ] Skills emergentes ranking
- [ ] Expandir con m√°s LLMs (si resultados son buenos)

---

## üìä OUTPUTS ESPERADOS

Despu√©s de ejecutar todo, tendr√°s:

**En base de datos:**
- `extracted_skills`: ~7,200 skills (300 jobs √ó 24 skills/job promedio) - Pipeline A
- `enhanced_skills`: ~31,200 skills (300 jobs √ó 26 skills/job √ó 4 LLMs) - Pipeline B

**En data/reports/:**
- `EVALUATION_REPORT.md`: Reporte ejecutivo con conclusiones
- `pipeline_comparison_pure.csv`: M√©tricas comparaci√≥n texto
- `pipeline_comparison_esco.csv`: M√©tricas comparaci√≥n ESCO
- `evaluation_results.json`: Datos completos en JSON

**Tiempo estimado total:** ~2-3 horas

---

**√öltima actualizaci√≥n:** 2025-11-05 14:55:23

